def add_bucket_default_owner bucket_name user_email storage_client storage Client bucket storage_client bucket bucket_name bucket acl reload bucket default_object_acl user user_email grant_owner bucket default_object_acl save print 'Addeduser{}asanownerinthedefaultaclonbucket{} ' format user_email bucket_name
def add_bucket_default_owner bucket_name user_email storage_client storage Client bucket storage_client bucket bucket_name bucket acl reload bucket default_object_acl user user_email grant_owner bucket default_object_acl save print 'Addeduser{}asanownerinthedefaultaclonbucket{} ' format user_email bucket_name
def add_bucket_default_owner bucket_name user_email storage_client storage Client bucket storage_client bucket bucket_name bucket acl reload bucket default_object_acl user user_email grant_owner bucket default_object_acl save print 'Addeduser{}asanownerinthedefaultaclonbucket{} ' format user_email bucket_name
def make_trivial_sdist dist_path setup_py setup_py_file tarfile TarInfo name 'setup py' try MemFile BytesIOexcept AttributeError MemFile StringIOsetup_py_bytes MemFile setup_py encode 'utf-8' setup_py_file size len setup_py_bytes getvalue dist tarfile open dist_path 'w gz' try dist addfile setup_py_file fileobj setup_py_bytes finally dist close
def make_trivial_sdist dist_path setup_py setup_py_file tarfile TarInfo name 'setup py' try MemFile BytesIOexcept AttributeError MemFile StringIOsetup_py_bytes MemFile setup_py encode 'utf-8' setup_py_file size len setup_py_bytes getvalue dist tarfile open dist_path 'w gz' try dist addfile setup_py_file fileobj setup_py_bytes finally dist close
def make_trivial_sdist dist_path setup_py setup_py_file tarfile TarInfo name 'setup py' try MemFile BytesIOexcept AttributeError MemFile StringIOsetup_py_bytes MemFile setup_py encode 'utf-8' setup_py_file size len setup_py_bytes getvalue dist tarfile open dist_path 'w gz' try dist addfile setup_py_file fileobj setup_py_bytes finally dist close
def create_le_config parent_dir config copy deepcopy constants CLI_DEFAULTS le_dir os path join parent_dir 'certbot' config['config_dir'] os path join le_dir 'config' config['work_dir'] os path join le_dir 'work' config['logs_dir'] os path join le_dir 'logs_dir' os makedirs config['config_dir'] os mkdir config['work_dir'] os mkdir config['logs_dir'] config['domains'] Nonereturn argparse Namespace **config
def create_le_config parent_dir config copy deepcopy constants CLI_DEFAULTS le_dir os path join parent_dir 'certbot' config['config_dir'] os path join le_dir 'config' config['work_dir'] os path join le_dir 'work' config['logs_dir'] os path join le_dir 'logs_dir' os makedirs config['config_dir'] os mkdir config['work_dir'] os mkdir config['logs_dir'] config['domains'] Nonereturn argparse Namespace **config
def create_le_config parent_dir config copy deepcopy constants CLI_DEFAULTS le_dir os path join parent_dir 'certbot' config['config_dir'] os path join le_dir 'config' config['work_dir'] os path join le_dir 'work' config['logs_dir'] os path join le_dir 'logs_dir' os makedirs config['config_dir'] os mkdir config['work_dir'] os mkdir config['logs_dir'] config['domains'] Nonereturn argparse Namespace **config
def policy_key location return u'{cat}/{name}' format cat location category name location name
def policy_key location return u'{cat}/{name}' format cat location category name location name
def remove_like obj user obj_type apps get_model 'contenttypes' 'ContentType' objects get_for_model obj with atomic qs Like objects filter content_type obj_type object_id obj id user user if not qs exists returnlike qs first project like projectqs delete if project is not None project refresh_totals
def remove_like obj user obj_type apps get_model 'contenttypes' 'ContentType' objects get_for_model obj with atomic qs Like objects filter content_type obj_type object_id obj id user user if not qs exists returnlike qs first project like projectqs delete if project is not None project refresh_totals
def test_message_delete db gmail_account api_client new_api_client db gmail_account namespace generic_thread add_fake_thread db session gmail_account namespace id gen_message add_fake_message db session gmail_account namespace id generic_thread category_ids []for i in xrange 10 po_data api_client post_data '/labels/' {'display_name' str i } assert po_data status_code 200 category_ids append json loads po_data data ['id'] data {'label_ids' category_ids}resp api_client put_data '/messages/{}' format gen_message public_id data assert resp status_code 200 associated_mcs db session query MessageCategory filter MessageCategory message_id gen_message id all assert len associated_mcs 10 db session delete gen_message db session commit assert db session query MessageCategory filter MessageCategory message_id gen_message id all []
def test_message_delete db gmail_account api_client new_api_client db gmail_account namespace generic_thread add_fake_thread db session gmail_account namespace id gen_message add_fake_message db session gmail_account namespace id generic_thread category_ids []for i in xrange 10 po_data api_client post_data '/labels/' {'display_name' str i } assert po_data status_code 200 category_ids append json loads po_data data ['id'] data {'label_ids' category_ids}resp api_client put_data '/messages/{}' format gen_message public_id data assert resp status_code 200 associated_mcs db session query MessageCategory filter MessageCategory message_id gen_message id all assert len associated_mcs 10 db session delete gen_message db session commit assert db session query MessageCategory filter MessageCategory message_id gen_message id all []
def test_message_delete db gmail_account api_client new_api_client db gmail_account namespace generic_thread add_fake_thread db session gmail_account namespace id gen_message add_fake_message db session gmail_account namespace id generic_thread category_ids []for i in xrange 10 po_data api_client post_data '/labels/' {'display_name' str i } assert po_data status_code 200 category_ids append json loads po_data data ['id'] data {'label_ids' category_ids}resp api_client put_data '/messages/{}' format gen_message public_id data assert resp status_code 200 associated_mcs db session query MessageCategory filter MessageCategory message_id gen_message id all assert len associated_mcs 10 db session delete gen_message db session commit assert db session query MessageCategory filter MessageCategory message_id gen_message id all []
def check_output *args cmd_output subprocess check_output args cmd_output cmd_output decode 'utf-8' return cmd_output strip
def check_output *args cmd_output subprocess check_output args cmd_output cmd_output decode 'utf-8' return cmd_output strip
def set_seed seed None if seed None random seed seed
def set_seed seed None if seed None random seed seed
def argmin x axis -1 if axis < 0 axis axis % len x get_shape return tf argmin x axis
def argmin x axis -1 if axis < 0 axis axis % len x get_shape return tf argmin x axis
def _get_query_handle_and_state query_history handle query_history get_handle if handle is None raise PopupException _ 'FailedtoretrievequerystatefromtheQueryServer ' state dbms get query_history owner query_history get_query_server_config get_state handle if state is None raise PopupException _ 'FailedtocontactServertocheckquerystatus ' return handle state
def _get_query_handle_and_state query_history handle query_history get_handle if handle is None raise PopupException _ 'FailedtoretrievequerystatefromtheQueryServer ' state dbms get query_history owner query_history get_query_server_config get_state handle if state is None raise PopupException _ 'FailedtocontactServertocheckquerystatus ' return handle state
def ThrottledHttpRpcServerFactory throttle throttle_class None def MakeRpcServer *args **kwargs 'FactorytoproduceaThrottledHttpRpcServer \n\nArgs \nargs PositionalargstopasstoThrottledHttpRpcServer \nkwargs KeywordargstopasstoThrottledHttpRpcServer \n\nReturns \nAThrottledHttpRpcServerinstance \n'kwargs['account_type'] 'HOSTED_OR_GOOGLE'kwargs['save_cookies'] Trueif throttle_class rpc_server throttle_class throttle *args **kwargs else rpc_server ThrottledHttpRpcServer throttle *args **kwargs return rpc_serverreturn MakeRpcServer
def ThrottledHttpRpcServerFactory throttle throttle_class None def MakeRpcServer *args **kwargs 'FactorytoproduceaThrottledHttpRpcServer \n\nArgs \nargs PositionalargstopasstoThrottledHttpRpcServer \nkwargs KeywordargstopasstoThrottledHttpRpcServer \n\nReturns \nAThrottledHttpRpcServerinstance \n'kwargs['account_type'] 'HOSTED_OR_GOOGLE'kwargs['save_cookies'] Trueif throttle_class rpc_server throttle_class throttle *args **kwargs else rpc_server ThrottledHttpRpcServer throttle *args **kwargs return rpc_serverreturn MakeRpcServer
def ThrottledHttpRpcServerFactory throttle throttle_class None def MakeRpcServer *args **kwargs 'FactorytoproduceaThrottledHttpRpcServer \n\nArgs \nargs PositionalargstopasstoThrottledHttpRpcServer \nkwargs KeywordargstopasstoThrottledHttpRpcServer \n\nReturns \nAThrottledHttpRpcServerinstance \n'kwargs['account_type'] 'HOSTED_OR_GOOGLE'kwargs['save_cookies'] Trueif throttle_class rpc_server throttle_class throttle *args **kwargs else rpc_server ThrottledHttpRpcServer throttle *args **kwargs return rpc_serverreturn MakeRpcServer
def ThrottledHttpRpcServerFactory throttle throttle_class None def MakeRpcServer *args **kwargs 'FactorytoproduceaThrottledHttpRpcServer \n\nArgs \nargs PositionalargstopasstoThrottledHttpRpcServer \nkwargs KeywordargstopasstoThrottledHttpRpcServer \n\nReturns \nAThrottledHttpRpcServerinstance \n'kwargs['account_type'] 'HOSTED_OR_GOOGLE'kwargs['save_cookies'] Trueif throttle_class rpc_server throttle_class throttle *args **kwargs else rpc_server ThrottledHttpRpcServer throttle *args **kwargs return rpc_serverreturn MakeRpcServer
@register as_tagdef blog_categories *args posts BlogPost objects published categories BlogCategory objects filter blogposts__in posts return list categories annotate post_count Count u'blogposts'
@register as_tagdef blog_categories *args posts BlogPost objects published categories BlogCategory objects filter blogposts__in posts return list categories annotate post_count Count u'blogposts'
def query database query time_precision 's' chunked False user None password None host None port None client _client user user password password host host port port client switch_database database return client query query time_precision time_precision chunked chunked
def query database query time_precision 's' chunked False user None password None host None port None client _client user user password password host host port port client switch_database database return client query query time_precision time_precision chunked chunked
def query database query time_precision 's' chunked False user None password None host None port None client _client user user password password host host port port client switch_database database return client query query time_precision time_precision chunked chunked
def final_repset_from_iteration_repsets repset_fasta_fs observed {}for repset_fasta_f in repset_fasta_fs for otu_id seq in parse_fasta repset_fasta_f o otu_id split [0]if not o in observed yield otu_id seq observed[o] Noneelse pass
def final_repset_from_iteration_repsets repset_fasta_fs observed {}for repset_fasta_f in repset_fasta_fs for otu_id seq in parse_fasta repset_fasta_f o otu_id split [0]if not o in observed yield otu_id seq observed[o] Noneelse pass
@register simple_tagdef bootstrap_formset *args **kwargs return render_formset *args **kwargs
@register simple_tagdef bootstrap_formset *args **kwargs return render_formset *args **kwargs
def _get_float data position dummy0 dummy1 dummy2 end position + 8 return _UNPACK_FLOAT data[position end] [0] end
def _get_float data position dummy0 dummy1 dummy2 end position + 8 return _UNPACK_FLOAT data[position end] [0] end
def str2css sourcestring colors None title '' markup 'css' header None footer None linenumbers 0 form None if markup lower not in ['css' 'xhtml'] markup 'css'stringIO StringIO StringIO parse Parser sourcestring colors colors title title out stringIO markup markup header header footer footer linenumbers linenumbers parse format form stringIO seek 0 if form None return parse _sendCSSStyle external 1 stringIO read else return None stringIO read
def str2css sourcestring colors None title '' markup 'css' header None footer None linenumbers 0 form None if markup lower not in ['css' 'xhtml'] markup 'css'stringIO StringIO StringIO parse Parser sourcestring colors colors title title out stringIO markup markup header header footer footer linenumbers linenumbers parse format form stringIO seek 0 if form None return parse _sendCSSStyle external 1 stringIO read else return None stringIO read
def str2css sourcestring colors None title '' markup 'css' header None footer None linenumbers 0 form None if markup lower not in ['css' 'xhtml'] markup 'css'stringIO StringIO StringIO parse Parser sourcestring colors colors title title out stringIO markup markup header header footer footer linenumbers linenumbers parse format form stringIO seek 0 if form None return parse _sendCSSStyle external 1 stringIO read else return None stringIO read
def validate_value tokens for token in tokens type_ token typeif type_ u'{' validate_block token content u'propertyvalue' else validate_any token u'propertyvalue'
def validate_value tokens for token in tokens type_ token typeif type_ u'{' validate_block token content u'propertyvalue' else validate_any token u'propertyvalue'
@login_required@ensure_csrf_cookiedef run_python request if not request user is_staff raise Http404c {}c['code'] ''c['results'] Noneif request method 'POST' py_code c['code'] request POST get 'code' g {}try safe_exec py_code g except Exception c['results'] traceback format_exc else c['results'] pprint pformat g return render_to_response 'debug/run_python_form html' c
@login_required@ensure_csrf_cookiedef run_python request if not request user is_staff raise Http404c {}c['code'] ''c['results'] Noneif request method 'POST' py_code c['code'] request POST get 'code' g {}try safe_exec py_code g except Exception c['results'] traceback format_exc else c['results'] pprint pformat g return render_to_response 'debug/run_python_form html' c
@login_required@ensure_csrf_cookiedef run_python request if not request user is_staff raise Http404c {}c['code'] ''c['results'] Noneif request method 'POST' py_code c['code'] request POST get 'code' g {}try safe_exec py_code g except Exception c['results'] traceback format_exc else c['results'] pprint pformat g return render_to_response 'debug/run_python_form html' c
@login_required@ensure_csrf_cookiedef run_python request if not request user is_staff raise Http404c {}c['code'] ''c['results'] Noneif request method 'POST' py_code c['code'] request POST get 'code' g {}try safe_exec py_code g except Exception c['results'] traceback format_exc else c['results'] pprint pformat g return render_to_response 'debug/run_python_form html' c
@login_required@ensure_csrf_cookiedef run_python request if not request user is_staff raise Http404c {}c['code'] ''c['results'] Noneif request method 'POST' py_code c['code'] request POST get 'code' g {}try safe_exec py_code g except Exception c['results'] traceback format_exc else c['results'] pprint pformat g return render_to_response 'debug/run_python_form html' c
def get_all_orgs return BACKEND get_all_orgs
def get_all_orgs return BACKEND get_all_orgs
def get_all_orgs return BACKEND get_all_orgs
def get_all_orgs return BACKEND get_all_orgs
def get_all_orgs return BACKEND get_all_orgs
def threadfunc callback callback Async callback try while True print ' 'callback time sleep 1 except print 'threadexiting'
def threadfunc callback callback Async callback try while True print ' 'callback time sleep 1 except print 'threadexiting'
def fix_url s charset 'utf-8' if isinstance s unicode s s encode charset 'ignore' scheme netloc path qs anchor urlparse urlsplit s path urllib quote path '/%' qs urllib quote_plus qs ' & ' return urlparse urlunsplit scheme netloc path qs anchor
def fix_url s charset 'utf-8' if isinstance s unicode s s encode charset 'ignore' scheme netloc path qs anchor urlparse urlsplit s path urllib quote path '/%' qs urllib quote_plus qs ' & ' return urlparse urlunsplit scheme netloc path qs anchor
def test_ast_bad_lambda cant_compile u' lambda '
def test_ast_bad_lambda cant_compile u' lambda '
def test_ast_bad_lambda cant_compile u' lambda '
def test_ast_bad_lambda cant_compile u' lambda '
def test_ast_bad_lambda cant_compile u' lambda '
def __virtual__ if not __detect_os return False 'Thelvsexecutionmodulecannotbeloaded theipvsadmbinaryisnotinthepath ' return 'lvs'
def __virtual__ if not __detect_os return False 'Thelvsexecutionmodulecannotbeloaded theipvsadmbinaryisnotinthepath ' return 'lvs'
def _local_server_get url session request HttpRequest request method 'GET'request session session view args kwargs resolve url response view request *args **kwargs return response content
def _local_server_get url session request HttpRequest request method 'GET'request session session view args kwargs resolve url response view request *args **kwargs return response content
def _local_server_get url session request HttpRequest request method 'GET'request session session view args kwargs resolve url response view request *args **kwargs return response content
def _local_server_get url session request HttpRequest request method 'GET'request session session view args kwargs resolve url response view request *args **kwargs return response content
def _local_server_get url session request HttpRequest request method 'GET'request session session view args kwargs resolve url response view request *args **kwargs return response content
def _local_server_get url session request HttpRequest request method 'GET'request session session view args kwargs resolve url response view request *args **kwargs return response content
@not_implemented_for 'undirected' def number_attracting_components G n len list attracting_components G return n
@not_implemented_for 'undirected' def number_attracting_components G n len list attracting_components G return n
def _find_guids guid_string guids []for found_guid in re finditer GUID_REGEX guid_string if found_guid groups guids append found_guid group 0 strip '{}' return sorted list set guids
def _find_guids guid_string guids []for found_guid in re finditer GUID_REGEX guid_string if found_guid groups guids append found_guid group 0 strip '{}' return sorted list set guids
def _find_guids guid_string guids []for found_guid in re finditer GUID_REGEX guid_string if found_guid groups guids append found_guid group 0 strip '{}' return sorted list set guids
def _verify_centralizer group arg centr None if centr is None centr group centralizer arg centr_list list centr generate_dimino af True centr_list_naive _naive_list_centralizer group arg af True return _cmp_perm_lists centr_list centr_list_naive
def _verify_centralizer group arg centr None if centr is None centr group centralizer arg centr_list list centr generate_dimino af True centr_list_naive _naive_list_centralizer group arg af True return _cmp_perm_lists centr_list centr_list_naive
def assertNotReading testCase reactor transport if IReactorFDSet providedBy reactor testCase assertNotIn transport reactor getReaders else testCase assertFalse transport reading
def assertNotReading testCase reactor transport if IReactorFDSet providedBy reactor testCase assertNotIn transport reactor getReaders else testCase assertFalse transport reading
def assertNotReading testCase reactor transport if IReactorFDSet providedBy reactor testCase assertNotIn transport reactor getReaders else testCase assertFalse transport reading
def zone_for_name name rdclass dns rdataclass IN tcp False resolver None if isinstance name str unicode name dns name from_text name dns name root if resolver is None resolver get_default_resolver if not name is_absolute raise NotAbsolute name while 1 try answer resolver query name dns rdatatype SOA rdclass tcp if answer rrset name name return nameexcept dns resolver NXDOMAIN dns resolver NoAnswer passtry name name parent except dns name NoParent raise NoRootSOA
def zone_for_name name rdclass dns rdataclass IN tcp False resolver None if isinstance name str unicode name dns name from_text name dns name root if resolver is None resolver get_default_resolver if not name is_absolute raise NotAbsolute name while 1 try answer resolver query name dns rdatatype SOA rdclass tcp if answer rrset name name return nameexcept dns resolver NXDOMAIN dns resolver NoAnswer passtry name name parent except dns name NoParent raise NoRootSOA
def zone_for_name name rdclass dns rdataclass IN tcp False resolver None if isinstance name str unicode name dns name from_text name dns name root if resolver is None resolver get_default_resolver if not name is_absolute raise NotAbsolute name while 1 try answer resolver query name dns rdatatype SOA rdclass tcp if answer rrset name name return nameexcept dns resolver NXDOMAIN dns resolver NoAnswer passtry name name parent except dns name NoParent raise NoRootSOA
def test_infomax_weights_ini X np random random 3 100 weights np array [[1 2 3] [4 5 6] [7 8 9]] dtype np float64 w1 infomax X max_iter 0 weights weights extended True w2 infomax X max_iter 0 weights weights extended False assert_almost_equal w1 weights assert_almost_equal w2 weights
def test_infomax_weights_ini X np random random 3 100 weights np array [[1 2 3] [4 5 6] [7 8 9]] dtype np float64 w1 infomax X max_iter 0 weights weights extended True w2 infomax X max_iter 0 weights weights extended False assert_almost_equal w1 weights assert_almost_equal w2 weights
def test_infomax_weights_ini X np random random 3 100 weights np array [[1 2 3] [4 5 6] [7 8 9]] dtype np float64 w1 infomax X max_iter 0 weights weights extended True w2 infomax X max_iter 0 weights weights extended False assert_almost_equal w1 weights assert_almost_equal w2 weights
def GenerateOAuthRequestTokenUrl oauth_input_params scopes request_token_url 'https //www google com/accounts/OAuthGetRequestToken' extra_parameters None scopes_string '' join [str scope for scope in scopes] parameters {'scope' scopes_string}if extra_parameters parameters update extra_parameters oauth_request oauth OAuthRequest from_consumer_and_token oauth_input_params GetConsumer http_url request_token_url parameters parameters oauth_request sign_request oauth_input_params GetSignatureMethod oauth_input_params GetConsumer None return atom url parse_url oauth_request to_url
def GenerateOAuthRequestTokenUrl oauth_input_params scopes request_token_url 'https //www google com/accounts/OAuthGetRequestToken' extra_parameters None scopes_string '' join [str scope for scope in scopes] parameters {'scope' scopes_string}if extra_parameters parameters update extra_parameters oauth_request oauth OAuthRequest from_consumer_and_token oauth_input_params GetConsumer http_url request_token_url parameters parameters oauth_request sign_request oauth_input_params GetSignatureMethod oauth_input_params GetConsumer None return atom url parse_url oauth_request to_url
def GenerateOAuthRequestTokenUrl oauth_input_params scopes request_token_url 'https //www google com/accounts/OAuthGetRequestToken' extra_parameters None scopes_string '' join [str scope for scope in scopes] parameters {'scope' scopes_string}if extra_parameters parameters update extra_parameters oauth_request oauth OAuthRequest from_consumer_and_token oauth_input_params GetConsumer http_url request_token_url parameters parameters oauth_request sign_request oauth_input_params GetSignatureMethod oauth_input_params GetConsumer None return atom url parse_url oauth_request to_url
def GenerateOAuthRequestTokenUrl oauth_input_params scopes request_token_url 'https //www google com/accounts/OAuthGetRequestToken' extra_parameters None scopes_string '' join [str scope for scope in scopes] parameters {'scope' scopes_string}if extra_parameters parameters update extra_parameters oauth_request oauth OAuthRequest from_consumer_and_token oauth_input_params GetConsumer http_url request_token_url parameters parameters oauth_request sign_request oauth_input_params GetSignatureMethod oauth_input_params GetConsumer None return atom url parse_url oauth_request to_url
def completed_chart series_id get_vars get 'series_id' if not series_id return 'ProgrammingError SeriesIDmissing'question_id get_vars get 'question_id' if not question_id return 'ProgrammingError QuestionIDmissing'q_type get_vars get 'type' if not q_type return 'ProgrammingError QuestionTypemissing'getAnswers s3db survey_getAllAnswersForQuestionInSeriesanswers getAnswers question_id series_id analysisTool s3db survey_analysis_type[q_type] question_id answers qstnName analysisTool qstnWidget question nameimage analysisTool drawChart series_id output 'png' return image
def completed_chart series_id get_vars get 'series_id' if not series_id return 'ProgrammingError SeriesIDmissing'question_id get_vars get 'question_id' if not question_id return 'ProgrammingError QuestionIDmissing'q_type get_vars get 'type' if not q_type return 'ProgrammingError QuestionTypemissing'getAnswers s3db survey_getAllAnswersForQuestionInSeriesanswers getAnswers question_id series_id analysisTool s3db survey_analysis_type[q_type] question_id answers qstnName analysisTool qstnWidget question nameimage analysisTool drawChart series_id output 'png' return image
def completed_chart series_id get_vars get 'series_id' if not series_id return 'ProgrammingError SeriesIDmissing'question_id get_vars get 'question_id' if not question_id return 'ProgrammingError QuestionIDmissing'q_type get_vars get 'type' if not q_type return 'ProgrammingError QuestionTypemissing'getAnswers s3db survey_getAllAnswersForQuestionInSeriesanswers getAnswers question_id series_id analysisTool s3db survey_analysis_type[q_type] question_id answers qstnName analysisTool qstnWidget question nameimage analysisTool drawChart series_id output 'png' return image
def completed_chart series_id get_vars get 'series_id' if not series_id return 'ProgrammingError SeriesIDmissing'question_id get_vars get 'question_id' if not question_id return 'ProgrammingError QuestionIDmissing'q_type get_vars get 'type' if not q_type return 'ProgrammingError QuestionTypemissing'getAnswers s3db survey_getAllAnswersForQuestionInSeriesanswers getAnswers question_id series_id analysisTool s3db survey_analysis_type[q_type] question_id answers qstnName analysisTool qstnWidget question nameimage analysisTool drawChart series_id output 'png' return image
def completed_chart series_id get_vars get 'series_id' if not series_id return 'ProgrammingError SeriesIDmissing'question_id get_vars get 'question_id' if not question_id return 'ProgrammingError QuestionIDmissing'q_type get_vars get 'type' if not q_type return 'ProgrammingError QuestionTypemissing'getAnswers s3db survey_getAllAnswersForQuestionInSeriesanswers getAnswers question_id series_id analysisTool s3db survey_analysis_type[q_type] question_id answers qstnName analysisTool qstnWidget question nameimage analysisTool drawChart series_id output 'png' return image
def importAndRunFunction path moduleName funcName **keywords import sysoriginalPath sys pathtry augmentedPath [path] + sys path sys path augmentedPathfunc getattr __import__ moduleName fromlist [funcName] funcName sys path originalPathexcept sys path originalPathraisereturn func **keywords
def importAndRunFunction path moduleName funcName **keywords import sysoriginalPath sys pathtry augmentedPath [path] + sys path sys path augmentedPathfunc getattr __import__ moduleName fromlist [funcName] funcName sys path originalPathexcept sys path originalPathraisereturn func **keywords
def regions from boto cloudsearchdomain layer1 import CloudSearchDomainConnectionreturn get_regions 'cloudsearchdomain' connection_cls CloudSearchDomainConnection
def regions from boto cloudsearchdomain layer1 import CloudSearchDomainConnectionreturn get_regions 'cloudsearchdomain' connection_cls CloudSearchDomainConnection
def _to_snake_case pascal_case snake_case re sub ' ^ [a-z] [A-Z] ' lambda match '{0}_{1}' format match group 1 lower match group 2 lower pascal_case return snake_case lower strip '_'
def _to_snake_case pascal_case snake_case re sub ' ^ [a-z] [A-Z] ' lambda match '{0}_{1}' format match group 1 lower match group 2 lower pascal_case return snake_case lower strip '_'
def cov_nw_panel results nlags groupidx weights_func weights_bartlett use_correction 'hac' if nlags 0 weights [1 0]else weights weights_func nlags xu hessian_inv _get_sandwich_arrays results S_hac S_nw_panel xu weights groupidx cov_hac _HCCM2 hessian_inv S_hac if use_correction nobs k_params xu shapeif use_correction 'hac' cov_hac * nobs / float nobs - k_params elif use_correction in ['c' 'clu' 'cluster'] n_groups len groupidx cov_hac * n_groups / n_groups - 1 0 cov_hac * nobs - 1 0 / float nobs - k_params return cov_hac
def cov_nw_panel results nlags groupidx weights_func weights_bartlett use_correction 'hac' if nlags 0 weights [1 0]else weights weights_func nlags xu hessian_inv _get_sandwich_arrays results S_hac S_nw_panel xu weights groupidx cov_hac _HCCM2 hessian_inv S_hac if use_correction nobs k_params xu shapeif use_correction 'hac' cov_hac * nobs / float nobs - k_params elif use_correction in ['c' 'clu' 'cluster'] n_groups len groupidx cov_hac * n_groups / n_groups - 1 0 cov_hac * nobs - 1 0 / float nobs - k_params return cov_hac
def _iter_dir dir_ saltenv ret []for fn_ in os listdir dir_ path os path join dir_ fn_ if os path isdir path yield _iter_dir path saltenv elif os path isfile path with salt utils fopen path as fp_ if salt utils istextfile fp_ ret append {'path' six text_type path 'saltenv' six text_type saltenv 'content' six text_type fp_ read } else ret append {'path' six text_type path 'saltenv' six text_type saltenv 'content' u'bin'} yield ret
def _iter_dir dir_ saltenv ret []for fn_ in os listdir dir_ path os path join dir_ fn_ if os path isdir path yield _iter_dir path saltenv elif os path isfile path with salt utils fopen path as fp_ if salt utils istextfile fp_ ret append {'path' six text_type path 'saltenv' six text_type saltenv 'content' six text_type fp_ read } else ret append {'path' six text_type path 'saltenv' six text_type saltenv 'content' u'bin'} yield ret
def _iter_dir dir_ saltenv ret []for fn_ in os listdir dir_ path os path join dir_ fn_ if os path isdir path yield _iter_dir path saltenv elif os path isfile path with salt utils fopen path as fp_ if salt utils istextfile fp_ ret append {'path' six text_type path 'saltenv' six text_type saltenv 'content' six text_type fp_ read } else ret append {'path' six text_type path 'saltenv' six text_type saltenv 'content' u'bin'} yield ret
def _iter_dir dir_ saltenv ret []for fn_ in os listdir dir_ path os path join dir_ fn_ if os path isdir path yield _iter_dir path saltenv elif os path isfile path with salt utils fopen path as fp_ if salt utils istextfile fp_ ret append {'path' six text_type path 'saltenv' six text_type saltenv 'content' six text_type fp_ read } else ret append {'path' six text_type path 'saltenv' six text_type saltenv 'content' u'bin'} yield ret
def mask_color clip color [0 0 0] thr 0 s 1 hill lambda x 1 0 * x 0 if thr 0 else x ** s / thr ** s + x ** s color np array color flim lambda im hill np sqrt im - color ** 2 sum axis 2 mask clip fl_image flim mask ismask Truenewclip clip set_mask mask return newclip
def mask_color clip color [0 0 0] thr 0 s 1 hill lambda x 1 0 * x 0 if thr 0 else x ** s / thr ** s + x ** s color np array color flim lambda im hill np sqrt im - color ** 2 sum axis 2 mask clip fl_image flim mask ismask Truenewclip clip set_mask mask return newclip
def mask_color clip color [0 0 0] thr 0 s 1 hill lambda x 1 0 * x 0 if thr 0 else x ** s / thr ** s + x ** s color np array color flim lambda im hill np sqrt im - color ** 2 sum axis 2 mask clip fl_image flim mask ismask Truenewclip clip set_mask mask return newclip
def mask_color clip color [0 0 0] thr 0 s 1 hill lambda x 1 0 * x 0 if thr 0 else x ** s / thr ** s + x ** s color np array color flim lambda im hill np sqrt im - color ** 2 sum axis 2 mask clip fl_image flim mask ismask Truenewclip clip set_mask mask return newclip
def _remove_original_values _config_vars for k in list _config_vars if k startswith _INITPRE del _config_vars[k]
def _remove_original_values _config_vars for k in list _config_vars if k startswith _INITPRE del _config_vars[k]
@step u'The" [^"]* "problemdisplaysa" [^"]* "answer' def assert_problem_has_answer step problem_type answer_class assert answer_class in ['correct' 'incorrect' 'blank'] assert problem_type in PROBLEM_DICT problem_has_answer world scenario_dict['COURSE'] number problem_type answer_class
@step u'The" [^"]* "problemdisplaysa" [^"]* "answer' def assert_problem_has_answer step problem_type answer_class assert answer_class in ['correct' 'incorrect' 'blank'] assert problem_type in PROBLEM_DICT problem_has_answer world scenario_dict['COURSE'] number problem_type answer_class
def approximate_fst desired_fst simulated_fst parameter_fst max_run_fst 1 min_run_fst 0 limit 0 005 if abs simulated_fst - desired_fst < limit return parameter_fst max_run_fst min_run_fst if simulated_fst > desired_fst max_run_fst parameter_fstnext_parameter_fst min_run_fst + parameter_fst / 2 else min_run_fst parameter_fstnext_parameter_fst max_run_fst + parameter_fst / 2 return next_parameter_fst max_run_fst min_run_fst
def approximate_fst desired_fst simulated_fst parameter_fst max_run_fst 1 min_run_fst 0 limit 0 005 if abs simulated_fst - desired_fst < limit return parameter_fst max_run_fst min_run_fst if simulated_fst > desired_fst max_run_fst parameter_fstnext_parameter_fst min_run_fst + parameter_fst / 2 else min_run_fst parameter_fstnext_parameter_fst max_run_fst + parameter_fst / 2 return next_parameter_fst max_run_fst min_run_fst
def approximate_fst desired_fst simulated_fst parameter_fst max_run_fst 1 min_run_fst 0 limit 0 005 if abs simulated_fst - desired_fst < limit return parameter_fst max_run_fst min_run_fst if simulated_fst > desired_fst max_run_fst parameter_fstnext_parameter_fst min_run_fst + parameter_fst / 2 else min_run_fst parameter_fstnext_parameter_fst max_run_fst + parameter_fst / 2 return next_parameter_fst max_run_fst min_run_fst
def genBhLobe x N 512f x * np pi * 2 / N df 2 * np pi / N y np zeros x size consts [0 35875 0 48829 0 14128 0 01168]for m in range 0 4 y + consts[m] / 2 * sinc f - df * m N + sinc f + df * m N y y / N / consts[0] return y
def genBhLobe x N 512f x * np pi * 2 / N df 2 * np pi / N y np zeros x size consts [0 35875 0 48829 0 14128 0 01168]for m in range 0 4 y + consts[m] / 2 * sinc f - df * m N + sinc f + df * m N y y / N / consts[0] return y
def get_cifar100 withlabel True ndim 3 scale 1 0 raw _retrieve_cifar_100 train _preprocess_cifar raw['train_x'] raw['train_y'] withlabel ndim scale test _preprocess_cifar raw['test_x'] raw['test_y'] withlabel ndim scale return train test
def get_cifar100 withlabel True ndim 3 scale 1 0 raw _retrieve_cifar_100 train _preprocess_cifar raw['train_x'] raw['train_y'] withlabel ndim scale test _preprocess_cifar raw['test_x'] raw['test_y'] withlabel ndim scale return train test
def _get_window start end from scipy signal import hannwindow 1 - np r_[ hann 4 [ 2] np ones np abs end - start - 4 hann 4 [ -2 ] ] T return window
def _get_window start end from scipy signal import hannwindow 1 - np r_[ hann 4 [ 2] np ones np abs end - start - 4 hann 4 [ -2 ] ] T return window
def _get_window start end from scipy signal import hannwindow 1 - np r_[ hann 4 [ 2] np ones np abs end - start - 4 hann 4 [ -2 ] ] T return window
def apply_operators e e e expand muls e atoms Mul subs_list [ m _apply_Mul m for m in iter muls ]return e subs subs_list
def apply_operators e e e expand muls e atoms Mul subs_list [ m _apply_Mul m for m in iter muls ]return e subs subs_list
def apply_operators e e e expand muls e atoms Mul subs_list [ m _apply_Mul m for m in iter muls ]return e subs subs_list
def map_url_out request env application controller function args other scheme host port language None map MapUrlOut request env application controller function args other scheme host port language return map acf
def map_url_out request env application controller function args other scheme host port language None map MapUrlOut request env application controller function args other scheme host port language return map acf
def map_url_out request env application controller function args other scheme host port language None map MapUrlOut request env application controller function args other scheme host port language return map acf
def map_url_out request env application controller function args other scheme host port language None map MapUrlOut request env application controller function args other scheme host port language return map acf
def map_url_out request env application controller function args other scheme host port language None map MapUrlOut request env application controller function args other scheme host port language return map acf
def map_url_out request env application controller function args other scheme host port language None map MapUrlOut request env application controller function args other scheme host port language return map acf
def getWideAnglePointIndex loop dotProductMinimum 9999999 9widestPointIndex 0for pointIndex in xrange len loop point loop[ pointIndex % len loop ]afterPoint loop[ pointIndex + 1 % len loop ]beforePoint loop[ pointIndex - 1 % len loop ]afterSegmentNormalized euclidean getNormalized afterPoint - point beforeSegmentNormalized euclidean getNormalized beforePoint - point dotProduct euclidean getDotProduct afterSegmentNormalized beforeSegmentNormalized if dotProduct < 0 99 return pointIndexif dotProduct < dotProductMinimum dotProductMinimum dotProductwidestPointIndex pointIndexreturn widestPointIndex
def getWideAnglePointIndex loop dotProductMinimum 9999999 9widestPointIndex 0for pointIndex in xrange len loop point loop[ pointIndex % len loop ]afterPoint loop[ pointIndex + 1 % len loop ]beforePoint loop[ pointIndex - 1 % len loop ]afterSegmentNormalized euclidean getNormalized afterPoint - point beforeSegmentNormalized euclidean getNormalized beforePoint - point dotProduct euclidean getDotProduct afterSegmentNormalized beforeSegmentNormalized if dotProduct < 0 99 return pointIndexif dotProduct < dotProductMinimum dotProductMinimum dotProductwidestPointIndex pointIndexreturn widestPointIndex
def getWideAnglePointIndex loop dotProductMinimum 9999999 9widestPointIndex 0for pointIndex in xrange len loop point loop[ pointIndex % len loop ]afterPoint loop[ pointIndex + 1 % len loop ]beforePoint loop[ pointIndex - 1 % len loop ]afterSegmentNormalized euclidean getNormalized afterPoint - point beforeSegmentNormalized euclidean getNormalized beforePoint - point dotProduct euclidean getDotProduct afterSegmentNormalized beforeSegmentNormalized if dotProduct < 0 99 return pointIndexif dotProduct < dotProductMinimum dotProductMinimum dotProductwidestPointIndex pointIndexreturn widestPointIndex
def getWideAnglePointIndex loop dotProductMinimum 9999999 9widestPointIndex 0for pointIndex in xrange len loop point loop[ pointIndex % len loop ]afterPoint loop[ pointIndex + 1 % len loop ]beforePoint loop[ pointIndex - 1 % len loop ]afterSegmentNormalized euclidean getNormalized afterPoint - point beforeSegmentNormalized euclidean getNormalized beforePoint - point dotProduct euclidean getDotProduct afterSegmentNormalized beforeSegmentNormalized if dotProduct < 0 99 return pointIndexif dotProduct < dotProductMinimum dotProductMinimum dotProductwidestPointIndex pointIndexreturn widestPointIndex
def getWideAnglePointIndex loop dotProductMinimum 9999999 9widestPointIndex 0for pointIndex in xrange len loop point loop[ pointIndex % len loop ]afterPoint loop[ pointIndex + 1 % len loop ]beforePoint loop[ pointIndex - 1 % len loop ]afterSegmentNormalized euclidean getNormalized afterPoint - point beforeSegmentNormalized euclidean getNormalized beforePoint - point dotProduct euclidean getDotProduct afterSegmentNormalized beforeSegmentNormalized if dotProduct < 0 99 return pointIndexif dotProduct < dotProductMinimum dotProductMinimum dotProductwidestPointIndex pointIndexreturn widestPointIndex
def attach_points queryset as_field 'points_attr' model queryset modelsql '\nSELECTjson_agg \nrow_to_json projects_points \nORDERBYprojects_points order\n \nFROMprojects_points\nWHEREprojects_points project_id {tbl} id\n'sql sql format tbl model _meta db_table queryset queryset extra select {as_field sql} return queryset
def attach_points queryset as_field 'points_attr' model queryset modelsql '\nSELECTjson_agg \nrow_to_json projects_points \nORDERBYprojects_points order\n \nFROMprojects_points\nWHEREprojects_points project_id {tbl} id\n'sql sql format tbl model _meta db_table queryset queryset extra select {as_field sql} return queryset
def _get_registered_option key return _registered_options get key
def _get_registered_option key return _registered_options get key
def move_entry from_list from_set to_list to_set i mac get from_list if mac is None returnfrom_list delete i to_list insert END mac mac EthAddr mac to_set add mac from_set remove mac if clear_tables_on_change core callLater clear_flows
def align_check device part_type partition _validate_device device if part_type not in set ['minimal' 'optimal'] raise CommandExecutionError 'Invalidpart_typepassedtopartition align_check' try int partition except Exception raise CommandExecutionError 'Invalidpartitionpassedtopartition align_check' cmd 'parted-m-s{0}align-check{1}{2}' format device part_type partition out __salt__['cmd run'] cmd splitlines return out
def align_check device part_type partition _validate_device device if part_type not in set ['minimal' 'optimal'] raise CommandExecutionError 'Invalidpart_typepassedtopartition align_check' try int partition except Exception raise CommandExecutionError 'Invalidpartitionpassedtopartition align_check' cmd 'parted-m-s{0}align-check{1}{2}' format device part_type partition out __salt__['cmd run'] cmd splitlines return out
def align_check device part_type partition _validate_device device if part_type not in set ['minimal' 'optimal'] raise CommandExecutionError 'Invalidpart_typepassedtopartition align_check' try int partition except Exception raise CommandExecutionError 'Invalidpartitionpassedtopartition align_check' cmd 'parted-m-s{0}align-check{1}{2}' format device part_type partition out __salt__['cmd run'] cmd splitlines return out
def create_custom_token uid valid_minutes 60 client_email app_identity get_service_account_name now int time time payload base64 b64encode json dumps {'iss' client_email 'sub' client_email 'aud' _IDENTITY_ENDPOINT 'uid' uid 'iat' now 'exp' now + valid_minutes * 60 } header base64 b64encode json dumps {'typ' 'JWT' 'alg' 'RS256'} to_sign '{} {}' format header payload return '{} {}' format to_sign base64 b64encode app_identity sign_blob to_sign [1]
def create_custom_token uid valid_minutes 60 client_email app_identity get_service_account_name now int time time payload base64 b64encode json dumps {'iss' client_email 'sub' client_email 'aud' _IDENTITY_ENDPOINT 'uid' uid 'iat' now 'exp' now + valid_minutes * 60 } header base64 b64encode json dumps {'typ' 'JWT' 'alg' 'RS256'} to_sign '{} {}' format header payload return '{} {}' format to_sign base64 b64encode app_identity sign_blob to_sign [1]
def _accumulateFrequencyCounts values freqCounts None values numpy array values numEntries values max + 1 if freqCounts is not None numEntries max numEntries freqCounts size if freqCounts is not None if freqCounts size numEntries newCounts numpy zeros numEntries dtype 'int32' newCounts[0 freqCounts size] freqCountselse newCounts freqCountselse newCounts numpy zeros numEntries dtype 'int32' for v in values newCounts[v] + 1return newCounts
def _accumulateFrequencyCounts values freqCounts None values numpy array values numEntries values max + 1 if freqCounts is not None numEntries max numEntries freqCounts size if freqCounts is not None if freqCounts size numEntries newCounts numpy zeros numEntries dtype 'int32' newCounts[0 freqCounts size] freqCountselse newCounts freqCountselse newCounts numpy zeros numEntries dtype 'int32' for v in values newCounts[v] + 1return newCounts
def test_badoptimization_opt_err @gof local_optimizer [theano tensor add] def insert_bigger_b_add node if node op theano tensor add inputs list node inputs if inputs[ -1 ] owner is None inputs[ -1 ] theano tensor concatenate inputs[ -1 ] inputs[ -1 ] return [node op *inputs ]return Falseedb gof EquilibriumDB edb register 'insert_bigger_b_add' insert_bigger_b_add 'all' opt edb query '+all' a theano tensor dvector b theano tensor dvector f theano function [a b] a + b mode debugmode DebugMode optimizer opt try f [1 0 2 0 3 0] [2 3 4] except Exception as e assert 'insert_bigger_b_add' in exc_message e returnassert False
def test_badoptimization_opt_err @gof local_optimizer [theano tensor add] def insert_bigger_b_add node if node op theano tensor add inputs list node inputs if inputs[ -1 ] owner is None inputs[ -1 ] theano tensor concatenate inputs[ -1 ] inputs[ -1 ] return [node op *inputs ]return Falseedb gof EquilibriumDB edb register 'insert_bigger_b_add' insert_bigger_b_add 'all' opt edb query '+all' a theano tensor dvector b theano tensor dvector f theano function [a b] a + b mode debugmode DebugMode optimizer opt try f [1 0 2 0 3 0] [2 3 4] except Exception as e assert 'insert_bigger_b_add' in exc_message e returnassert False
def test_badoptimization_opt_err @gof local_optimizer [theano tensor add] def insert_bigger_b_add node if node op theano tensor add inputs list node inputs if inputs[ -1 ] owner is None inputs[ -1 ] theano tensor concatenate inputs[ -1 ] inputs[ -1 ] return [node op *inputs ]return Falseedb gof EquilibriumDB edb register 'insert_bigger_b_add' insert_bigger_b_add 'all' opt edb query '+all' a theano tensor dvector b theano tensor dvector f theano function [a b] a + b mode debugmode DebugMode optimizer opt try f [1 0 2 0 3 0] [2 3 4] except Exception as e assert 'insert_bigger_b_add' in exc_message e returnassert False
def test_badoptimization_opt_err @gof local_optimizer [theano tensor add] def insert_bigger_b_add node if node op theano tensor add inputs list node inputs if inputs[ -1 ] owner is None inputs[ -1 ] theano tensor concatenate inputs[ -1 ] inputs[ -1 ] return [node op *inputs ]return Falseedb gof EquilibriumDB edb register 'insert_bigger_b_add' insert_bigger_b_add 'all' opt edb query '+all' a theano tensor dvector b theano tensor dvector f theano function [a b] a + b mode debugmode DebugMode optimizer opt try f [1 0 2 0 3 0] [2 3 4] except Exception as e assert 'insert_bigger_b_add' in exc_message e returnassert False
def test_badoptimization_opt_err @gof local_optimizer [theano tensor add] def insert_bigger_b_add node if node op theano tensor add inputs list node inputs if inputs[ -1 ] owner is None inputs[ -1 ] theano tensor concatenate inputs[ -1 ] inputs[ -1 ] return [node op *inputs ]return Falseedb gof EquilibriumDB edb register 'insert_bigger_b_add' insert_bigger_b_add 'all' opt edb query '+all' a theano tensor dvector b theano tensor dvector f theano function [a b] a + b mode debugmode DebugMode optimizer opt try f [1 0 2 0 3 0] [2 3 4] except Exception as e assert 'insert_bigger_b_add' in exc_message e returnassert False
def construct_query_part model_cls prefixes query_part if not query_part return query TrueQuery query_classes {}for k t in itertools chain model_cls _fields items model_cls _types items query_classes[k] t query key pattern query_class negate parse_query_part query_part query_classes prefixes if key is None if issubclass query_class query FieldQuery q query AnyFieldQuery pattern model_cls _search_fields query_class if negate return query NotQuery q else return qelif negate return query NotQuery query_class pattern else return query_class pattern key key lower q query_class key lower pattern key in model_cls _fields if negate return query NotQuery q return q
def construct_query_part model_cls prefixes query_part if not query_part return query TrueQuery query_classes {}for k t in itertools chain model_cls _fields items model_cls _types items query_classes[k] t query key pattern query_class negate parse_query_part query_part query_classes prefixes if key is None if issubclass query_class query FieldQuery q query AnyFieldQuery pattern model_cls _search_fields query_class if negate return query NotQuery q else return qelif negate return query NotQuery query_class pattern else return query_class pattern key key lower q query_class key lower pattern key in model_cls _fields if negate return query NotQuery q return q
def rectangle terminal position dimension top left position width height dimensionterminal selectCharacterSet insults CS_DRAWING insults G0 terminal cursorPosition top left terminal write chr 108 terminal write chr 113 * width - 2 terminal write chr 107 for n in range height - 2 terminal cursorPosition left top + n + 1 terminal write chr 120 terminal cursorForward width - 2 terminal write chr 120 terminal cursorPosition 0 top + height - 1 terminal write chr 109 terminal write chr 113 * width - 2 terminal write chr 106 terminal selectCharacterSet insults CS_US insults G0
def rectangle terminal position dimension top left position width height dimensionterminal selectCharacterSet insults CS_DRAWING insults G0 terminal cursorPosition top left terminal write chr 108 terminal write chr 113 * width - 2 terminal write chr 107 for n in range height - 2 terminal cursorPosition left top + n + 1 terminal write chr 120 terminal cursorForward width - 2 terminal write chr 120 terminal cursorPosition 0 top + height - 1 terminal write chr 109 terminal write chr 113 * width - 2 terminal write chr 106 terminal selectCharacterSet insults CS_US insults G0
def terminate instance_id None name None region None key None keyid None profile None filters None instances find_instances instance_id instance_id name name region region key key keyid keyid profile profile return_objs True filters filters if instances in False None [] return instancesif len instances 1 instances[0] terminate return Trueelse log warning 'Refusingtoterminatemultipleinstancesatonce' return False
def terminate instance_id None name None region None key None keyid None profile None filters None instances find_instances instance_id instance_id name name region region key key keyid keyid profile profile return_objs True filters filters if instances in False None [] return instancesif len instances 1 instances[0] terminate return Trueelse log warning 'Refusingtoterminatemultipleinstancesatonce' return False
def _make_container_root name path _root name if os path exists path __context__['retcode'] salt defaults exitcodes SALT_BUILD_FAILraise CommandExecutionError 'Container{0}alreadyexists' format name else try os makedirs path return pathexcept OSError as exc raise CommandExecutionError 'Unabletomakecontainerrootdirectory{0} {1}' format name exc
def _make_container_root name path _root name if os path exists path __context__['retcode'] salt defaults exitcodes SALT_BUILD_FAILraise CommandExecutionError 'Container{0}alreadyexists' format name else try os makedirs path return pathexcept OSError as exc raise CommandExecutionError 'Unabletomakecontainerrootdirectory{0} {1}' format name exc
def _split_gcd *a g a[0]b1 [g]b2 []for x in a[1 ] g1 gcd g x if g1 1 b2 append x else g g1b1 append x return g b1 b2
def _split_gcd *a g a[0]b1 [g]b2 []for x in a[1 ] g1 gcd g x if g1 1 b2 append x else g g1b1 append x return g b1 b2
def johnson G weight 'weight' if not nx is_weighted G weight weight raise nx NetworkXError 'Graphisnotweighted ' dist {v 0 for v in G}pred {v [None] for v in G}weight _weight_function G weight dist_bellman _bellman_ford G list G weight pred pred dist dist scale lambda u v dist_bellman[u] - dist_bellman[v] new_weight lambda u v d weight u v d + scale u v def dist_path v paths {v [v]}_dijkstra G v new_weight paths paths return pathsreturn {v dist_path v for v in G}
def johnson G weight 'weight' if not nx is_weighted G weight weight raise nx NetworkXError 'Graphisnotweighted ' dist {v 0 for v in G}pred {v [None] for v in G}weight _weight_function G weight dist_bellman _bellman_ford G list G weight pred pred dist dist scale lambda u v dist_bellman[u] - dist_bellman[v] new_weight lambda u v d weight u v d + scale u v def dist_path v paths {v [v]}_dijkstra G v new_weight paths paths return pathsreturn {v dist_path v for v in G}
def johnson G weight 'weight' if not nx is_weighted G weight weight raise nx NetworkXError 'Graphisnotweighted ' dist {v 0 for v in G}pred {v [None] for v in G}weight _weight_function G weight dist_bellman _bellman_ford G list G weight pred pred dist dist scale lambda u v dist_bellman[u] - dist_bellman[v] new_weight lambda u v d weight u v d + scale u v def dist_path v paths {v [v]}_dijkstra G v new_weight paths paths return pathsreturn {v dist_path v for v in G}
def packages pkg_list name None prefix None **kwargs install pkg_list name name prefix prefix **kwargs
def packages pkg_list name None prefix None **kwargs install pkg_list name name prefix prefix **kwargs
def connect creds credentials get_credentials return storage Client credentials credentials http ThreadSafeHttp credentials
def connect creds credentials get_credentials return storage Client credentials credentials http ThreadSafeHttp credentials
def connect creds credentials get_credentials return storage Client credentials credentials http ThreadSafeHttp credentials
def quote s return '"' + s replace '\\' '\\\\' replace '"' '\\"' + '"'
def quote s return '"' + s replace '\\' '\\\\' replace '"' '\\"' + '"'
def redirect_with_get view_name get_querydict do_reverse True if do_reverse url reverse view_name else url view_nameif get_querydict return redirect '%s?%s' % url get_querydict urlencode safe '/' return redirect view_name
def redirect_with_get view_name get_querydict do_reverse True if do_reverse url reverse view_name else url view_nameif get_querydict return redirect '%s?%s' % url get_querydict urlencode safe '/' return redirect view_name
def ParseHostPort address host_port_re re match ' [a-zA-Z0-9-\\ ]+ [0-9]{1 5} $' address if not host_port_re raise TypeError 'badhost port %s' % address host host_port_re group 1 port int host_port_re group 2 if port > 65536 raise TypeError 'invalidport %d' % port return host port
def ParseHostPort address host_port_re re match ' [a-zA-Z0-9-\\ ]+ [0-9]{1 5} $' address if not host_port_re raise TypeError 'badhost port %s' % address host host_port_re group 1 port int host_port_re group 2 if port > 65536 raise TypeError 'invalidport %d' % port return host port
def ParseHostPort address host_port_re re match ' [a-zA-Z0-9-\\ ]+ [0-9]{1 5} $' address if not host_port_re raise TypeError 'badhost port %s' % address host host_port_re group 1 port int host_port_re group 2 if port > 65536 raise TypeError 'invalidport %d' % port return host port
def _wider_test_jpeg data if data[ 2] '\xff\xd8' return 'jpeg'
def _wider_test_jpeg data if data[ 2] '\xff\xd8' return 'jpeg'
def _wider_test_jpeg data if data[ 2] '\xff\xd8' return 'jpeg'
def _wider_test_jpeg data if data[ 2] '\xff\xd8' return 'jpeg'
def _wider_test_jpeg data if data[ 2] '\xff\xd8' return 'jpeg'
def _wider_test_jpeg data if data[ 2] '\xff\xd8' return 'jpeg'
def _wider_test_jpeg data if data[ 2] '\xff\xd8' return 'jpeg'
def _wider_test_jpeg data if data[ 2] '\xff\xd8' return 'jpeg'
def _wider_test_jpeg data if data[ 2] '\xff\xd8' return 'jpeg'
def get_course_enrollment username course_id course_key CourseKey from_string course_id try enrollment CourseEnrollment objects get user__username username course_id course_key return CourseEnrollmentSerializer enrollment dataexcept CourseEnrollment DoesNotExist return None
def get_course_enrollment username course_id course_key CourseKey from_string course_id try enrollment CourseEnrollment objects get user__username username course_id course_key return CourseEnrollmentSerializer enrollment dataexcept CourseEnrollment DoesNotExist return None
def get_course_enrollment username course_id course_key CourseKey from_string course_id try enrollment CourseEnrollment objects get user__username username course_id course_key return CourseEnrollmentSerializer enrollment dataexcept CourseEnrollment DoesNotExist return None
def get_course_enrollment username course_id course_key CourseKey from_string course_id try enrollment CourseEnrollment objects get user__username username course_id course_key return CourseEnrollmentSerializer enrollment dataexcept CourseEnrollment DoesNotExist return None
def _get_volume_type_extra_spec type_id spec_key possible_values None default_value None if not type_id return default_valuespec_key 'vmware %s' % spec_key spec_value volume_types get_volume_type_extra_specs type_id spec_key if not spec_value LOG debug 'Returningdefaultspecvalue %s ' default_value return default_valueif possible_values is None return spec_valueif spec_value in possible_values LOG debug 'Returningspecvalue%s' spec_value return spec_valueLOG debug 'Invalidspecvalue %sspecified ' spec_value
def _get_volume_type_extra_spec type_id spec_key possible_values None default_value None if not type_id return default_valuespec_key 'vmware %s' % spec_key spec_value volume_types get_volume_type_extra_specs type_id spec_key if not spec_value LOG debug 'Returningdefaultspecvalue %s ' default_value return default_valueif possible_values is None return spec_valueif spec_value in possible_values LOG debug 'Returningspecvalue%s' spec_value return spec_valueLOG debug 'Invalidspecvalue %sspecified ' spec_value
def _display s if not isinstance s unicode s s decode 'utf-8' s _indent _escaped_text_from_text s 'whitespace' 4 if not s endswith '\n' s + '\n'return s
@cache_permissiondef can_commit_translation user project return check_permission user project 'trans commit_translation'
@cache_permissiondef can_commit_translation user project return check_permission user project 'trans commit_translation'
def _synchronized method def synchronized_wrapper self *args with self _lock return method self *args return synchronized_wrapper
def _synchronized method def synchronized_wrapper self *args with self _lock return method self *args return synchronized_wrapper
def _orbits degree generators seen set orbs []sorted_I list range degree I set sorted_I while I i sorted_I[0]orb _orbit degree generators i orbs append orb I - orbsorted_I [i for i in sorted_I if i not in orb ]return orbs
def _orbits degree generators seen set orbs []sorted_I list range degree I set sorted_I while I i sorted_I[0]orb _orbit degree generators i orbs append orb I - orbsorted_I [i for i in sorted_I if i not in orb ]return orbs
def _saferepr obj repr py io saferepr obj if py builtin _istext repr t py builtin textelse t py builtin bytesreturn repr replace t '\n' t '\\n'
def _saferepr obj repr py io saferepr obj if py builtin _istext repr t py builtin textelse t py builtin bytesreturn repr replace t '\n' t '\\n'
def parse_rarefaction lines col_headers []comments []rarefaction_data []rarefaction_fns []for line in lines if line[0] '#' comments append line elif line[0] ' DCTB ' col_headers map strip line split ' DCTB ' else rarefaction_fn data parse_rarefaction_record line rarefaction_fns append rarefaction_fn rarefaction_data append data return col_headers comments rarefaction_fns rarefaction_data
@contextmanagerdef flock lockfile shared False fcntl flock lockfile fcntl LOCK_SH if shared else fcntl LOCK_EX try yield finally fcntl flock lockfile fcntl LOCK_UN
@contextmanagerdef flock lockfile shared False fcntl flock lockfile fcntl LOCK_SH if shared else fcntl LOCK_EX try yield finally fcntl flock lockfile fcntl LOCK_UN
def ms_module exploit return {'1' 'exploit/multi/browser/adobe_flash_hacking_team_uaf' '2' 'exploit/multi/browser/adobe_flash_nellymoser_bof' '3' 'exploit/multi/browser/adobe_flash_shader_drawing_fill' '4' 'exploit/windows/browser/ms14_012_textrange' '5' 'exploit/windows/browser/ms14_012_cmarkup_uaf' '6' 'exploit/windows/browser/ms13_080_cdisplaypointer' '7' 'exploit/windows/browser/ie_setmousecapture_uaf' '8' 'exploit/multi/browser/java_jre17_jmxbean_2' '9' 'exploit/multi/browser/java_jre17_jmxbean' '10' 'exploit/windows/browser/ms13_009_ie_slayoutrun_uaf' '11' 'exploit/windows/browser/ie_cbutton_uaf' '12' 'exploit/multi/browser/java_jre17_exec' '13' 'exploit/windows/browser/ie_execcommand_uaf' '14' 'exploit/multi/browser/java_atomicreferencearray' '15' 'exploit/multi/browser/java_verifier_field_access' '16' 'exploit/windows/browser/ms12_037_same_id' '17' 'exploit/windows/browser/msxml_get_definition_code_exec' '18' 'exploit/windows/browser/adobe_flash_rtmp' '19' 'exploit/windows/browser/adobe_flash_mp4_cprt' '20' 'exploit/windows/browser/ms12_004_midi' '21' 'multi/browser/java_rhino\nsettarget1' '22' 'windows/browser/ms11_050_mshtml_cobjectelement' '23' 'windows/browser/adobe_flashplayer_flash10o' '24' 'windows/browser/cisco_anyconnect_exec' '25' 'windows/browser/ms11_003_ie_css_import' '26' 'windows/browser/wmi_admintools' '27' 'windows/browser/ms10_090_ie_css_clip' '28' 'windows/browser/java_codebase_trust' '29' 'windows/browser/java_docbase_bof' '30' 'windows/browser/webdav_dll_hijacker' '31' 'windows/browser/adobe_flashplayer_avm' '32' 'windows/browser/adobe_shockwave_rcsl_corruption' '33' 'windows/browser/adobe_cooltype_sing' '34' 'windows/browser/apple_quicktime_marshaled_punk' '35' 'windows/browser/ms10_042_helpctr_xss_cmd_exec' '36' 'windows/browser/ms10_018_ie_behaviors' '37' 'windows/browser/ms10_002_aurora' '38' 'windows/browser/ms10_018_ie_tabular_activex' '39' 'windows/browser/ms09_002_memory_corruption' '40' 'windows/browser/ms09_072_style_object' '41' 'windows/browser/ie_iscomponentinstalled' '42' 'windows/browser/ms08_078_xml_corruption' '43' 'windows/browser/ie_unsafe_scripting' '44' 'multi/browser/firefox_escape_retval' '45' 'windows/browser/mozilla_mchannel' '46' 'auxiliary/server/browser_autopwn'} get exploit 'ERROR'
def ms_module exploit return {'1' 'exploit/multi/browser/adobe_flash_hacking_team_uaf' '2' 'exploit/multi/browser/adobe_flash_nellymoser_bof' '3' 'exploit/multi/browser/adobe_flash_shader_drawing_fill' '4' 'exploit/windows/browser/ms14_012_textrange' '5' 'exploit/windows/browser/ms14_012_cmarkup_uaf' '6' 'exploit/windows/browser/ms13_080_cdisplaypointer' '7' 'exploit/windows/browser/ie_setmousecapture_uaf' '8' 'exploit/multi/browser/java_jre17_jmxbean_2' '9' 'exploit/multi/browser/java_jre17_jmxbean' '10' 'exploit/windows/browser/ms13_009_ie_slayoutrun_uaf' '11' 'exploit/windows/browser/ie_cbutton_uaf' '12' 'exploit/multi/browser/java_jre17_exec' '13' 'exploit/windows/browser/ie_execcommand_uaf' '14' 'exploit/multi/browser/java_atomicreferencearray' '15' 'exploit/multi/browser/java_verifier_field_access' '16' 'exploit/windows/browser/ms12_037_same_id' '17' 'exploit/windows/browser/msxml_get_definition_code_exec' '18' 'exploit/windows/browser/adobe_flash_rtmp' '19' 'exploit/windows/browser/adobe_flash_mp4_cprt' '20' 'exploit/windows/browser/ms12_004_midi' '21' 'multi/browser/java_rhino\nsettarget1' '22' 'windows/browser/ms11_050_mshtml_cobjectelement' '23' 'windows/browser/adobe_flashplayer_flash10o' '24' 'windows/browser/cisco_anyconnect_exec' '25' 'windows/browser/ms11_003_ie_css_import' '26' 'windows/browser/wmi_admintools' '27' 'windows/browser/ms10_090_ie_css_clip' '28' 'windows/browser/java_codebase_trust' '29' 'windows/browser/java_docbase_bof' '30' 'windows/browser/webdav_dll_hijacker' '31' 'windows/browser/adobe_flashplayer_avm' '32' 'windows/browser/adobe_shockwave_rcsl_corruption' '33' 'windows/browser/adobe_cooltype_sing' '34' 'windows/browser/apple_quicktime_marshaled_punk' '35' 'windows/browser/ms10_042_helpctr_xss_cmd_exec' '36' 'windows/browser/ms10_018_ie_behaviors' '37' 'windows/browser/ms10_002_aurora' '38' 'windows/browser/ms10_018_ie_tabular_activex' '39' 'windows/browser/ms09_002_memory_corruption' '40' 'windows/browser/ms09_072_style_object' '41' 'windows/browser/ie_iscomponentinstalled' '42' 'windows/browser/ms08_078_xml_corruption' '43' 'windows/browser/ie_unsafe_scripting' '44' 'multi/browser/firefox_escape_retval' '45' 'windows/browser/mozilla_mchannel' '46' 'auxiliary/server/browser_autopwn'} get exploit 'ERROR'
def expire key seconds host None port None db None password None server _connect host port db password return server expire key seconds
def expire key seconds host None port None db None password None server _connect host port db password return server expire key seconds
def expire key seconds host None port None db None password None server _connect host port db password return server expire key seconds
def expand_empty tensor_var size if size 0 return tensor_varshapes [tensor_var shape[x] for x in xrange tensor_var ndim ]new_shape [ size + shapes[0] ] + shapes[1 ] empty tensor AllocEmpty tensor_var dtype *new_shape ret tensor set_subtensor empty[ shapes[0]] tensor_var ret tag nan_guard_mode_check Falsereturn ret
def expand_empty tensor_var size if size 0 return tensor_varshapes [tensor_var shape[x] for x in xrange tensor_var ndim ]new_shape [ size + shapes[0] ] + shapes[1 ] empty tensor AllocEmpty tensor_var dtype *new_shape ret tensor set_subtensor empty[ shapes[0]] tensor_var ret tag nan_guard_mode_check Falsereturn ret
def expand_empty tensor_var size if size 0 return tensor_varshapes [tensor_var shape[x] for x in xrange tensor_var ndim ]new_shape [ size + shapes[0] ] + shapes[1 ] empty tensor AllocEmpty tensor_var dtype *new_shape ret tensor set_subtensor empty[ shapes[0]] tensor_var ret tag nan_guard_mode_check Falsereturn ret
def expand_empty tensor_var size if size 0 return tensor_varshapes [tensor_var shape[x] for x in xrange tensor_var ndim ]new_shape [ size + shapes[0] ] + shapes[1 ] empty tensor AllocEmpty tensor_var dtype *new_shape ret tensor set_subtensor empty[ shapes[0]] tensor_var ret tag nan_guard_mode_check Falsereturn ret
def test_json assert hug types json {'this' 'works'} {'this' 'works'} assert hug types json json dumps {'this' 'works'} {'this' 'works'} with pytest raises ValueError hug types json 'InvalidJSON'
def test_json assert hug types json {'this' 'works'} {'this' 'works'} assert hug types json json dumps {'this' 'works'} {'this' 'works'} with pytest raises ValueError hug types json 'InvalidJSON'
def test_json assert hug types json {'this' 'works'} {'this' 'works'} assert hug types json json dumps {'this' 'works'} {'this' 'works'} with pytest raises ValueError hug types json 'InvalidJSON'
@LocalContextdef unpack_many data word_size None word_size word_size or context word_size endianness context endiannesssign context signif word_size 'all' return [unpack data word_size ]if word_size % 8 0 raise ValueError 'unpack_many word_sizemustbeamultipleof8' out []n word_size // 8 for i in range 0 len data n out append unpack data[i i + n ] word_size return list map int out
@LocalContextdef unpack_many data word_size None word_size word_size or context word_size endianness context endiannesssign context signif word_size 'all' return [unpack data word_size ]if word_size % 8 0 raise ValueError 'unpack_many word_sizemustbeamultipleof8' out []n word_size // 8 for i in range 0 len data n out append unpack data[i i + n ] word_size return list map int out
def maximum_flow_value G s t capacity 'capacity' flow_func None **kwargs if flow_func is None if kwargs raise nx NetworkXError 'Youhavetoexplicitlysetaflow_funcifyouneedtopassparametersviakwargs ' flow_func default_flow_funcif not callable flow_func raise nx NetworkXError 'flow_funchastobecallable ' R flow_func G s t capacity capacity value_only True **kwargs return R graph['flow_value']
def maximum_flow_value G s t capacity 'capacity' flow_func None **kwargs if flow_func is None if kwargs raise nx NetworkXError 'Youhavetoexplicitlysetaflow_funcifyouneedtopassparametersviakwargs ' flow_func default_flow_funcif not callable flow_func raise nx NetworkXError 'flow_funchastobecallable ' R flow_func G s t capacity capacity value_only True **kwargs return R graph['flow_value']
def RemoveSelfDependencies targets for target_name target_dict in targets iteritems for dependency_key in dependency_sections dependencies target_dict get dependency_key [] if dependencies for t in dependencies if t target_name if targets[t] get 'variables' {} get 'prune_self_dependency' 0 target_dict[dependency_key] Filter dependencies target_name
def RemoveSelfDependencies targets for target_name target_dict in targets iteritems for dependency_key in dependency_sections dependencies target_dict get dependency_key [] if dependencies for t in dependencies if t target_name if targets[t] get 'variables' {} get 'prune_self_dependency' 0 target_dict[dependency_key] Filter dependencies target_name
def RemoveSelfDependencies targets for target_name target_dict in targets iteritems for dependency_key in dependency_sections dependencies target_dict get dependency_key [] if dependencies for t in dependencies if t target_name if targets[t] get 'variables' {} get 'prune_self_dependency' 0 target_dict[dependency_key] Filter dependencies target_name
def RemoveSelfDependencies targets for target_name target_dict in targets iteritems for dependency_key in dependency_sections dependencies target_dict get dependency_key [] if dependencies for t in dependencies if t target_name if targets[t] get 'variables' {} get 'prune_self_dependency' 0 target_dict[dependency_key] Filter dependencies target_name
def as_sparse_or_tensor_variable x name None try return as_sparse_variable x name except ValueError TypeError return theano tensor as_tensor_variable x name
def as_sparse_or_tensor_variable x name None try return as_sparse_variable x name except ValueError TypeError return theano tensor as_tensor_variable x name
def get_python_os_info info platform system_alias platform system platform release platform version os_type os_ver _ infoos_type os_type lower if os_type startswith 'linux' info platform linux_distribution if info[0] os_type info[0]if info[1] os_ver info[1]elif os_type startswith 'darwin' os_ver subprocess Popen ['sw_vers' '-productVersion'] stdout subprocess PIPE communicate [0] rstrip '\n' elif os_type startswith 'freebsd' os_ver os_ver partition '-' [0]os_ver os_ver partition ' ' [0]elif platform win32_ver [1] os_ver platform win32_ver [1]else os_ver ''return os_type os_ver
def get_python_os_info info platform system_alias platform system platform release platform version os_type os_ver _ infoos_type os_type lower if os_type startswith 'linux' info platform linux_distribution if info[0] os_type info[0]if info[1] os_ver info[1]elif os_type startswith 'darwin' os_ver subprocess Popen ['sw_vers' '-productVersion'] stdout subprocess PIPE communicate [0] rstrip '\n' elif os_type startswith 'freebsd' os_ver os_ver partition '-' [0]os_ver os_ver partition ' ' [0]elif platform win32_ver [1] os_ver platform win32_ver [1]else os_ver ''return os_type os_ver
def mkdirs path mode if not path or os path exists path return [] head _ os path split path res mkdirs head mode os mkdir path os chmod path mode res + [path]return res
@dispatch Broadcast MongoQuery def post_compute e q scope None columns dict col 1 for qry in q query for col in qry get '$project' [] scope {'$project' toolz merge {'_id' 0} dict col 1 for col in columns }q q append scope dicts get_result q coll aggregate list q query assert len columns 1 return list pluck first columns keys dicts
def get_modules package src_directory blacklist STD_BLACKLIST modules []for directory dirnames filenames in os walk src_directory _handle_blacklist blacklist dirnames filenames if not '__init__ py' in filenames dirnames[ ] continueif directory src_directory dir_package directory[len src_directory ] replace os sep ' ' modules append package + dir_package for filename in filenames if _is_python_file filename and filename '__init__ py' src join directory filename module package + src[len src_directory -3 ] modules append module replace os sep ' ' return modules
def _predict_binary estimator X if is_regressor estimator return estimator predict X try score np ravel estimator decision_function X except AttributeError NotImplementedError score estimator predict_proba X [ 1]return score
def apns_fetch_inactive_ids certfile None with closing _apns_create_socket_to_feedback certfile as socket inactive_ids []for ts registration_id in _apns_receive_feedback socket inactive_ids append codecs encode registration_id 'hex_codec' return inactive_ids
def _gen_vol_xml vmname diskname size hypervisor **kwargs size int size * 1024 disk_info _get_image_info hypervisor vmname **kwargs context {'name' vmname 'filename' '{0} {1}' format diskname disk_info['disktype'] 'volname' diskname 'disktype' disk_info['disktype'] 'size' str size 'pool' disk_info['pool']}fn_ 'libvirt_volume jinja'try template JINJA get_template fn_ except jinja2 exceptions TemplateNotFound log error 'Couldnotloadtemplate{0}' format fn_ return ''return template render **context
def read_chunk fp chunk []lines []while 1 line fp readline if not line breakif line sep1 if lines chunk append lines breakif line sep2 if lines chunk append lines lines []else lines append line return chunk
def shrunk_covariance emp_cov shrinkage 0 1 emp_cov check_array emp_cov n_features emp_cov shape[0]mu np trace emp_cov / n_features shrunk_cov 1 0 - shrinkage * emp_cov shrunk_cov flat[ n_features + 1 ] + shrinkage * mu return shrunk_cov
def create_var size var_id None if var_id is None var_id get_id return lo LinOp lo VARIABLE size [] var_id
def from_html html_code **kwargs parser TableHandler **kwargs parser feed html_code return parser tables
def memoize_stacks func cache {}@wraps func def memoizer be optree optree_key tensor_index_map index_tensor_map optree intrinsic_key_maps optree_key optree_key id be if optree_key in cache stacks cached_tensor_index_map cache[optree_key]for stack in stacks for i in range len stack if isinstance stack[i] Tensor if stack[i] in cached_tensor_index_map stack[i] index_tensor_map[cached_tensor_index_map[stack[i]]]cache[optree_key] stacks tensor_index_map else stacks func be optree cache[optree_key] stacks tensor_index_map return stacksreturn memoizer
def memoize_stacks func cache {}@wraps func def memoizer be optree optree_key tensor_index_map index_tensor_map optree intrinsic_key_maps optree_key optree_key id be if optree_key in cache stacks cached_tensor_index_map cache[optree_key]for stack in stacks for i in range len stack if isinstance stack[i] Tensor if stack[i] in cached_tensor_index_map stack[i] index_tensor_map[cached_tensor_index_map[stack[i]]]cache[optree_key] stacks tensor_index_map else stacks func be optree cache[optree_key] stacks tensor_index_map return stacksreturn memoizer
def memoize_stacks func cache {}@wraps func def memoizer be optree optree_key tensor_index_map index_tensor_map optree intrinsic_key_maps optree_key optree_key id be if optree_key in cache stacks cached_tensor_index_map cache[optree_key]for stack in stacks for i in range len stack if isinstance stack[i] Tensor if stack[i] in cached_tensor_index_map stack[i] index_tensor_map[cached_tensor_index_map[stack[i]]]cache[optree_key] stacks tensor_index_map else stacks func be optree cache[optree_key] stacks tensor_index_map return stacksreturn memoizer
def update_simple_binding jboss_config binding_name value profile None log debug ' MODULEFUNCTION jboss7 update_simple_binding binding_name %s value %s profile %s' binding_name value profile operation '/subsystem naming/binding "{binding_name}" write-attribute name value value "{value}" ' format binding_name binding_name value __escape_binding_value value if profile is not None operation '/profile "{profile}"' format profile profile + operation return __salt__['jboss7_cli run_operation'] jboss_config operation
def get_action_manager mod SETTINGS get 'MANAGER' 'actstream managers ActionManager' mod_path mod split ' ' try return getattr __import__ ' ' join mod_path[ -1 ] {} {} [mod_path[ -1 ]] mod_path[ -1 ] except ImportError raise ImportError 'Cannotimport%stryfixingACTSTREAM_SETTINGS[MANAGER]setting ' % mod
def get_action_manager mod SETTINGS get 'MANAGER' 'actstream managers ActionManager' mod_path mod split ' ' try return getattr __import__ ' ' join mod_path[ -1 ] {} {} [mod_path[ -1 ]] mod_path[ -1 ] except ImportError raise ImportError 'Cannotimport%stryfixingACTSTREAM_SETTINGS[MANAGER]setting ' % mod
def test_input_discard mlp MLP layers [FlattenerLayer CompositeLayer 'composite' [Linear 10 'h0' 0 1 ] {0 [0] 1 []} Softmax 5 'softmax' 0 1 ] input_space CompositeSpace [VectorSpace 15 VectorSpace 20 ] input_source 'features0' 'features1' dataset VectorSpacesDataset np random rand 20 20 astype theano config floatX np random rand 20 15 astype theano config floatX np random rand 20 5 astype theano config floatX CompositeSpace [VectorSpace 20 VectorSpace 15 VectorSpace 5 ] 'features1' 'features0' 'targets' train Train dataset mlp SGD 0 1 batch_size 5 train algorithm termination_criterion EpochCounter 1 train main_loop
def test_input_discard mlp MLP layers [FlattenerLayer CompositeLayer 'composite' [Linear 10 'h0' 0 1 ] {0 [0] 1 []} Softmax 5 'softmax' 0 1 ] input_space CompositeSpace [VectorSpace 15 VectorSpace 20 ] input_source 'features0' 'features1' dataset VectorSpacesDataset np random rand 20 20 astype theano config floatX np random rand 20 15 astype theano config floatX np random rand 20 5 astype theano config floatX CompositeSpace [VectorSpace 20 VectorSpace 15 VectorSpace 5 ] 'features1' 'features0' 'targets' train Train dataset mlp SGD 0 1 batch_size 5 train algorithm termination_criterion EpochCounter 1 train main_loop
def get_or_set_hash name length 8 chars 'abcdefghijklmnopqrstuvwxyz0123456789 @#$%^&* -_ + ' ret get name None if ret is None val '' join [random SystemRandom choice chars for _ in range length ] if DEFAULT_TARGET_DELIM in name root rest name split DEFAULT_TARGET_DELIM 1 curr get root _infinitedict val _dict_from_path rest val curr update val setval root curr else setval name val return get name
def domains_for_certname config certname def update_domains_for_name_match candidate_lineage rv 'Returndomainsifcertnamematches elsereturnrv\n'matching_domains rvif candidate_lineage lineagename certname matching_domains candidate_lineage names return matching_domainsreturn _search_lineages config update_domains_for_name_match None
def get_decoder t return _decoders[_aliases get t t ]
def download name if not update_available name raise SaltInvocationError 'Updatenotavailable {0}' format name if name in list_downloads return Truecmd ['softwareupdate' '--download' name]salt utils mac_utils execute_return_success cmd return name in list_downloads
def get_os_sslcertfile l get_os_sslcertfile_searchpath if l None return Nonefor f in l assert type f type '' if os path exists f and os path isfile f or os path islink f return freturn None
def yaml_dquote text with io StringIO as ostream yemitter yaml emitter Emitter ostream width six MAXSIZE yemitter write_double_quoted six text_type text return ostream getvalue
def yaml_dquote text with io StringIO as ostream yemitter yaml emitter Emitter ostream width six MAXSIZE yemitter write_double_quoted six text_type text return ostream getvalue
def yaml_dquote text with io StringIO as ostream yemitter yaml emitter Emitter ostream width six MAXSIZE yemitter write_double_quoted six text_type text return ostream getvalue
def constructor_float loader node value loader construct_scalar node return float value
def find_checks argument_name checks []function_type type find_checks for name function in globals iteritems if type function is function_type args inspect getargspec function [0]if len args > 1 and args[0] startswith argument_name checks append name function args checks sort return checks
def client *args **kwargs return _get_default_session client *args **kwargs
def _toOPM plate d dict plate qualifiers items d[_csvData] {}d[_csvData][_plate] plate idd[_measurements] {}d[_measurements][_hour] []times set for wid w in plate _wells items d[_measurements][wid] []for hour in w _signals times add hour for hour in sorted times d[_measurements][_hour] append hour for wid w in plate _wells items if hour in w _signals d[_measurements][wid] append w[hour] else d[_measurements][wid] append float 'nan' return d
def quote_unix value value six moves shlex_quote value return value
def generate_random_string length None entropy None pool ALPHANUMERIC pool list set pool if length and entropy raise ValueError 'Uselengthorentropy notboth ' if length < 0 and entropy < 0 raise ValueError 'Lengthorentropymustbegreaterthan0 ' if entropy log_of_2 0 6931471805599453length long math ceil log_of_2 / math log len pool * entropy return '' join _rng choice pool for _ in xrange length
def create_index table_name session *column_names index_name u'_' join [u'ix' table_name] + list column_names table table_schema table_name session columns [getattr table c column for column in column_names]try Index index_name *columns create bind session bind except OperationalError log debug u'Errorcreatingindex ' exc_info True
def create_index table_name session *column_names index_name u'_' join [u'ix' table_name] + list column_names table table_schema table_name session columns [getattr table c column for column in column_names]try Index index_name *columns create bind session bind except OperationalError log debug u'Errorcreatingindex ' exc_info True
def split_policy_string policy_string if '-' in policy_string base policy_index policy_string rsplit '-' 1 else base policy_index policy_string None policy POLICIES get_by_index policy_index if get_policy_string base policy policy_string raise PolicyError 'Unknownpolicy' index policy_index return base policy
def _convert_input x y z None n_models 1 model_set_axis 0 x np asarray x dtype np float y np asarray y dtype np float if z is not None z np asarray z dtype np float if n_models > 1 if z is None if y shape[model_set_axis] n_models raise ValueError u'Numberofdatasets yarrayisexpectedtoequalthenumberofparametersets ' y np rollaxis y model_set_axis y ndim else z_shape z shape[ model_set_axis] + z shape[ model_set_axis + 1 ] if not x shape y shape z_shape raise ValueError u'x yandzshouldhavethesameshape' if z is None farg x y else farg x y z return farg
def gaussian_random_partition_graph n s v p_in p_out directed False seed None if s > n raise nx NetworkXError 'smustbe< n' assigned 0sizes []while True size int random normalvariate s float s / v + 0 5 if size < 1 continueif assigned + size > n sizes append n - assigned breakassigned + sizesizes append size return random_partition_graph sizes p_in p_out directed seed
def readwav file wav _wave open file rate wav getframerate nchannels wav getnchannels sampwidth wav getsampwidth nframes wav getnframes data wav readframes nframes wav close array _wav2array nchannels sampwidth data return rate sampwidth array
def info pretty False best False return _distro info pretty best
def read handle format seq_count None alphabet None iterator parse handle format seq_count alphabet try first next iterator except StopIteration first Noneif first is None raise ValueError 'Norecordsfoundinhandle' try second next iterator except StopIteration second Noneif second is not None raise ValueError 'Morethanonerecordfoundinhandle' if seq_count assert len first seq_count return first
def change_node_state deployer desired_configuration state_persister InMemoryStatePersister def converge d deployer discover_state DeploymentState nodes {NodeState hostname deployer hostname uuid deployer node_uuid applications [] manifestations {} paths {} devices {} } persistent_state state_persister get_state def got_changes local_state changes local_state shared_state_changes cluster_state DeploymentState for change in changes cluster_state change update_cluster_state cluster_state return deployer calculate_changes desired_configuration cluster_state local_state d addCallback got_changes d addCallback lambda change run_state_change change deployer deployer state_persister state_persister return dresult converge result addCallback lambda _ converge result addCallback lambda _ converge return result
def preBuildPage page context data return context data
@pytest fixturedef user default_groups user User username 'test_normal' email 'test_normal@example org' password 'test' primary_group default_groups[3] activated True user save return user
def break_long_words value max_word_length 8 tagless django utils html strip_tags value re_capitalized_word re compile ' [A-Z][a-z][a-z]+ ' re UNICODE words re_capitalized_word split tagless re_too_many_letters_in_a_row re compile ' [\\w]{%d} [ \\_^/] ' % max_word_length re UNICODE broken_words []for word in words if word broken_words + re_too_many_letters_in_a_row split word broken_words filter lambda x x broken_words return u'\u200b' join broken_words
def break_long_words value max_word_length 8 tagless django utils html strip_tags value re_capitalized_word re compile ' [A-Z][a-z][a-z]+ ' re UNICODE words re_capitalized_word split tagless re_too_many_letters_in_a_row re compile ' [\\w]{%d} [ \\_^/] ' % max_word_length re UNICODE broken_words []for word in words if word broken_words + re_too_many_letters_in_a_row split word broken_words filter lambda x x broken_words return u'\u200b' join broken_words
def get_suffixes return [_PY_SOURCE_SUFFIX]
@manager commanddef worker print u'StartingSQLCeleryworker ' if config get u'CELERY_CONFIG' print u'Celerybrokerurl ' print config get u'CELERY_CONFIG' BROKER_URL application celery current_app _get_current_object c_worker celery_worker worker app application options {u'broker' config get u'CELERY_CONFIG' BROKER_URL u'loglevel' u'INFO' u'traceback' True}c_worker run **options
def check_feature_enabled feature_name def _check_feature_enabled view_func def _decorator request *args **kwargs if not settings FEATURES get feature_name False return HttpResponseBadRequest return view_func request *args **kwargs return wraps view_func _decorator return _check_feature_enabled
def find_stim_steps raw pad_start None pad_stop None merge 0 stim_channel None stim_channel _get_stim_channel stim_channel raw info picks pick_channels raw info['ch_names'] include stim_channel if len picks 0 raise ValueError 'Nostimchannelfoundtoextracteventtriggers ' data _ raw[picks ]if np any data < 0 warn 'Triggerchannelcontainsnegativevalues usingabsolutevalue ' data np abs data data data astype np int return _find_stim_steps data raw first_samp pad_start pad_start pad_stop pad_stop merge merge
def simple_class_factory model attrs return model
def skip_because *args **kwargs def decorator f @functools wraps f def wrapper self *func_args **func_kwargs skip Falseif 'condition' in kwargs if kwargs['condition'] is True skip Trueelse skip Trueif 'bug' in kwargs and skip is True if not kwargs['bug'] isdigit raise ValueError 'bugmustbeavalidbugnumber' msg 'SkippeduntilBug %sisresolved ' % kwargs['bug'] raise testtools TestCase skipException msg return f self *func_args **func_kwargs return wrapperreturn decorator
def skip_because *args **kwargs def decorator f @functools wraps f def wrapper self *func_args **func_kwargs skip Falseif 'condition' in kwargs if kwargs['condition'] is True skip Trueelse skip Trueif 'bug' in kwargs and skip is True if not kwargs['bug'] isdigit raise ValueError 'bugmustbeavalidbugnumber' msg 'SkippeduntilBug %sisresolved ' % kwargs['bug'] raise testtools TestCase skipException msg return f self *func_args **func_kwargs return wrapperreturn decorator
def print_all_links res rules r []for host in res host_name _apply_rules host rules print '%s esxhostname %s' % host_name host_name print '%s isesxhost 1' % host_name for vm in res[host] vm_name _apply_rules vm rules print '%s vmname %s' % vm_name vm_name print '%s isesxvm 1' % vm_name print '%s esxhost %s' % vm_name host_name return r
def _unit_file_changed name return "'systemctldaemon-reload'" in _systemctl_status name ['stdout'] lower
def _unit_file_changed name return "'systemctldaemon-reload'" in _systemctl_status name ['stdout'] lower
def key_description character ascii_code ord character if ascii_code < 32 return 'Ctrl+{ c}' format ord '@' + ascii_code else return repr character
def test_multiseries_donut chart Pie inner_radius 0 3 pretty_print True chart title 'BrowserusagebyversioninFebruary2012 in% 'chart add 'IE' [5 7 10 2 2 6 1] chart add 'Firefox' [0 6 16 8 7 4 2 2 1 2 1 1 1 1 4 3 1] chart add 'Chrome' [0 3 0 9 17 1 15 3 0 6 0 5 1 6] chart add 'Safari' [4 4 0 1] chart add 'Opera' [0 1 1 6 0 1 0 5] assert chart render
def setup_platform hass config add_devices discovery_info None if discovery_info is not None host urlparse discovery_info[1] hostnameelse host config get CONF_HOST if host is None _LOGGER error 'NoTVfoundinconfigurationfileorwithdiscovery' return Falseif host in _CONFIGURING returnmac config get CONF_MAC name config get CONF_NAME customize config get CONF_CUSTOMIZE setup_tv host mac name customize hass add_devices
def clean_directory directory if not os path exists directory returnfor entry in os listdir directory if entry startswith u' ' continuepath os path join directory entry if os path isdir path shutil rmtree path True else os unlink path
def add_node workflow name node_type parents attrs {} NodeClass NODE_TYPES[node_type]node NodeClass workflow workflow node_type node_type name name for attr in attrs setattr node attr attrs[attr] node save if parents for parent in parents name 'ok'if parent node_type 'start' or parent node_type 'join' name 'to'elif parent node_type 'fork' or parent node_type 'decision' name 'start'link Link parent parent child node name name link save if node_type 'fork' and node_type 'decision' and node_type 'join' link Link parent node child Kill objects get name 'kill' workflow workflow name 'error' link save return node
def add_node workflow name node_type parents attrs {} NodeClass NODE_TYPES[node_type]node NodeClass workflow workflow node_type node_type name name for attr in attrs setattr node attr attrs[attr] node save if parents for parent in parents name 'ok'if parent node_type 'start' or parent node_type 'join' name 'to'elif parent node_type 'fork' or parent node_type 'decision' name 'start'link Link parent parent child node name name link save if node_type 'fork' and node_type 'decision' and node_type 'join' link Link parent node child Kill objects get name 'kill' workflow workflow name 'error' link save return node
def notify_list_user_unsubscribed e target e['target']if target['screen_name'] c['original_name'] returnsource e['source']target_object [e['target_object']]created_at e['created_at']source_user cycle_color source['name'] + color_func c['NOTIFICATION']['source_nick'] '@' + source['screen_name'] notify color_func c['NOTIFICATION']['notify'] 'unsubscribedfromyourlist' date parser parse created_at clock fallback_humanize date clock color_func c['NOTIFICATION']['clock'] clock meta c['NOTIFY_FORMAT']meta source_user join meta split '#source_user' meta notify join meta split '#notify' meta clock join meta split '#clock' meta emojize meta printNicely '' printNicely meta print_list target_object noti True
def qualities quality_ids def q qid try return quality_ids index qid except ValueError return -1 return q
def generateArgList arglist args []extra []count 0for i in range len arglist elt arglist[i]if isinstance elt str args append elt elif isinstance elt tuple args append TupleArg i * 2 elt extra extend misc flatten elt count count + 1 else raise ValueError 'unexpectargumenttype ' eltreturn args + extra count
def generateArgList arglist args []extra []count 0for i in range len arglist elt arglist[i]if isinstance elt str args append elt elif isinstance elt tuple args append TupleArg i * 2 elt extra extend misc flatten elt count count + 1 else raise ValueError 'unexpectargumenttype ' eltreturn args + extra count
def generateArgList arglist args []extra []count 0for i in range len arglist elt arglist[i]if isinstance elt str args append elt elif isinstance elt tuple args append TupleArg i * 2 elt extra extend misc flatten elt count count + 1 else raise ValueError 'unexpectargumenttype ' eltreturn args + extra count
def make_xontribs_wiz md xontrib_metadata pkgs [md['packages'] get d get 'package' None {} for d in md['xontribs']]w _make_flat_wiz make_xontrib md['xontribs'] pkgs return w
def get_pointer ctypes_func return ctypes cast ctypes_func ctypes c_void_p value
def user_with_role user role return {'id' user id 'username' user username 'email' user email 'role' role}
def print_class class_name filename sys argv[0]with open filename as f data f read regex '^ class' + class_name + '\\b *? ^\\S' match re search regex data flags re MULTILINE re DOTALL if match print match group 1
def print_class class_name filename sys argv[0]with open filename as f data f read regex '^ class' + class_name + '\\b *? ^\\S' match re search regex data flags re MULTILINE re DOTALL if match print match group 1
def count_total_params layers layer_set None if layer_set is None layer_set set trainable_count 0non_trainable_count 0for layer in layers if layer in layer_set continuelayer_set add layer if isinstance layer Model Sequential t nt count_total_params layer layers layer_set trainable_count + tnon_trainable_count + ntelse trainable_count + sum [K count_params p for p in layer trainable_weights] non_trainable_count + sum [K count_params p for p in layer non_trainable_weights] return trainable_count non_trainable_count
def parse_db_config config_key 'sqlalchemy url' from ckan common import configurl config[config_key]regex ['^\\s* ?P<db_type>\\w* ' ' //' ' ?P<db_user>[^ ]* ' ' ?' ' ?P<db_pass>[^@]* ' '@' ' ?P<db_host>[^/ ]* ' ' ?' ' ?P<db_port>[^/]* ' '/' ' ?P<db_name>[\\w -]* ']db_details_match re match '' join regex url if not db_details_match raise Exception 'Couldnotextractdbdetailsfromurl %r' % url db_details db_details_match groupdict return db_details
def _create_collection committer_id collection commit_message commit_cmds collection validate strict False rights_manager create_new_collection_rights collection id committer_id model collection_models CollectionModel id collection id category collection category title collection title objective collection objective language_code collection language_code tags collection tags schema_version collection schema_version nodes [collection_node to_dict for collection_node in collection nodes] model commit committer_id commit_message commit_cmds collection version + 1create_collection_summary collection id committer_id
def test_topic_save forum user post Post content 'TestContent' topic Topic title 'TestTitle' assert forum last_post_id is None assert forum post_count 0 assert forum topic_count 0 topic save forum forum post post user user assert topic title 'TestTitle' topic title 'TestEditTitle'topic save assert topic title 'TestEditTitle' assert topic first_post_id post id assert topic last_post_id post id assert forum last_post_id post id assert forum post_count 1 assert forum topic_count 1
@functools wrapsdef process_request_body fn def wrapped *args **kwargs if cherrypy request process_request_body is not False fn *args **kwargs return wrapped
def get_reader data_format data_class readers [ fmt cls for fmt cls in _readers if fmt data_format ]for reader_format reader_class in readers if _is_best_match data_class reader_class readers return _readers[ reader_format reader_class ]else format_table_str _get_format_table_str data_class u'Read' raise IORegistryError u"Noreaderdefinedforformat'{0}'andclass'{1}' \nTheavailableformatsare \n{2}" format data_format data_class __name__ format_table_str
def for_float_dtypes name 'dtype' no_float16 False if no_float16 return for_dtypes _regular_float_dtypes name name else return for_dtypes _float_dtypes name name
def splitext p tail head split p if '/' in head q len head - string rfind head '/' return p[ - q ] p[ - q ] return p ''
def add_discussion page menu_index 0 page wait_for_component_menu click_css page 'button>span large-discussion-icon' menu_index
def generate_user email username slugify email user _ UserProfile objects get_or_create email email defaults {'username' username} return user
def filter_match_kwargs kwargs children False kwargs kwargs copy for key in 'pattern' 'start' 'end' 'parent' 'formatter' 'value' if key in kwargs del kwargs[key]if children for key in 'name' if key in kwargs del kwargs[key]return kwargs
def find_repeats arr return RepeatedResults *_find_repeats np array arr dtype np float64
def bugreport app import billiardimport celeryimport kombutry conn app connection driver_v u'{0} {1}' format conn transport driver_name conn transport driver_version transport conn transport_clsexcept Exception transport driver_v u''return BUGREPORT_INFO format system _platform system arch u' ' join x for x in _platform architecture if x py_i pyimplementation celery_v celery VERSION_BANNER kombu_v kombu __version__ billiard_v billiard __version__ py_v _platform python_version driver_v driver_v transport transport results maybe_sanitize_url app conf result_backend or u'disabled' human_settings app conf humanize loader qualname app loader __class__
def showInfo text parent False help '' type 'info' title 'Anki' if parent is False parent aqt mw app activeWindow or aqt mw if type 'warning' icon QMessageBox Warningelif type 'critical' icon QMessageBox Criticalelse icon QMessageBox Informationmb QMessageBox parent mb setText text mb setIcon icon mb setWindowModality Qt WindowModal mb setWindowTitle title b mb addButton QMessageBox Ok b setDefault True if help b mb addButton QMessageBox Help b clicked connect lambda openHelp help b setAutoDefault False return mb exec_
def install reactor WxReactor from twisted internet main import installReactorinstallReactor reactor return reactor
def install reactor WxReactor from twisted internet main import installReactorinstallReactor reactor return reactor
def _color_text text color color_mapping {u'black' u'0 30' u'red' u'0 31' u'green' u'0 32' u'brown' u'0 33' u'blue' u'0 34' u'magenta' u'0 35' u'cyan' u'0 36' u'lightgrey' u'0 37' u'default' u'0 39' u'darkgrey' u'1 30' u'lightred' u'1 31' u'lightgreen' u'1 32' u'yellow' u'1 33' u'lightblue' u'1 34' u'lightmagenta' u'1 35' u'lightcyan' u'1 36' u'white' u'1 37'}if sys platform u'win32' and OutStream is None return textcolor_code color_mapping get color u'0 39' return u'\x1b[{0}m{1}\x1b[0m' format color_code text
def _color_text text color color_mapping {u'black' u'0 30' u'red' u'0 31' u'green' u'0 32' u'brown' u'0 33' u'blue' u'0 34' u'magenta' u'0 35' u'cyan' u'0 36' u'lightgrey' u'0 37' u'default' u'0 39' u'darkgrey' u'1 30' u'lightred' u'1 31' u'lightgreen' u'1 32' u'yellow' u'1 33' u'lightblue' u'1 34' u'lightmagenta' u'1 35' u'lightcyan' u'1 36' u'white' u'1 37'}if sys platform u'win32' and OutStream is None return textcolor_code color_mapping get color u'0 39' return u'\x1b[{0}m{1}\x1b[0m' format color_code text
def _color_text text color color_mapping {u'black' u'0 30' u'red' u'0 31' u'green' u'0 32' u'brown' u'0 33' u'blue' u'0 34' u'magenta' u'0 35' u'cyan' u'0 36' u'lightgrey' u'0 37' u'default' u'0 39' u'darkgrey' u'1 30' u'lightred' u'1 31' u'lightgreen' u'1 32' u'yellow' u'1 33' u'lightblue' u'1 34' u'lightmagenta' u'1 35' u'lightcyan' u'1 36' u'white' u'1 37'}if sys platform u'win32' and OutStream is None return textcolor_code color_mapping get color u'0 39' return u'\x1b[{0}m{1}\x1b[0m' format color_code text
def _color_text text color color_mapping {u'black' u'0 30' u'red' u'0 31' u'green' u'0 32' u'brown' u'0 33' u'blue' u'0 34' u'magenta' u'0 35' u'cyan' u'0 36' u'lightgrey' u'0 37' u'default' u'0 39' u'darkgrey' u'1 30' u'lightred' u'1 31' u'lightgreen' u'1 32' u'yellow' u'1 33' u'lightblue' u'1 34' u'lightmagenta' u'1 35' u'lightcyan' u'1 36' u'white' u'1 37'}if sys platform u'win32' and OutStream is None return textcolor_code color_mapping get color u'0 39' return u'\x1b[{0}m{1}\x1b[0m' format color_code text
def _color_text text color color_mapping {u'black' u'0 30' u'red' u'0 31' u'green' u'0 32' u'brown' u'0 33' u'blue' u'0 34' u'magenta' u'0 35' u'cyan' u'0 36' u'lightgrey' u'0 37' u'default' u'0 39' u'darkgrey' u'1 30' u'lightred' u'1 31' u'lightgreen' u'1 32' u'yellow' u'1 33' u'lightblue' u'1 34' u'lightmagenta' u'1 35' u'lightcyan' u'1 36' u'white' u'1 37'}if sys platform u'win32' and OutStream is None return textcolor_code color_mapping get color u'0 39' return u'\x1b[{0}m{1}\x1b[0m' format color_code text
def _color_text text color color_mapping {u'black' u'0 30' u'red' u'0 31' u'green' u'0 32' u'brown' u'0 33' u'blue' u'0 34' u'magenta' u'0 35' u'cyan' u'0 36' u'lightgrey' u'0 37' u'default' u'0 39' u'darkgrey' u'1 30' u'lightred' u'1 31' u'lightgreen' u'1 32' u'yellow' u'1 33' u'lightblue' u'1 34' u'lightmagenta' u'1 35' u'lightcyan' u'1 36' u'white' u'1 37'}if sys platform u'win32' and OutStream is None return textcolor_code color_mapping get color u'0 39' return u'\x1b[{0}m{1}\x1b[0m' format color_code text
def isLoopListIntersecting loops for loopIndex in xrange len loops - 1 loop loops[loopIndex]if isLoopIntersectingLoops loop loops[ loopIndex + 1 ] return Truereturn False
def isLoopListIntersecting loops for loopIndex in xrange len loops - 1 loop loops[loopIndex]if isLoopIntersectingLoops loop loops[ loopIndex + 1 ] return Truereturn False
def update_key_description key_id description region None key None keyid None profile None conn _get_conn region region key key keyid keyid profile profile r {}try conn update_key_description key_id description r['result'] Trueexcept boto exception BotoServerError as e r['result'] Falser['error'] __utils__['boto get_error'] e return r
def read *parts with codecs open os path join HERE *parts 'r' as f return f read
def read *parts with codecs open os path join HERE *parts 'r' as f return f read
def get key default '' if not isinstance key six string_types log debug "{0} 'key'argumentisnotastringtype '{1}'" format __name__ key return Falsereturn os environ get key default
def get_session global _SESSIONif tf get_default_session is not None session tf get_default_session else if _SESSION is None if not os environ get 'OMP_NUM_THREADS' config tf ConfigProto allow_soft_placement True else nb_thread int os environ get 'OMP_NUM_THREADS' config tf ConfigProto intra_op_parallelism_threads nb_thread allow_soft_placement True _SESSION tf Session config config session _SESSIONif not _MANUAL_VAR_INIT _initialize_variables return session
def __virtual__ if HAS_JUNOS and 'proxy' in __opts__ return __virtualname__else return False 'Thejunosmodulecouldnotbeloaded junos-ezncorjxmleaseorproxycouldnotbeloaded '
@given 'abehavemodelwith' def step_given_a_behave_model_with_table context assert context table 'REQUIRE context table'context table require_columns BehaveModelBuilder REQUIRED_COLUMNS model_builder BehaveModelBuilder context behave_model model_builder build_model_from_table context table
def generate_tasks **kwargs tasks _local_tasks for fun args in tasks fun *args tasks clear
def _exec_query_ps cmd fields if not WINDOWS returnps sp Popen [conf prog powershell] + cmd + [' ' 'select%s' % ' ' join fields ' ' 'fl'] stdout sp PIPE universal_newlines True l []for line in ps stdout if not line strip continuesl line split ' ' 1 if len sl 1 l[ -1 ] + sl[0] strip continueelse l append sl[1] strip if len l len fields yield l l []
def clean_tables for f in TABLES if os path isfile f os remove f print u'Removed' + f
def _apply_if_callable maybe_callable obj **kwargs if callable maybe_callable return maybe_callable obj **kwargs return maybe_callable
def _apply_if_callable maybe_callable obj **kwargs if callable maybe_callable return maybe_callable obj **kwargs return maybe_callable
def serve config_file None dev_addr None strict None theme None theme_dir None livereload u'livereload' tempdir tempfile mkdtemp def builder log info u'Buildingdocumentation ' config load_config config_file config_file dev_addr dev_addr strict strict theme theme theme_dir theme_dir config[u'site_dir'] tempdirlive_server livereload in [u'dirty' u'livereload'] dirty livereload u'dirty' build config live_server live_server dirty dirty return configconfig builder host port config[u'dev_addr'] split u' ' 1 try if livereload in [u'livereload' u'dirty'] _livereload host port config builder tempdir else _static_server host port tempdir finally shutil rmtree tempdir
def serve config_file None dev_addr None strict None theme None theme_dir None livereload u'livereload' tempdir tempfile mkdtemp def builder log info u'Buildingdocumentation ' config load_config config_file config_file dev_addr dev_addr strict strict theme theme theme_dir theme_dir config[u'site_dir'] tempdirlive_server livereload in [u'dirty' u'livereload'] dirty livereload u'dirty' build config live_server live_server dirty dirty return configconfig builder host port config[u'dev_addr'] split u' ' 1 try if livereload in [u'livereload' u'dirty'] _livereload host port config builder tempdir else _static_server host port tempdir finally shutil rmtree tempdir
def serve config_file None dev_addr None strict None theme None theme_dir None livereload u'livereload' tempdir tempfile mkdtemp def builder log info u'Buildingdocumentation ' config load_config config_file config_file dev_addr dev_addr strict strict theme theme theme_dir theme_dir config[u'site_dir'] tempdirlive_server livereload in [u'dirty' u'livereload'] dirty livereload u'dirty' build config live_server live_server dirty dirty return configconfig builder host port config[u'dev_addr'] split u' ' 1 try if livereload in [u'livereload' u'dirty'] _livereload host port config builder tempdir else _static_server host port tempdir finally shutil rmtree tempdir
def serve config_file None dev_addr None strict None theme None theme_dir None livereload u'livereload' tempdir tempfile mkdtemp def builder log info u'Buildingdocumentation ' config load_config config_file config_file dev_addr dev_addr strict strict theme theme theme_dir theme_dir config[u'site_dir'] tempdirlive_server livereload in [u'dirty' u'livereload'] dirty livereload u'dirty' build config live_server live_server dirty dirty return configconfig builder host port config[u'dev_addr'] split u' ' 1 try if livereload in [u'livereload' u'dirty'] _livereload host port config builder tempdir else _static_server host port tempdir finally shutil rmtree tempdir
def validate_twitter username if username username re sub 'https? // www\\ ?twitter\\ com/ @' '' username if len username > 15 raise ValidationError _ 'Twitterusernamescannotbelongerthan15characters ' if not re match '^\\w+$' username raise ValidationError _ 'Twitterusernamesmustcontainonlyalphanumericcharactersandtheunderscore ' return username
def validate_twitter username if username username re sub 'https? // www\\ ?twitter\\ com/ @' '' username if len username > 15 raise ValidationError _ 'Twitterusernamescannotbelongerthan15characters ' if not re match '^\\w+$' username raise ValidationError _ 'Twitterusernamesmustcontainonlyalphanumericcharactersandtheunderscore ' return username
def create_template_register return django template Library
def create_template_register return django template Library
def register_jitable *args **kwargs def wrap fn @overload fn jit_options kwargs def ov_wrap *args **kwargs return fnreturn fnif kwargs return wrapelse return wrap *args
def get_service hass config discovery_info None filename config[CONF_FILENAME]timestamp config[CONF_TIMESTAMP]return FileNotificationService hass filename timestamp
def getZComponentCrossProduct vec3First vec3Second return vec3First x * vec3Second y - vec3First y * vec3Second x
@taskdef remove_incomplete_accounts days INCOMPLETE_ACC_MAX_DAYS from mozillians users models import UserProfilenow datetime now - timedelta days days UserProfile objects filter full_name '' filter user__date_joined__lt now delete
def mount dmg temp_dir __salt__['temp dir'] prefix 'dmg-' cmd 'hdiutilattach-readonly-nobrowse-mountpoint{0}"{1}"' format temp_dir dmg return __salt__['cmd run'] cmd temp_dir
@njitdef _repeat_1d x K out N x shape[0]L out shape[0] // K * N for n in range N val x[n]for k in range K for l in range L ind k * N * L + n * L + l out[ind] val
def upgrade migrate_engine meta MetaData meta bind migrate_engineTable 'volume_attachment' meta autoload True attachment_specs Table 'attachment_specs' meta Column 'created_at' DateTime timezone False Column 'updated_at' DateTime timezone False Column 'deleted_at' DateTime timezone False Column 'deleted' Boolean default False Column 'id' Integer primary_key True nullable False Column 'attachment_id' String 36 ForeignKey 'volume_attachment id' nullable False Column 'key' String 255 Column 'value' String 255 mysql_engine 'InnoDB' mysql_charset 'utf8' attachment_specs create
@verbosedef data_path url path None force_update False update_path None verbose None key 'MNE_DATASETS_EEGBCI_PATH'name 'EEGBCI'path _get_path path key name destination _url_to_local_path url op join path 'MNE-eegbci-data' destinations [destination]if not op isfile destination or force_update if op isfile destination os remove destination if not op isdir op dirname destination os makedirs op dirname destination _fetch_file url destination print_destination False _do_path_update path update_path key name return destinations
@require_POST@login_requireddef get_coupon_info request course_id coupon_id request POST get 'id' None if not coupon_id return JsonResponse {'message' _ 'couponidnotfound' } status 400 try coupon Coupon objects get id coupon_id except ObjectDoesNotExist return JsonResponse {'message' _ 'couponwiththecouponid {coupon_id} DoesNotExist' format coupon_id coupon_id } status 400 if not coupon is_active return JsonResponse {'message' _ 'couponwiththecouponid {coupon_id} isalreadyinactive' format coupon_id coupon_id } status 400 expiry_date coupon display_expiry_datereturn JsonResponse {'coupon_code' coupon code 'coupon_description' coupon description 'coupon_course_id' coupon course_id to_deprecated_string 'coupon_discount' coupon percentage_discount 'expiry_date' expiry_date 'message' _ 'couponwiththecouponid {coupon_id} updatedsuccessfully' format coupon_id coupon_id }
def diffsets_with_comments review current_pair if not review returndiffsets DiffSet objects filter files__comments__review review diffsets diffsets filter files__comments__interfilediff__isnull True diffsets diffsets distinct for diffset in diffsets yield {u'diffset' diffset u'is_current' current_pair[0] diffset and current_pair[1] is None }
def binskim sample signature _check_challenge signature if len re findall ' [a-f\\d]{32} ' sample 0 return 'WrongInput 'binskim_path config['binskim']['file_x64']command 'analyze'path config['MobSF']['samples'] + sample output_p '-o'output_d config['MobSF']['samples'] + sample + '_binskim' policy_p '--config'policy_d 'default'params [binskim_path command path output_p output_d policy_p policy_d]pipe subprocess Popen subprocess list2cmdline params pipe wait out_file open output_d return out_file read
def _mean_image_subtraction image means if image get_shape ndims 3 raise ValueError 'Inputmustbeofsize[height width C>0]' num_channels image get_shape as_list [ -1 ]if len means num_channels raise ValueError 'len means mustmatchthenumberofchannels' channels tf split 2 num_channels image for i in range num_channels channels[i] - means[i]return tf concat 2 channels
def webapi_token_saved_cb instance created **kwargs if created op u'created'else op u'updated'mail_webapi_token instance op
def interval *args def add_attribute function if not hasattr function u'interval' function interval []for arg in args function interval append arg return functionreturn add_attribute
def interval *args def add_attribute function if not hasattr function u'interval' function interval []for arg in args function interval append arg return functionreturn add_attribute
def autofill_unprovided_options options package_name get_formatted_string u'packagename' options package_name options extension_name LowerCaseWithUnderscores options class_name get_formatted_string u'classname' options class_name options extension_name CamelCase if options description is None options description u'Extension%s' % options extension_name
def setup_platform hass config add_devices discovery_info None emails config get CONF_EMAIL data HaveIBeenPwnedData emails devices []for email in emails devices append HaveIBeenPwnedSensor data hass email add_devices devices for sensor in devices sensor update_nothrottle
def dfs command None *args if command return _hadoop_cmd 'dfs' command *args else return 'Error commandmustbeprovided'
def hvals key host None port None db None password None server _connect host port db password return server hvals key
def hvals key host None port None db None password None server _connect host port db password return server hvals key
def l1_regularizer weight 1 0 scope None def regularizer tensor with tf name_scope scope 'L1Regularizer' [tensor] l1_weight tf convert_to_tensor weight dtype tensor dtype base_dtype name 'weight' return tf multiply l1_weight tf reduce_sum tf abs tensor name 'value' return regularizer
def b level 1 if config DEBUG > level pdb set_trace
def get_version return ' ' join str each for each in VERSION[ 3]
def teardown_paste_factories global app_factory filter_factorydel app_factorydel filter_factory
def write_theme return theme_xml
def get_pending_servermanager vname 'CurrentRebootAttempts'key 'SOFTWARE\\Microsoft\\ServerManager'reg_ret __salt__['reg read_value'] 'HKLM' key vname if reg_ret['success'] log debug 'Foundkey %s' key try if int reg_ret['vdata'] > 0 return Trueexcept ValueError passelse log debug 'Unabletoaccesskey %s' key return False
def pause_trace return replace_trace
def group_volume_type_mapping_create context group_id volume_type_id return IMPL group_volume_type_mapping_create context group_id volume_type_id
def add_kernel_arrays_1D array_1 array_2 if array_1 size > array_2 size new_array array_1 copy center array_1 size // 2 slice_ slice center - array_2 size // 2 center + array_2 size // 2 + 1 new_array[slice_] + array_2return new_arrayelif array_2 size > array_1 size new_array array_2 copy center array_2 size // 2 slice_ slice center - array_1 size // 2 center + array_1 size // 2 + 1 new_array[slice_] + array_1return new_arrayreturn array_2 + array_1
def apply_web_specific_fixes font unhinted family_name hhea font['hhea']hhea ascent 1900hhea descent -500 os2 font['OS/2']os2 sTypoAscender 1536os2 sTypoDescender -512 os2 sTypoLineGap 102os2 usWinAscent 1946os2 usWinDescent 512apply_web_cros_common_fixes font unhinted family_name
def apply_web_specific_fixes font unhinted family_name hhea font['hhea']hhea ascent 1900hhea descent -500 os2 font['OS/2']os2 sTypoAscender 1536os2 sTypoDescender -512 os2 sTypoLineGap 102os2 usWinAscent 1946os2 usWinDescent 512apply_web_cros_common_fixes font unhinted family_name
def call context topic msg timeout None check_for_lock False if check_for_lock _check_for_lock return _get_impl call CONF context topic msg timeout
def shell_env **kw return _setenv {'shell_env' kw}
def shell_env **kw return _setenv {'shell_env' kw}
def semanage_fcontext_exists sefcontext target ftype record target option_to_file_type_str[ftype] records sefcontext get_all try return records[record]except KeyError return None
def non_translated_locals global _non_translated_localsif not _non_translated_locals locales config get 'ckan locale_order' '' split _non_translated_locals [x for x in locales if x not in get_locales ]return _non_translated_locals
def translation x 0 y 0 z 0 m np array [[1 0 0 x] [0 1 0 y] [0 0 1 z] [0 0 0 1]] dtype float return m
def is_widget_with_placeholder widget return isinstance widget TextInput Textarea PasswordInput
def make_decreasing_candle open high low close dates **kwargs decrease_x decrease_y _Candlestick open high low close dates **kwargs get_candle_decrease if 'line' in kwargs kwargs setdefault 'fillcolor' kwargs['line']['color'] else kwargs setdefault 'fillcolor' _DEFAULT_DECREASING_COLOR kwargs setdefault 'showlegend' False kwargs setdefault 'line' dict color _DEFAULT_DECREASING_COLOR kwargs setdefault 'name' 'Decreasing' candle_decr_data dict type 'box' x decrease_x y decrease_y whiskerwidth 0 boxpoints False **kwargs return [candle_decr_data]
def _to_datetime iso8601_time if iso8601_time is None return Nonereturn iso8601_to_datetime iso8601_time
def forget func *xs return Forget func *xs
def hex_to_filename path hex if getattr path 'encode' None is not None hex hex decode 'ascii' dir hex[ 2]file hex[2 ]return os path join path dir file
def create_ipsecpolicy name profile None **kwargs conn _auth profile return conn create_ipsecpolicy name **kwargs
def get_page_context page content toc meta config if config[u'site_url'] page set_canonical_url config[u'site_url'] if config[u'repo_url'] page set_edit_url config[u'repo_url'] config[u'edit_uri'] page content contentpage toc tocpage meta metaif page is_homepage or page title is None page_title Noneelse page_title page titleif page is_homepage page_description config[u'site_description']else page_description Nonereturn {u'page' page u'page_title' page_title u'page_description' page_description u'content' content u'toc' toc u'meta' meta u'canonical_url' page canonical_url u'current_page' page u'previous_page' page previous_page u'next_page' page next_page}
def get_page_context page content toc meta config if config[u'site_url'] page set_canonical_url config[u'site_url'] if config[u'repo_url'] page set_edit_url config[u'repo_url'] config[u'edit_uri'] page content contentpage toc tocpage meta metaif page is_homepage or page title is None page_title Noneelse page_title page titleif page is_homepage page_description config[u'site_description']else page_description Nonereturn {u'page' page u'page_title' page_title u'page_description' page_description u'content' content u'toc' toc u'meta' meta u'canonical_url' page canonical_url u'current_page' page u'previous_page' page previous_page u'next_page' page next_page}
def blockcut expr rowsizes colsizes rowbounds bounds rowsizes colbounds bounds colsizes return BlockMatrix [[MatrixSlice expr rowbound colbound for colbound in colbounds] for rowbound in rowbounds]
def in_words integer in_million True locale u'en_IN' if not in_million else frappe local lang integer int integer try ret num2words integer lang locale except NotImplementedError ret num2words integer lang u'en' return ret replace u'-' u''
@click command 'get-app' @click argument 'name' nargs -1 @click argument 'git-url' @click option '--branch' default None help 'branchtocheckout' def get_app git_url branch name None from bench app import get_appget_app git_url branch branch
def mountCgroups mounts quietRun 'cat/proc/mounts' cgdir '/sys/fs/cgroup'csdir cgdir + '/cpuset' if 'cgroup%s' % cgdir not in mounts and 'cgroups%s' % cgdir not in mounts raise Exception 'cgroupsnotmountedon' + cgdir if 'cpuset%s' % csdir not in mounts errRun 'mkdir-p' + csdir errRun 'mount-tcgroup-ocpusetcpuset' + csdir
@must_be_logged_indef oauth_application_detail auth **kwargs client_id kwargs get 'client_id' try record ApiOAuth2Application find_one Q 'client_id' 'eq' client_id except NoResultsFound raise HTTPError http NOT_FOUND except ValueError raise HTTPError http NOT_FOUND if record owner auth user raise HTTPError http FORBIDDEN if record is_active is False raise HTTPError http GONE app_detail_url api_v2_url 'applications/{}/' format client_id return {'app_list_url' '' 'app_detail_url' app_detail_url}
def test_xml_filters_round_trip plot Bar plot add 'A' [60 75 80 78 83 90] plot add 'B' [92 87 81 73 68 55] before plot render plot add_xml_filter lambda T T after plot render assert before after
def task_cli_pip_install venv_name 'flocker-client' package_source PackageSource url 'https //{bucket} s3 amazonaws com/{key}/Flocker-{version}-py2-none-any whl' format bucket ARCHIVE_BUCKET key 'python' version _get_wheel_version package_source return sequence [run_from_args ['virtualenv' '--python /usr/bin/python2 7' venv_name] run_from_args ['source' '{}/bin/activate' format venv_name ] run_from_args ['pip' 'install' '--upgrade' 'pip'] run_from_args ['pip' 'install' url] ]
def diskfull if cfg email_full return send T 'To %s\nFrom %s\nDate %s\nSubject SABnzbdreportsDiskFull\n\nHi \n\nSABnzbdhasstoppeddownloading becausethediskisalmostfull \nPleasemakeroomandresumeSABnzbdmanually \n\n' % cfg email_to get_string cfg email_from get_email_date cfg email_to else return ''
def get_valid_name layer_name name _clean_string layer_name proposed_name namecount 1while Layer objects filter name proposed_name exists proposed_name '%s_%d' % name count count count + 1 logger info 'Requestednamealreadyused adjustingname[%s] >[%s]' layer_name proposed_name else logger info 'Usingnameasrequested' return proposed_name
def reactivate domain_name opts salt utils namecheap get_opts 'namecheap domains reactivate' opts['DomainName'] domain_nameresponse_xml salt utils namecheap post_request opts if response_xml is None return {}domainreactivateresult response_xml getElementsByTagName 'DomainReactivateResult' [0]return salt utils namecheap xml_to_dict domainreactivateresult
def reactivate domain_name opts salt utils namecheap get_opts 'namecheap domains reactivate' opts['DomainName'] domain_nameresponse_xml salt utils namecheap post_request opts if response_xml is None return {}domainreactivateresult response_xml getElementsByTagName 'DomainReactivateResult' [0]return salt utils namecheap xml_to_dict domainreactivateresult
def _string_to_rgb color if not color startswith '#' if color lower not in _color_dict raise ValueError 'Color"%s"unknown' % color color _color_dict[color]assert color[0] '#' color color[1 ]lc len color if lc in 3 4 color '' join c + c for c in color lc len color if lc not in 6 8 raise ValueError 'Hexcolormusthaveexactlysixoreightelementsfollowingthe#sign' color np array [ int color[i i + 2 ] 16 / 255 0 for i in range 0 lc 2 ] return color
def _string_to_rgb color if not color startswith '#' if color lower not in _color_dict raise ValueError 'Color"%s"unknown' % color color _color_dict[color]assert color[0] '#' color color[1 ]lc len color if lc in 3 4 color '' join c + c for c in color lc len color if lc not in 6 8 raise ValueError 'Hexcolormusthaveexactlysixoreightelementsfollowingthe#sign' color np array [ int color[i i + 2 ] 16 / 255 0 for i in range 0 lc 2 ] return color
def get_component_by_name app name sa_session app model context currentreturn sa_session query app model Component filter app model Component table c name name first
@gen enginedef SetupAndDispatch callback EmailManager SetInstance SendGridEmailManager ThrottleUsage client db_client DBClient Instance yield gen Task Dispatch client callback
def additions_installed name reboot False upgrade_os False ret {'name' name 'changes' {} 'result' False 'comment' ''}current_state __salt__['vbox_guest additions_version'] if current_state ret['result'] Trueret['comment'] 'Systemalreadyinthecorrectstate'return retif __opts__['test'] ret['comment'] 'ThestateofVirtualBoxGuestAdditionswillbechanged 'ret['changes'] {'old' current_state 'new' True}ret['result'] Nonereturn retnew_state __salt__['vbox_guest additions_install'] reboot reboot upgrade_os upgrade_os ret['comment'] 'ThestateofVirtualBoxGuestAdditionswaschanged 'ret['changes'] {'old' current_state 'new' new_state}ret['result'] bool new_state return ret
def parse_extra_model_fields extra_model_fields fields defaultdict list for entry in extra_model_fields model_key field_name parse_field_path entry[0] field_class import_field entry[1] field_args field_kwargs entry[2 ]try field field_class *field_args **field_kwargs except TypeError as e raise ImproperlyConfigured u"TheEXTRA_MODEL_FIELDSsettingcontainsargumentsforthefield'%s'whichcouldnotbeapplied %s" % entry[1] e fields[model_key] append field_name field return fields
def dmp_to_tuple f u if not u return tuple f v u - 1 return tuple dmp_to_tuple c v for c in f
def volume_mute hass hass services call DOMAIN SERVICE_VOLUME_MUTE
def service_get_all_computes_by_hv_type context hv_type include_disabled False return IMPL service_get_all_computes_by_hv_type context hv_type include_disabled include_disabled
def path_exists path return os path exists path
def path_exists path return os path exists path
def path_exists path return os path exists path
def path_exists path return os path exists path
def path_exists path return os path exists path
def path_exists path return os path exists path
def path_exists path return os path exists path
def path_exists path return os path exists path
def path_exists path return os path exists path
def wait_for_notification page def _is_saving 'Whetherornotthenotificationiscurrentlyshowing 'return page q css ' wrapper-notification-mini is-shown' presentdef _is_saving_done 'Whetherornotthenotificationisfinishedshowing 'return page q css ' wrapper-notification-mini is-hiding' presentEmptyPromise _is_saving 'Notificationshouldhavebeenshown ' try_interval 0 1 timeout 60 fulfill EmptyPromise _is_saving_done 'Notificationshouldhavebeenhidden ' try_interval 0 1 timeout 60 fulfill
def writeFeatureFile font path fout open path 'w' fout write font features text fout close
def sleep sleep_time 0 25 time sleep sleep_time
def nsdecls *prefixes return u'' join [ u'xmlns %s "%s"' % pfx nsmap[pfx] for pfx in prefixes]
def to_unicode value if not isinstance value six string_types raise ValueError 'Value"%s"mustbeastring ' % value if not isinstance value six text_type value six u value return value
def cli *commands return __proxy__['napalm call'] 'cli' **{'commands' list commands }
def sameTPParams tp1 tp2 result Truefor param in ['numberOfCols' 'cellsPerColumn' 'initialPerm' 'connectedPerm' 'minThreshold' 'newSynapseCount' 'permanenceInc' 'permanenceDec' 'permanenceMax' 'globalDecay' 'activationThreshold' 'doPooling' 'segUpdateValidDuration' 'seed' 'burnIn' 'pamLength' 'maxAge'] if getattr tp1 param getattr tp2 param print param 'isdifferent'print getattr tp1 param 'vs' getattr tp2 param result Falsereturn result
def check_password_strength password password unicode password n math log len set password num re search '[0-9]' password is not None and re match '^[0-9]*$' password is None caps password password upper and password password lower extra re match '^[a-zA-Z0-9]*$' password is None score len password * n + caps + num + extra / 20 password_strength {0 'Weak' 1 'Medium' 2 'Strong' 3 'VeryStrong'}return password_strength[min 3 int score ]
def _update_config_from_env config env srcdir os path join os path dirname os path realpath __file__ ' ' if env in ['prod' 'staging'] base_cfg_path ['/etc/inboxapp/secrets yml' '/etc/inboxapp/config json']else v {'env' env 'srcdir' srcdir}base_cfg_path ['{srcdir}/etc/secrets-{env} yml' format **v '{srcdir}/etc/config-{env} json' format **v ]if 'SYNC_ENGINE_CFG_PATH' in os environ cfg_path os environ get 'SYNC_ENGINE_CFG_PATH' '' split os path pathsep cfg_path list p strip for p in cfg_path if p strip else cfg_path []path cfg_path + base_cfg_path for filename in reversed path try f open filename except IOError OSError as e if e errno errno ENOENT raiseelse with f config update yaml safe_load f
def SerialCorr series lag 1 xs series[lag ]ys series shift lag [lag ]corr Corr xs ys return corr
@pytest fixture autouse True def reset_cache_backend_state celery_app yield backend celery_app __dict__ get u'backend' if backend is not None if isinstance backend CacheBackend if isinstance backend client DummyClient backend client cache clear backend _cache clear
def create_image_bdm image_ref boot_index 0 return BlockDeviceDict {'source_type' 'image' 'image_id' image_ref 'delete_on_termination' True 'boot_index' boot_index 'device_type' 'disk' 'destination_type' 'local'}
def isFixedGroup kexAlgorithm return _IFixedGroupKexAlgorithm providedBy getKex kexAlgorithm
def isFixedGroup kexAlgorithm return _IFixedGroupKexAlgorithm providedBy getKex kexAlgorithm
def _delete_cookie response response set_cookie settings SESSION_COOKIE_NAME max_age 0 expires 'Thu 01-Jan-197000 00 00GMT' domain settings SESSION_COOKIE_DOMAIN secure settings SESSION_COOKIE_SECURE or None httponly settings SESSION_COOKIE_HTTPONLY or None
def _delete_cookie response response set_cookie settings SESSION_COOKIE_NAME max_age 0 expires 'Thu 01-Jan-197000 00 00GMT' domain settings SESSION_COOKIE_DOMAIN secure settings SESSION_COOKIE_SECURE or None httponly settings SESSION_COOKIE_HTTPONLY or None
def fake_service_orm **updates db_service fake_db_service **updates service models Service **db_service return service
def get_eip_address_info addresses None allocation_ids None region None key None keyid None profile None if type addresses type 'string' addresses [addresses]if type allocation_ids type 'string' allocation_ids [allocation_ids]ret _get_all_eip_addresses addresses addresses allocation_ids allocation_ids region region key key keyid keyid profile profile interesting ['allocation_id' 'association_id' 'domain' 'instance_id' 'network_interface_id' 'network_interface_owner_id' 'public_ip' 'private_ip_address']return [dict [ x getattr address x for x in interesting] for address in ret]
def write_name_list fid kind data write_string fid kind ' ' join data
def format_by_pattern numobj number_format user_defined_formats country_code numobj country_codensn national_significant_number numobj if not _has_valid_country_calling_code country_code return nsnregion_code region_code_for_country_code country_code metadata PhoneMetadata metadata_for_region_or_calling_code country_code region_code formatted_number U_EMPTY_STRINGformatting_pattern _choose_formatting_pattern_for_number user_defined_formats nsn if formatting_pattern is None formatted_number nsnelse num_format_copy _copy_number_format formatting_pattern np_formatting_rule formatting_pattern national_prefix_formatting_ruleif np_formatting_rule national_prefix metadata national_prefixif national_prefix np_formatting_rule re sub _NP_PATTERN national_prefix np_formatting_rule count 1 np_formatting_rule re sub _FG_PATTERN unicod '\\\\1' np_formatting_rule count 1 num_format_copy national_prefix_formatting_rule np_formatting_ruleelse num_format_copy national_prefix_formatting_rule Noneformatted_number _format_nsn_using_pattern nsn num_format_copy number_format formatted_number _maybe_append_formatted_extension numobj metadata number_format formatted_number formatted_number _prefix_number_with_country_calling_code country_code number_format formatted_number return formatted_number
def get_config_root *append from hadoop conf import HDFS_CLUSTERSyarn_site_path HDFS_CLUSTERS['default'] HADOOP_CONF_DIR get return os path abspath os path join yarn_site_path ' ' *append
def _mergetreejinja src dst context for item in os listdir src s os path join src item d os path join dst item if os path isdir s log info 'Copyingfolder{0}to{1}' format s d if os path exists d _mergetreejinja s d context else os mkdir d _mergetreejinja s d context elif item TEMPLATE_FILE_NAME d Template d render context log info 'Copyingfile{0}to{1}' format s d with salt utils fopen s 'r' as source_file src_contents source_file read dest_contents Template src_contents render context with salt utils fopen d 'w' as dest_file dest_file write dest_contents
def hg_hook ui repo node None **kwargs seen set paths []if len repo for rev in range repo[node] len repo for file_ in repo[rev] files file_ op join repo root file_ if file_ in seen or not op exists file_ continueseen add file_ paths append file_ options parse_options setup_logger options if paths process_paths options candidates paths
def convert_to_http_date dt if timezone is_naive dt default_timezone timezone get_default_timezone dt timezone make_aware dt default_timezone utc_dt dt astimezone timezone utc epoch_dt calendar timegm utc_dt utctimetuple return http_date epoch_dt
def is_android return sys platform startswith 'linux' and 'ANDROID_PRIVATE' in os environ
def is_android return sys platform startswith 'linux' and 'ANDROID_PRIVATE' in os environ
def is_android return sys platform startswith 'linux' and 'ANDROID_PRIVATE' in os environ
def macroexpand tree compiler load_macros compiler module_name old Nonewhile old tree old treetree macroexpand_1 tree compiler return tree
def sanitize_text text valid_characters valid_chars character_map mapped_chars invalid_character 'X' if isinstance text list return [sanitize_text x valid_characters valid_characters character_map character_map invalid_character invalid_character for x in text]if not isinstance text string_types text smart_str text return _sanitize_text_helper text valid_characters valid_characters character_map character_map
def sanitize_text text valid_characters valid_chars character_map mapped_chars invalid_character 'X' if isinstance text list return [sanitize_text x valid_characters valid_characters character_map character_map invalid_character invalid_character for x in text]if not isinstance text string_types text smart_str text return _sanitize_text_helper text valid_characters valid_characters character_map character_map
def get_request_ip_resolver module attribute get_cms_setting 'REQUEST_IP_RESOLVER' rsplit ' ' 1 try ip_resolver_module importlib import_module module ip_resolver getattr ip_resolver_module attribute except ImportError raise ImproperlyConfigured _ 'UnabletofindthespecifiedCMS_REQUEST_IP_RESOLVERmodule "{0}" ' format module except AttributeError raise ImproperlyConfigured _ 'UnabletofindthespecifiedCMS_REQUEST_IP_RESOLVERfunction "{0}"inmodule"{1}" ' format attribute module return ip_resolver
def cons el seq yield el for s in seq yield s
def Get setting_name return _style[setting_name]
def TaskMessage1 name id None args kwargs {} callbacks None errbacks None chain None **options from celery import uuidfrom kombu serialization import dumpsid id or uuid message Mock name u'TaskMessage-{0}' format id message headers {}message payload {u'task' name u'id' id u'args' args u'kwargs' kwargs u'callbacks' callbacks u'errbacks' errbacks}message payload update options message content_type message content_encoding message body dumps message payload return message
def system_data if sys platform startswith 'linux' path '/usr/share/qutebrowser'if not os path exists path path data else path data return path
def keepalived_ipv6_supported br_name ha_port gw_port common_utils get_related_rand_device_names ['ka-test-' ha_router HA_DEV_PREFIX namespaces INTERNAL_DEV_PREFIX] gw_vip 'fdf8 f53b 82e4 10/64'expected_default_gw 'fe80 f816 1'with ovs_lib OVSBridge br_name as br with KeepalivedIPv6Test ha_port gw_port gw_vip expected_default_gw as ka br add_port ha_port 'type' 'internal' br add_port gw_port 'type' 'internal' ha_dev ip_lib IPDevice ha_port gw_dev ip_lib IPDevice gw_port ha_dev link set_netns ka nsname gw_dev link set_netns ka nsname ha_dev link set_up gw_dev link set_up ka configure ka start_keepalived_process ka verify_ipv6_address_assignment gw_dev default_gw gw_dev route get_gateway ip_version 6 if default_gw default_gw default_gw['gateway']return expected_default_gw default_gw
def keepalived_ipv6_supported br_name ha_port gw_port common_utils get_related_rand_device_names ['ka-test-' ha_router HA_DEV_PREFIX namespaces INTERNAL_DEV_PREFIX] gw_vip 'fdf8 f53b 82e4 10/64'expected_default_gw 'fe80 f816 1'with ovs_lib OVSBridge br_name as br with KeepalivedIPv6Test ha_port gw_port gw_vip expected_default_gw as ka br add_port ha_port 'type' 'internal' br add_port gw_port 'type' 'internal' ha_dev ip_lib IPDevice ha_port gw_dev ip_lib IPDevice gw_port ha_dev link set_netns ka nsname gw_dev link set_netns ka nsname ha_dev link set_up gw_dev link set_up ka configure ka start_keepalived_process ka verify_ipv6_address_assignment gw_dev default_gw gw_dev route get_gateway ip_version 6 if default_gw default_gw default_gw['gateway']return expected_default_gw default_gw
@_ensure_existsdef stop name kill False if _sd_version > 219 if kill action 'terminate'else action 'poweroff'ret _machinectl '{0}{1}' format action name else cmd 'systemctlstopsystemd-nspawn@{0}' format name ret __salt__['cmd run_all'] cmd python_shell False if ret['retcode'] 0 __context__['retcode'] salt defaults exitcodes EX_UNAVAILABLEreturn Falsereturn True
def SequenceToConjunction node return ConvertNodes node QueryParser SEQUENCE QueryParser CONJUNCTION 'CONJUNCTION'
def build_real_request wsgi_environ config testing setUp settings DEFAULT_SETTINGS includeme config request pyramid_request Request wsgi_environ request registry config registryreturn request
def _strip_uri repo splits repo split for idx in range len splits if any splits[idx] startswith x for x in 'http //' 'https //' 'ftp //' splits[idx] splits[idx] rstrip '/' return '' join splits
def _strip_uri repo splits repo split for idx in range len splits if any splits[idx] startswith x for x in 'http //' 'https //' 'ftp //' splits[idx] splits[idx] rstrip '/' return '' join splits
def get_uid path follow_symlinks True if not os path exists path return Falseif follow_symlinks and sys getwindowsversion major > 6 path _resolve_symlink path try secdesc win32security GetFileSecurity path win32security OWNER_SECURITY_INFORMATION except MemoryError return 'S-1-1-0'except pywinerror as exc if exc winerror 1 or exc winerror 50 return 'S-1-1-0'raiseowner_sid secdesc GetSecurityDescriptorOwner return win32security ConvertSidToStringSid owner_sid
def _any_pandas_objects terms return any isinstance term value pd core generic PandasObject for term in terms
def test_read_hdf_multiply_open pytest importorskip 'tables' df pd DataFrame {'x' ['a' 'b' 'c' 'd'] 'y' [1 2 3 4]} index [1 0 2 0 3 0 4 0] with tmpfile 'h5' as fn df to_hdf fn '/data' format 'table' with pd HDFStore fn mode 'r' dd read_hdf fn '/data' chunksize 2 mode 'r'
def _getAvatar acc pimg 'avatars/' + acc + ' jpeg' return pimg None if os path exists pimg else '/Applications/Skype app' 'fileicon'
def _transform_index index func if isinstance index MultiIndex items [tuple func y for y in x for x in index]return MultiIndex from_tuples items names index names else items [func x for x in index]return Index items name index name
def md5_digest instr if six PY3 b salt utils to_bytes instr return hashlib md5 b hexdigest return hashlib md5 instr hexdigest
def mongo_db registry xml_parent data mongodb XML SubElement xml_parent 'org jenkinsci plugins mongodb MongoBuildWrapper' mongodb set 'plugin' 'mongodb' mapping [ 'name' 'mongodbName' None 'port' 'port' '' 'data-directory' 'dbpath' '' 'startup-params' 'parameters' '' 'start-timeout' 'startTimeout' 0 ]convert_mapping_to_xml mongodb data mapping fail_required True
def mongo_db registry xml_parent data mongodb XML SubElement xml_parent 'org jenkinsci plugins mongodb MongoBuildWrapper' mongodb set 'plugin' 'mongodb' mapping [ 'name' 'mongodbName' None 'port' 'port' '' 'data-directory' 'dbpath' '' 'startup-params' 'parameters' '' 'start-timeout' 'startTimeout' 0 ]convert_mapping_to_xml mongodb data mapping fail_required True
def test_bootstrap_random_seed data rs randn 50 seed 42boots1 algo bootstrap data random_seed seed boots2 algo bootstrap data random_seed seed assert_array_equal boots1 boots2
def test_bootstrap_random_seed data rs randn 50 seed 42boots1 algo bootstrap data random_seed seed boots2 algo bootstrap data random_seed seed assert_array_equal boots1 boots2
def test_bootstrap_random_seed data rs randn 50 seed 42boots1 algo bootstrap data random_seed seed boots2 algo bootstrap data random_seed seed assert_array_equal boots1 boots2
@lower_builtin types NumberClass types Any def number_constructor context builder sig args if isinstance sig return_type types Array impl context get_function np array sig return impl builder args else [val] args[valty] sig argsreturn context cast builder val valty sig return_type
def null_image return QImage
def get_all_params layer unwrap_shared True **tags layers get_all_layers layer params chain from_iterable l get_params unwrap_shared unwrap_shared **tags for l in layers return utils unique params
def m_quadratic_sum A B max_it 50 gamma1 solve_discrete_lyapunov A B max_it return gamma1
def _addHook registry lookup registry lookup1def _hook iface ob factory lookup declarations providedBy ob iface if factory is None return Noneelse return factory ob interface adapter_hooks append _hook return _hook
def _addHook registry lookup registry lookup1def _hook iface ob factory lookup declarations providedBy ob iface if factory is None return Noneelse return factory ob interface adapter_hooks append _hook return _hook
def _addHook registry lookup registry lookup1def _hook iface ob factory lookup declarations providedBy ob iface if factory is None return Noneelse return factory ob interface adapter_hooks append _hook return _hook
def _addHook registry lookup registry lookup1def _hook iface ob factory lookup declarations providedBy ob iface if factory is None return Noneelse return factory ob interface adapter_hooks append _hook return _hook
def serialize format queryset **options s get_serializer format s serialize queryset **options return s getvalue
def dblog msg module return insert 'INSERTINTOlogVALUES ? ? ? ' module _timestamp msg rstrip
def refactor_with_2to3 source_text fixer_names filename u'' from lib2to3 refactor import RefactoringToolfixers [ u'lib2to3 fixes fix_' + name for name in fixer_names]tool RefactoringTool fixer_names fixers explicit fixers from lib2to3 pgen2 import tokenize as lib2to3_tokenizetry return unicode tool refactor_string source_text name filename except lib2to3_tokenize TokenError return source_text
def refactor_with_2to3 source_text fixer_names filename u'' from lib2to3 refactor import RefactoringToolfixers [ u'lib2to3 fixes fix_' + name for name in fixer_names]tool RefactoringTool fixer_names fixers explicit fixers from lib2to3 pgen2 import tokenize as lib2to3_tokenizetry return unicode tool refactor_string source_text name filename except lib2to3_tokenize TokenError return source_text
@event u'task execute started' def load_task task if not SimplePersistence class_store[task name] SimplePersistence load task name
def find_node v cls ignore_clients False if v owner is not None and ignore_clients or len v clients 1 if isinstance v owner op cls return v ownerelif isinstance v owner op GpuFromHost and v owner inputs[0] owner is not None and ignore_clients or len v owner inputs[0] clients 1 and isinstance v owner inputs[0] owner op HostFromGpu return find_node v owner inputs[0] owner inputs[0] cls else return None
def find_node v cls ignore_clients False if v owner is not None and ignore_clients or len v clients 1 if isinstance v owner op cls return v ownerelif isinstance v owner op GpuFromHost and v owner inputs[0] owner is not None and ignore_clients or len v owner inputs[0] clients 1 and isinstance v owner inputs[0] owner op HostFromGpu return find_node v owner inputs[0] owner inputs[0] cls else return None
def post_upgrade name if name not in _tracker['post_upgrade'] return Falsereturn _tracker['post_upgrade'][name]
def _legalize_stage path replacements length extension fragment path sanitize_path path replacements if not fragment path bytestring_path path path + extension lower pre_truncate_path pathpath truncate_path path length return path path pre_truncate_path
def make_full_schema data schema flattened_schema flatten_schema schema key_combinations get_all_key_combinations data flattened_schema full_schema {}for combination in key_combinations sub_schema schemafor key in combination[ 2] sub_schema sub_schema[key]for key value in sub_schema iteritems if isinstance value list full_schema[ combination + key ] valuereturn full_schema
def generatePersistentJobGUID return 'JOB_UUID1-' + str uuid uuid1
def slack registry xml_parent data logger logging getLogger __name__ plugin_info registry get_plugin_info 'SlackNotificationPlugin' plugin_ver pkg_resources parse_version plugin_info get 'version' '0' if plugin_ver > pkg_resources parse_version '2 0' logger warn 'propertiessectionisnotusedwithpluginversion> 2 0' mapping 'notify-start' 'startNotification' False 'notify-success' 'notifySuccess' False 'notify-aborted' 'notifyAborted' False 'notify-not-built' 'notifyNotBuilt' False 'notify-unstable' 'notifyUnstable' False 'notify-failure' 'notifyFailure' False 'notify-back-to-normal' 'notifyBackToNormal' False 'notify-repeated-failure' 'notifyRepeatedFailure' False 'include-test-summary' 'includeTestSummary' False 'include-custom-message' 'includeCustomMessage' False 'custom-message' 'customMessage' '' 'room' 'room' '' slack XML SubElement xml_parent 'jenkins plugins slack SlackNotifier_-SlackJobProperty' if data get 'include-custom-message' False if not data get 'custom-message' '' raise MissingAttributeError 'custom-message' helpers convert_mapping_to_xml slack data mapping fail_required True
def slack registry xml_parent data logger logging getLogger __name__ plugin_info registry get_plugin_info 'SlackNotificationPlugin' plugin_ver pkg_resources parse_version plugin_info get 'version' '0' if plugin_ver > pkg_resources parse_version '2 0' logger warn 'propertiessectionisnotusedwithpluginversion> 2 0' mapping 'notify-start' 'startNotification' False 'notify-success' 'notifySuccess' False 'notify-aborted' 'notifyAborted' False 'notify-not-built' 'notifyNotBuilt' False 'notify-unstable' 'notifyUnstable' False 'notify-failure' 'notifyFailure' False 'notify-back-to-normal' 'notifyBackToNormal' False 'notify-repeated-failure' 'notifyRepeatedFailure' False 'include-test-summary' 'includeTestSummary' False 'include-custom-message' 'includeCustomMessage' False 'custom-message' 'customMessage' '' 'room' 'room' '' slack XML SubElement xml_parent 'jenkins plugins slack SlackNotifier_-SlackJobProperty' if data get 'include-custom-message' False if not data get 'custom-message' '' raise MissingAttributeError 'custom-message' helpers convert_mapping_to_xml slack data mapping fail_required True
def describe_api_deployments restApiId region None key None keyid None profile None try conn _get_conn region region key key keyid keyid profile profile deployments []_deployments conn get_deployments restApiId restApiId while True if _deployments deployments deployments + _deployments['items'] if 'position' not in _deployments break_deployments conn get_deployments restApiId restApiId position _deployments['position'] return {'deployments' [_convert_datetime_str deployment for deployment in deployments]}except ClientError as e return {'error' salt utils boto3 get_error e }
def write_metadata_tag stream metadata SCRIPT_TAG '\x12'FLV_TAG_HEADER_LEN 11if metadata stream write SCRIPT_TAG write_unsigned_int_24 stream len metadata stream write '\x00\x00\x00\x00\x00\x00\x00' stream write metadata write_unsigned_int stream FLV_TAG_HEADER_LEN + len metadata
def users_to_remove source_event source_node new_node NotificationSubscription apps get_model 'osf NotificationSubscription' removed_users {key [] for key in constants NOTIFICATION_TYPES}if source_node new_node return removed_usersold_sub NotificationSubscription load to_subscription_key source_node _id source_event old_node_sub NotificationSubscription load to_subscription_key source_node _id '_' join source_event split '_' [ -2 ] if not old_sub and not old_node_sub return removed_usersfor notification_type in constants NOTIFICATION_TYPES users list getattr old_sub notification_type values_list 'guids___id' flat True + list getattr old_node_sub notification_type values_list 'guids___id' flat True subbed removed_users[notification_type] separate_users new_node users return removed_users
def users_to_remove source_event source_node new_node NotificationSubscription apps get_model 'osf NotificationSubscription' removed_users {key [] for key in constants NOTIFICATION_TYPES}if source_node new_node return removed_usersold_sub NotificationSubscription load to_subscription_key source_node _id source_event old_node_sub NotificationSubscription load to_subscription_key source_node _id '_' join source_event split '_' [ -2 ] if not old_sub and not old_node_sub return removed_usersfor notification_type in constants NOTIFICATION_TYPES users list getattr old_sub notification_type values_list 'guids___id' flat True + list getattr old_node_sub notification_type values_list 'guids___id' flat True subbed removed_users[notification_type] separate_users new_node users return removed_users
def users_to_remove source_event source_node new_node NotificationSubscription apps get_model 'osf NotificationSubscription' removed_users {key [] for key in constants NOTIFICATION_TYPES}if source_node new_node return removed_usersold_sub NotificationSubscription load to_subscription_key source_node _id source_event old_node_sub NotificationSubscription load to_subscription_key source_node _id '_' join source_event split '_' [ -2 ] if not old_sub and not old_node_sub return removed_usersfor notification_type in constants NOTIFICATION_TYPES users list getattr old_sub notification_type values_list 'guids___id' flat True + list getattr old_node_sub notification_type values_list 'guids___id' flat True subbed removed_users[notification_type] separate_users new_node users return removed_users
def users_to_remove source_event source_node new_node NotificationSubscription apps get_model 'osf NotificationSubscription' removed_users {key [] for key in constants NOTIFICATION_TYPES}if source_node new_node return removed_usersold_sub NotificationSubscription load to_subscription_key source_node _id source_event old_node_sub NotificationSubscription load to_subscription_key source_node _id '_' join source_event split '_' [ -2 ] if not old_sub and not old_node_sub return removed_usersfor notification_type in constants NOTIFICATION_TYPES users list getattr old_sub notification_type values_list 'guids___id' flat True + list getattr old_node_sub notification_type values_list 'guids___id' flat True subbed removed_users[notification_type] separate_users new_node users return removed_users
def _GetGroupByKey entity property_names return frozenset prop name prop value SerializeToString for prop in entity property_list if prop name in property_names
def _GetGroupByKey entity property_names return frozenset prop name prop value SerializeToString for prop in entity property_list if prop name in property_names
def test_predefined_string_roundtrip with u magnitude_zero_points enable assert u Unit u STmag to_string u STmag assert u Unit u ABmag to_string u ABmag assert u Unit u M_bol to_string u M_bol assert u Unit u m_bol to_string u m_bol
def split_de_casteljau beta t beta np asarray beta beta_list [beta]while True beta _de_casteljau1 beta t beta_list append beta if len beta 1 breakleft_beta [beta[0] for beta in beta_list]right_beta [beta[ -1 ] for beta in reversed beta_list ]return left_beta right_beta
def split_de_casteljau beta t beta np asarray beta beta_list [beta]while True beta _de_casteljau1 beta t beta_list append beta if len beta 1 breakleft_beta [beta[0] for beta in beta_list]right_beta [beta[ -1 ] for beta in reversed beta_list ]return left_beta right_beta
def compare_partial_dicts source comparee for key value in six iteritems source if key not in comparee or value comparee[key] return Falsereturn True
def jsonFileLogObserver outFile recordSeparator u'\x1e' return FileLogObserver outFile lambda event u'{0}{1}\n' format recordSeparator eventAsJSON event
def update cwd rev force False user None cmd ['hg' 'update' '{0}' format rev ]if force cmd append '-C' ret __salt__['cmd run_all'] cmd cwd cwd runas user python_shell False if ret['retcode'] 0 raise CommandExecutionError 'Hgcommandfailed {0}' format ret get 'stderr' ret['stdout'] return ret['stdout']
def computed_values d *args **kwargs result {}for k v in six iteritems d if callable v v v *args **kwargs if isinstance v dict v computed_values v *args **kwargs result[k] vreturn result
def intersect_chunks old_chunks new_chunks cmo cumdims_label old_chunks 'o' cmn cumdims_label new_chunks 'n' sums [sum o for o in old_chunks]sums2 [sum n for n in old_chunks]if not sums sums2 raise ValueError 'Cannotchangedimensionsfromto%r' % sums2 old_to_new [_intersect_1d _breakpoints cm[0] cm[1] for cm in zip cmo cmn ]cross1 product *old_to_new cross chain tuple product *cr for cr in cross1 return cross
def run_script pycode if pycode[0] '\n' pycode pycode[1 ]pycode rstrip pycode textwrap dedent pycode globs {}exec pycode in globs globsreturn globs
@pytest mark parametrize u'testframe' totest_frames def test_gcrs_icrs_moonish testframe moonish GCRS MOONDIST_CART obstime testframe obstime moonicrs moonish transform_to ICRS assert 0 97 * u au < moonicrs distance < 1 03 * u au
def prep_course_for_grading course request course _field_data_cache {}course set_grading_policy course grading_policy
def prep_course_for_grading course request course _field_data_cache {}course set_grading_policy course grading_policy
def _convert_nnn val word '' mod rem val % 100 val // 100 if rem > 0 word to_19[rem] + 'Hundred' if mod > 0 word + ''if mod > 0 word + _convert_nn mod return word
@staff_member_requireddef report request context admin_context request context[u'subprojects'] SubProject objects all return render request u'admin/report html' context
def _has_unicode_fields array dtypes d[0] for d in array dtype fields values return any d kind 'U' for d in dtypes
def _has_unicode_fields array dtypes d[0] for d in array dtype fields values return any d kind 'U' for d in dtypes
def test_convert_np_array mixin_cols t QTable mixin_cols ta t as_array m mixin_cols['m']dtype_kind m dtype kind if hasattr m 'dtype' else 'O' assert ta['m'] dtype kind dtype_kind
def test_convert_np_array mixin_cols t QTable mixin_cols ta t as_array m mixin_cols['m']dtype_kind m dtype kind if hasattr m 'dtype' else 'O' assert ta['m'] dtype kind dtype_kind
def test_convert_np_array mixin_cols t QTable mixin_cols ta t as_array m mixin_cols['m']dtype_kind m dtype kind if hasattr m 'dtype' else 'O' assert ta['m'] dtype kind dtype_kind
def test_convert_np_array mixin_cols t QTable mixin_cols ta t as_array m mixin_cols['m']dtype_kind m dtype kind if hasattr m 'dtype' else 'O' assert ta['m'] dtype kind dtype_kind
def mutESLogNormal individual c indpb size len individual t c / math sqrt 2 0 * math sqrt size t0 c / math sqrt 2 0 * size n random gauss 0 1 t0_n t0 * n for indx in xrange size if random random < indpb individual strategy[indx] * math exp t0_n + t * random gauss 0 1 individual[indx] + individual strategy[indx] * random gauss 0 1 return individual
def mutESLogNormal individual c indpb size len individual t c / math sqrt 2 0 * math sqrt size t0 c / math sqrt 2 0 * size n random gauss 0 1 t0_n t0 * n for indx in xrange size if random random < indpb individual strategy[indx] * math exp t0_n + t * random gauss 0 1 individual[indx] + individual strategy[indx] * random gauss 0 1 return individual
def _check_pillar kwargs if kwargs get 'force' return Trueif '_errors' in __pillar__ return Falsereturn True
def get_discovery_srid36s srs Subreddit _by_name g live_config['discovery_srs'] return [sr _id36 for sr in srs itervalues ]
def get_discovery_srid36s srs Subreddit _by_name g live_config['discovery_srs'] return [sr _id36 for sr in srs itervalues ]
def get_discovery_srid36s srs Subreddit _by_name g live_config['discovery_srs'] return [sr _id36 for sr in srs itervalues ]
def get_discovery_srid36s srs Subreddit _by_name g live_config['discovery_srs'] return [sr _id36 for sr in srs itervalues ]
def ignore_nanwarnings return WarningContext 'ignore' {'category' RuntimeWarning 'module' 'numpy lib nanfunctions'}
def ignore_nanwarnings return WarningContext 'ignore' {'category' RuntimeWarning 'module' 'numpy lib nanfunctions'}
@click group def cli pass
@click group def cli pass
def b0_indices in_bval max_b 10 0 import numpy as npbval np loadtxt in_bval return np argwhere bval < max_b flatten tolist
def clear_layers_name set_keep['_layers_name_list'] []
def clear_layers_name set_keep['_layers_name_list'] []
def bfs_edges G source reverse False if reverse and G is_directed successors G predecessorselse successors G neighborsfor e in generic_bfs_edges G source successors yield e
def bfs_edges G source reverse False if reverse and G is_directed successors G predecessorselse successors G neighborsfor e in generic_bfs_edges G source successors yield e
@printing_funcdef select_package_address jvm_package_name address_families addresses [address for address_family in address_families for address in address_family addressables keys ]if len addresses 0 raise ValueError u'Notargetsexistedin{}toprovide{}' format address_families jvm_package_name elif len addresses > 1 raise ValueError u'Multipletargetsmightbeabletoprovide{} \n{}' format jvm_package_name u'\n' join str a for a in addresses return addresses[0]
@printing_funcdef select_package_address jvm_package_name address_families addresses [address for address_family in address_families for address in address_family addressables keys ]if len addresses 0 raise ValueError u'Notargetsexistedin{}toprovide{}' format address_families jvm_package_name elif len addresses > 1 raise ValueError u'Multipletargetsmightbeabletoprovide{} \n{}' format jvm_package_name u'\n' join str a for a in addresses return addresses[0]
def populate_entry_fields entry parser config entry[u'series_parser'] copy parser entry[u'series_name'] parser nameif u'quality' in entry and entry[u'quality'] parser quality log verbose u'Founddifferentqualityfor%s Was%s overridingwith%s ' % entry[u'title'] entry[u'quality'] parser quality entry[u'quality'] parser qualityentry[u'proper'] parser properentry[u'proper_count'] parser proper_countentry[u'release_group'] parser groupif parser id_type u'ep' entry[u'series_season'] parser seasonentry[u'series_episode'] parser episodeelif parser id_type u'date' entry[u'series_date'] parser identry[u'series_season'] parser id yearelse entry[u'series_season'] time gmtime tm_yearentry[u'series_episodes'] parser episodesentry[u'series_id'] parser pack_identifierentry[u'series_id_type'] parser id_typeif config if u'path' in config log debug u'setting%scustompathto%s' entry[u'title'] config get u'path' config setdefault u'set' {} update path config[u'path'] if u'set' in config set plugin get_plugin_by_name u'set' set instance modify entry config get u'set'
def get_module_path module *joins module scrub module return get_pymodule_path local module_app[module] + u' ' + module *joins
def groupsstats_1d y x labelsunique labelmeans np array ndimage mean x labels y index labelsunique labelvars np array ndimage var x labels y index labelsunique return labelmeans labelvars
def groupsstats_1d y x labelsunique labelmeans np array ndimage mean x labels y index labelsunique labelvars np array ndimage var x labels y index labelsunique return labelmeans labelvars
def list_aliases ret dict alias target for alias target comment in __parse_aliases if alias return ret
def list_aliases ret dict alias target for alias target comment in __parse_aliases if alias return ret
def requires_special_home_display name return name '~' and Color HOME_SPECIAL_DISPLAY
def requires_special_home_display name return name '~' and Color HOME_SPECIAL_DISPLAY
def add_lookup namespace directory package None prepend False templates LOOKUP get namespace if not templates LOOKUP[namespace] templates DynamicTemplateLookup module_directory settings MAKO_MODULE_DIR output_encoding 'utf-8' input_encoding 'utf-8' default_filters ['decode utf8'] encoding_errors 'replace' if package directory pkg_resources resource_filename package directory templates add_directory directory prepend prepend
def snapshot_create request conf request bodyif isinstance conf basestring config json loads conf snapshot MapSnapshot objects create config clean_config conf map Map objects get id config['id'] return HttpResponse num_encode snapshot id content_type 'text/plain' else return HttpResponse 'InvalidJSON' content_type 'text/plain' status 500
def _get_dev_url backend instance None return 'http //%s' % _get_dev_hostname backend instance
def validate_string s try return len s > 0 except TypeError return False
def validate_string s try return len s > 0 except TypeError return False
def reset *args **kwargs for pool in _all_pools try pool force_close_all except Exception passfor group in _groups group clear
def playMovieInWindow theWindow theFile movieBox theMovie loadMovie theFile theMovie SetMovieBox movieBox theMovie GoToBeginningOfMovie theMovie MoviesTask 0 theMovie StartMovie while not theMovie IsMovieDone and not Evt Button theMovie MoviesTask 0
def make_blob_public bucket_name blob_name storage_client storage Client bucket storage_client get_bucket bucket_name blob bucket blob blob_name blob make_public print 'Blob{}ispubliclyaccessibleat{}' format blob name blob public_url
def _mixed_join iterable sentinel iterator iter iterable first_item next iterator sentinel if isinstance first_item bytes return first_item + '' join iterator return first_item + u'' join iterator
def quota_usage_update_resource context old_res new_res return IMPL quota_usage_update_resource context old_res new_res
def quota_usage_update_resource context old_res new_res return IMPL quota_usage_update_resource context old_res new_res
def build_path graph node1 node2 path None if path is None path []if node1 is node2 return pathpath append node2 for pred in graph all_preds node2 if pred in path continuebuild_path graph node1 pred path return path
def irc_prefix var if isinstance var basestring return u'irc_%s' % var lower
@before each_scenariodef process_requires_tags scenario tag_re re compile 'requires_stub_ ?P<server>[^_]+ ' for tag in scenario tags requires tag_re match tag if requires if requires group 'server' 'youtube' if not is_youtube_available YOUTUBE_API_URLS scenario steps []returnstart_stub requires group 'server'
@before each_scenariodef process_requires_tags scenario tag_re re compile 'requires_stub_ ?P<server>[^_]+ ' for tag in scenario tags requires tag_re match tag if requires if requires group 'server' 'youtube' if not is_youtube_available YOUTUBE_API_URLS scenario steps []returnstart_stub requires group 'server'
def execute_until_empty taskqueue queue 'default' handlers_map None task_run_counts collections defaultdict lambda 0 while taskqueue GetTasks queue new_counts execute_all_tasks taskqueue queue handlers_map for handler_cls in new_counts task_run_counts[handler_cls] + new_counts[handler_cls]return task_run_counts
def support_enumeration g return list support_enumeration_gen g
def upload_to_mugshot instance filename extension filename split ' ' [ -1 ] lower salt hash generate_sha1 instance pk path userena_settings USERENA_MUGSHOT_PATH % {'username' instance user username 'id' instance user id 'date' instance user date_joined 'date_now' get_datetime_now date } return '% path s% hash s % extension s' % {'path' path 'hash' hash[ 10] 'extension' extension}
def upload_to_mugshot instance filename extension filename split ' ' [ -1 ] lower salt hash generate_sha1 instance pk path userena_settings USERENA_MUGSHOT_PATH % {'username' instance user username 'id' instance user id 'date' instance user date_joined 'date_now' get_datetime_now date } return '% path s% hash s % extension s' % {'path' path 'hash' hash[ 10] 'extension' extension}
def upload_to_mugshot instance filename extension filename split ' ' [ -1 ] lower salt hash generate_sha1 instance pk path userena_settings USERENA_MUGSHOT_PATH % {'username' instance user username 'id' instance user id 'date' instance user date_joined 'date_now' get_datetime_now date } return '% path s% hash s % extension s' % {'path' path 'hash' hash[ 10] 'extension' extension}
def exit status message None if isinstance status ValueConstant code status valueelse code int status if message if code 0 out stdoutelse out stderrout write message out write '\n' sysexit code
def write_weighted_edgelist G path comments '#' delimiter '' encoding 'utf-8' write_edgelist G path comments comments delimiter delimiter data 'weight' encoding encoding
def absent name **kwargs ret {'name' name 'result' True 'changes' {} 'comment' []}current_schedule __salt__['schedule list'] show_all True return_yaml False if name in current_schedule if 'test' in __opts__ and __opts__['test'] kwargs['test'] Trueresult __salt__['schedule delete'] name **kwargs ret['comment'] append result['comment'] else result __salt__['schedule delete'] name **kwargs if not result['result'] ret['result'] result['result']ret['comment'] result['comment']return retelse ret['comment'] append 'Removedjob{0}fromschedule' format name else ret['comment'] append 'Job{0}notpresentinschedule' format name ret['comment'] '\n' join ret['comment'] return ret
def get_service hass config discovery_info None return FreeSMSNotificationService config[CONF_USERNAME] config[CONF_ACCESS_TOKEN]
def addClassAdvisor callback depth 2 frame sys _getframe depth kind module caller_locals caller_globals getFrameInfo frame previousMetaclass caller_locals get '__metaclass__' if __python3 defaultMetaclass caller_globals get '__metaclass__' type else defaultMetaclass caller_globals get '__metaclass__' ClassType def advise name bases cdict if '__metaclass__' in cdict del cdict['__metaclass__']if previousMetaclass is None if bases meta determineMetaclass bases else meta defaultMetaclasselif isClassAdvisor previousMetaclass meta previousMetaclasselse meta determineMetaclass bases previousMetaclass newClass meta name bases cdict return callback newClass advise previousMetaclass previousMetaclassadvise callback callbackcaller_locals['__metaclass__'] advise
def addClassAdvisor callback depth 2 frame sys _getframe depth kind module caller_locals caller_globals getFrameInfo frame previousMetaclass caller_locals get '__metaclass__' if __python3 defaultMetaclass caller_globals get '__metaclass__' type else defaultMetaclass caller_globals get '__metaclass__' ClassType def advise name bases cdict if '__metaclass__' in cdict del cdict['__metaclass__']if previousMetaclass is None if bases meta determineMetaclass bases else meta defaultMetaclasselif isClassAdvisor previousMetaclass meta previousMetaclasselse meta determineMetaclass bases previousMetaclass newClass meta name bases cdict return callback newClass advise previousMetaclass previousMetaclassadvise callback callbackcaller_locals['__metaclass__'] advise
def addClassAdvisor callback depth 2 frame sys _getframe depth kind module caller_locals caller_globals getFrameInfo frame previousMetaclass caller_locals get '__metaclass__' if __python3 defaultMetaclass caller_globals get '__metaclass__' type else defaultMetaclass caller_globals get '__metaclass__' ClassType def advise name bases cdict if '__metaclass__' in cdict del cdict['__metaclass__']if previousMetaclass is None if bases meta determineMetaclass bases else meta defaultMetaclasselif isClassAdvisor previousMetaclass meta previousMetaclasselse meta determineMetaclass bases previousMetaclass newClass meta name bases cdict return callback newClass advise previousMetaclass previousMetaclassadvise callback callbackcaller_locals['__metaclass__'] advise
def diagnose try import pynotifyexcept ImportError return u'<p>Error pynotifyisn\'tinstalled OnUbuntu/Debian installthe<ahref "apt python-notify">python-notify</a>package 'if 'DISPLAY' not in os environ and 'DBUS_SESSION_BUS_ADDRESS' not in os environ return u"<p>Error EnvironmentvariablesDISPLAYandDBUS_SESSION_BUS_ADDRESSaren'tset libnotifywillonlyworkwhenyourunSickBeardfromadesktoplogin "try import dbusexcept ImportError passelse try bus dbus SessionBus except dbus DBusException as e return u'<p>Error unabletoconnecttoD-Bussessionbus <code>%s</code> <p>AreyourunningSickBeardinadesktopsession?' % cgi escape e try bus get_object 'org freedesktop Notifications' '/org/freedesktop/Notifications' except dbus DBusException as e return u"<p>Error theredoesn'tseemtobeanotificationdaemonavailable <code>%s</code><p>Tryinstallingnotification-daemonornotify-osd " % cgi escape e return u'<p>Error Unabletosendnotification '
def fancify_summary expr seen_names clear name_dict clear exprs pipe expr values map Expr _traverse concat filter lambda x isinstance x Reduction set one summary **dict _name expr expr for expr in exprs two dict _name expr symbol _name expr datashape var * expr dshape for expr in exprs d dict expr two[_name expr ] for expr in exprs three dict name value _subs d for name value in zip expr names expr values return one two three
def fancify_summary expr seen_names clear name_dict clear exprs pipe expr values map Expr _traverse concat filter lambda x isinstance x Reduction set one summary **dict _name expr expr for expr in exprs two dict _name expr symbol _name expr datashape var * expr dshape for expr in exprs d dict expr two[_name expr ] for expr in exprs three dict name value _subs d for name value in zip expr names expr values return one two three
def returns_arg function def call_and_assert arg context None if context is None context {}result function arg context context assert result arg 'Shouldreturntheargumentthatwaspassedtoit unchanged {arg} ' format arg repr arg return resultreturn call_and_assert
def guess_scheme url parts urlparse url if parts scheme return urlif re match '^#startwith \n \n\\ # asingledot \n \n\\ [^/\\ ]+#optionallyfollowedby\n ?#eitheraseconddotorsomecharacters\n ?#optionalmatchof" " " "or" blabla"\n/#atleastone"/"forafilepath \n #andsomethingafterthe"/"\n' parts path flags re VERBOSE return any_to_uri url else return add_http_if_no_scheme url
def guess_scheme url parts urlparse url if parts scheme return urlif re match '^#startwith \n \n\\ # asingledot \n \n\\ [^/\\ ]+#optionallyfollowedby\n ?#eitheraseconddotorsomecharacters\n ?#optionalmatchof" " " "or" blabla"\n/#atleastone"/"forafilepath \n #andsomethingafterthe"/"\n' parts path flags re VERBOSE return any_to_uri url else return add_http_if_no_scheme url
def _flip_codons codon_seq target_seq a b '' '' for char1 char2 in zip codon_seq target_seq if char1 '' a + char1b + char2else a + char2b + char1return a b
@synchronized IO_LOCK def load_admin _id remove False do_pickle True silent False path os path join cfg admin_dir get_path _id logging info 'Loadingdatafor%sfrom%s' _id path if not os path exists path logging info '%smissing' path return Nonetry f open path 'rb' if do_pickle data cPickle load f else data f read f close if remove os remove path except if not silent excepterror str sys exc_info [0] logging error T 'Loading%sfailedwitherror%s' path excepterror logging info 'Traceback ' exc_info True return Nonereturn data
@contextfilterdef currency_format context value currency None if not currency currency Currency objects get is_default True if not currency symbol return unicode value + '' + currency code else return currency symbol + unicode value
@contextfilterdef currency_format context value currency None if not currency currency Currency objects get is_default True if not currency symbol return unicode value + '' + currency code else return currency symbol + unicode value
@contextfilterdef currency_format context value currency None if not currency currency Currency objects get is_default True if not currency symbol return unicode value + '' + currency code else return currency symbol + unicode value
@contextfilterdef currency_format context value currency None if not currency currency Currency objects get is_default True if not currency symbol return unicode value + '' + currency code else return currency symbol + unicode value
def only name hostnames ret {'name' name 'changes' {} 'result' None 'comment' ''}if isinstance hostnames six string_types hostnames [hostnames]old '' join __salt__['hosts get_alias'] name new '' join x strip for x in hostnames if old new ret['comment'] 'IPaddress{0}alreadysetto"{1}"' format name new ret['result'] Truereturn retif __opts__['test'] ret['comment'] 'Wouldchange{0}from"{1}"to"{2}"' format name old new return retret['result'] __salt__['hosts set_host'] name new if not ret['result'] ret['comment'] 'hosts set_hostfailedtochange{0}' + 'from"{1}"to"{2}"' format name old new return retret['comment'] 'successfullychanged{0}from"{1}"to"{2}"' format name old new ret['changes'] {name {'old' old 'new' new}}return ret
def apply_temporary_fixes font is_for_cros False font_name font_data font_name font weight noto_fonts parse_weight font_name weight_number noto_fonts WEIGHTS[weight]if is_for_cros and weight 'Thin' weight_number 100font['OS/2'] usWeightClass weight_numbername_records font_data get_name_records font family_name name_records[1]if family_name endswith 'Black' font['head'] macStyle 1 << 0 font['OS/2'] fsSelection 1 << 5 font['OS/2'] fsSelection & ~ 1 << 6
def LanguageFacet locale_ident extra_weights {} def score searcher docnum doc searcher stored_fields docnum weight extra_weights get doc['name'] 1 0 doc_language doc['language']if doc_language locale_ident weight * 2 0elif doc_language u'roomaji' weight * 1 4return - weight return whoosh sorting FunctionFacet score
def LanguageFacet locale_ident extra_weights {} def score searcher docnum doc searcher stored_fields docnum weight extra_weights get doc['name'] 1 0 doc_language doc['language']if doc_language locale_ident weight * 2 0elif doc_language u'roomaji' weight * 1 4return - weight return whoosh sorting FunctionFacet score
def LanguageFacet locale_ident extra_weights {} def score searcher docnum doc searcher stored_fields docnum weight extra_weights get doc['name'] 1 0 doc_language doc['language']if doc_language locale_ident weight * 2 0elif doc_language u'roomaji' weight * 1 4return - weight return whoosh sorting FunctionFacet score
def _calc_end text start if len text 1 new_end start + Position 0 len text[0] else new_end Position start line + len text - 1 len text[ -1 ] return new_end
def _unwindk z return int np ceil z imag - np pi / 2 * np pi
def peer2str addr if isinstance addr IPv4Address res u'tcp4 {0} {1}' format addr host addr port elif _HAS_IPV6 and isinstance addr IPv6Address res u'tcp6 {0} {1}' format addr host addr port elif isinstance addr UNIXAddress res u'unix {0}' format addr name elif isinstance addr PipeAddress res u'<pipe>'else res u'? {0}' format addr return res
def h2_safe_headers headers stripped {i lower strip for k v in headers if k 'connection' for i in v split ' ' }stripped add 'connection' return [header for header in headers if header[0] not in stripped ]
def get_position_size fd 1 info get_console_screen_buffer_info fd return info dwCursorPosition X info dwCursorPosition Y info dwSize X info dwSize Y
def getFillOfSurroundings nestedRings penultimateFillLoops fillOfSurroundings []for nestedRing in nestedRings fillOfSurroundings + nestedRing getFillLoops penultimateFillLoops return fillOfSurroundings
@pick_context_manager_writerdef instance_group_update context group_uuid values group model_query context models InstanceGroup filter_by uuid group_uuid first if not group raise exception InstanceGroupNotFound group_uuid group_uuid policies values get 'policies' if policies is not None _instance_group_policies_add context group id values pop 'policies' set_delete True members values get 'members' if members is not None _instance_group_members_add context group id values pop 'members' set_delete True group update values if policies values['policies'] policiesif members values['members'] members
def which_prefix path prefix abspath path while True if isdir join prefix u'conda-meta' return prefixif prefix dirname prefix return Noneprefix dirname prefix
def which_prefix path prefix abspath path while True if isdir join prefix u'conda-meta' return prefixif prefix dirname prefix return Noneprefix dirname prefix
def which_prefix path prefix abspath path while True if isdir join prefix u'conda-meta' return prefixif prefix dirname prefix return Noneprefix dirname prefix
def create_logger common_config use_daemon_threads True ext u'__unknown__' import_paths None imported_modules None stream None logger logging Logger u'powerline' level getattr logging common_config[u'log_level'] logger setLevel level pl PowerlineLogger use_daemon_threads logger ext get_module_attr gen_module_attr_getter pl common_config[u'paths'] set if imported_modules is None else imported_modules _set_log_handlers common_config logger get_module_attr stream return logger pl get_module_attr
def convert_coord_data_to_dict data coord_header data['coord'][0]coords data['coord'][1]pct_var data['coord'][3]coords_dict {}pct_var_dict {}coords_dict['pcvectornumber'] coord_headerfor x in range len coords coords_dict[str x + 1 ] coords[0 x]pct_var_dict[str x + 1 ] pct_var[x]return coords_dict pct_var_dict
def get_max_data_extent net layer rc is_conv if is_conv conv_size net blobs[layer] data shape[2 4]layer_slice_middle conv_size[0] / 2 conv_size[0] / 2 + 1 conv_size[1] / 2 conv_size[1] / 2 + 1 data_slice rc convert_region layer 'data' layer_slice_middle return data_slice[1] - data_slice[0] data_slice[3] - data_slice[2] else return net blobs['data'] data shape[2 4]
def get_max_data_extent net layer rc is_conv if is_conv conv_size net blobs[layer] data shape[2 4]layer_slice_middle conv_size[0] / 2 conv_size[0] / 2 + 1 conv_size[1] / 2 conv_size[1] / 2 + 1 data_slice rc convert_region layer 'data' layer_slice_middle return data_slice[1] - data_slice[0] data_slice[3] - data_slice[2] else return net blobs['data'] data shape[2 4]
def get_max_data_extent net layer rc is_conv if is_conv conv_size net blobs[layer] data shape[2 4]layer_slice_middle conv_size[0] / 2 conv_size[0] / 2 + 1 conv_size[1] / 2 conv_size[1] / 2 + 1 data_slice rc convert_region layer 'data' layer_slice_middle return data_slice[1] - data_slice[0] data_slice[3] - data_slice[2] else return net blobs['data'] data shape[2 4]
def generic_bfs_edges G source neighbors None visited {source}queue deque [ source neighbors source ] while queue parent children queue[0]try child next children if child not in visited yield parent child visited add child queue append child neighbors child except StopIteration queue popleft
def generic_bfs_edges G source neighbors None visited {source}queue deque [ source neighbors source ] while queue parent children queue[0]try child next children if child not in visited yield parent child visited add child queue append child neighbors child except StopIteration queue popleft
def cleanedUpClassificationNetwork original_network num_categories network caffe_pb2 NetParameter network CopyFrom original_network for i layer in enumerate network layer if 'Data' in layer type assert layer type in ['Data' 'HDF5Data'] 'Unsupporteddatalayertype%s' % layer type elif layer type 'Input' del network layer[i]elif layer type 'Accuracy' if layer accuracy_param HasField 'top_k' and layer accuracy_param top_k > num_categories del network layer[i]elif layer type 'InnerProduct' if not layer inner_product_param HasField 'num_output' layer inner_product_param num_output num_categoriesreturn network
def cleanedUpClassificationNetwork original_network num_categories network caffe_pb2 NetParameter network CopyFrom original_network for i layer in enumerate network layer if 'Data' in layer type assert layer type in ['Data' 'HDF5Data'] 'Unsupporteddatalayertype%s' % layer type elif layer type 'Input' del network layer[i]elif layer type 'Accuracy' if layer accuracy_param HasField 'top_k' and layer accuracy_param top_k > num_categories del network layer[i]elif layer type 'InnerProduct' if not layer inner_product_param HasField 'num_output' layer inner_product_param num_output num_categoriesreturn network
def utcnow now datetime datetime utcnow now now replace tzinfo TimeZoneInfo utc return now
def utcnow now datetime datetime utcnow now now replace tzinfo TimeZoneInfo utc return now
def package pkg_name update False if not is_installed pkg_name install pkg_name update update
def notify_about_volume_swap context instance host action phase old_volume_id new_volume_id exception None ips _get_instance_ips instance flavor flavor_notification FlavorPayload instance flavor fault priority _get_fault_and_priority_from_exc exception payload instance_notification InstanceActionVolumeSwapPayload instance instance fault fault ip_addresses ips flavor flavor old_volume_id old_volume_id new_volume_id new_volume_id instance_notification InstanceActionVolumeSwapNotification context context priority priority publisher notification_base NotificationPublisher context context host host binary 'nova-compute' event_type notification_base EventType object 'instance' action action phase phase payload payload emit context
def setup_standalone_signals instance window instance get_widget 'config-window' window connect 'delete-event' gtk main_quit button instance get_widget 'button1' button handler_block_by_func instance gtk_widget_destroy button connect 'clicked' gtk main_quit return instance
def setup_standalone_signals instance window instance get_widget 'config-window' window connect 'delete-event' gtk main_quit button instance get_widget 'button1' button handler_block_by_func instance gtk_widget_destroy button connect 'clicked' gtk main_quit return instance
def is_course_blocked request redeemed_registration_codes course_key blocked Falsefor redeemed_registration in redeemed_registration_codes if redeemed_registration invoice_item if not redeemed_registration invoice_item invoice is_valid blocked TrueOptout objects get_or_create user request user course_id course_key log info u'User%s %s optedoutofreceivingemailsfromcourse%s' request user username request user email course_key track views server_track request 'change-email1-settings' {'receive_emails' 'no' 'course' course_key to_deprecated_string } page 'dashboard' breakreturn blocked
def libvlc_audio_output_list_get p_instance f _Cfunctions get 'libvlc_audio_output_list_get' None or _Cfunction 'libvlc_audio_output_list_get' 1 None ctypes POINTER AudioOutput Instance return f p_instance
@sopel module require_privmsg@sopel module require_admin@sopel module commands u'mode' @sopel module priority u'low' def mode bot trigger mode trigger group 3 bot write u'MODE' bot nick + u'' + mode
def get_price_info shop customer product quantity pricing_mod get_pricing_module pricing_ctx pricing_mod get_context_from_data shop shop customer customer or AnonymousContact return product get_price_info pricing_ctx quantity quantity
def libvlc_video_get_cursor p_mi num f _Cfunctions get 'libvlc_video_get_cursor' None or _Cfunction 'libvlc_video_get_cursor' 1 1 2 2 None ctypes c_int MediaPlayer ctypes c_uint ctypes POINTER ctypes c_int ctypes POINTER ctypes c_int return f p_mi num
def libvlc_video_get_cursor p_mi num f _Cfunctions get 'libvlc_video_get_cursor' None or _Cfunction 'libvlc_video_get_cursor' 1 1 2 2 None ctypes c_int MediaPlayer ctypes c_uint ctypes POINTER ctypes c_int ctypes POINTER ctypes c_int return f p_mi num
@auth s3_requires_membership 1 def clean from subprocess import check_callinstance settings get_instance_name try check_call [ 'sudo/usr/local/bin/clean%s' % instance ] shell True except import syserror sys exc_info [1]status current xml json_message False 400 'Scriptcannotberun %s' % error raise HTTP 400 body status
@auth s3_requires_membership 1 def clean from subprocess import check_callinstance settings get_instance_name try check_call [ 'sudo/usr/local/bin/clean%s' % instance ] shell True except import syserror sys exc_info [1]status current xml json_message False 400 'Scriptcannotberun %s' % error raise HTTP 400 body status
def emr_ssl_host_for_region region region _fix_region region if not region return _EMR_REGIONLESS_ENDPOINTelse return _EMR_REGION_SSL_HOST % {'region' region}
def emr_ssl_host_for_region region region _fix_region region if not region return _EMR_REGIONLESS_ENDPOINTelse return _EMR_REGION_SSL_HOST % {'region' region}
def list sandbox_name results 15 start 0 result util callm '%s/%s' % 'sandbox' 'list' {'sandbox' sandbox_name 'results' results 'start' start} assets result['response']['assets']start result['response']['start']total result['response']['total']return ResultList assets start total
def xml_readlines source encoding get_xml_encoding source with data get_readable_fileobj source encoding encoding as input input seek 0 xml_lines input readlines return xml_lines
def ulps_check expected got ulps 20 ulps_error to_ulps got - to_ulps expected if abs ulps_error < ulps return Nonereturn 'error {}ulps permittederror {}ulps' format ulps_error ulps
@check_job_permissiondef set_job_priority request job priority request GET get 'priority' jid request jt thriftjobid_from_string job jobId request jt set_job_priority jid ThriftJobPriority _NAMES_TO_VALUES[priority] return render_json {}
def volume_delete name profile None conn _auth profile return conn volume_delete name
def replace_new_vars expr id_to_new_var if expr type lo VARIABLE and expr data in id_to_new_var return id_to_new_var[expr data]else new_args []for arg in expr args new_args append replace_new_vars arg id_to_new_var return lo LinOp expr type expr size new_args expr data
def delete_ref refname oldvalue None assert refname startswith 'refs/' oldvalue [] if not oldvalue else [oldvalue] p subprocess Popen ['git' 'update-ref' '-d' refname] + oldvalue preexec_fn _gitenv _git_wait 'gitupdate-ref' p
def _ConvertToCygpath path if sys platform 'cygwin' p subprocess Popen ['cygpath' path] stdout subprocess PIPE path p communicate [0] strip return path
def present name provider ret {'name' name 'result' True 'comment' '' 'changes' {}}is_present list __salt__['cloud action'] 'queues_exists' provider provider name name [provider] values [0]if not is_present if __opts__['test'] msg 'Rackspacequeue{0}issettobecreated ' format name ret['comment'] msgret['result'] Nonereturn retcreated __salt__['cloud action'] 'queues_create' provider provider name name if created queue __salt__['cloud action'] 'queues_show' provider provider name name ret['changes']['old'] {}ret['changes']['new'] {'queue' queue}else ret['result'] Falseret['comment'] 'Failedtocreate{0}Rackspacequeue ' format name return retelse ret['comment'] '{0}present ' format name return ret
def _write_files output_root contents generated_suffix_map None _ensure_dir output_root to_delete set file basename for file in output_root files - set contents keys if generated_suffix_map for output_file in contents keys for suffix generated_suffix in generated_suffix_map items if output_file endswith suffix to_delete discard output_file replace suffix generated_suffix for extra_file in to_delete output_root / extra_file remove_p for filename file_content in contents iteritems output_file output_root / filename not_file not output_file isfile write_file not_file or output_file read_md5 hashlib md5 file_content digest if write_file LOG debug 'Writing%s' output_file output_file write_bytes file_content else LOG debug '%sunchanged skipping' output_file
def _write_files output_root contents generated_suffix_map None _ensure_dir output_root to_delete set file basename for file in output_root files - set contents keys if generated_suffix_map for output_file in contents keys for suffix generated_suffix in generated_suffix_map items if output_file endswith suffix to_delete discard output_file replace suffix generated_suffix for extra_file in to_delete output_root / extra_file remove_p for filename file_content in contents iteritems output_file output_root / filename not_file not output_file isfile write_file not_file or output_file read_md5 hashlib md5 file_content digest if write_file LOG debug 'Writing%s' output_file output_file write_bytes file_content else LOG debug '%sunchanged skipping' output_file
def concatenate tensors axis -1 if axis < 0 dims ndim tensors[0] if dims axis axis % dims else axis 0if py_all [is_sparse x for x in tensors] return tf sparse_concat axis tensors else try return tf concat_v2 [to_dense x for x in tensors] axis except AttributeError return tf concat axis [to_dense x for x in tensors]
def _pos_match pos_tuple if pos_tuple[0] 's' pos_tuple 'a' pos_tuple[1] pos_tuple[2] for n x in enumerate pos_tuple if x is not None breakfor pt in _pos_tuples if pt[n] pos_tuple[n] return ptreturn None
def set_partition ssli part []for s in sorted list set ssli key len [ -1 ] s_ set s copy if not any set s_ intersection set t for t in part part append s missing list set i for ll in ssli for i in ll - set i for ll in part for i in ll return part missing
def assert_application_calculated_changes case node_state node_config nonmanifest_datasets expected_changes additional_node_states frozenset additional_node_config frozenset deployer ApplicationNodeDeployer hostname node_state hostname node_uuid node_state uuid docker_client FakeDockerClient return assert_calculated_changes_for_deployer case deployer node_state node_config nonmanifest_datasets additional_node_states additional_node_config expected_changes NodeLocalState node_state node_state
def default_missing_value_for_dtype dtype try return _FILLVALUE_DEFAULTS[dtype]except KeyError raise NoDefaultMissingValue 'Nodefaultvalueregisteredfordtype%s ' % dtype
@lru_cache def _get_firebase_db_url regex re compile '\\bdatabaseURL\\b *?["\\\'] [^"\\\']+ ' cwd os path dirname __file__ try with open os path join cwd 'templates' _FIREBASE_CONFIG as f url next regex search line for line in f if regex search line except StopIteration raise ValueError 'ErrorparsingdatabaseURL PleasecopyFirebasewebsnippetintotemplates/{}' format _FIREBASE_CONFIG return url group 1
def render_alert content alert_type None dismissable True button u''if not alert_type alert_type u'info'css_classes [u'alert' u'alert-' + text_value alert_type ]if dismissable css_classes append u'alert-dismissable' button u'<buttontype "button"class "close"' + u'data-dismiss "alert"aria-hidden "true">&times </button>' button_placeholder u'__BUTTON__'return mark_safe render_tag u'div' attrs {u'class' u'' join css_classes } content button_placeholder + text_value content replace button_placeholder button
def last_update_time STAMP '/var/lib/apt/periodic/fabtools-update-success-stamp'if not is_file STAMP return -1 return getmtime STAMP
def _U_func_numpy x1 y1 x2 y2 if x1 x2 and y1 y2 return 0 0r_2 x2 - x1 ** 2 + y2 - y1 ** 2 return r_2 * np log r_2
def _U_func_numpy x1 y1 x2 y2 if x1 x2 and y1 y2 return 0 0r_2 x2 - x1 ** 2 + y2 - y1 ** 2 return r_2 * np log r_2
def get_locales_by_visit start_date end_date cache_key 'locales_sorted_by_visits {start} {end}' format start start_date end end_date sorted_locales cache get cache_key if sorted_locales is None try results visitors_by_locale start_date end_date locales_and_visits results items sorted_locales list reversed sorted locales_and_visits key lambda x x[1] cache add cache_key sorted_locales CACHE_TIMEOUT except GoogleAPIError Oauth2Error OpenSSLError log exception 'SomethingwentwronggettingvisitorsbylocalefromGoogleAnalytics Nobodygota500though ' sorted_locales [ l 0 for l in settings SUMO_LANGUAGES]return sorted_locales
def multisig_type wallet_type match re match ' \\d+ of \\d+ ' wallet_type if match match [int x for x in match group 1 2 ]return match
def multisig_type wallet_type match re match ' \\d+ of \\d+ ' wallet_type if match match [int x for x in match group 1 2 ]return match
def openshift_img_tagger registry xml_parent data osb XML SubElement xml_parent 'com openshift jenkins plugins pipeline OpenShiftImageTagger' mapping [ 'api-url' 'apiURL' 'https //openshift default svc cluster local' 'test-tag' 'testTag' 'origin-nodejs-sample latest' 'prod-tag' 'prodTag' 'origin-nodejs-sample prod' 'namespace' 'namespace' 'test' 'auth-token' 'authToken' '' 'verbose' 'verbose' False ]convert_mapping_to_xml osb data mapping fail_required True
def openshift_img_tagger registry xml_parent data osb XML SubElement xml_parent 'com openshift jenkins plugins pipeline OpenShiftImageTagger' mapping [ 'api-url' 'apiURL' 'https //openshift default svc cluster local' 'test-tag' 'testTag' 'origin-nodejs-sample latest' 'prod-tag' 'prodTag' 'origin-nodejs-sample prod' 'namespace' 'namespace' 'test' 'auth-token' 'authToken' '' 'verbose' 'verbose' False ]convert_mapping_to_xml osb data mapping fail_required True
def openshift_img_tagger registry xml_parent data osb XML SubElement xml_parent 'com openshift jenkins plugins pipeline OpenShiftImageTagger' mapping [ 'api-url' 'apiURL' 'https //openshift default svc cluster local' 'test-tag' 'testTag' 'origin-nodejs-sample latest' 'prod-tag' 'prodTag' 'origin-nodejs-sample prod' 'namespace' 'namespace' 'test' 'auth-token' 'authToken' '' 'verbose' 'verbose' False ]convert_mapping_to_xml osb data mapping fail_required True
def install_python python runas None python re sub '^python-' '' python env Noneenv_list []if __grains__['os'] in 'FreeBSD' 'NetBSD' 'OpenBSD' env_list append 'MAKE gmake' if __salt__['config option'] 'pyenv build_env' env_list append __salt__['config option'] 'pyenv build_env' if env_list env '' join env_list ret {}ret _pyenv_exec 'install' python env env runas runas ret ret if ret['retcode'] 0 rehash runas runas return ret['stderr']else uninstall_python python runas runas return False
def select_filters filters level return [f for f in filters if f max_debug_level is None or cmp_debug_levels level f max_debug_level < 0 ]
def select_filters filters level return [f for f in filters if f max_debug_level is None or cmp_debug_levels level f max_debug_level < 0 ]
def make_XAxis xaxis_title xaxis_range xaxis graph_objs XAxis title xaxis_title range xaxis_range showgrid False zeroline False showline False mirror False ticks '' showticklabels False return xaxis
def get_selections with settings hide 'stdout' res run_as_root 'dpkg--get-selections' selections dict for line in res splitlines package status line split selections setdefault status list append package return selections
def header_elements fieldname fieldvalue if not fieldvalue return []result []for element in fieldvalue split ' ' if fieldname startswith 'Accept' or fieldname 'TE' hv AcceptElement from_str element else hv HeaderElement from_str element result append hv result sort result reverse return result
def header_elements fieldname fieldvalue if not fieldvalue return []result []for element in fieldvalue split ' ' if fieldname startswith 'Accept' or fieldname 'TE' hv AcceptElement from_str element else hv HeaderElement from_str element result append hv result sort result reverse return result
@cwd_at ' ' def test_completion_docstring def docstr src result c Script src completions [0]assert c docstring raw True fast False cleandoc result c Script 'importjedi\njed' completions [0]assert c docstring fast False cleandoc jedi_doc docstr 'importjedi\njedi Scr' cleandoc Script __doc__ docstr 'abcd 3 abcd' '' docstr '"hello"\nabcd 3\nabcd' 'hello' docstr '"hello"\nabcd 3 abcd' 'hello' docstr '"hello" 0\nabcd 3\nabcd' ''
@intercept_errors UserAPIInternalError ignore_errors [UserAPIRequestError] def get_user_preferences requesting_user username None existing_user _get_authorized_user requesting_user username allow_staff True context {'request' get_request_or_stub }user_serializer UserSerializer existing_user context context return user_serializer data['preferences']
def create_account caller text 'Enteryournewaccountname 'options {'key' '_default' 'goto' 'create_username'} return text options
@cacheitdef _expand_delta expr index if not expr is_Mul return exprdelta Nonefunc Addterms [S 1 ]for h in expr args if delta is None and h is_Add and _has_simple_delta h index delta Truefunc h functerms [ terms[0] * t for t in h args]else terms [ t * h for t in terms]return func *terms
@cacheitdef _expand_delta expr index if not expr is_Mul return exprdelta Nonefunc Addterms [S 1 ]for h in expr args if delta is None and h is_Add and _has_simple_delta h index delta Truefunc h functerms [ terms[0] * t for t in h args]else terms [ t * h for t in terms]return func *terms
def getCentersFromLoopDirection isWiddershins loop radius centers getCentersFromLoop loop radius return getLoopsFromLoopsDirection isWiddershins centers
@lazyobjectdef SetConsoleCursorPosition sccp ctypes windll kernel32 SetConsoleCursorPositionsccp errcheck check_zerosccp argtypes HANDLE COORD sccp restype BOOLreturn sccp
@lazyobjectdef SetConsoleCursorPosition sccp ctypes windll kernel32 SetConsoleCursorPositionsccp errcheck check_zerosccp argtypes HANDLE COORD sccp restype BOOLreturn sccp
def get_languages_for_locale locale locale locale replace '-' '_' if locale not in AVAILABLE_LANGUAGES try local_lang babel Locale locale languagesexcept babel UnknownLocaleError return AVAILABLE_LANGUAGES['en']diff [lc for lc in REFERENCE_LANGUAGE keys if lc not in local_lang keys ]for lc in diff local_lang[lc] REFERENCE_LANGUAGE[lc]for lang in REMOVE_LANGS if lang in local_lang local_lang pop lang local_lang sorted [ key value capitalize for key value in local_lang items ] key lambda language language[1] AVAILABLE_LANGUAGES[locale] local_langreturn AVAILABLE_LANGUAGES[locale]
def get_languages_for_locale locale locale locale replace '-' '_' if locale not in AVAILABLE_LANGUAGES try local_lang babel Locale locale languagesexcept babel UnknownLocaleError return AVAILABLE_LANGUAGES['en']diff [lc for lc in REFERENCE_LANGUAGE keys if lc not in local_lang keys ]for lc in diff local_lang[lc] REFERENCE_LANGUAGE[lc]for lang in REMOVE_LANGS if lang in local_lang local_lang pop lang local_lang sorted [ key value capitalize for key value in local_lang items ] key lambda language language[1] AVAILABLE_LANGUAGES[locale] local_langreturn AVAILABLE_LANGUAGES[locale]
def _cast_other binary_op def cast_op self other 'Awrappedbinaryoperatorthatcanhandlenon-Expressionarguments \n'other self cast_to_const other return binary_op self other return cast_op
def rush value True realtime False if importCtypesFailed return Falseif value schedParams _SchedParams schedParams sched_priority c sched_get_priority_max SCHED_RR err c sched_setscheduler 0 SCHED_RR ctypes byref schedParams if err -1 logging warning warnMax % sys executable sys executable else schedParams _SchedParams schedParams sched_priority c sched_get_priority_min SCHED_NORMAL err c sched_setscheduler 0 SCHED_NORMAL ctypes byref schedParams if err -1 logging warning warnNormal % sys executable return True
def rush value True realtime False if importCtypesFailed return Falseif value schedParams _SchedParams schedParams sched_priority c sched_get_priority_max SCHED_RR err c sched_setscheduler 0 SCHED_RR ctypes byref schedParams if err -1 logging warning warnMax % sys executable sys executable else schedParams _SchedParams schedParams sched_priority c sched_get_priority_min SCHED_NORMAL err c sched_setscheduler 0 SCHED_NORMAL ctypes byref schedParams if err -1 logging warning warnNormal % sys executable return True
def rush value True realtime False if importCtypesFailed return Falseif value schedParams _SchedParams schedParams sched_priority c sched_get_priority_max SCHED_RR err c sched_setscheduler 0 SCHED_RR ctypes byref schedParams if err -1 logging warning warnMax % sys executable sys executable else schedParams _SchedParams schedParams sched_priority c sched_get_priority_min SCHED_NORMAL err c sched_setscheduler 0 SCHED_NORMAL ctypes byref schedParams if err -1 logging warning warnNormal % sys executable return True
def rush value True realtime False if importCtypesFailed return Falseif value schedParams _SchedParams schedParams sched_priority c sched_get_priority_max SCHED_RR err c sched_setscheduler 0 SCHED_RR ctypes byref schedParams if err -1 logging warning warnMax % sys executable sys executable else schedParams _SchedParams schedParams sched_priority c sched_get_priority_min SCHED_NORMAL err c sched_setscheduler 0 SCHED_NORMAL ctypes byref schedParams if err -1 logging warning warnNormal % sys executable return True
def _validate_cert_path name cmd "Test-Path-Path'{0}'" format name if not ast literal_eval _cmd_run cmd cmd raise SaltInvocationError 'Invalidpathspecified {0}' format name
def _get_boolean data position dummy0 dummy1 dummy2 end position + 1 boolean_byte data[position end]if boolean_byte '\x00' return False end elif boolean_byte '\x01' return True end raise InvalidBSON 'invalidbooleanvalue %r' % boolean_byte
def assert_not_match pattern string msg None assert_none re search pattern string msg
def assert_not_match pattern string msg None assert_none re search pattern string msg
def assert_not_match pattern string msg None assert_none re search pattern string msg
def escape x if "'" not in x return "'" + x + "'" s '"'for c in x if c in '\\$"`' s s + '\\' s s + c s s + '"' return s
def bingify s return "'{}'" format s
def bingify s return "'{}'" format s
def _create_blockdevice_id_for_test dataset_id return 'blockdevice-' + unicode dataset_id
def _create_blockdevice_id_for_test dataset_id return 'blockdevice-' + unicode dataset_id
def _create_blockdevice_id_for_test dataset_id return 'blockdevice-' + unicode dataset_id
def _create_blockdevice_id_for_test dataset_id return 'blockdevice-' + unicode dataset_id
def callUser acc try getUserName except _skypeError return skype 'CALL' + acc
def get_cursor_offset fd 1 csbi get_console_screen_buffer_info fd fd pos csbi dwCursorPositionsize csbi dwSizereturn pos Y * size X + pos X
def FindUnixSocket for path in _POTENTIAL_SOCKET_LOCATIONS if os path exists path return path
def FindUnixSocket for path in _POTENTIAL_SOCKET_LOCATIONS if os path exists path return path
def _get_defaults func code func __code__pos_count code co_argcountarg_names code co_varnamesarg_names arg_names[ pos_count]defaults func __defaults__ or kwdefaults func __kwdefaults__res dict kwdefaults if kwdefaults else {} pos_offset pos_count - len defaults for name value in zip arg_names[pos_offset ] defaults assert name not in res res[name] valuereturn res
def ParseNolintSuppressions filename raw_line linenum error matched Search '\\bNOLINT NEXTLINE ?\\b \\ [^ ]+\\ ?' raw_line if matched if matched group 1 suppressed_line linenum + 1 else suppressed_line linenumcategory matched group 2 if category in None ' * ' _error_suppressions setdefault None set add suppressed_line elif category startswith ' ' and category endswith ' ' category category[1 -1 ]if category in _ERROR_CATEGORIES _error_suppressions setdefault category set add suppressed_line elif category not in _LEGACY_ERROR_CATEGORIES error filename linenum 'readability/nolint' 5 'UnknownNOLINTerrorcategory %s' % category
def get_int_len _int power 1while True if _int 0 return 0elif _int < 10 ** power return powerpower + 1
def evalUnits unitStr pass
def getDescriptionMultiply lines activateMultiplyString getSettingString lines 'multiply' 'ActivateMultiply' if activateMultiplyString None or activateMultiplyString 'False' return ''columnsString getSettingString lines 'multiply' 'NumberofColumns' rowsString getSettingString lines 'multiply' 'NumberofRows' if columnsString '1' and rowsString '1' return ''return '_%scx%sr' % columnsString rowsString
def available_distributions flocker_source_path return set path basename for path in flocker_source_path descendant BUILD_TARGETS_SEGMENTS children if path isdir and path child 'Dockerfile' exists
def get_gmond_format val tp type val __name__if tp 'int' return 'uint' '%u' elif tp 'float' return 'float' '% 4f' elif tp 'string' return 'string' '%u' else return 'string' '%u'
@yield_oncedef icollect_bears bear_dir_glob bear_globs kinds log_printer for bear_dir dir_glob in filter lambda x os path isdir x[0] icollect bear_dir_glob bear_dir glob_escape bear_dir for bear_glob in bear_globs for matching_file in iglob os path join bear_dir bear_glob + ' py' try for bear in _import_bears matching_file kinds yield bear bear_glob except pkg_resources VersionConflict as exception log_printer log_exception 'Unabletocollectbearsfrom{file}becausethereisaconflictwiththeversionofadependencyyouhaveinstalled Thismayberesolvedbycreatingaseparatevirtualenvironmentforcoalaorrunning`pipinstall"{pkg}"` Beawarethatthelattersolutionmightbreakotherpythonpackagesthatdependonthecurrentlyinstalledversion ' format file matching_file pkg exception req exception log_level LOG_LEVEL WARNING except BaseException as exception log_printer log_exception 'Unabletocollectbearsfrom{file} Probablythefileismalformedorthemodulecoderaisesanexception ' format file matching_file exception log_level LOG_LEVEL WARNING
def _create_diffs_for_sets current_path set_a set_b resulting_diffs pvector [] evolver for item in set_a difference set_b resulting_diffs append _Remove path current_path item item for item in set_b difference set_a resulting_diffs append _Add path current_path item item return resulting_diffs persistent
def _validate_footer_timestamp vdi_path check_cmd 'vhd-utilcheck-n% vdi_path s-p' % locals check_proc make_subprocess check_cmd stdout True stderr True out err finish_subprocess check_proc check_cmd ok_exit_codes [0 22] first_line out splitlines [0] strip if 'primaryfooterinvalid' in first_line raise Exception "VDI'% vdi_path s'hastimestampinthefuture ensuresourceanddestinationhostmachineshavetimesetcorrectly" % locals elif check_proc returncode 0 raise Exception "Unexpectedoutput'% out s'fromvhd-util" % locals
def _validate_footer_timestamp vdi_path check_cmd 'vhd-utilcheck-n% vdi_path s-p' % locals check_proc make_subprocess check_cmd stdout True stderr True out err finish_subprocess check_proc check_cmd ok_exit_codes [0 22] first_line out splitlines [0] strip if 'primaryfooterinvalid' in first_line raise Exception "VDI'% vdi_path s'hastimestampinthefuture ensuresourceanddestinationhostmachineshavetimesetcorrectly" % locals elif check_proc returncode 0 raise Exception "Unexpectedoutput'% out s'fromvhd-util" % locals
def _validate_footer_timestamp vdi_path check_cmd 'vhd-utilcheck-n% vdi_path s-p' % locals check_proc make_subprocess check_cmd stdout True stderr True out err finish_subprocess check_proc check_cmd ok_exit_codes [0 22] first_line out splitlines [0] strip if 'primaryfooterinvalid' in first_line raise Exception "VDI'% vdi_path s'hastimestampinthefuture ensuresourceanddestinationhostmachineshavetimesetcorrectly" % locals elif check_proc returncode 0 raise Exception "Unexpectedoutput'% out s'fromvhd-util" % locals
def to_text value if value < 0 or value > 65535 raise ValueError 'classmustbebetween> 0and< 65535' text _by_value get value if text is None text 'CLASS' + `value` return text
def count_params layer **tags params get_all_params layer **tags shapes [p get_value shape for p in params]counts [np prod shape for shape in shapes]return sum counts
def check_builtin_matches_remote download_url True builtin_registry EarthLocation _get_site_registry force_builtin True dl_registry EarthLocation _get_site_registry force_download download_url in_dl {}matches {}for name in builtin_registry names in_dl[name] name in dl_registry if in_dl[name] matches[name] quantity_allclose builtin_registry[name] dl_registry[name] else matches[name] Falseif not all matches values print u'Inbuiltinregistrybutnotindownload ' for name in in_dl if not in_dl[name] print u'' name print u'Inbothbutnotthesamevalue ' for name in matches if not matches[name] and in_dl[name] print u'' name u'builtin ' builtin_registry[name] u'download ' dl_registry[name] assert False u"Builtinanddownloadregistryaren'tconsistent-failuresprintedtostdout"
def get_fun fun conn mdb _get_conn ret None ret {}rdata mdb saltReturns find_one {'fun' fun} {'_id' 0} if rdata ret rdatareturn ret
def get_fun fun conn mdb _get_conn ret None ret {}rdata mdb saltReturns find_one {'fun' fun} {'_id' 0} if rdata ret rdatareturn ret
def row_by_value idl_ table column match default _NO_DEFAULT tab idl_ tables[table]for r in tab rows values if getattr r column match return rif default is not _NO_DEFAULT return defaultraise RowNotFound table table col column match match
def close *args if len args 0 figManager _pylab_helpers Gcf get_active if figManager is None returnelse figManager canvas mpl_disconnect figManager _cidgcf _pylab_helpers Gcf destroy figManager num elif len args 1 arg args[0]if arg 'all' for manager in _pylab_helpers Gcf get_all_fig_managers manager canvas mpl_disconnect manager _cidgcf _pylab_helpers Gcf destroy manager num elif isinstance arg int _pylab_helpers Gcf destroy arg elif isinstance arg Figure for manager in _pylab_helpers Gcf get_all_fig_managers if manager canvas figure arg manager canvas mpl_disconnect manager _cidgcf _pylab_helpers Gcf destroy manager num else raise TypeError 'Unrecognizedargumenttype%stoclose' % type arg else raise TypeError 'closetakes0or1arguments'
def set_hsa_kernel fn mod fn modulefn calling_convention CC_SPIR_KERNELocl_kernels mod get_or_insert_named_metadata 'opencl kernels' ocl_kernels add lc MetaData get mod [fn gen_arg_addrspace_md fn gen_arg_access_qual_md fn gen_arg_type fn gen_arg_type_qual fn gen_arg_base_type fn ] make_constant lambda x lc Constant int lc Type int x spir_version_constant [make_constant x for x in SPIR_VERSION]spir_version mod get_or_insert_named_metadata 'opencl spir version' if not spir_version operands spir_version add lc MetaData get mod spir_version_constant ocl_version mod get_or_insert_named_metadata 'opencl ocl version' if not ocl_version operands ocl_version add lc MetaData get mod spir_version_constant
def correlation_row_generator bt pmf category data array [i for i in bt iter_data axis 'observation' ] try cat_vect array [pmf[s][category] for s in bt ids ] dtype float return row cat_vect for row in data except ValueError raise ValueError "Mappingfilecategorycontaineddatathatcouldn't" + "beconvertedtofloat Can'tcontinue "
def test_cache_clear_activated config_stub tmpdir config_stub data {'storage' {'cache-size' 1024} 'general' {'private-browsing' False}}disk_cache cache DiskCache str tmpdir assert disk_cache cacheSize 0 preload_cache disk_cache assert disk_cache cacheSize 0 disk_cache clear assert disk_cache cacheSize 0
@pytest mark skipif 'notHAS_PATHLIB' def test_votable_path_object fpath pathlib Path get_pkg_data_filename 'data/names xml' table parse fpath get_first_table to_table assert len table 1 assert int table[0][3] 266
def metric_init params global descriptorsd1 {'name' 'scribe_overall_messages_per_second' 'call_back' GetOverallMessagesPerSecond 'time_max' 90 'value_type' 'float' 'units' 'msg/sec' 'slope' 'both' 'format' '%f' 'description' 'Averagenumberofmessagessentpersecond' 'groups' 'scribe'}descriptors [d1]return descriptors
def pull_repository repo repository_clone_url ctx_rev commands pull get_configured_ui repo source repository_clone_url rev [ctx_rev]
def outparam key type_ None return BindParameter key None type_ type_ unique False isoutparam True
def env_script registry xml_parent data el XML SubElement xml_parent 'com lookout jenkins EnvironmentScript' XML SubElement el 'script' text data get 'script-content' '' valid_script_types {'unix-script' 'unixScript' 'power-shell' 'powerShell' 'batch-script' 'batchScript'}script_type data get 'script-type' 'unix-script' if script_type not in valid_script_types raise InvalidAttributeError 'script-type' script_type valid_script_types XML SubElement el 'scriptType' text valid_script_types[script_type]only_on_parent str data get 'only-run-on-parent' False lower XML SubElement el 'onlyRunOnParent' text only_on_parent
def wait_ready_prefix popen prefix emulator_ready Falsewhile not emulator_ready emulator_ready popen stderr readline startswith prefix
def wait_ready_prefix popen prefix emulator_ready Falsewhile not emulator_ready emulator_ready popen stderr readline startswith prefix
def get_associations context qos_specs_id try types objects VolumeTypeList get_all_types_for_qos context qos_specs_id except db_exc DBError LOG exception _LE 'DBerror ' msg _ 'Failedtogetallassociationsofqosspecs%s' % qos_specs_id LOG warning msg raise exception CinderException message msg result []for vol_type in types result append {'association_type' 'volume_type' 'name' vol_type name 'id' vol_type id} return result
def compiler_type dist Distribution dist parse_config_files cmd dist get_command_obj 'build' cmd ensure_finalized compiler new_compiler compiler cmd compiler return compiler compiler_type
def CalculateVariables default_variables params generator_flags params get 'generator_flags' {} msvs_version MSVSVersion SelectVisualStudioVersion generator_flags get 'msvs_version' 'auto' params['msvs_version'] msvs_versiondefault_variables['MSVS_VERSION'] msvs_version ShortName if os environ get 'PROCESSOR_ARCHITECTURE' '' find '64' > 0 or os environ get 'PROCESSOR_ARCHITEW6432' '' find '64' > 0 default_variables['MSVS_OS_BITS'] 64else default_variables['MSVS_OS_BITS'] 32if gyp common GetFlavor params 'ninja' default_variables['SHARED_INTERMEDIATE_DIR'] '$ OutDir gen'
def CalculateVariables default_variables params generator_flags params get 'generator_flags' {} msvs_version MSVSVersion SelectVisualStudioVersion generator_flags get 'msvs_version' 'auto' params['msvs_version'] msvs_versiondefault_variables['MSVS_VERSION'] msvs_version ShortName if os environ get 'PROCESSOR_ARCHITECTURE' '' find '64' > 0 or os environ get 'PROCESSOR_ARCHITEW6432' '' find '64' > 0 default_variables['MSVS_OS_BITS'] 64else default_variables['MSVS_OS_BITS'] 32if gyp common GetFlavor params 'ninja' default_variables['SHARED_INTERMEDIATE_DIR'] '$ OutDir gen'
@taskdef tag vs push False patch_version vs repo_root with cd repo_root run 'gitcommit-a-m"release{}"' format vs run 'gittag-a-m"release{0}"{0}' format vs if push run 'gitpush' run 'gitpush--tags'
def putInToC document toc tocOrig domhelpers findElementsWithAttribute document 'class' 'toc' if tocOrig tocOrig tocOrig[0]tocOrig childNodes [toc]
def to_timestamp value return value - epoch total_seconds
def get_data_cache_dir data_dir subdir None data_cache_dir os environ get 'NEON_DATA_CACHE_DIR' if data_cache_dir is None data_cache_dir data_dirif subdir subdir subdir if isinstance subdir list else [subdir] data_cache_dir os path join data_cache_dir *subdir if not os path exists data_cache_dir os makedirs data_cache_dir return data_cache_dir
def xblock_has_own_studio_page xblock parent_xblock None category xblock categoryif is_unit xblock parent_xblock return Trueelif category 'vertical' if parent_xblock is None parent_xblock get_parent_xblock xblock return is_unit parent_xblock if parent_xblock else False return xblock has_children
def xblock_has_own_studio_page xblock parent_xblock None category xblock categoryif is_unit xblock parent_xblock return Trueelif category 'vertical' if parent_xblock is None parent_xblock get_parent_xblock xblock return is_unit parent_xblock if parent_xblock else False return xblock has_children
def add_output_compression_type parser group parser add_mutually_exclusive_group group add_argument u'--gzip' default False action u'store_true' help u'Compressoutputusinggzip' group add_argument u'--bzip' default False action u'store_true' help u'Compressoutputusingbzip2'
def add_variables_to_context generator context generator contextcontext['relpath_to_site'] relpath_to_sitecontext['main_siteurl'] _MAIN_SITEURLcontext['main_lang'] _MAIN_LANGcontext['lang_siteurls'] _SITE_DBcurrent_lang generator settings['DEFAULT_LANG']extra_siteurls _SITE_DB copy extra_siteurls pop current_lang context['extra_siteurls'] extra_siteurls
def check_action_edition_permission view_func def decorate request *args **kwargs action kwargs get 'action' Job objects can_edit_or_exception request action workflow return view_func request *args **kwargs return wraps view_func decorate
def check_action_edition_permission view_func def decorate request *args **kwargs action kwargs get 'action' Job objects can_edit_or_exception request action workflow return view_func request *args **kwargs return wraps view_func decorate
def check_action_edition_permission view_func def decorate request *args **kwargs action kwargs get 'action' Job objects can_edit_or_exception request action workflow return view_func request *args **kwargs return wraps view_func decorate
def interpolate_cache_slug string cache_slug utils get_asset_dir_prefix return parse_string string {'cache_slug' cache_slug}
def interpolate_cache_slug string cache_slug utils get_asset_dir_prefix return parse_string string {'cache_slug' cache_slug}
def reinit hub _get_hub if hub is not None hub loop reinit if hasattr hub threadpool '_on_fork' hub threadpool _on_fork if hasattr hub resolver '_on_fork' hub resolver _on_fork
def reinit hub _get_hub if hub is not None hub loop reinit if hasattr hub threadpool '_on_fork' hub threadpool _on_fork if hasattr hub resolver '_on_fork' hub resolver _on_fork
def kill_processtree pid signum family_pids get_processtree_pids pid for _pid in family_pids os kill _pid signum
def looks_like_issubclass obj classname t objif t __name__ classname return Truefor klass in t __mro__ if klass __name__ classname return Truereturn False
def json_encode obj **kwargs return json dumps obj default json_default **kwargs
def json_encode obj **kwargs return json dumps obj default json_default **kwargs
def create_key policy None description None key_usage None region None key None keyid None profile None conn _get_conn region region key key keyid keyid profile profile r {}_policy json serialize policy try key_metadata conn create_key _policy description description key_usage key_usage r['key_metadata'] key_metadata['KeyMetadata']except boto exception BotoServerError as e r['error'] __utils__['boto get_error'] e return r
def do_votes_by_user parser token bits token contents split if len bits 6 raise template TemplateSyntaxError "'%s'tagtakesexactlyfourarguments" % bits[0] if bits[2] 'on' raise template TemplateSyntaxError "secondargumentto'%s'tagmustbe'on'" % bits[0] if bits[4] 'as' raise template TemplateSyntaxError "fourthargumentto'%s'tagmustbe'as'" % bits[0] return VotesByUserNode bits[1] bits[3] bits[5]
def do_votes_by_user parser token bits token contents split if len bits 6 raise template TemplateSyntaxError "'%s'tagtakesexactlyfourarguments" % bits[0] if bits[2] 'on' raise template TemplateSyntaxError "secondargumentto'%s'tagmustbe'on'" % bits[0] if bits[4] 'as' raise template TemplateSyntaxError "fourthargumentto'%s'tagmustbe'as'" % bits[0] return VotesByUserNode bits[1] bits[3] bits[5]
def assert_has_element_with_path output path if xml_find output path is None errmsg 'ExpectedtofindXMLelementmatchingexpression%s notsuchmatchwasfound ' % path raise AssertionError errmsg
def assert_has_element_with_path output path if xml_find output path is None errmsg 'ExpectedtofindXMLelementmatchingexpression%s notsuchmatchwasfound ' % path raise AssertionError errmsg
def assert_has_element_with_path output path if xml_find output path is None errmsg 'ExpectedtofindXMLelementmatchingexpression%s notsuchmatchwasfound ' % path raise AssertionError errmsg
@register inclusion_tag 'inclusion html' takes_context True def inclusion_no_params_with_context context return {'result' 'inclusion_no_params_with_context-Expectedresult contextvalue %s ' % context['value'] }
def unique_labels list_of_labels list_of_labels [idx for idx labels in enumerate list_of_labels ]return list_of_labels
def unique_labels list_of_labels list_of_labels [idx for idx labels in enumerate list_of_labels ]return list_of_labels
def get_snapshot_paths service file_list []if service 'cassandra' return file_listlook_for 'snapshots'data_dir '{0}/{1}' format APPSCALE_DATA_DIR service for full_path _ file in os walk data_dir if look_for in full_path file_list append full_path logging debug "Listofdatapathsfor'{0}' {1}" format service file_list return file_list
def get_snapshot_paths service file_list []if service 'cassandra' return file_listlook_for 'snapshots'data_dir '{0}/{1}' format APPSCALE_DATA_DIR service for full_path _ file in os walk data_dir if look_for in full_path file_list append full_path logging debug "Listofdatapathsfor'{0}' {1}" format service file_list return file_list
def build_cors_request url origin_header_value headers Headers if origin_header_value is not None headers['Origin'] origin_header_value strip forged_req FuzzableRequest url 'GET' headers headers return forged_req
def get_gridline_path world pixel mask np isnan pixel[ 0] np isnan pixel[ 1] codes np zeros world shape[0] dtype np uint8 codes[ ] Path LINETOcodes[0] Path MOVETOcodes[mask] Path MOVETOcodes[1 ][mask[ -1 ]] Path MOVETOpath Path pixel codes codes return path
def fileobj_is_binary f if hasattr f 'binary' return f binaryif io is not None and isinstance f io TextIOBase return Falsemode fileobj_mode f if mode return 'b' in mode else return True
def fileobj_is_binary f if hasattr f 'binary' return f binaryif io is not None and isinstance f io TextIOBase return Falsemode fileobj_mode f if mode return 'b' in mode else return True
@utils arg 'host' metavar '<host>' help _ 'Nameofhost ' @utils arg 'action' metavar '<action>' choices ['set' 'delete'] help _ "Actions 'set'or'delete'" @utils arg 'metadata' metavar '<key value>' nargs '+' action 'append' default [] help _ 'Metadatatosetordelete onlykeyisnecessaryondelete ' def do_host_meta cs args hypervisors cs hypervisors search args host servers True for hyper in hypervisors metadata _extract_metadata args if hasattr hyper 'servers' for server in hyper servers if args action 'set' cs servers set_meta server['uuid'] metadata elif args action 'delete' cs servers delete_meta server['uuid'] metadata keys
def rebuild_all_translation_files for lang in get_all_languages for app in frappe get_all_apps write_translations_file app lang
def _copy_dist_from_dir link_path location if os path isdir location rmtree location setup_py 'setup py'sdist_args [sys executable]sdist_args append '-c' sdist_args append SETUPTOOLS_SHIM % setup_py sdist_args append 'sdist' sdist_args + ['--dist-dir' location]logger info 'Runningsetup pysdistfor%s' link_path with indent_log call_subprocess sdist_args cwd link_path show_stdout False sdist os path join location os listdir location [0] logger info 'Unpackingsdist%sinto%s' sdist location unpack_file sdist location content_type None link None
def _copy_dist_from_dir link_path location if os path isdir location rmtree location setup_py 'setup py'sdist_args [sys executable]sdist_args append '-c' sdist_args append SETUPTOOLS_SHIM % setup_py sdist_args append 'sdist' sdist_args + ['--dist-dir' location]logger info 'Runningsetup pysdistfor%s' link_path with indent_log call_subprocess sdist_args cwd link_path show_stdout False sdist os path join location os listdir location [0] logger info 'Unpackingsdist%sinto%s' sdist location unpack_file sdist location content_type None link None
def GetLocalUser return pwd getpwuid os getuid [0] or os getlogin
@pytest mark parametrize u'gframe' gcrs_frames def test_icrs_gcrs_dist_diff gframe gcrsnod icrs_coords[0] transform_to gframe gcrswd icrs_coords[1] transform_to gframe assert not allclose gcrswd ra gcrsnod ra rtol 1e-08 atol 1e-10 * u deg assert not allclose gcrswd dec gcrsnod dec rtol 1e-08 atol 1e-10 * u deg assert not allclose gcrswd distance icrs_coords[1] distance rtol 1e-08 atol 1e-10 * u pc
def feature_permission url option msg yes_action no_action abort_on config_val config get *option if config_val 'ask' if url isValid text 'Allowthewebsiteat<b>{}</b>to{}?' format html escape url toDisplayString msg else text 'Allowthewebsiteto{}?' format msg return message confirm_async yes_action yes_action no_action no_action cancel_action no_action abort_on abort_on title 'Permissionrequest' text text elif config_val yes_action return Noneelse no_action return None
def rm_subdirs path onerror None names []try names os listdir path except os error as err if onerror is not None onerror os listdir path sys exc_info else raisefor name in names fullname os path join path name if os path isdir fullname if onerror is not None shutil rmtree fullname False onerror else err_count 0while True try shutil rmtree fullname False None breakexcept os error if err_count > 0 raiseerr_count + 1time sleep RM_SUBDIRS_RETRY_TIME
def _cherry_pick_args func args dargs if func func_code co_flags & 4 p_args argselse p_args if func func_code co_flags & 8 p_dargs dargselse p_dargs {}for param in _get_nonstar_args func if param in dargs p_dargs[param] dargs[param]return p_args p_dargs
def _cherry_pick_args func args dargs if func func_code co_flags & 4 p_args argselse p_args if func func_code co_flags & 8 p_dargs dargselse p_dargs {}for param in _get_nonstar_args func if param in dargs p_dargs[param] dargs[param]return p_args p_dargs
def disable_signing **kwargs return botocore UNSIGNED
def disable_signing **kwargs return botocore UNSIGNED
def disable_signing **kwargs return botocore UNSIGNED
def local_bitwidth return struct calcsize 'P' * 8
def local_bitwidth return struct calcsize 'P' * 8
def threads thread_yields [100 200 500 1000]thread_locks [2 4 8 16]test_command 'sysbench--num-threads 64--test threads'test_command + '--thread-yields {0}--thread-locks {1}run'result Noneret_val {}for yields locks in zip thread_yields thread_locks key 'Yields {0}Locks {1}' format yields locks run_command test_command format yields locks result __salt__['cmd run'] run_command ret_val[key] _parser result return ret_val
def _aggregate input options output timeFieldName aggregator Aggregator aggregationInfo options inputFields input getFields timeFieldName timeFieldName while True inRecord input getNextRecord print 'Feedingin ' inRecord outRecord aggBookmark aggregator next record inRecord curInputBookmark None print 'Recordout ' outRecordif outRecord is not None output appendRecord outRecord None if inRecord is None and outRecord is None break
def get_datastore_ref si datastore_name inventory get_inventory si container inventory viewManager CreateContainerView inventory rootFolder [vim Datastore] True for item in container view if item name datastore_name return itemreturn None
def get_datastore_ref si datastore_name inventory get_inventory si container inventory viewManager CreateContainerView inventory rootFolder [vim Datastore] True for item in container view if item name datastore_name return itemreturn None
def filter_factory global_conf **local_conf conf global_conf copy conf update local_conf def swift3_filter app return Swift3Middleware app conf return swift3_filter
def filter_factory global_conf **local_conf conf global_conf copy conf update local_conf def swift3_filter app return Swift3Middleware app conf return swift3_filter
@pytest mark skipif 'no_real_s3_credentials ' def test_subdomain_compatible creds Credentials os getenv 'AWS_ACCESS_KEY_ID' os getenv 'AWS_SECRET_ACCESS_KEY' bucket_name bucket_name_mangle 'wal-e-test-us-west-1-no-dots' cinfo calling_format from_store_name bucket_name with FreshBucket bucket_name host 's3-us-west-1 amazonaws com' calling_format connection OrdinaryCallingFormat as fb fb create location 'us-west-1' conn cinfo connect creds assert cinfo region is None assert cinfo calling_format is connection SubdomainCallingFormat assert isinstance conn calling_format connection SubdomainCallingFormat
def output_file filename title 'BokehPlot' mode 'cdn' root_dir None _state output_file filename title title mode mode root_dir root_dir
@pytest mark parametrize 'fast_writer' [True False] def test_write_fill_masked_different fast_writer data ascii read tab_to_fill data table Table data masked True data['a'] mask [True False]data['c'] mask [False True]for test_def in test_def_masked_fill_value check_write_table test_def data fast_writer
def _get_username username __salt__['config get'] 'mattermost username' or __salt__['config get'] 'mattermost username' return username
def test_start_after_join group worker WalTransferGroup FakeWalUploader group join seg FakeWalSegment 'arbitrary' with pytest raises UserCritical group start seg
def get_container_view service_instance obj_type container None if not container container service_instance content rootFolderview_ref service_instance content viewManager CreateContainerView container container type obj_type recursive True return view_ref
def get_container_view service_instance obj_type container None if not container container service_instance content rootFolderview_ref service_instance content viewManager CreateContainerView container container type obj_type recursive True return view_ref
def bytes text if text is None return ''if sys version_info < 3 0 import __builtin__return __builtin__ bytes text else import builtinsif isinstance text builtins bytes return textif isinstance text list return builtins bytes text else return builtins bytes text encoding 'utf-8'
def bytes text if text is None return ''if sys version_info < 3 0 import __builtin__return __builtin__ bytes text else import builtinsif isinstance text builtins bytes return textif isinstance text list return builtins bytes text else return builtins bytes text encoding 'utf-8'
def add_doc_link app pagename templatename context doctree if not app config github_user and app config github_project returnsource_suffix app config source_suffixsource_suffix source_suffix if isinstance source_suffix basestring else source_suffix[0] context['github_link'] lambda mode 'edit' make_github_link app 'doc/%s%s' % pagename source_suffix mode mode
@pytest fixture scope u'function' def remove_test_dir request def fin_remove_test_dir if os path exists u'test_copy_without_render' utils rmtree u'test_copy_without_render' request addfinalizer fin_remove_test_dir
def divergence vect coord_sys return coord_sys delop dot vect doit
def send_alert_confirmation alert ctx Context {'alert' alert 'site' Site objects get_current } subject_tpl loader get_template 'customer/alerts/emails/confirmation_subject txt' body_tpl loader get_template 'customer/alerts/emails/confirmation_body txt' mail send_mail subject_tpl render ctx strip body_tpl render ctx settings OSCAR_FROM_EMAIL [alert email]
def _clean_order order order re findall '\\ [0-9]*?\\ ' order clean lambda x tuple map int re sub '[ ]' '' x split '' if len order > 1 order sorder map clean order else order clean order[0] sorder 0 0 0 return order sorder
def parse template delimiters None if type template is not unicode raise Exception 'Templateisnotunicode %s' % type template parser _Parser delimiters return parser parse template
def get_switchport port module command 'showinterface{0}switchport' format port body execute_show_command command module try body execute_show_command command module [0]except IndexError body []if body key_map {'interface' 'interface' 'oper_mode' 'mode' 'switchport' 'switchport' 'access_vlan' 'access_vlan' 'access_vlan_name' 'access_vlan_name' 'native_vlan' 'native_vlan' 'native_vlan_name' 'native_vlan_name' 'trunk_vlans' 'trunk_vlans'}sp_table body['TABLE_interface']['ROW_interface']sp apply_key_map key_map sp_table return spelse return {}
def ansible_dict_to_boto3_filter_list filters_dict filters_list []for k v in filters_dict items filter_dict {'Name' k}if isinstance v string_types filter_dict['Values'] [v]else filter_dict['Values'] vfilters_list append filter_dict return filters_list
def ansible_dict_to_boto3_filter_list filters_dict filters_list []for k v in filters_dict items filter_dict {'Name' k}if isinstance v string_types filter_dict['Values'] [v]else filter_dict['Values'] vfilters_list append filter_dict return filters_list
def canonicalize_email email if not email return ''email _force_utf8 email lower localpart at domain email partition '@' if not at or '@' in domain return ''localpart localpart replace ' ' '' localpart localpart partition '+' [0]return localpart + '@' + domain
def cleanup style None def make_cleanup func if inspect isgeneratorfunction func @functools wraps func def wrapped_callable *args **kwargs original_units_registry matplotlib units registry copy original_settings mpl rcParams copy matplotlib style use style try for yielded in func *args **kwargs yield yielded finally _do_cleanup original_units_registry original_settings else @functools wraps func def wrapped_callable *args **kwargs original_units_registry matplotlib units registry copy original_settings mpl rcParams copy matplotlib style use style try func *args **kwargs finally _do_cleanup original_units_registry original_settings return wrapped_callableif isinstance style six string_types return make_cleanupelse result make_cleanup style style u'classic'return result
def regions return get_regions 'cloudwatch' connection_cls CloudWatchConnection
def OutHeader text dash n 64 - len text Output Output '/*%s%s%s*/' dash * n / 2 text dash * n - n / 2 Output
def OutHeader text dash n 64 - len text Output Output '/*%s%s%s*/' dash * n / 2 text dash * n - n / 2 Output
def IsDerivedFunction clean_lines linenum for i in xrange linenum max -1 linenum - 10 -1 match Match '^ [^ ]*\\w+ \\ ' clean_lines elided[i] if match line _ closing_paren CloseExpression clean_lines i len match group 1 return closing_paren > 0 and Search '\\boverride\\b' line[closing_paren ] return False
def test_merge_type_mismatch merge_log_err packages {'pack_1' {'input_boolean' [{'ib1' None}]} 'pack_11' {'input_select' {'ib1' None}} 'pack_2' {'light' {'ib1' None}}}config {config_util CONF_CORE {config_util CONF_PACKAGES packages} 'input_boolean' {'ib2' None} 'input_select' [{'ib2' None}] 'light' [{'platform' 'two'}]}config_util merge_packages_config config packages assert merge_log_err call_count 2 assert len config 4 assert len config['input_boolean'] 1 assert len config['light'] 2
def wraps function def wrap decorator decorator functools wraps function decorator if not hasattr function 'original' decorator original functionelse decorator original function originaldelattr function 'original' return decoratorreturn wrap
def twinx ax None if ax is None ax gca ax1 ax twinx return ax1
def twinx ax None if ax is None ax gca ax1 ax twinx return ax1
def generate_video v params [] sources []if v webm sources append {'src' _get_video_url v webm 'type' 'webm'} if v ogv sources append {'src' _get_video_url v ogv 'type' 'ogg'} data_fallback ''if v flv data_fallback _get_video_url v flv return render_to_string 'wikiparser/hook_video html' {'fallback' data_fallback 'sources' sources 'params' params 'video' v 'height' settings WIKI_VIDEO_HEIGHT 'width' settings WIKI_VIDEO_WIDTH}
def generate_video v params [] sources []if v webm sources append {'src' _get_video_url v webm 'type' 'webm'} if v ogv sources append {'src' _get_video_url v ogv 'type' 'ogg'} data_fallback ''if v flv data_fallback _get_video_url v flv return render_to_string 'wikiparser/hook_video html' {'fallback' data_fallback 'sources' sources 'params' params 'video' v 'height' settings WIKI_VIDEO_HEIGHT 'width' settings WIKI_VIDEO_WIDTH}
def namespace_to_regex namespace db_name coll_name namespace split ' ' 1 db_regex re escape db_name replace '\\*' ' [^ ]* ' coll_regex re escape coll_name replace '\\*' ' * ' return re compile '\\A' + db_regex + '\\ ' + coll_regex + '\\Z'
def scale_taxa_data_matrix coords pct_var return coords[ len pct_var ] * pct_var / pct_var max
def date_to_key date return int date strftime '%Y%m%d'
def group_to_gid group if group is None return ''try if isinstance group int return groupreturn grp getgrnam group gr_gidexcept KeyError return ''
def GetActiveFileName bAutoSave 1 pathName Noneactive GetActiveView if active is None return Nonetry doc active GetDocument pathName doc GetPathName if bAutoSave and len pathName > 0 or doc GetTitle [ 8] 'Untitled' or doc GetTitle [ 6] 'Script' if doc IsModified try doc OnSaveDocument pathName pathName doc GetPathName linecache clearcache except win32ui error raise KeyboardInterruptexcept win32ui error AttributeError passif not pathName return Nonereturn pathName
def pPca data dim num data shape[1]data asmatrix makeCentered data W asmatrix standard_normal num dim W_ W[ ]while True E inv W T * W * W T * data T W W_ data T * E T * inv E * E T W if abs W - W_ max < 0 001 breakreturn W T
def run_shell use_plain False from django db models loading import get_modelsloaded_models get_models try if use_plain raise ImportErrorimport IPythonshell IPython Shell IPShell argv [] shell mainloop except ImportError import codetry import readlineexcept ImportError passelse import rlcompleterreadline parse_and_bind 'tab complete' code interact
def src filename if filename is None return filenameif sys platform startswith 'java' and filename endswith '$py class' return ' ' join filename[ -9 ] 'py' base ext os path splitext filename if ext in ' pyc' ' pyo' ' py' return ' ' join base 'py' return filename
def normpath path path syspath path prefix False path os path normpath os path abspath os path expanduser path return bytestring_path path
def tiny2zero x eps 1e-15 mask np abs x copy < eps x[mask] 0return x
def get completion try return _instances[completion]except KeyError if completion in INITIALIZERS INITIALIZERS[completion] return _instances[completion]else raise
def data_for_url url path url path host url host log misc debug 'url {} path {} host{}' format url toDisplayString path host try handler _HANDLERS[path]except KeyError try handler _HANDLERS[host]except KeyError raise NoHandlerFound url try mimetype data handler url except OSError as e raise QuteSchemeOSError e except QuteSchemeError as e raiseassert mimetype is not None urlif mimetype 'text/html' and isinstance data str data data encode 'utf-8' errors 'xmlcharrefreplace' return mimetype data
def _ssh_run_with_recursion ssh_bin address ec2_key_pair_file keyfile cmd_args if ' ' in address if keyfile is None raise ValueError 'SSHkeyfilepathcannotbeNone' host1 host2 address split ' ' more_args ['ssh' '-i' keyfile '-o' 'StrictHostKeyChecking no' '-o' 'UserKnownHostsFile /dev/null' 'hadoop@%s' % host2 ]return _ssh_run ssh_bin host1 ec2_key_pair_file more_args + list cmd_args else return _ssh_run ssh_bin address ec2_key_pair_file cmd_args
def list_states saltenv 'base' return __context__['fileclient'] list_states saltenv
def json_from_url url error_message ''url_handle urllib urlopen url url_contents url_handle read try parsed_json json loads url_contents except Exception as e error_message str url_contents print 'ErrorparsingJSONdatainjson_from_url ' str e return None error_message return parsed_json error_message
def show_path_changes path_changes sources destinations zip *path_changes sources list map util displayable_path sources destinations list map util displayable_path destinations col_width term_width - len '->' // 2 max_width len max sources + destinations key len if max_width > col_width for source dest in zip sources destinations log info u'{0}\n->{1}' source dest else title_pad max_width - len 'Source' + len '->' log info u'Source{0}Destination' '' * title_pad for source dest in zip sources destinations pad max_width - len source log info u'{0}{1}->{2}' source '' * pad dest
@core_helperdef resource_view_display_preview resource_view view_plugin datapreview get_view_plugin resource_view['view_type'] return view_plugin info get 'preview_enabled' True
def test_temporary_files_failed_cleanup caplog qtbot py_proc runner cmd args py_proc "\nimportos\nos remove os environ['QUTE_HTML'] \n" with caplog at_level logging ERROR with qtbot waitSignal runner finished timeout 10000 runner prepare_run cmd *args runner store_text '' runner store_html '' assert len caplog records 1 expected 'Failedtodeletetempfile'assert caplog records[0] message startswith expected
def must_have_write_permission_or_public_wiki func @functools wraps func def wrapped *args **kwargs _inject_nodes kwargs wiki kwargs['node'] get_addon 'wiki' if wiki and wiki is_publicly_editable return func *args **kwargs else return must_have_permission 'write' func *args **kwargs return wrapped
@register filterdef friends user try return Relationship objects get_friends_for_user user except AttributeError return []
def assert_arping src_namespace dst_ip source None timeout 1 count 1 ns_ip_wrapper ip_lib IPWrapper src_namespace arping_cmd ['arping' '-c' count '-w' timeout]if source arping_cmd extend ['-s' source] arping_cmd append dst_ip ns_ip_wrapper netns execute arping_cmd
def log_loss actual predicted return np mean ll actual predicted
def stash model StashModel view StashView model qtutils active_window view show view raise_ return view
def stash model StashModel view StashView model qtutils active_window view show view raise_ return view
def send_message token chat_id text disable_web_page_preview None reply_to_message_id None reply_markup None parse_mode None disable_notification None method_url 'sendMessage'payload {'chat_id' str chat_id 'text' text}if disable_web_page_preview payload['disable_web_page_preview'] disable_web_page_previewif reply_to_message_id payload['reply_to_message_id'] reply_to_message_idif reply_markup payload['reply_markup'] _convert_markup reply_markup if parse_mode payload['parse_mode'] parse_modeif disable_notification payload['disable_notification'] disable_notificationreturn _make_request token method_url params payload method 'post'
def Condition *args **kwargs return _Condition *args **kwargs
def get_move_page_id_list user site check_global True use_cache True page_ids _get_page_ids_for_action user user site site action 'move_page' check_global check_global use_cache use_cache return page_ids
def serve_webapp webapp port None host None server Noneif port is not None server httpserver serve webapp host host port port start_loop False else random seed for i in range 0 9 try port str random randint 8000 10000 server httpserver serve webapp host host port port start_loop False breakexcept socket error as e if e[0] 98 continueraiseelse raise Exception 'Unabletoopenaportbetween%sand%stostartGalaxyserver' % 8000 1000 t threading Thread target server serve_forever t start return server port
def _example_short_number_for_cost region_code cost metadata PhoneMetadata short_metadata_for_region region_code if metadata is None return U_EMPTY_STRINGdesc Noneif cost ShortNumberCost TOLL_FREE desc metadata toll_freeelif cost ShortNumberCost STANDARD_RATE desc metadata standard_rateelif cost ShortNumberCost PREMIUM_RATE desc metadata premium_rateelse passif desc is not None and desc example_number is not None return desc example_numberreturn U_EMPTY_STRING
def test_pmf_hist_basics out utils pmf_hist a_norm assert_equal len out 3 x h w outassert_equal len x len h a np arange 10 x h w utils pmf_hist a 10 nose tools assert_true np all h h[0]
def expand_path path return os path expandvars os path expanduser path
def group_snapshot_create context values return IMPL group_snapshot_create context values
@contextmanagerdef use_step_import_modules step_container orig_modules {}import_context StepImportModuleContext step_container with _step_import_lock try for module_name fake_module in six iteritems import_context modules orig_module sys modules get module_name unknown orig_modules[module_name] orig_modulesys modules[module_name] fake_module yield import_context finally for module_name orig_module in six iteritems orig_modules if orig_module is unknown del sys modules[module_name]else sys modules[module_name] orig_module
@contextmanagerdef use_step_import_modules step_container orig_modules {}import_context StepImportModuleContext step_container with _step_import_lock try for module_name fake_module in six iteritems import_context modules orig_module sys modules get module_name unknown orig_modules[module_name] orig_modulesys modules[module_name] fake_module yield import_context finally for module_name orig_module in six iteritems orig_modules if orig_module is unknown del sys modules[module_name]else sys modules[module_name] orig_module
@contextmanagerdef use_step_import_modules step_container orig_modules {}import_context StepImportModuleContext step_container with _step_import_lock try for module_name fake_module in six iteritems import_context modules orig_module sys modules get module_name unknown orig_modules[module_name] orig_modulesys modules[module_name] fake_module yield import_context finally for module_name orig_module in six iteritems orig_modules if orig_module is unknown del sys modules[module_name]else sys modules[module_name] orig_module
def map_bits fn n while n b n & ~ n + 1 yield fn b n ^ b
def generate_secret from Crypto Random import get_random_bytesbytes get_random_bytes 20 encoded base64 b32encode bytes return encoded
def createXYs x y None if y is None y xxs numpy resize x len x * len y ys numpy repeat y len x return numpy vstack [xs ys] transpose
def get_category_or_404 path path_bits [p for p in path split '/' if p]return get_object_or_404 Category slug path_bits[ -1 ]
def get_category_or_404 path path_bits [p for p in path split '/' if p]return get_object_or_404 Category slug path_bits[ -1 ]
def transform cls if cls name in NEED_FIX for f in FIX_MEMBERS cls locals[f] [scoped_nodes Class f None ]
def transform cls if cls name in NEED_FIX for f in FIX_MEMBERS cls locals[f] [scoped_nodes Class f None ]
def transform cls if cls name in NEED_FIX for f in FIX_MEMBERS cls locals[f] [scoped_nodes Class f None ]
def default_fused_keys_renamer keys typ type keys[0] if typ is str or typ is unicode names [key_split x for x in keys[ 0 -1 ]]names append keys[0] return '-' join names elif typ is tuple and len keys[0] > 0 and isinstance keys[0][0] str unicode names [key_split x for x in keys[ 0 -1 ]]names append keys[0][0] return '-' join names + keys[0][1 ] else return None
def test_commented_scenarios scenario Scenario from_string COMMENTED_SCENARIO assert_equals scenario name u'Addingsomestudentstomyuniversitydatabase' assert_equals len scenario steps 4
def test_commented_scenarios scenario Scenario from_string COMMENTED_SCENARIO assert_equals scenario name u'Addingsomestudentstomyuniversitydatabase' assert_equals len scenario steps 4
def virtual_interface_get_by_uuid context vif_uuid return IMPL virtual_interface_get_by_uuid context vif_uuid
def _symlink_check name target force user group pchanges {}if not os path exists name and not __salt__['file is_link'] name pchanges['new'] namereturn None 'Symlink{0}to{1}issetforcreation' format name target pchanges if __salt__['file is_link'] name if __salt__['file readlink'] name target pchanges['change'] namereturn None 'Link{0}targetissettobechangedto{1}' format name target pchanges else result Truemsg 'Thesymlink{0}ispresent' format name if not _check_symlink_ownership name user group result Nonepchanges['ownership'] '{0} {1}' format *_get_symlink_ownership name msg + ' buttheownershipofthesymlinkwouldbechangedfrom{2} {3}to{0} {1}' format user group *_get_symlink_ownership name return result msg pchanges else if force return None 'Thefileordirectory{0}issetforremovaltomakewayforanewsymlinktargeting{1}' format name target pchanges return False 'Fileordirectoryexistswherethesymlink{0}shouldbe Didyoumeantouseforce?' format name pchanges
def service_get_by_args context host binary return IMPL service_get_by_args context host binary
def clear color None if color is None _sensehat clear else _sensehat clear color return {'color' color}
def get_server_certs iam name None results dict try if name server_certs [iam get_server_certificate ServerCertificateName name ['ServerCertificate']]else server_certs iam list_server_certificates ['ServerCertificateMetadataList']for server_cert in server_certs if not name server_cert iam get_server_certificate ServerCertificateName server_cert['ServerCertificateName'] ['ServerCertificate']cert_md server_cert['ServerCertificateMetadata']results[cert_md['ServerCertificateName']] {'certificate_body' server_cert['CertificateBody'] 'server_certificate_id' cert_md['ServerCertificateId'] 'server_certificate_name' cert_md['ServerCertificateName'] 'arn' cert_md['Arn'] 'path' cert_md['Path'] 'expiration' cert_md['Expiration'] isoformat 'upload_date' cert_md['UploadDate'] isoformat }except botocore exceptions ClientError passreturn results
def blame_upstream registry xml_parent data XML SubElement xml_parent 'hudson plugins blame__upstream__commiters BlameUpstreamCommitersPublisher'
def blame_upstream registry xml_parent data XML SubElement xml_parent 'hudson plugins blame__upstream__commiters BlameUpstreamCommitersPublisher'
def blame_upstream registry xml_parent data XML SubElement xml_parent 'hudson plugins blame__upstream__commiters BlameUpstreamCommitersPublisher'
def is_exp_summary_editable exp_summary user_id None return user_id is not None and user_id in exp_summary editor_ids or user_id in exp_summary owner_ids or exp_summary community_owned
def is_exp_summary_editable exp_summary user_id None return user_id is not None and user_id in exp_summary editor_ids or user_id in exp_summary owner_ids or exp_summary community_owned
def isValidQuicktimeVideoURL field_data all_data uc URLMimeTypeCheck 'video/quicktime' 'video/mpeg' try uc field_data all_data except URLMimeTypeCheck InvalidContentType raise ValidationError gettext 'TheURL%sdoesnotpointtoavalidQuickTimevideo ' % field_data
def get_array_memory_extents context builder arrty arr shapes strides data lower upper offset_bounds_from_strides context builder arrty arr shapes strides return compute_memory_extents context builder lower upper data
def _classOfMethod methodObject if _PY3 return methodObject __self__ __class__return methodObject im_class
def parse_userdata options userdata options['userdata']if userdata try if userdata startswith '@' try with open userdata[1 ] as f return json load f except IOError as e usage options 'Invaliduserdatafile {}' format e strerror else return json loads userdata except ValueError as e usage options 'Invaliduserdata {}' format e args[0] return None
def _resize_part_and_fs dev start old_sectors new_sectors size new_sectors - start end new_sectors - 1 dev_path utils make_dev_path dev partition_path utils make_dev_path dev partition 1 utils execute 'e2fsck' '-f' '-y' partition_path run_as_root True check_exit_code [0 1 2] utils execute 'tune2fs' '-O^has_journal' partition_path run_as_root True if new_sectors < old_sectors utils execute 'resize2fs' partition_path '%ds' % size run_as_root True utils execute 'parted' '--script' dev_path 'rm' '1' run_as_root True utils execute 'parted' '--script' dev_path 'mkpart' 'primary' '%ds' % start '%ds' % end run_as_root True if new_sectors > old_sectors utils execute 'resize2fs' partition_path run_as_root True utils execute 'tune2fs' '-j' partition_path run_as_root True
def _get_cost_functions cost_fns_conf CONF least_cost_functionsif cost_fns_conf is None fn_str 'nova scheduler least_cost compute_fill_first_cost_fn'cost_fns_conf [fn_str]cost_fns []for cost_fn_str in cost_fns_conf short_name cost_fn_str split ' ' [ -1 ]if not short_name startswith 'compute_' or short_name startswith 'noop' continueif cost_fn_str startswith 'nova scheduler least_cost ' cost_fn_str 'nova scheduler weights least_cost' + cost_fn_str[25 ] try cost_fn importutils import_class cost_fn_str except ImportError raise exception SchedulerCostFunctionNotFound cost_fn_str cost_fn_str try flag_name '%s_weight' % cost_fn __name__ weight getattr CONF flag_name except AttributeError raise exception SchedulerWeightFlagNotFound flag_name flag_name if flag_name 'compute_fill_first_cost_fn_weight' and weight is None weight -1 0 cost_fns append weight cost_fn return cost_fns
def cleanup print green '%s Cleaningup' % env host with cd '/home/web2py/applications/eden/' run '/bin/rm-rfcompiled' pty True run 'find -name* BASE-print xargs/bin/rm-f' pty True run 'find -name* THIS-print xargs/bin/rm-f' pty True run 'foriin`find -name* OTHER` domv$i${i/ OTHER/} done' pty True run 'bzrresolve' pty True print green '%s RestoringCustomisations' % env host env warn_only Truerun 'patch-f-p0</root/custom diff' pty True env warn_only False
@parametrize 'varname' [attr for attr in dir tables if not attr startswith '_' ] def test_variable_names varname table getattr tables varname try if not issubclass table tables TableBase or table is tables TableBase returnexcept TypeError returnclassname table __name__if classname and varname[0] isupper assert varname classname '%srefersto%s' % varname classname
def clear_persistent_graph_cache request request facebook Nonerequest session delete 'graph' if request user is_authenticated profile get_profile request user profile clear_access_token
def polarify eq subs True lift False if lift subs Falseeq _polarify sympify eq lift if not subs return eqreps {s Dummy s name polar True for s in eq free_symbols}eq eq subs reps return eq {r s for s r in reps items }
def CheckRedundantOverrideOrFinal filename clean_lines linenum error line clean_lines elided[linenum]declarator_end line rfind ' ' if declarator_end > 0 fragment line[declarator_end ]elif linenum > 1 and clean_lines elided[ linenum - 1 ] rfind ' ' > 0 fragment lineelse returnif Search '\\boverride\\b' fragment and Search '\\bfinal\\b' fragment error filename linenum 'readability/inheritance' 4 '"override"isredundantsincefunctionisalreadydeclaredas"final"'
def maybe_shift_divisions df periods freq if isinstance freq str freq pd tseries frequencies to_offset freq if isinstance freq pd DateOffset and freq isAnchored or not hasattr freq 'delta' return df clear_divisions if df known_divisions divs pd Series range len df divisions index df divisions divisions divs shift periods freq freq indexreturn type df df dask df _name df _meta divisions return df
def maybe_shift_divisions df periods freq if isinstance freq str freq pd tseries frequencies to_offset freq if isinstance freq pd DateOffset and freq isAnchored or not hasattr freq 'delta' return df clear_divisions if df known_divisions divs pd Series range len df divisions index df divisions divisions divs shift periods freq freq indexreturn type df df dask df _name df _meta divisions return df
def maybe_shift_divisions df periods freq if isinstance freq str freq pd tseries frequencies to_offset freq if isinstance freq pd DateOffset and freq isAnchored or not hasattr freq 'delta' return df clear_divisions if df known_divisions divs pd Series range len df divisions index df divisions divisions divs shift periods freq freq indexreturn type df df dask df _name df _meta divisions return df
def maybe_shift_divisions df periods freq if isinstance freq str freq pd tseries frequencies to_offset freq if isinstance freq pd DateOffset and freq isAnchored or not hasattr freq 'delta' return df clear_divisions if df known_divisions divs pd Series range len df divisions index df divisions divisions divs shift periods freq freq indexreturn type df df dask df _name df _meta divisions return df
def humanize_speed speed if speed > 1000000000 and speed % 1000000000 0 return '{}Tbps' format speed / 1000000000 elif speed > 1000000 and speed % 1000000 0 return '{}Gbps' format speed / 1000000 elif speed > 1000 and speed % 1000 0 return '{}Mbps' format speed / 1000 elif speed > 1000 return '{}Mbps' format float speed / 1000 else return '{}Kbps' format speed
def humanize_speed speed if speed > 1000000000 and speed % 1000000000 0 return '{}Tbps' format speed / 1000000000 elif speed > 1000000 and speed % 1000000 0 return '{}Gbps' format speed / 1000000 elif speed > 1000 and speed % 1000 0 return '{}Mbps' format speed / 1000 elif speed > 1000 return '{}Mbps' format float speed / 1000 else return '{}Kbps' format speed
def to_pgraster rast if rast is None or rast '' returnrasterheader 1 0 len rast bands rast scale x rast scale y rast origin x rast origin y rast skew x rast skew y rast srs srid rast width rast height result pack POSTGIS_HEADER_STRUCTURE rasterheader for band in rast bands structure 'B' + GDAL_TO_STRUCT[band datatype ] pixeltype GDAL_TO_POSTGIS[band datatype ]if band nodata_value is not None pixeltype + 64bandheader pack structure pixeltype band nodata_value or 0 band_data_hex binascii hexlify band data as_memoryview True upper result + bandheader + band_data_hex return result decode
def makeCgiPrintEncoding encoding def cgiPrint s 'Encodethegivenstringusingthe%sencoding andreplace\ncharsoutsidethegivencharsetwithXMLcharreferences ' % encoding s escape s quote 1 if isinstance s unicode s s encode encoding 'xmlcharrefreplace' return sreturn cgiPrint
def _run_subsuite args runner_class subsuite_index subsuite failfast argsrunner runner_class failfast failfast result runner run subsuite return subsuite_index result events
def searchForNeededEpisodes results []for curShow in sickrage srCore SHOWLIST if curShow paused continueepisodes wantedEpisodes curShow date fromordinal 1 result searchProviders curShow episodes cacheOnly True if result results + resultreturn results
def searchForNeededEpisodes results []for curShow in sickrage srCore SHOWLIST if curShow paused continueepisodes wantedEpisodes curShow date fromordinal 1 result searchProviders curShow episodes cacheOnly True if result results + resultreturn results
def get_filter_name field_name lookup_expr filter_name LOOKUP_SEP join [field_name lookup_expr] _exact LOOKUP_SEP + u'exact' if filter_name endswith _exact filter_name filter_name[ - len _exact ]return filter_name
def register request success_url None form_class RegistrationForm profile_callback None template_name 'registration/registration_form html' extra_context None if request method 'POST' form form_class data request POST files request FILES if form is_valid new_user form save profile_callback profile_callback return HttpResponseRedirect success_url or reverse 'registration_complete' else form form_class if extra_context is None extra_context {}context RequestContext request for key value in extra_context items context[key] callable value and value or value return render_to_response template_name {'form' form} context_instance context
@pytest mark parametrize 'elidemode check' [ Qt ElideRight lambda s s endswith '\xe2\x80\xa6' or s endswith ' ' Qt ElideLeft lambda s s startswith '\xe2\x80\xa6' or s startswith ' ' Qt ElideMiddle lambda s '\xe2\x80\xa6' in s or ' ' in s Qt ElideNone lambda s '\xe2\x80\xa6' not in s and ' ' not in s ] def test_elided_text fake_statusbar qtbot elidemode check label TextBase elidemode elidemode qtbot add_widget label fake_statusbar hbox addWidget label long_string 'Helloworld ' * 100 label setText long_string label show assert check label _elided_text
def get_service hass config discovery_info None secret config get CONF_API_KEY recipient config get CONF_TO device config get CONF_DEVICE return AutomateNotificationService secret recipient device
def IndexDefinitionsToKeys indexes keyset set if indexes is not None if indexes indexes for index in indexes indexes keyset add IndexToKey index return keyset
def IndexDefinitionsToKeys indexes keyset set if indexes is not None if indexes indexes for index in indexes indexes keyset add IndexToKey index return keyset
def unregister config unused_plugins account_storage account AccountFileStorage config accounts account_storage find_all reporter_util zope component getUtility interfaces IReporter if not accounts return 'Couldnotfindexistingaccounttodeactivate 'yesno zope component getUtility interfaces IDisplay yesnoprompt 'Areyousureyouwouldliketoirrevocablydeactivateyouraccount?'wants_deactivate yesno prompt yes_label 'Deactivate' no_label 'Abort' default True if not wants_deactivate return 'Deactivationaborted ' acc acme _determine_account config acme_client client Client config acc None None acme acme acme_client acme deactivate_registration acc regr account_files account AccountFileStorage config account_files delete config account reporter_util add_message 'Accountdeactivated ' reporter_util MEDIUM_PRIORITY
@contextmanagerdef ensure_clean filename None return_filelike False filename filename or '' fd Noneif return_filelike f tempfile TemporaryFile suffix filename try yield f finally f close else if len os path dirname filename raise ValueError "Can'tpassaqualifiednametoensure_clean " try fd filename tempfile mkstemp suffix filename except UnicodeEncodeError import noseraise nose SkipTest 'nounicodefilenamesonthissystem' try yield filename finally try os close fd except Exception as e print "Couldn'tclosefiledescriptor %d file %s " % fd filename try if os path exists filename os remove filename except Exception as e print 'Exceptiononremovingfile %s' % e
def event2json event return jdumps dict type event type send_event event send_event time event time root str event root window str event window same_screen event same_screen child str event child root_x event root_x root_y event root_y event_x event event_x event_y event event_y state event state detail event detail
def _nt_quote_args args for i in range len args if string find args[i] '' -1 args[i] '"%s"' % args[i] return args
def _nt_quote_args args for i in range len args if string find args[i] '' -1 args[i] '"%s"' % args[i] return args
def keywords func if isinstance func type return keywords func __init__ elif isinstance func partial return keywords func func return inspect getargspec func args
def approximate_taylor_polynomial f x degree scale order None if order is None order degreen order + 1 xs scale * np cos np linspace 0 np pi n endpoint n % 1 + x P KroghInterpolator xs f xs d P derivatives x der degree + 1 return np poly1d d / factorial np arange degree + 1 [ -1 ]
def approximate_taylor_polynomial f x degree scale order None if order is None order degreen order + 1 xs scale * np cos np linspace 0 np pi n endpoint n % 1 + x P KroghInterpolator xs f xs d P derivatives x der degree + 1 return np poly1d d / factorial np arange degree + 1 [ -1 ]
@utils memoizedef get_pkgfile command try command command strip if command startswith 'sudo' command command[5 ]command command split '' [0]packages subprocess check_output ['pkgfile' '-b' '-v' command] universal_newlines True stderr utils DEVNULL splitlines return [package split [0] for package in packages]except subprocess CalledProcessError return None
@utils memoizedef get_pkgfile command try command command strip if command startswith 'sudo' command command[5 ]command command split '' [0]packages subprocess check_output ['pkgfile' '-b' '-v' command] universal_newlines True stderr utils DEVNULL splitlines return [package split [0] for package in packages]except subprocess CalledProcessError return None
def get_role_ids course_id roles Role objects filter course_id course_id exclude name FORUM_ROLE_STUDENT return dict [ role name list role users values_list 'id' flat True for role in roles]
def _load_params global _ANSIBLE_ARGSif _ANSIBLE_ARGS is not None buffer _ANSIBLE_ARGSelse if len sys argv > 1 if os path isfile sys argv[1] fd open sys argv[1] 'rb' buffer fd read fd close else buffer sys argv[1]if PY3 buffer buffer encode 'utf-8' errors 'surrogateescape' elif PY2 buffer sys stdin read else buffer sys stdin buffer read _ANSIBLE_ARGS buffertry params json loads buffer decode 'utf-8' except ValueError print '\n{"msg" "Error ModuleunabletodecodevalidJSONonstdin Unabletofigureoutwhatparameterswerepassed" "failed" true}'sys exit 1 if PY2 params json_dict_unicode_to_bytes params try return params['ANSIBLE_MODULE_ARGS']except KeyError print '\n{"msg" "Error ModuleunabletolocateANSIBLE_MODULE_ARGSinjsondatafromstdin Unabletofigureoutwhatparameterswerepassed" "failed" true}'sys exit 1
def get_date_regex timestring prev ''curr ''regex ''for s in range 0 len timestring curr timestring[s]if curr '%' passelif curr in settings date_regex and prev '%' regex + '\\d{' + settings date_regex [curr] + '}' elif curr in [' ' '-'] regex + '\\' + curr else regex + currprev currlogger debug 'regex {0}' format regex return regex
def uptime format False uptime time - SERVER_START_TIME if format return _format uptime 31536000 2628000 604800 86400 3600 60 return uptime
def uptime format False uptime time - SERVER_START_TIME if format return _format uptime 31536000 2628000 604800 86400 3600 60 return uptime
def update pathtozip patterns filepaths names compression zipfile ZIP_DEFLATED verbose True assert len patterns len filepaths len names z zipfile ZipFile pathtozip mode 'a' for name in z namelist for pat fname new_name in zip patterns filepaths names if pat search name if verbose print 'Updating%swith%s' % name fname if new_name is None z replace fname arcname name compress_type compression else z delete name z write fname new_name compress_type compression breakz close
def update pathtozip patterns filepaths names compression zipfile ZIP_DEFLATED verbose True assert len patterns len filepaths len names z zipfile ZipFile pathtozip mode 'a' for name in z namelist for pat fname new_name in zip patterns filepaths names if pat search name if verbose print 'Updating%swith%s' % name fname if new_name is None z replace fname arcname name compress_type compression else z delete name z write fname new_name compress_type compression breakz close
def test_user_link_unicode u UserProfile username u'jm\xfcller' display_name u'J\xfcrgenM\xfcller' pk 1 assert user_link u u'<ahref "%s"title "%s">J\xfcrgenM\xfcller</a>' % u get_url_path u name u UserProfile username '\xe5\xaf\x92\xe6\x98\x9f' pk 1 assert user_link u u'<ahref "%s"title "%s">%s</a>' % u get_url_path u name u username
def create_xml_runner test_pkg test_name results_file None is_rostest False test_name os path basename test_name if not results_file results_file xml_results_file test_pkg test_name is_rostest test_dir os path abspath os path dirname results_file if not os path exists test_dir try makedirs_with_parent_perms test_dir except OSError as error raise IOError 'cannotcreatetestresultsdirectory[%s] %s' % test_dir str error elif os path isfile test_dir raise Exception 'ERROR cannotruntestsuite fileispreventingcreationoftestdir %s' % test_dir print '[ROSUNIT]Outputtingtestresultsto' + results_file outstream open results_file 'w' outstream write '<?xmlversion "1 0"encoding "utf-8"?>\n' return XMLTestRunner stream outstream
def getDataDirectory moduleName None if not moduleName caller currentframe 1 moduleName inspect getmodule caller __name__return appdirs user_data_dir moduleName
def _FindStmtParent node if pytree_utils NodeName node in _STATEMENT_NODES return nodeelse return _FindStmtParent node parent
def get_current_view return getattr g '_admin_view' None
def isPathAdded edges faces loops remainingEdgeTable vertexes z if len remainingEdgeTable < 1 return FalsepathIndexes []remainingEdgeIndexKey remainingEdgeTable keys [0]pathIndexes append remainingEdgeIndexKey del remainingEdgeTable[remainingEdgeIndexKey]nextEdgeIndexAroundZ getNextEdgeIndexAroundZ edges[remainingEdgeIndexKey] faces remainingEdgeTable while nextEdgeIndexAroundZ -1 pathIndexes append nextEdgeIndexAroundZ del remainingEdgeTable[nextEdgeIndexAroundZ]nextEdgeIndexAroundZ getNextEdgeIndexAroundZ edges[nextEdgeIndexAroundZ] faces remainingEdgeTable if len pathIndexes < 3 print 'Danglingedges willuseintersectingcirclestogetimportlayeratheight%s' % z del loops[ ]return Falseloops append getPath edges pathIndexes vertexes z return True
def isPathAdded edges faces loops remainingEdgeTable vertexes z if len remainingEdgeTable < 1 return FalsepathIndexes []remainingEdgeIndexKey remainingEdgeTable keys [0]pathIndexes append remainingEdgeIndexKey del remainingEdgeTable[remainingEdgeIndexKey]nextEdgeIndexAroundZ getNextEdgeIndexAroundZ edges[remainingEdgeIndexKey] faces remainingEdgeTable while nextEdgeIndexAroundZ -1 pathIndexes append nextEdgeIndexAroundZ del remainingEdgeTable[nextEdgeIndexAroundZ]nextEdgeIndexAroundZ getNextEdgeIndexAroundZ edges[nextEdgeIndexAroundZ] faces remainingEdgeTable if len pathIndexes < 3 print 'Danglingedges willuseintersectingcirclestogetimportlayeratheight%s' % z del loops[ ]return Falseloops append getPath edges pathIndexes vertexes z return True
def install_git path hook op join path 'pre-commit' with open hook 'w' as fd fd write "# /usr/bin/envpython\nimportsys\nfrompylama hookimportgit_hook\n\nif__name__ '__main__' \nsys exit git_hook \n" chmod hook 484
def test_positive_integer_2 assert 1 positive_integer '1'
def prepare_query_string params params sorted params items key lambda x x[0] return '?%s' % parse urlencode params if params else ''
def prepare_query_string params params sorted params items key lambda x x[0] return '?%s' % parse urlencode params if params else ''
def get_attr method_string app_name method_string split u' ' [0]if not local flags in_install and app_name not in get_installed_apps throw _ u'App{0}isnotinstalled' format app_name AppNotInstalledError modulename u' ' join method_string split u' ' [ -1 ] methodname method_string split u' ' [ -1 ]return getattr get_module modulename methodname
def start_webserver config args logger info 'Startwebservermode' global webserverfrom glances webserver import GlancesWebServerwebserver GlancesWebServer config config args args webserver serve_forever
def systemd_running_state name path None try ret run_all name 'systemctlis-system-running' path path ignore_retcode True ['stdout']except CommandExecutionError ret ''return ret
def gimme_json_for_portfolio request "JSONincludes \n*Theperson'sdata \n*otherstuff"if not request user is_authenticated return HttpResponseServerError "Oops you'renotloggedin " person request user get_profile citations list Citation untrashed filter portfolio_entry__person person portfolio_entries_unserialized PortfolioEntry objects filter person person is_deleted False projects_unserialized [p project for p in portfolio_entries_unserialized]summaries {}for c in citations summaries[c pk] render_to_string 'profile/portfolio/citation_summary html' {'citation' c} five_minutes_ago datetime datetime utcnow - datetime timedelta minutes 5 portfolio_entries json loads serializers serialize 'json' portfolio_entries_unserialized projects json loads serializers serialize 'json' projects_unserialized citations json loads serializers serialize 'json' citations portfolio_json json dumps {'citations' citations 'portfolio_entries' portfolio_entries 'projects' projects 'summaries' summaries} return HttpResponse portfolio_json mimetype 'application/json'
def speakerDiarizationEvaluateScript folderName LDAs types '* wav' wavFilesList []for files in types wavFilesList extend glob glob os path join folderName files wavFilesList sorted wavFilesList N []for wavFile in wavFilesList gtFile wavFile replace ' wav' ' segments' if os path isfile gtFile [segStart segEnd segLabels] readSegmentGT gtFile N append len list set segLabels else N append -1 for l in LDAs print 'LDA {0 d}' format l for i wavFile in enumerate wavFilesList speakerDiarization wavFile N[i] 2 0 0 2 0 05 l PLOT False print
def speakerDiarizationEvaluateScript folderName LDAs types '* wav' wavFilesList []for files in types wavFilesList extend glob glob os path join folderName files wavFilesList sorted wavFilesList N []for wavFile in wavFilesList gtFile wavFile replace ' wav' ' segments' if os path isfile gtFile [segStart segEnd segLabels] readSegmentGT gtFile N append len list set segLabels else N append -1 for l in LDAs print 'LDA {0 d}' format l for i wavFile in enumerate wavFilesList speakerDiarization wavFile N[i] 2 0 0 2 0 05 l PLOT False print
def get_default_hparams return HParams batch_size 64 residual_blocks 2 n_couplings 2 n_scale 4 learning_rate 0 001 momentum 0 1 decay 0 001 l2_coeff 5e-05 clip_gradient 100 0 optimizer 'adam' dropout_mask 0 base_dim 32 bottleneck 0 use_batch_norm 1 alternate 1 use_aff 1 skip 1 data_constraint 0 9 n_opt 0
def print_timing msg None debug False prefix msgif isinstance msg types FunctionType prefix msg func_namedef wrap_f func *arg **kargs 'Rawtimingfunction'time1 time time res func *arg **kargs time2 time time msg '%stook%0 3fmins' % prefix time2 - time1 / 60 0 if debug log debug msg else log info msg return resif isinstance msg types FunctionType return decorator decorator wrap_f msg else return decorator decorator wrap_f
def print_timing msg None debug False prefix msgif isinstance msg types FunctionType prefix msg func_namedef wrap_f func *arg **kargs 'Rawtimingfunction'time1 time time res func *arg **kargs time2 time time msg '%stook%0 3fmins' % prefix time2 - time1 / 60 0 if debug log debug msg else log info msg return resif isinstance msg types FunctionType return decorator decorator wrap_f msg else return decorator decorator wrap_f
def sanitize_redirect host redirect_to if redirect_to try netloc urlparse redirect_to [1] or host except TypeError AttributeError passelse if netloc host return redirect_to
@ensure_csrf_cookie@cache_control no_cache True no_store True must_revalidate True @require_level 'staff' @require_POSTdef get_registration_codes request course_id course_id SlashSeparatedCourseKey from_deprecated_string course_id registration_codes CourseRegistrationCode objects filter course_id course_id order_by 'invoice_item__invoice__company_name' company_name request POST['download_company_name']if company_name registration_codes registration_codes filter invoice_item__invoice__company_name company_name csv_type 'download'return registration_codes_csv 'Registration_Codes csv' registration_codes csv_type
def get_collection_by_alias bus alias service_obj bus_get_object bus SS_PATH service_iface dbus Interface service_obj SERVICE_IFACE collection_path service_iface ReadAlias alias signature 's' if len collection_path < 1 raise ItemNotFoundException 'Nocollectionwithsuchalias ' return Collection bus collection_path
def get_main_running_hub hubs get_running_hubs if not hubs raise SAMPHubError u'UnabletofindarunningSAMPHub ' if u'SAMP_HUB' in os environ if os environ[u'SAMP_HUB'] startswith u'std-lockurl ' lockfilename os environ[u'SAMP_HUB'][len u'std-lockurl ' ]else raise SAMPHubError u'SAMPHubprofilenotsupported ' else lockfilename os path join _find_home u' samp' return hubs[lockfilename]
def create_appscale_user password uaserver does_user_exist uaserver does_user_exist hermes_constants USER_EMAIL appscale_info get_secret if does_user_exist 'true' logging debug 'User{0}alreadyexists sonotcreatingitagain ' format hermes_constants USER_EMAIL return Trueelif uaserver commit_new_user hermes_constants USER_EMAIL password hermes_constants ACCOUNT_TYPE appscale_info get_secret 'true' return Trueelse logging error 'ErrorwhilecreatinganAppscaleuser ' return False
def clear_time_override utcnow override_time None
def _has_constant_term p x R p ringiv R gens index x zm R zero_monoma [0] * R ngens a[iv] 1miv tuple a for expv in p if monomial_min expv miv zm return Truereturn False
def _make_link volume_path backup_path vol_id try utils execute 'ln' volume_path backup_path run_as_root True check_exit_code True except processutils ProcessExecutionError as exc err _ 'backup % vol_id sfailedtocreatedevicehardlinkfrom% vpath sto% bpath s \nstdout % out s\nstderr % err s' % {'vol_id' vol_id 'vpath' volume_path 'bpath' backup_path 'out' exc stdout 'err' exc stderr} LOG error err raise exception InvalidBackup reason err
def pretty_bool value bool_dict [True 'True' 'true' 'T' 't' '1']return value in bool_dict
def cmpPeople p1 p2 p1b getattr p1 'billingPos' None or _last p2b getattr p2 'billingPos' None or _last if p1b > p2b return 1if p1b < p2b return -1 p1n p1 get 'canonicalname' _last p2n p2 get 'canonicalname' _last if p1n is _last and p2n is _last p1n p1 get 'name' _last p2n p2 get 'name' _last if p1n > p2n return 1if p1n < p2n return -1 p1i p1 get 'imdbIndex' _last p2i p2 get 'imdbIndex' _last if p1i > p2i return 1if p1i < p2i return -1 return 0
def cmpPeople p1 p2 p1b getattr p1 'billingPos' None or _last p2b getattr p2 'billingPos' None or _last if p1b > p2b return 1if p1b < p2b return -1 p1n p1 get 'canonicalname' _last p2n p2 get 'canonicalname' _last if p1n is _last and p2n is _last p1n p1 get 'name' _last p2n p2 get 'name' _last if p1n > p2n return 1if p1n < p2n return -1 p1i p1 get 'imdbIndex' _last p2i p2 get 'imdbIndex' _last if p1i > p2i return 1if p1i < p2i return -1 return 0
def current_service request if request matched_route services request registry cornice_servicespattern request matched_route patterntry service services[pattern]except KeyError return Noneelse return service
def current_service request if request matched_route services request registry cornice_servicespattern request matched_route patterntry service services[pattern]except KeyError return Noneelse return service
def expanding_apply arg func min_periods 1 freq None args kwargs {} return ensure_compat 'expanding' 'apply' arg freq freq min_periods min_periods func_kw ['func' 'args' 'kwargs'] func func args args kwargs kwargs
def sha1_hash_digest payload return base64 b64encode hashlib sha1 payload digest
def sha1_hash_digest payload return base64 b64encode hashlib sha1 payload digest
@open_file 0 mode 'rt' def read_graph6 path glist []for line in path line line strip if not len line continueglist append parse_graph6 line if len glist 1 return glist[0]else return glist
def decint raw forward True val 0byts bytearray src bytearray raw if not forward src reverse for bnum in src byts append bnum & 127 if bnum & 128 breakif not forward byts reverse for byte in byts val << 7val bytereturn val len byts
def decint raw forward True val 0byts bytearray src bytearray raw if not forward src reverse for bnum in src byts append bnum & 127 if bnum & 128 breakif not forward byts reverse for byte in byts val << 7val bytereturn val len byts
def _find_label_paths subject 'fsaverage' pattern None subjects_dir None subjects_dir get_subjects_dir subjects_dir raise_error True subject_dir os path join subjects_dir subject lbl_dir os path join subject_dir 'label' if pattern is None paths []for dirpath _ filenames in os walk lbl_dir rel_dir os path relpath dirpath lbl_dir for filename in fnmatch filter filenames '* label' path os path join rel_dir filename paths append path else paths [os path relpath path lbl_dir for path in iglob pattern ]return paths
def _find_label_paths subject 'fsaverage' pattern None subjects_dir None subjects_dir get_subjects_dir subjects_dir raise_error True subject_dir os path join subjects_dir subject lbl_dir os path join subject_dir 'label' if pattern is None paths []for dirpath _ filenames in os walk lbl_dir rel_dir os path relpath dirpath lbl_dir for filename in fnmatch filter filenames '* label' path os path join rel_dir filename paths append path else paths [os path relpath path lbl_dir for path in iglob pattern ]return paths
def memory memory_oper ['read' 'write']memory_scope ['local' 'global']test_command 'sysbench--num-threads 64--test memory'test_command + '--memory-oper {0}--memory-scope {1}'test_command + '--memory-block-size 1K--memory-total-size 32Grun'result Noneret_val {}for oper in memory_oper for scope in memory_scope key 'Operation {0}Scope {1}' format oper scope run_command test_command format oper scope result __salt__['cmd run'] run_command ret_val[key] _parser result return ret_val
def compile_device_template pyfunc debug False inline False from descriptor import CUDATargetDescdft DeviceFunctionTemplate pyfunc debug debug inline inline class device_function_template AbstractTemplate key dftdef generic self args kws assert not kws return dft compile args typingctx CUDATargetDesc typingctxtypingctx insert_user_function dft device_function_template return dft
def to_rgba c alpha None if _is_nth_color c from matplotlib import rcParamsprop_cycler rcParams[u'axes prop_cycle']colors prop_cycler by_key get u'color' [u'k'] c colors[ int c[1] % len colors ]try rgba _colors_full_map cache[ c alpha ]except KeyError TypeError rgba _to_rgba_no_colorcycle c alpha try _colors_full_map cache[ c alpha ] rgbaexcept TypeError passreturn rgba
def spawn_personal_stream args stuff None g['keyword'] g['listname'] ''g['PREFIX'] u2str emojize format_prefix th threading Thread target stream args c['USER_DOMAIN'] args g['original_name'] th daemon Trueth start
def stopOnError case reactor publisher None if publisher is None from twisted python import log as publisherrunning [None]def stopIfError event if running and event get 'isError' running pop reactor stop publisher addObserver stopIfError case addCleanup publisher removeObserver stopIfError
def stopOnError case reactor publisher None if publisher is None from twisted python import log as publisherrunning [None]def stopIfError event if running and event get 'isError' running pop reactor stop publisher addObserver stopIfError case addCleanup publisher removeObserver stopIfError
def stopOnError case reactor publisher None if publisher is None from twisted python import log as publisherrunning [None]def stopIfError event if running and event get 'isError' running pop reactor stop publisher addObserver stopIfError case addCleanup publisher removeObserver stopIfError
def has_pattern try from pattern en import parsereturn Trueexcept ImportError return False
def has_pattern try from pattern en import parsereturn Trueexcept ImportError return False
def Potential name var model None model modelcontext model var name model name_for name model potentials append var return var
@verbosedef fetch_hcp_mmp_parcellation subjects_dir None verbose None subjects_dir get_subjects_dir subjects_dir raise_error True destination op join subjects_dir 'fsaverage' 'label' fnames [op join destination 'lh HCPMMP1 annot' op join destination 'rh HCPMMP1 annot' ]if all op isfile fname for fname in fnames returnif '--accept-hcpmmp-license' in sys argv answer 'y'else answer input '%s\nAgree y/[n] ?' % _hcp_mmp_license_text if answer lower 'y' raise RuntimeError 'Youmustagreetothelicensetousethisdataset' _fetch_file 'https //ndownloader figshare com/files/5528816' fnames[0] hash_ '46a102b59b2fb1bb4bd62d51bf02e975' _fetch_file 'https //ndownloader figshare com/files/5528819' fnames[1] hash_ '75e96b331940227bbcb07c1c791c2463'
def install widget ms 10 reactor None installTkFunctions global _task_task task LoopingCall widget update _task start ms / 1000 0 False
def normalize_resource_url resource_url try protocol name split_resource_url resource_url except ValueError protocol u'nltk'name resource_urlif protocol u'nltk' and os path isabs name protocol u'file //'name normalize_resource_name name False None elif protocol u'file' protocol u'file //'name normalize_resource_name name False None elif protocol u'nltk' protocol u'nltk 'name normalize_resource_name name True else protocol + u' //'return u'' join [protocol name]
def _fake_exists path return False
def is_process_running node name command ['pidof' '-x' name]d node run_as_root command def not_existing failure failure trap ProcessTerminated return Falsed addCallbacks lambda result True not_existing return d
def is_process_running node name command ['pidof' '-x' name]d node run_as_root command def not_existing failure failure trap ProcessTerminated return Falsed addCallbacks lambda result True not_existing return d
def test_from_castra_with_selection castra pytest importorskip 'castra' blosc pytest importorskip 'blosc' if LooseVersion blosc __version__ '1 3 0' or LooseVersion castra __version__ < '0 1 8' pytest skip df pd DataFrame {'x' ['a' 'b' 'c' 'd'] 'y' [2 3 4 5]} index pd Index [1 0 2 0 3 0 4 0] name 'ind' a dd from_pandas df 2 b dd from_castra a to_castra assert_eq b[ b y > 3 ] x df[ df y > 3 ] x
def in6_or a1 a2 return _in6_bitops a1 a2 0
@frappe whitelist def get_users doctype name return frappe db sql u'select\n DCTB DCTB DCTB `name` `user` `read` `write` `share` `everyone`\n DCTB DCTB from\n DCTB DCTB DCTB tabDocShare\n DCTB DCTB where\n DCTB DCTB DCTB share_doctype %sandshare_name %s' doctype name as_dict True
@open_file 0 mode 'rt' def read_sparse6 path glist []for line in path line line strip if not len line continueglist append parse_sparse6 line if len glist 1 return glist[0]else return glist
def requirejs_xmodule request return render_to_response 'xmodule js' {'urls' get_xmodule_urls } content_type 'text/javascript'
def requirejs_xmodule request return render_to_response 'xmodule js' {'urls' get_xmodule_urls } content_type 'text/javascript'
def requirejs_xmodule request return render_to_response 'xmodule js' {'urls' get_xmodule_urls } content_type 'text/javascript'
def requirejs_xmodule request return render_to_response 'xmodule js' {'urls' get_xmodule_urls } content_type 'text/javascript'
def in6_iseui64 x eui64 inet_pton socket AF_INET6 ' ff fe00 0' x in6_and inet_pton socket AF_INET6 x eui64 return x eui64
def in6_iseui64 x eui64 inet_pton socket AF_INET6 ' ff fe00 0' x in6_and inet_pton socket AF_INET6 x eui64 return x eui64
def python_revision return _sys_version [3]
def get_repository_categories app id sa_session app model context currentreturn sa_session query app model RepositoryCategoryAssociation filter app model RepositoryCategoryAssociation table c repository_id app security decode_id id
def create_api_error_from_http_exception e response e responsetry explanation response json ['message']except ValueError explanation response content strip cls APIErrorif response status_code 404 if explanation and 'Nosuchimage' in str explanation or 'notfound doesnotexistornoreadaccess' in str explanation cls ImageNotFoundelse cls NotFoundraise cls e response response explanation explanation
@utils expects_func_args 'migration' def errors_out_migration function @functools wraps function def decorated_function self context *args **kwargs try return function self context *args **kwargs except Exception as ex with excutils save_and_reraise_exception wrapped_func safe_utils get_wrapped_function function keyed_args inspect getcallargs wrapped_func self context *args **kwargs migration keyed_args['migration']if not isinstance ex exception InstanceNotFound status migration statusif status not in ['migrating' 'post-migrating'] returnmigration status 'error'try with migration obj_as_admin migration save except Exception LOG debug 'Errorsettingmigrationstatusforinstance%s ' migration instance_uuid exc_info True return decorated_function
def task_id_str task_family params param_str json dumps params separators ' ' ' ' sort_keys True param_hash hashlib md5 param_str encode 'utf-8' hexdigest param_summary '_' join p[ TASK_ID_TRUNCATE_PARAMS] for p in params[p] for p in sorted params [ TASK_ID_INCLUDE_PARAMS] param_summary TASK_ID_INVALID_CHAR_REGEX sub '_' param_summary return '{}_{}_{}' format task_family param_summary param_hash[ TASK_ID_TRUNCATE_HASH]
def task_id_str task_family params param_str json dumps params separators ' ' ' ' sort_keys True param_hash hashlib md5 param_str encode 'utf-8' hexdigest param_summary '_' join p[ TASK_ID_TRUNCATE_PARAMS] for p in params[p] for p in sorted params [ TASK_ID_INCLUDE_PARAMS] param_summary TASK_ID_INVALID_CHAR_REGEX sub '_' param_summary return '{}_{}_{}' format task_family param_summary param_hash[ TASK_ID_TRUNCATE_HASH]
def get_method java_object method_name return JavaMember method_name java_object java_object _target_id java_object _gateway_client
def validate_config_section filename config section if not isinstance config dict raise ConfigurationError u"Infile'{filename}' {section}mustbeamapping not{type} " format filename filename section section type anglicize_json_type python_type_to_yaml_type config for key value in config items if not isinstance key six string_types raise ConfigurationError u"Infile'{filename}' the{section}name{name}mustbeaquotedstring i e '{name}' " format filename filename section section name key if not isinstance value dict type None raise ConfigurationError u"Infile'{filename}' {section}'{name}'mustbeamappingnot{type} " format filename filename section section name key type anglicize_json_type python_type_to_yaml_type value
def color_func func_name if str func_name isdigit return term_color int func_name return globals [func_name]
def task_cli_pkg_install distribution package_source PackageSource commands task_package_install 'clusterhq-flocker-cli' distribution package_source return sequence [ Effect Sudo command e intent command log_command_filter e intent log_command_filter if isinstance e intent Run else e for e in commands intent effects]
def node_redundancy G nodes None if nodes is None nodes Gif any len G[v] < 2 for v in nodes raise NetworkXError 'Cannotcomputeredundancycoefficientforanodethathasfewerthantwoneighbors ' return {v _node_redundancy G v for v in nodes}
def parse_one_rule_from_dict rule rule pop 'description' None trigger rule pop 'trigger' None case_sensitive rule pop 'caseSensitive' True if case_sensitive 'false' case_sensitive Falseelif case_sensitive 'true' case_sensitive Truetry search rule pop 'search' except KeyError raise ValueError 'Redactionruleismissing`search`field' try replace rule pop 'replace' except KeyError raise ValueError 'Redactionruleismissing`replace`field' if rule raise ValueError 'Redactionrulecontainsunknownfield s %s' % rule keys return RedactionRule trigger search replace case_sensitive
def parse_one_rule_from_dict rule rule pop 'description' None trigger rule pop 'trigger' None case_sensitive rule pop 'caseSensitive' True if case_sensitive 'false' case_sensitive Falseelif case_sensitive 'true' case_sensitive Truetry search rule pop 'search' except KeyError raise ValueError 'Redactionruleismissing`search`field' try replace rule pop 'replace' except KeyError raise ValueError 'Redactionruleismissing`replace`field' if rule raise ValueError 'Redactionrulecontainsunknownfield s %s' % rule keys return RedactionRule trigger search replace case_sensitive
def subtractXIntersectionsTable subtractFromTable subtractTable subtractFromTableKeys subtractFromTable keys subtractFromTableKeys sort for subtractFromTableKey in subtractFromTableKeys xIntersectionIndexList []addXIntersectionIndexesFromXIntersections -1 xIntersectionIndexList subtractFromTable[subtractFromTableKey] if subtractFromTableKey in subtractTable addXIntersectionIndexesFromXIntersections 0 xIntersectionIndexList subtractTable[subtractFromTableKey] xIntersections getXIntersectionsFromIntersections xIntersectionIndexList if len xIntersections > 0 subtractFromTable[subtractFromTableKey] xIntersectionselse del subtractFromTable[subtractFromTableKey]
def set_style style None rc None style_object axes_style style rc mpl rcParams update style_object
def isNetworkRoaming try mContext autoclass 'android content Context' pythonActivity autoclass 'org renpy android PythonService' telephonyManager cast 'android telephony TelephonyManager' pythonActivity mService getSystemService mContext TELEPHONY_SERVICE isNetworkRoaming telephonyManager isNetworkRoaming return isNetworkRoamingexcept Exception as e return None
def isNetworkRoaming try mContext autoclass 'android content Context' pythonActivity autoclass 'org renpy android PythonService' telephonyManager cast 'android telephony TelephonyManager' pythonActivity mService getSystemService mContext TELEPHONY_SERVICE isNetworkRoaming telephonyManager isNetworkRoaming return isNetworkRoamingexcept Exception as e return None
@dec skip_without 'sqlite3' def test_macro_run ip get_ipython ip history_manager reset cmds ['a 10' 'a+ 1' py3compat doctest_refactor_print 'printa' '%macrotest2-3']for cmd in cmds ip run_cell cmd store_history True nt assert_equal ip user_ns['test'] value py3compat doctest_refactor_print 'a+ 1\nprinta\n' with tt AssertPrints '12' ip run_cell 'test' with tt AssertPrints '13' ip run_cell 'test'
def isleap year return year % 4 0 and year % 100 0 or year % 400 0
def vb_wait_for_network_address timeout step None machine_name None machine None kwargs {'machine_name' machine_name 'machine' machine}return wait_for vb_get_network_addresses timeout timeout step step default [] func_kwargs kwargs
def add_jinja2_ext pelican if u'JINJA_ENVIRONMENT' in pelican settings pelican settings[u'JINJA_ENVIRONMENT'][u'extensions'] append AssetsExtension else pelican settings[u'JINJA_EXTENSIONS'] append AssetsExtension
def add_jinja2_ext pelican if u'JINJA_ENVIRONMENT' in pelican settings pelican settings[u'JINJA_ENVIRONMENT'][u'extensions'] append AssetsExtension else pelican settings[u'JINJA_EXTENSIONS'] append AssetsExtension
def unroll deep_autoencoder net Network deep_autoencoder layers net weights deep_autoencoder weights[ len deep_autoencoder layers - 1 ]net biases deep_autoencoder biases[ len deep_autoencoder layers - 1 ]return net
@with_setup prepare_stderr def test_many_features_a_file filename syntax_feature_name 'many_features_a_file' runner Runner filename assert_raises LettuceRunnerError runner run assert_stderr_lines 'Syntaxerrorat %s\nAfeaturefilemustcontainONLYONEfeature \n' % filename
def _fetch_latest_from_memcache app_version proto_string memcache get app_version namespace NAMESPACE if proto_string logging debug 'Loadedmostrecentconfdatafrommemcache ' return db model_from_protobuf proto_string logging debug 'Triedtoloadconfdatafrommemcache butfoundnothing ' return None
def _set_reply_headers new_message previous_message if previous_message message_id_header new_message in_reply_to previous_message message_id_headerif previous_message references new_message references previous_message references + [previous_message message_id_header] else new_message references [previous_message message_id_header]
def translation language global _translationsif language not in _translations _translations[language] DjangoTranslation language return _translations[language]
def translation language global _translationsif language not in _translations _translations[language] DjangoTranslation language return _translations[language]
def _find_review_request request review_request_id local_site review_request _find_review_request_object review_request_id local_site if review_request is_accessible_by request user return review_request None else return None _render_permission_denied request
def _find_review_request request review_request_id local_site review_request _find_review_request_object review_request_id local_site if review_request is_accessible_by request user return review_request None else return None _render_permission_denied request
def ec2_client region zone access_key_id secret_access_key session_token None validate_region True connection boto3 session Session aws_access_key_id access_key_id aws_secret_access_key secret_access_key aws_session_token session_token connection _session set_config_variable 'metadata_service_num_attempts' BOTO_NUM_RETRIES ec2_resource connection resource 'ec2' region_name region if validate_region try zones ec2_resource meta client describe_availability_zones except EndpointConnectionError raise InvalidRegionError region available_zones [available_zone['ZoneName'] for available_zone in zones['AvailabilityZones']]if zone not in available_zones raise InvalidZoneError zone available_zones return _EC2 zone zone connection ec2_resource
def fetch_token_mock *args **kwargs return
def fetch_token_mock *args **kwargs return
def _describe_volume volume return {'id' volume id 'creation_time' _format_time _get_volume_creation_time volume 'provider' volume driver name 'region' _get_volume_region volume 'extra' repr volume extra }
def randperm n r range n x []while r i random choice r x append i r remove i return x
def check_conflicts unmerged if prefs check_conflicts unmerged [path for path in unmerged if is_conflict_free path ]return unmerged
@taskdef prepare prepare_apt checkout_cache
def nn model text vectors query k 5 qf encode model [query] qf / norm qf scores numpy dot qf vectors T flatten sorted_args numpy argsort scores [ -1 ]sentences [text[a] for a in sorted_args[ k]]print 'QUERY ' + query print 'NEAREST 'for i s in enumerate sentences print s sorted_args[i]
def setUpModule global ENGINEglobal SESSIONENGINE create_engine 'sqlite //' Base metadata create_all ENGINE session_factory sessionmaker bind ENGINE SESSION scoped_session session_factory
def increment_ipv4_segments segments segments [int segment for segment in segments]segments[3] + 1if segments[3] 256 segments[3] 0segments[2] + 1if segments[2] 256 segments[2] 0segments[1] + 1if segments[1] 256 segments[1] 0segments[0] + 1return segments
def update_user_email user new_email validate_email_unique new_email validate_email new_email user email new_emailuser save
def raises exc func *args try func *args except exc return Trueelse return False
def conlleval p g w filename script_path out ''for sl sp sw in zip g p w out + 'BOSOO\n'for wl wp w in zip sl sp sw out + w + '' + wl + '' + wp + '\n' out + 'EOSOO\n\n'f open filename 'w' f writelines out f close return get_perf filename script_path
@register filterdef prepend_http_if_necessary value parsed urlparse value if not parsed scheme return 'http //' + parsed geturl return value
def get_model_classes result []for module_name in MODEL_MODULE_NAMES module importlib import_module module_name model_classes getattr module 'MODELS' [] result extend model_classes return result
def _get_jid_snapshots jid config 'root' jid_snapshots [x for x in list_snapshots config if x['userdata'] get 'salt_jid' jid ]pre_snapshot [x for x in jid_snapshots if x['type'] 'pre' ]post_snapshot [x for x in jid_snapshots if x['type'] 'post' ]if not pre_snapshot or not post_snapshot raise CommandExecutionError "Jid'{0}'snapshotsnotfound" format jid return pre_snapshot[0]['id'] post_snapshot[0]['id']
@webauth SecurityCheck@csrf ensure_csrf_cookiedef Homepage request renderers_js_files set ['statistics js' 'usage js'] create_time psutil Process os getpid create_time context {'heading' config_lib CONFIG['AdminUI heading'] 'report_url' config_lib CONFIG['AdminUI report_url'] 'help_url' config_lib CONFIG['AdminUI help_url'] 'use_precompiled_js' config_lib CONFIG['AdminUI use_precompiled_js'] 'renderers_js' renderers_js_files 'timestamp' create_time}response shortcuts render_to_response 'base html' context context_instance template RequestContext request request REQ request GET dict request token BuildToken request 60 user_record aff4 FACTORY Create aff4 ROOT_URN Add 'users' Add request user aff4_type aff4_users GRRUser mode 'r' token request token canary_mode user_record Get user_record Schema GUI_SETTINGS canary_modeif canary_mode response set_cookie 'canary_mode' 'true' else response delete_cookie 'canary_mode' return response
@webauth SecurityCheck@csrf ensure_csrf_cookiedef Homepage request renderers_js_files set ['statistics js' 'usage js'] create_time psutil Process os getpid create_time context {'heading' config_lib CONFIG['AdminUI heading'] 'report_url' config_lib CONFIG['AdminUI report_url'] 'help_url' config_lib CONFIG['AdminUI help_url'] 'use_precompiled_js' config_lib CONFIG['AdminUI use_precompiled_js'] 'renderers_js' renderers_js_files 'timestamp' create_time}response shortcuts render_to_response 'base html' context context_instance template RequestContext request request REQ request GET dict request token BuildToken request 60 user_record aff4 FACTORY Create aff4 ROOT_URN Add 'users' Add request user aff4_type aff4_users GRRUser mode 'r' token request token canary_mode user_record Get user_record Schema GUI_SETTINGS canary_modeif canary_mode response set_cookie 'canary_mode' 'true' else response delete_cookie 'canary_mode' return response
def filter_roidb roidb def is_valid entry overlaps entry['max_overlaps']fg_inds np where overlaps > cfg TRAIN FG_THRESH [0]bg_inds np where overlaps < cfg TRAIN BG_THRESH_HI & overlaps > cfg TRAIN BG_THRESH_LO [0]valid len fg_inds > 0 or len bg_inds > 0 return validnum len roidb filtered_roidb [entry for entry in roidb if is_valid entry ]num_after len filtered_roidb print 'Filtered{}roidbentries {}->{}' format num - num_after num num_after return filtered_roidb
def filter_roidb roidb def is_valid entry overlaps entry['max_overlaps']fg_inds np where overlaps > cfg TRAIN FG_THRESH [0]bg_inds np where overlaps < cfg TRAIN BG_THRESH_HI & overlaps > cfg TRAIN BG_THRESH_LO [0]valid len fg_inds > 0 or len bg_inds > 0 return validnum len roidb filtered_roidb [entry for entry in roidb if is_valid entry ]num_after len filtered_roidb print 'Filtered{}roidbentries {}->{}' format num - num_after num num_after return filtered_roidb
def filter_roidb roidb def is_valid entry overlaps entry['max_overlaps']fg_inds np where overlaps > cfg TRAIN FG_THRESH [0]bg_inds np where overlaps < cfg TRAIN BG_THRESH_HI & overlaps > cfg TRAIN BG_THRESH_LO [0]valid len fg_inds > 0 or len bg_inds > 0 return validnum len roidb filtered_roidb [entry for entry in roidb if is_valid entry ]num_after len filtered_roidb print 'Filtered{}roidbentries {}->{}' format num - num_after num num_after return filtered_roidb
def generate_argument_parser_for_metadata metadata parameters metadata['parameters']parser argparse ArgumentParser description metadata['description'] for parameter_name parameter_options in parameters items name parameter_name replace '_' '-' description parameter_options['description']_type parameter_options['type']required parameter_options get 'required' False default_value parameter_options get 'default' None immutable parameter_options get 'immutable' False if immutable continueargs [ '--%s' % name ]kwargs {'help' description 'required' required}if default_value is not None kwargs['default'] default_valueif _type 'string' kwargs['type'] strelif _type 'integer' kwargs['type'] intelif _type 'boolean' if default_value is False kwargs['action'] 'store_false'else kwargs['action'] 'store_true'parser add_argument *args **kwargs return parser
def server mailname if not is_installed 'postfix' preseed_package 'postfix' {'postfix/main_mailer_type' 'select' 'InternetSite' 'postfix/mailname' 'string' mailname 'postfix/destinations' 'string' '%s localhost localdomain localhost' % mailname } install 'postfix' started 'postfix'
def EvalExponentialCdf x lam return 1 - math exp - lam * x
def compose_views request user_profile method_kwarg_pairs json_dict {}with transaction atomic for method kwargs in method_kwarg_pairs response method request user_profile **kwargs if response status_code 200 raise JsonableError response content json_dict update ujson loads response content return json_success json_dict
def _mark_asn1_named_ec_curve backend ec_cdata backend _lib EC_KEY_set_asn1_flag ec_cdata backend _lib OPENSSL_EC_NAMED_CURVE
def _mark_asn1_named_ec_curve backend ec_cdata backend _lib EC_KEY_set_asn1_flag ec_cdata backend _lib OPENSSL_EC_NAMED_CURVE
def add_data_sharing_consent_field request form_desc enterprise_customer get_enterprise_customer_for_request request required data_sharing_consent_required_at_login request if not data_sharing_consent_requested request returnlabel _ 'Iagreetoallow{platform_name}tosharedataaboutmyenrollment completionandperformanceinall{platform_name}coursesandprogramswheremyenrollmentissponsoredby{ec_name} ' format platform_name configuration_helpers get_value 'PLATFORM_NAME' settings PLATFORM_NAME ec_name enterprise_customer name error_msg _ 'Tolinkyouraccountwith{ec_name} youarerequiredtoconsenttodatasharing ' format ec_name enterprise_customer name form_desc add_field 'data_sharing_consent' label label field_type 'checkbox' default False required required error_messages {'required' error_msg}
def add_data_sharing_consent_field request form_desc enterprise_customer get_enterprise_customer_for_request request required data_sharing_consent_required_at_login request if not data_sharing_consent_requested request returnlabel _ 'Iagreetoallow{platform_name}tosharedataaboutmyenrollment completionandperformanceinall{platform_name}coursesandprogramswheremyenrollmentissponsoredby{ec_name} ' format platform_name configuration_helpers get_value 'PLATFORM_NAME' settings PLATFORM_NAME ec_name enterprise_customer name error_msg _ 'Tolinkyouraccountwith{ec_name} youarerequiredtoconsenttodatasharing ' format ec_name enterprise_customer name form_desc add_field 'data_sharing_consent' label label field_type 'checkbox' default False required required error_messages {'required' error_msg}
def StandardizedMoment xs k var CentralMoment xs 2 std math sqrt var return CentralMoment xs k / std ** k
def record_action action request data None if data is None data {}data['user-agent'] request META get 'HTTP_USER_AGENT' data['locale'] request LANGdata['src'] request GET get 'src' '' record_stat action request **data
def magma rc u'image' cmap u'magma' im gci if im is not None im set_cmap cm magma
def regexp_extraction string _regexp groups 1 regexp_search re search string str string pattern str _regexp if regexp_search if regexp_search group groups '' return str regexp_search group groups return None
def regexp_extraction string _regexp groups 1 regexp_search re search string str string pattern str _regexp if regexp_search if regexp_search group groups '' return str regexp_search group groups return None
def init_cachedir base None if base is None base __opts__['cachedir']needed_dirs base os path join base 'requested' os path join base 'active' for dir_ in needed_dirs if not os path exists dir_ os makedirs dir_ os chmod base 493 return base
def init_cachedir base None if base is None base __opts__['cachedir']needed_dirs base os path join base 'requested' os path join base 'active' for dir_ in needed_dirs if not os path exists dir_ os makedirs dir_ os chmod base 493 return base
def init_cachedir base None if base is None base __opts__['cachedir']needed_dirs base os path join base 'requested' os path join base 'active' for dir_ in needed_dirs if not os path exists dir_ os makedirs dir_ os chmod base 493 return base
def generate_filename_and_delete_previous ffile name before_delete None new_filename ffile field generate_filename ffile instance name try orig_instance ffile instance __class__ objects get id ffile instance id orig_field_file getattr orig_instance ffile field name orig_filename orig_field_file nameif orig_filename and new_filename orig_filename if before_delete before_delete orig_field_file orig_field_file delete except ffile instance __class__ DoesNotExist passreturn new_filename
def install_le_auto contents venv_dir venv_le_auto_path join venv_dir 'letsencrypt-auto' with open venv_le_auto_path 'w' as le_auto le_auto write contents chmod venv_le_auto_path S_IRUSR S_IXUSR
def install_le_auto contents venv_dir venv_le_auto_path join venv_dir 'letsencrypt-auto' with open venv_le_auto_path 'w' as le_auto le_auto write contents chmod venv_le_auto_path S_IRUSR S_IXUSR
def _call_with_retry max_attempts def wrapper f def func_wrapper *args **kwargs action '% func s' % {'func' getattr f '__name__' f } for attempt in range 1 max_attempts + 1 try return f *args **kwargs except oslo_messaging MessagingException with excutils save_and_reraise_exception reraise False as ctxt LOG warning _LW 'Failedtoexecute% action s % attempt doutof% max_attempts d' {'attempt' attempt 'max_attempts' max_attempts 'action' action} if attempt max_attempts ctxt reraise Truereturn func_wrapperreturn wrapper
def _call_with_retry max_attempts def wrapper f def func_wrapper *args **kwargs action '% func s' % {'func' getattr f '__name__' f } for attempt in range 1 max_attempts + 1 try return f *args **kwargs except oslo_messaging MessagingException with excutils save_and_reraise_exception reraise False as ctxt LOG warning _LW 'Failedtoexecute% action s % attempt doutof% max_attempts d' {'attempt' attempt 'max_attempts' max_attempts 'action' action} if attempt max_attempts ctxt reraise Truereturn func_wrapperreturn wrapper
def check_course_access course_key user None ip_address None url None if not settings FEATURES get 'EMBARGO' return Truecourse_is_restricted RestrictedCourse is_restricted_course course_key if not course_is_restricted return Trueif user is not None and has_course_author_access user course_key return Trueif ip_address is not None user_country_from_ip _country_code_from_ip ip_address if not CountryAccessRule check_country_access course_key user_country_from_ip log info u"Blockinguser%sfromaccessingcourse%sat%sbecausetheuser'sIPaddress%sappearstobelocatedin%s " getattr user 'id' '<NotAuthenticated>' course_key url ip_address user_country_from_ip return Falseif user is not None user_country_from_profile _get_user_country_from_profile user if not CountryAccessRule check_country_access course_key user_country_from_profile log info u"Blockinguser%sfromaccessingcourse%sat%sbecausetheuser'sprofilecountryis%s " user id course_key url user_country_from_profile return Falsereturn True
def check_course_access course_key user None ip_address None url None if not settings FEATURES get 'EMBARGO' return Truecourse_is_restricted RestrictedCourse is_restricted_course course_key if not course_is_restricted return Trueif user is not None and has_course_author_access user course_key return Trueif ip_address is not None user_country_from_ip _country_code_from_ip ip_address if not CountryAccessRule check_country_access course_key user_country_from_ip log info u"Blockinguser%sfromaccessingcourse%sat%sbecausetheuser'sIPaddress%sappearstobelocatedin%s " getattr user 'id' '<NotAuthenticated>' course_key url ip_address user_country_from_ip return Falseif user is not None user_country_from_profile _get_user_country_from_profile user if not CountryAccessRule check_country_access course_key user_country_from_profile log info u"Blockinguser%sfromaccessingcourse%sat%sbecausetheuser'sprofilecountryis%s " user id course_key url user_country_from_profile return Falsereturn True
def check_course_access course_key user None ip_address None url None if not settings FEATURES get 'EMBARGO' return Truecourse_is_restricted RestrictedCourse is_restricted_course course_key if not course_is_restricted return Trueif user is not None and has_course_author_access user course_key return Trueif ip_address is not None user_country_from_ip _country_code_from_ip ip_address if not CountryAccessRule check_country_access course_key user_country_from_ip log info u"Blockinguser%sfromaccessingcourse%sat%sbecausetheuser'sIPaddress%sappearstobelocatedin%s " getattr user 'id' '<NotAuthenticated>' course_key url ip_address user_country_from_ip return Falseif user is not None user_country_from_profile _get_user_country_from_profile user if not CountryAccessRule check_country_access course_key user_country_from_profile log info u"Blockinguser%sfromaccessingcourse%sat%sbecausetheuser'sprofilecountryis%s " user id course_key url user_country_from_profile return Falsereturn True
def _validate_named_port_params params if not params['named_ports'] return True '' if not isinstance params['named_ports'] list return False 'named_ports expectedlistofname portdictionaries ' req_fields [{'name' 'name' 'required' True 'type' str} {'name' 'port' 'required' True 'type' int}]for np in params['named_ports'] valid_named_ports np_msg _check_params np req_fields if not valid_named_ports return False np_msg return True ''
@task@timeddef uninstall_python_packages hasher hashlib sha1 hasher update repr PACKAGES_TO_UNINSTALL expected_version hasher hexdigest state_file_path os path join PREREQS_STATE_DIR 'Python_uninstall sha1' create_prereqs_cache_dir if os path isfile state_file_path with open state_file_path as state_file version state_file read if version expected_version print 'Pythonuninstallsunchanged skipping 'returnfor _ in range 3 uninstalled Falsefrozen sh 'pipfreeze' capture True for package_name in PACKAGES_TO_UNINSTALL if package_in_frozen package_name frozen sh 'pipuninstall--disable-pip-version-check-y{}' format package_name uninstalled Trueif not uninstalled breakelse print "Couldn'tuninstallunwantedPythonpackages "returnwith open state_file_path 'w' as state_file state_file write expected_version
@task@timeddef uninstall_python_packages hasher hashlib sha1 hasher update repr PACKAGES_TO_UNINSTALL expected_version hasher hexdigest state_file_path os path join PREREQS_STATE_DIR 'Python_uninstall sha1' create_prereqs_cache_dir if os path isfile state_file_path with open state_file_path as state_file version state_file read if version expected_version print 'Pythonuninstallsunchanged skipping 'returnfor _ in range 3 uninstalled Falsefrozen sh 'pipfreeze' capture True for package_name in PACKAGES_TO_UNINSTALL if package_in_frozen package_name frozen sh 'pipuninstall--disable-pip-version-check-y{}' format package_name uninstalled Trueif not uninstalled breakelse print "Couldn'tuninstallunwantedPythonpackages "returnwith open state_file_path 'w' as state_file state_file write expected_version
def define_vol_xml_str xml poolname __salt__['config get'] 'libvirt storagepool' 'default' conn __get_conn pool conn storagePoolLookupByName str poolname return pool createXML xml 0 is not None
def get_ptb_words_vocabulary return _retrieve_word_vocabulary
def s_size block_name offset 0 length 4 endian '<' format 'binary' inclusive False signed False math None fuzzable False name None if block_name in blocks CURRENT block_stack raise sex SullyRuntimeError 'CANNOTADDASIZEFORABLOCKCURRENTLYINTHESTACK' size blocks size block_name blocks CURRENT offset length endian format inclusive signed math fuzzable name blocks CURRENT push size
def __validate__ config if not isinstance config dict return False 'Configurationforstatusbeaconmustbeadictionary ' return True 'Validbeaconconfiguration'
def hostgroup_exists name None groupid None node None nodeids None **connection_args conn_args _login **connection_args zabbix_version apiinfo_version **connection_args try if conn_args if LooseVersion zabbix_version > LooseVersion '2 5' if not groupid groupid Noneif not name name Noneret hostgroup_get name groupid **connection_args return bool ret else params {}method 'hostgroup exists'if groupid params['groupid'] groupidif name params['name'] nameif LooseVersion zabbix_version < LooseVersion '2 4' if node params['node'] nodeif nodeids params['nodeids'] nodeidsif not groupid and not name and not node and not nodeids return {'result' False 'comment' 'Pleasesubmitgroupid name nodeornodeidsparametertocheckifatleastonehostgroupthatmatchesthegivenfiltercriteriaexists '}ret _query method params conn_args['url'] conn_args['auth'] return ret['result']else raise KeyErrorexcept KeyError return False
def make_browser_model browser filter_type None factories {FilterType instrument_hotswap make_instruments_browser_model FilterType drum_pad_hotswap make_drum_pad_browser_model FilterType audio_effect_hotswap make_audio_effect_browser_model FilterType midi_effect_hotswap make_midi_effect_browser_model}if filter_type None filter_type filter_type_for_browser browser return factories get filter_type make_fallback_browser_model browser
def _get_file_from_s3 creds metadata saltenv bucket path cached_file_path if os path isfile cached_file_path file_meta _find_file_meta metadata bucket saltenv path file_md5 '' join list filter str isalnum file_meta['ETag'] if file_meta else None cached_md5 salt utils get_hash cached_file_path 'md5' log debug 'Cachedfile path {0} md5 {1} etag {2}' format cached_file_path cached_md5 file_md5 log debug 'Cachedfile path {0} md5 {1} etag {2}' format cached_file_path cached_md5 file_md5 if cached_md5 file_md5 return__utils__['s3 query'] key creds key keyid creds keyid kms_keyid creds kms_keyid bucket bucket service_url creds service_url path _quote path local_file cached_file_path verify_ssl creds verify_ssl location creds location path_style creds path_style https_enable creds https_enable
def setDefaultPrivacyList disp listname None return setActivePrivacyList disp listname 'default'
def setDefaultPrivacyList disp listname None return setActivePrivacyList disp listname 'default'
def getSequenceIndexPlusOneFromText fileText craftSequence getReadCraftSequence for craftSequenceIndex in xrange len craftSequence - 1 -1 -1 procedure craftSequence[craftSequenceIndex]if gcodec isProcedureDone fileText procedure return craftSequenceIndex + 1 return 0
def keyed_md5 secret challenge warnings warn 'keyed_md5 isdeprecated Usethestdlibmodulehmacinstead ' DeprecationWarning stacklevel 2 return hmac HMAC secret challenge hexdigest
def get_ip_addresses include_loopback True system platform system if system lower in ['linux' 'darwin' 'macosx'] ips [iface get 'inet' for iface in ifcfg interfaces values ]elif system lower 'windows' ipconfig os popen 'ipconfig/all' read ips [match[1] for match in re findall 'IP v4 ?Address[\\ \\ ]+ [\\d\\ ]+ ' ipconfig ]else ips []ips set ips - set [None ''] if include_loopback ips ips union ['127 0 0 1'] else ips ips - set ['127 0 0 1'] return list ips
def validate file_ listed None subtask None from utils import ValidationAnnotatorannotator ValidationAnnotator file_ listed listed task_id cache get annotator cache_key if task_id return AsyncResult task_id else chain annotator taskif subtask is not None chain subtaskresult chain delay cache set annotator cache_key result task_id 5 * 60 return result
def _pr_compile regex cleanup None return _re_compile regex cleanup
def setup_platform hass config add_devices disc_info None if disc_info is None returnif not any [data[CONF_TRACK] for data in disc_info[CONF_ENTITIES]] returncalendar_service GoogleCalendarService hass config path TOKEN_FILE add_devices [GoogleCalendarEventDevice hass calendar_service disc_info[CONF_CAL_ID] data for data in disc_info[CONF_ENTITIES] if data[CONF_TRACK]]
def to_dict sentence token_str list children [[] for token in sentence token]root -1 for i in range 0 len sentence token token sentence token[i]token_str append '%s%s%s@%d' % token word token tag token label i + 1 if token head -1 root ielse children[token head] append i def _get_dict i d collections OrderedDict for c in children[i] d[token_str[c]] _get_dict c return dtree collections OrderedDict tree[token_str[root]] _get_dict root return tree
def write obj handle **kwargs trees list obj writer NewickIO Writer trees nexus_trees [ TREE_TEMPLATE % {'index' idx + 1 'tree' nwk} for idx nwk in enumerate writer to_strings plain False plain_newick True **kwargs ]tax_labels [str x name for x in chain * t get_terminals for t in trees ]text NEX_TEMPLATE % {'count' len tax_labels 'labels' '' join tax_labels 'trees' '\n' join nexus_trees } handle write text return len nexus_trees
def write obj handle **kwargs trees list obj writer NewickIO Writer trees nexus_trees [ TREE_TEMPLATE % {'index' idx + 1 'tree' nwk} for idx nwk in enumerate writer to_strings plain False plain_newick True **kwargs ]tax_labels [str x name for x in chain * t get_terminals for t in trees ]text NEX_TEMPLATE % {'count' len tax_labels 'labels' '' join tax_labels 'trees' '\n' join nexus_trees } handle write text return len nexus_trees
def write obj handle **kwargs trees list obj writer NewickIO Writer trees nexus_trees [ TREE_TEMPLATE % {'index' idx + 1 'tree' nwk} for idx nwk in enumerate writer to_strings plain False plain_newick True **kwargs ]tax_labels [str x name for x in chain * t get_terminals for t in trees ]text NEX_TEMPLATE % {'count' len tax_labels 'labels' '' join tax_labels 'trees' '\n' join nexus_trees } handle write text return len nexus_trees
def get_sample sample_output check_output [NODE_TOOL 'rangekeysample'] keys [{'key' key strip decode 'hex' 'size' 0} for key in sample_output splitlines [1 ]]sorted keys key lambda key key['key'] return keys
def CreateBinaryConfigPaths token None required_urns set try for platform in SUPPORTED_PLATFORMS required_urns add 'aff4 /config/executables/%s/agentupdates' % platform required_urns add 'aff4 /config/executables/%s/installers' % platform existing_urns [x['urn'] for x in aff4 FACTORY Stat list required_urns token token ]missing_urns required_urns - set existing_urns for urn in missing_urns aff4 FACTORY Create urn aff4 AFF4Volume token token Flush except access_control UnauthorizedAccess logging info 'Userisnotadmin cannotcheckconfigurationtree ' return
def create_warning path error_message skip_file True print_string 'warning 'if skip_file print_string print_string + 'Skippingfile' + path + ' ' print_string print_string + error_message warning_message WarningResult message print_string error False warning True return warning_message
def create_warning path error_message skip_file True print_string 'warning 'if skip_file print_string print_string + 'Skippingfile' + path + ' ' print_string print_string + error_message warning_message WarningResult message print_string error False warning True return warning_message
def test_strongly_typed_events def f global calledcalled Trueglobal calledcalled Falseev IronPythonTest Events ev InstanceTest + IronPythonTest EventTestDelegate f ev CallInstance AreEqual called True called Falseev InstanceTest - IronPythonTest EventTestDelegate f ev CallInstance AreEqual called False
def test_strongly_typed_events def f global calledcalled Trueglobal calledcalled Falseev IronPythonTest Events ev InstanceTest + IronPythonTest EventTestDelegate f ev CallInstance AreEqual called True called Falseev InstanceTest - IronPythonTest EventTestDelegate f ev CallInstance AreEqual called False
def test_strongly_typed_events def f global calledcalled Trueglobal calledcalled Falseev IronPythonTest Events ev InstanceTest + IronPythonTest EventTestDelegate f ev CallInstance AreEqual called True called Falseev InstanceTest - IronPythonTest EventTestDelegate f ev CallInstance AreEqual called False
def test_strongly_typed_events def f global calledcalled Trueglobal calledcalled Falseev IronPythonTest Events ev InstanceTest + IronPythonTest EventTestDelegate f ev CallInstance AreEqual called True called Falseev InstanceTest - IronPythonTest EventTestDelegate f ev CallInstance AreEqual called False
def test_strongly_typed_events def f global calledcalled Trueglobal calledcalled Falseev IronPythonTest Events ev InstanceTest + IronPythonTest EventTestDelegate f ev CallInstance AreEqual called True called Falseev InstanceTest - IronPythonTest EventTestDelegate f ev CallInstance AreEqual called False
def test_strongly_typed_events def f global calledcalled Trueglobal calledcalled Falseev IronPythonTest Events ev InstanceTest + IronPythonTest EventTestDelegate f ev CallInstance AreEqual called True called Falseev InstanceTest - IronPythonTest EventTestDelegate f ev CallInstance AreEqual called False
def guess_repo path None global repodirif path repodir pathif not repodir repodir os environ get 'BUP_DIR' if not repodir repodir os path expanduser '~/ bup'
def rankedFitness R res zeros_like R l list zip R list range len R l sort for i _ j in enumerate l res[j] ireturn res
def reap_fileserver_cache_dir cache_base find_func for saltenv in os listdir cache_base env_base os path join cache_base saltenv for root dirs files in os walk env_base if len dirs 0 and len files 0 if time time - os path getctime root > 60 os rmdir root continuefor file_ in files file_path os path join root file_ file_rel_path os path relpath file_path env_base try filename _ hash_type file_rel_path rsplit ' ' 2 except ValueError log warning 'Foundinvalidhashfile[{0}]whenattemptingtoreapcachedirectory ' format file_ continueret find_func filename saltenv saltenv if ret['path'] '' os unlink file_path
def reap_fileserver_cache_dir cache_base find_func for saltenv in os listdir cache_base env_base os path join cache_base saltenv for root dirs files in os walk env_base if len dirs 0 and len files 0 if time time - os path getctime root > 60 os rmdir root continuefor file_ in files file_path os path join root file_ file_rel_path os path relpath file_path env_base try filename _ hash_type file_rel_path rsplit ' ' 2 except ValueError log warning 'Foundinvalidhashfile[{0}]whenattemptingtoreapcachedirectory ' format file_ continueret find_func filename saltenv saltenv if ret['path'] '' os unlink file_path
def reap_fileserver_cache_dir cache_base find_func for saltenv in os listdir cache_base env_base os path join cache_base saltenv for root dirs files in os walk env_base if len dirs 0 and len files 0 if time time - os path getctime root > 60 os rmdir root continuefor file_ in files file_path os path join root file_ file_rel_path os path relpath file_path env_base try filename _ hash_type file_rel_path rsplit ' ' 2 except ValueError log warning 'Foundinvalidhashfile[{0}]whenattemptingtoreapcachedirectory ' format file_ continueret find_func filename saltenv saltenv if ret['path'] '' os unlink file_path
@contextmanagerdef temporary_file_path root_dir None cleanup True suffix u'' permissions None with temporary_file root_dir cleanup cleanup suffix suffix permissions permissions as fd fd close yield fd name
@pytest fixture scope u'function' def remove_output_folder request def finalizer_remove_output_folder if os path exists u'output_folder' utils rmtree u'output_folder' request addfinalizer finalizer_remove_output_folder
@must_be_contributor_or_publicdef claim_user_post node **kwargs request_data request jsonunclaimed_user User load request_data['pk'] unclaimed_data unclaimed_user get_unclaimed_record node _primary_key if 'value' in request_data email request_data['value'] lower strip claimer get_user email email if claimer and claimer is_registered send_claim_registered_email claimer unclaimed_user node else send_claim_email email unclaimed_user node notify True elif 'claimerId' in request_data claimer_id request_data['claimerId']claimer User load claimer_id send_claim_registered_email claimer unclaimed_user node email claimer usernameelse raise HTTPError http BAD_REQUEST return {'status' 'success' 'email' email 'fullname' unclaimed_data['name']}
def set_num_instances instances server None version None if not isinstance instances long int raise TypeError "'instances'argmustbeoftypelongorint " req servers_service_pb SetNumInstancesRequest req set_instances instances if server req set_server server if version req set_version version resp servers_service_pb SetNumInstancesResponse try apiproxy_stub_map MakeSyncCall 'servers' 'SetNumInstances' req resp except apiproxy_errors ApplicationError as e if e application_error servers_service_pb ServersServiceError INVALID_VERSION raise InvalidVersionError elif e application_error servers_service_pb ServersServiceError TRANSIENT_ERROR raise TransientError else raise Error
def cellname x y assert x > 0 return colnum2name x + str y
def _dotUnquoter line if line startswith ' ' return line[1 ]return line
def _dotUnquoter line if line startswith ' ' return line[1 ]return line
def setup_platform hass config add_devices discovery_info None add_devices [DemoWeather 'South' 'Sunshine' 21 92 1099 0 5 TEMP_CELSIUS DemoWeather 'North' 'Showerrain' -12 54 987 4 8 TEMP_FAHRENHEIT ]
def _create_array_of_type t if t in _array_types return _array_types[t] array_type_name 'ArrayOf%s' % t array_type type array_type_name DataObject {} def __init__ self super array_type self __init__ array_type_name setattr self t [] setattr array_type '__init__' __init__ _array_types[t] array_typereturn array_type
def _get_service_manager host_reference return host_reference configManager serviceSystem
def get_upload_pipeline in_fd out_fd rate_limit None gpg_key None lzop True commands []if rate_limit is not None commands append PipeViewerRateLimitFilter rate_limit if lzop commands append LZOCompressionFilter if gpg_key is not None commands append GPGEncryptionFilter gpg_key return Pipeline commands in_fd out_fd
def get_upload_pipeline in_fd out_fd rate_limit None gpg_key None lzop True commands []if rate_limit is not None commands append PipeViewerRateLimitFilter rate_limit if lzop commands append LZOCompressionFilter if gpg_key is not None commands append GPGEncryptionFilter gpg_key return Pipeline commands in_fd out_fd
def get_upload_pipeline in_fd out_fd rate_limit None gpg_key None lzop True commands []if rate_limit is not None commands append PipeViewerRateLimitFilter rate_limit if lzop commands append LZOCompressionFilter if gpg_key is not None commands append GPGEncryptionFilter gpg_key return Pipeline commands in_fd out_fd
@scope definedef callpipe1 fn_list arg for f in fn_list arg f arg return arg
@scope definedef callpipe1 fn_list arg for f in fn_list arg f arg return arg
def libvlc_audio_get_mute p_mi f _Cfunctions get 'libvlc_audio_get_mute' None or _Cfunction 'libvlc_audio_get_mute' 1 None ctypes c_int MediaPlayer return f p_mi
def cbPickMailbox result proto mboxes mbox mboxes[ int result or '1' - 1 ]return proto examine mbox addCallback cbExamineMbox proto
def agent_build_get_by_triple context hypervisor os architecture return IMPL agent_build_get_by_triple context hypervisor os architecture
def eval_location2 pymodule offset pyname_finder ScopeNameFinder pymodule return pyname_finder get_primary_and_pyname_at offset
def eval_location2 pymodule offset pyname_finder ScopeNameFinder pymodule return pyname_finder get_primary_and_pyname_at offset
def biDiText text text s3_unicode text if biDiImported and current deployment_settings get_pdf_bidi isArabic FalseisBidi Falsefor c in text cat unicodedata bidirectional c if cat in 'AL' 'AN' isArabic TrueisBidi Truebreakelif cat in 'R' 'RLE' 'RLO' isBidi Trueif isArabic text arabic_reshaper reshape text if isBidi text get_display text return text
def biDiText text text s3_unicode text if biDiImported and current deployment_settings get_pdf_bidi isArabic FalseisBidi Falsefor c in text cat unicodedata bidirectional c if cat in 'AL' 'AN' isArabic TrueisBidi Truebreakelif cat in 'R' 'RLE' 'RLO' isBidi Trueif isArabic text arabic_reshaper reshape text if isBidi text get_display text return text
def membership_required function None def decorator request *args **kwargs group get_object_or_404 Group slug kwargs['slug'] if request user is_anonymous return HttpResponseRedirect reverse 'django contrib auth views login' if GroupMember objects is_member group request user return function request *args **kwargs else return HttpResponseRedirect reverse 'groups join' args [group slug] return decorator
def get_resource_manager_extra_kwargs f args allow_conflicts False hooks getattr f 'resource_manager_kwargs_hooks' [] extra_kwargs {}for hook in hooks hook_kwargs hook args hook_name hook __name__conflicting_keys set hook_kwargs keys & set extra_kwargs keys if conflicting_keys and not allow_conflicts msg _ "Hook'% hook_name s'isattemptingtoredefineattributes'% conflicting_keys s'" % {'hook_name' hook_name 'conflicting_keys' conflicting_keys} raise exceptions NoUniqueMatch msg extra_kwargs update hook_kwargs return extra_kwargs
def file_iter fname sep None for line in open fname if line and line[0] '#' yield line split sep
def file_iter fname sep None for line in open fname if line and line[0] '#' yield line split sep
def from_wire wire keyring None request_mac '' xfr False origin None tsig_ctx None multi False first True question_only False one_rr_per_rrset False ignore_trailing False m Message id 0 m keyring keyringm request_mac request_macm xfr xfrm origin originm tsig_ctx tsig_ctxm multi multim first firstreader _WireReader wire m question_only one_rr_per_rrset ignore_trailing reader read return m
def load_external_templates spec_base spider_name spider_dir join spec_base spider_name if not isdir spider_dir raise StopIterationfor name in os listdir spider_dir if not name endswith ' json' continuepath join spider_dir name with open path as f sample json load f sample_dir path[ - len ' json' ]if isdir sample_dir for fname in os listdir sample_dir if fname endswith ' html' with open join sample_dir fname as f attr fname[ - len ' html' ]sample[attr] read f version sample get 'version' '' yield _build_sample sample legacy version < '0 13 0'
def reset repo mode committish 'HEAD' if mode 'hard' raise ValueError 'hardistheonlymodecurrentlysupported' with open_repo_closing repo as r tree r[committish] treer reset_index tree
def reset repo mode committish 'HEAD' if mode 'hard' raise ValueError 'hardistheonlymodecurrentlysupported' with open_repo_closing repo as r tree r[committish] treer reset_index tree
def get_word_blacklist_regex return re compile '\\b ' + ' ' join map re escape settings CC_WORD_BLACKLIST + ' \\b'
def test_read_right_indented_table table '\n#comment withblanklineabove \n \nCol1Col2Col3\n \n33 4foo\n14 5bar\n \n'reader ascii get_reader Reader ascii RST dat reader read table assert_equal dat colnames ['Col1' 'Col2' 'Col3'] assert_equal dat[0][2] 'foo' assert_equal dat[1][0] 1
def _to_ANP_poly f ring domain ring domainf_ ring zeroif isinstance f ring domain PolynomialRing for monom coeff in f iterterms for mon coef in coeff iterterms m monom[0] + mon c domain [domain domain coef ] + [0] * monom[1] if m not in f_ f_[m] celse f_[m] + celse for monom coeff in f iterterms m monom[0] c domain [domain domain coeff ] + [0] * monom[1] if m not in f_ f_[m] celse f_[m] + creturn f_
def quality mime_type ranges parsed_ranges [parse_media_range r for r in ranges split ' ' ]return quality_parsed mime_type parsed_ranges
def _time_to_micros time seconds time hour * 60 * 60 + time minute * 60 + time second return 1000000 * seconds + time microsecond
def post_save_user instance raw created **kwargs from cms utils permissions import get_current_usercreator get_current_user if not creator or not created or creator is_anonymous returnpage_user PageUser user_ptr_id instance pk created_by creator page_user __dict__ update instance __dict__ page_user save
def pool_add pool_name **kwargs return ceph_cfg pool_add pool_name **kwargs
def install_middlewares app settings if settings get 'newrelic_config' ini_file settings['newrelic_config']env settings['newrelic_env']newrelic agent initialize ini_file env app newrelic agent WSGIApplicationWrapper app if asbool settings get 'profiler_enabled' profile_dir settings['profiler_dir']app ProfilerMiddleware app profile_dir profile_dir restrictions '*kinto core*' return app
def hamming_loss y_true y_pred labels None sample_weight None classes None if classes is not None warnings warn "'classes'wasrenamedto'labels'inversion0 18andwillberemovedin0 20 " DeprecationWarning labels classes y_type y_true y_pred _check_targets y_true y_pred if labels is None labels unique_labels y_true y_pred else labels np asarray labels if sample_weight is None weight_average 1 0else weight_average np mean sample_weight if y_type startswith 'multilabel' n_differences count_nonzero y_true - y_pred sample_weight sample_weight return n_differences / y_true shape[0] * len labels * weight_average elif y_type in ['binary' 'multiclass'] return _weighted_sum y_true y_pred sample_weight normalize True else raise ValueError '{0}isnotsupported' format y_type
def latin_to_vcg st for char in st if char not in string ascii_letters try num ord char if num > 192 st st replace char '\\fi%d' % ord char except passreturn st
def masscan_x509 output certificate output decode 'base64' newout []for hashtype hashname in [ 'md5' 'MD5 ' 'sha1' 'SHA-1 ' ] hashvalue hashlib new hashtype cert hexdigest newout append '%-7s%s\n' % hashname '' join hashvalue[i i + 4 ] for i in xrange 0 len hashvalue 4 b64cert certificate encode 'base64' newout append '-----BEGINCERTIFICATE-----\n' newout extend '%s\n' % b64cert[i i + 64 ] for i in xrange 0 len b64cert 64 newout append '-----ENDCERTIFICATE-----\n' return '' join newout
def grader_from_conf conf if isinstance conf CourseGrader return confsubgraders []for subgraderconf in conf subgraderconf subgraderconf copy weight subgraderconf pop 'weight' 0 try if 'min_count' in subgraderconf subgrader_class AssignmentFormatGraderelse raise ValueError 'Configurationhasnoappropriategraderclass ' bad_args invalid_args subgrader_class __init__ subgraderconf if len bad_args > 0 log warning 'Invalidargumentsforasubgrader %s' bad_args for key in bad_args del subgraderconf[key]subgrader subgrader_class **subgraderconf subgraders append subgrader subgrader category weight except TypeError ValueError as error msg 'Unabletoparsegraderconfiguration \n' + str subgraderconf + '\nErrorwas \n' + str error raise ValueError msg None sys exc_info [2]return WeightedSubsectionsGrader subgraders
@pytest mark cmddef test_add_vfolders_user_nofile with pytest raises CommandError as e call_command 'add_vfolders' assert 'toofewarguments' in str e
def build_addon_button text action title '' button {'text' text 'action' action}if title button['attributes'] 'title "{title}"data-toggle "tooltip"data-placement "right"' format title title return button
def _plot_topo_onpick event show_func orig_ax event inaxesif event inaxes is None or not hasattr orig_ax '_mne_ch_idx' and not hasattr orig_ax '_mne_axs' returnimport matplotlib pyplot as plttry if hasattr orig_ax '_mne_axs' x y event xdata event ydata for ax in orig_ax _mne_axs if x > ax pos[0] and y > ax pos[1] and x < ax pos[0] + ax pos[2] and y < ax pos[1] + ax pos[3] orig_ax axbreakelse returnch_idx orig_ax _mne_ch_idxface_color orig_ax _mne_ax_face_color fig ax plt subplots 1 plt title orig_ax _mne_ch_name ax set_axis_bgcolor face_color show_func ax ch_idx except Exception as err print err raise
def _ratings_success_msg app old_status old_modified if old_modified old_modified datetime strptime old_modified '%Y-%m-%dT%H %M %S' if old_status app status return _submission_msgs ['complete']elif old_modified app last_rated_time return _submission_msgs ['content_ratings_saved']
def _ratings_success_msg app old_status old_modified if old_modified old_modified datetime strptime old_modified '%Y-%m-%dT%H %M %S' if old_status app status return _submission_msgs ['complete']elif old_modified app last_rated_time return _submission_msgs ['content_ratings_saved']
def _ratings_success_msg app old_status old_modified if old_modified old_modified datetime strptime old_modified '%Y-%m-%dT%H %M %S' if old_status app status return _submission_msgs ['complete']elif old_modified app last_rated_time return _submission_msgs ['content_ratings_saved']
def _copy_gl_functions source dest constants False if isinstance source BaseGLProxy s {}for key in dir source s[key] getattr source key source selif not isinstance source dict source source __dict__if not isinstance dest dict dest dest __dict__funcnames [name for name in source keys if name startswith 'gl' ]for name in funcnames dest[name] source[name]if constants constnames [name for name in source keys if name startswith 'GL_' ]for name in constnames dest[name] source[name]
def _copy_gl_functions source dest constants False if isinstance source BaseGLProxy s {}for key in dir source s[key] getattr source key source selif not isinstance source dict source source __dict__if not isinstance dest dict dest dest __dict__funcnames [name for name in source keys if name startswith 'gl' ]for name in funcnames dest[name] source[name]if constants constnames [name for name in source keys if name startswith 'GL_' ]for name in constnames dest[name] source[name]
def require_params oauth_request parameters None if parameters is None parameters []params ['oauth_consumer_key' 'oauth_nonce' 'oauth_signature' 'oauth_signature_method' 'oauth_timestamp']params extend parameters missing list param for param in params if not oauth_request or param not in oauth_request if missing response HttpResponse 'MissingOAuthparameters %s' % ' ' join missing response status_code 401return responsereturn None
def clone_via_serialize obj s cPickle dumps obj get_pickle_protocol return cPickle loads s
def clone_via_serialize obj s cPickle dumps obj get_pickle_protocol return cPickle loads s
def rastrigin_scaled individual N len individual return 10 * N + sum 10 ** i / N - 1 * x ** 2 - 10 * cos 2 * pi * 10 ** i / N - 1 * x for i x in enumerate individual
def get_header_from_yaml lines try import yamlexcept ImportError raise ImportError '`importyaml`failed PyYAMLpackageisrequiredforECSVformat' from io misc yaml import AstropyLoaderclass TableLoader AstropyLoader '\nCustomLoaderthatconstructsOrderedDictfroman omapobject \nThisdoesnothingbutprovideanamespaceforaddingthe\ncustomodictconstructor \n'TableLoader add_constructor u'tag yaml org 2002 omap' _construct_odict header_yaml textwrap dedent '\n' join lines try header yaml load header_yaml Loader TableLoader except Exception as err raise YamlParseError str err return header
def _words_group input strlen words []nblanks input count '' nmax max nblanks len input // strlen + 1 arr np fromstring input + '' dtype binary_type 1 blank_loc np nonzero arr '' [0]offset 0xoffset 0for idx in range nmax try loc np nonzero blank_loc > strlen + offset [0][0]offset blank_loc[ loc - 1 ] + 1 if loc 0 offset -1 except Exception offset len input if offset < xoffset offset xoffset + strlen words append input[xoffset offset] if len input offset breakxoffset offsetreturn words
def _words_group input strlen words []nblanks input count '' nmax max nblanks len input // strlen + 1 arr np fromstring input + '' dtype binary_type 1 blank_loc np nonzero arr '' [0]offset 0xoffset 0for idx in range nmax try loc np nonzero blank_loc > strlen + offset [0][0]offset blank_loc[ loc - 1 ] + 1 if loc 0 offset -1 except Exception offset len input if offset < xoffset offset xoffset + strlen words append input[xoffset offset] if len input offset breakxoffset offsetreturn words
def _colonHex val bytecount pieces []for i in range bytecount - 1 -1 -1 piece 255 << i * 8 & val >> i * 8 pieces append '%02x' % piece chStr ' ' join pieces return chStr
def data2proddummy x groups np unique lmap tuple x tolist return x groups[ None ] all -1 T astype int [ -1 ]
def data2proddummy x groups np unique lmap tuple x tolist return x groups[ None ] all -1 T astype int [ -1 ]
@pytest mark parametrize 'fast_reader' [True False 'force'] def test_read_with_names_arg fast_reader with pytest raises ValueError dat ascii read ['cd' 'ef'] names 'a' guess False fast_reader fast_reader
def remove_hop_by_hop_headers headers headers[ ] [ key value for key value in headers if not is_hop_by_hop_header key ]
@docstring dedent_interpddef xscale *args **kwargs gca set_xscale *args **kwargs
def TimestampFromTicks ticks return Timestamp *time localtime ticks [ 6]
def isNegative phrase return bool re search "\\b no t ? don\\'t stop end \\b" phrase re IGNORECASE
def make_pidlockfile path acquire_timeout if not isinstance path basestring error ValueError 'Notafilesystempath % path r' % vars raise errorif not os path isabs path error ValueError 'Notanabsolutepath % path r' % vars raise errorlockfile pidlockfile TimeoutPIDLockFile path acquire_timeout return lockfile
def _check_marks quteproc quickmarks expected contains if quickmarks mark_file os path join quteproc basedir 'config' 'quickmarks' else mark_file os path join quteproc basedir 'config' 'bookmarks' 'urls' quteproc clear_data quteproc send_cmd ' save' quteproc wait_for message 'Savedto{}' format mark_file with open mark_file 'r' encoding 'utf-8' as f lines f readlines matched_line any utils pattern_match pattern expected value line rstrip '\n' for line in lines assert matched_line contains lines
def _check_marks quteproc quickmarks expected contains if quickmarks mark_file os path join quteproc basedir 'config' 'quickmarks' else mark_file os path join quteproc basedir 'config' 'bookmarks' 'urls' quteproc clear_data quteproc send_cmd ' save' quteproc wait_for message 'Savedto{}' format mark_file with open mark_file 'r' encoding 'utf-8' as f lines f readlines matched_line any utils pattern_match pattern expected value line rstrip '\n' for line in lines assert matched_line contains lines
def collect_all_bears_from_sections sections log_printer local_bears {}global_bears {}for section in sections bear_dirs sections[section] bear_dirs local_bears[section] global_bears[section] collect_bears bear_dirs ['**'] [BEAR_KIND LOCAL BEAR_KIND GLOBAL] log_printer warn_if_unused_glob False return local_bears global_bears
def collect_all_bears_from_sections sections log_printer local_bears {}global_bears {}for section in sections bear_dirs sections[section] bear_dirs local_bears[section] global_bears[section] collect_bears bear_dirs ['**'] [BEAR_KIND LOCAL BEAR_KIND GLOBAL] log_printer warn_if_unused_glob False return local_bears global_bears
def replaced fmri return _fmadm_action_fmri 'replaced' fmri
def sniffing interface cb try sniff iface interface prn cb stop_filter stopfilter store False lfilter lambda p Dot11Beacon in p or Dot11ProbeResp in p except socket error as e if e errno 100 passelse raise
def ae actual predicted return np abs np array actual - np array predicted
def available_content_databases pattern re compile 'content_ ?P<channel>[^_]+ _ ?P<language>[^_]+ sqlite' for filename in glob iglob django_settings DEFAULT_DATABASE_DIR match pattern search filename if match yield match group 1 2
def available_content_databases pattern re compile 'content_ ?P<channel>[^_]+ _ ?P<language>[^_]+ sqlite' for filename in glob iglob django_settings DEFAULT_DATABASE_DIR match pattern search filename if match yield match group 1 2
def responsive res space_compress None if space_compress is None space_compress not g template_debug if is_api res res or u'' if not c allowed_callback and request environ get 'WANT_RAW_JSON' res scriptsafe_dumps res else res websafe_json simplejson dumps res if c allowed_callback res '/**/%s %s ' % websafe_json c allowed_callback res elif space_compress res spaceCompress res return res
def limited_join sep items max_chars 30 overflow_marker ' ' full_str sep join items if len full_str < max_chars return full_strn_chars 0n_items 0for j item in enumerate items n_chars + len item + len sep if n_chars < max_chars - len overflow_marker n_items + 1else breakreturn sep join list items[ n_items] + [overflow_marker]
def increment_lineno node n 1 for node in zip node walk node if 'lineno' in node _attributes node lineno getattr node 'lineno' 0 + n
@step CHECK_PREFIX + 'Ihavesentanemailwiththefollowinginthebody ' def mail_sent_content_multiline step return mail_sent_content step step multiline 'body'
def get_field_precision df doc None currency None from frappe utils import get_number_format_infoif cint df precision precision cint df precision elif df fieldtype u'Currency' number_format Noneif not currency and doc currency get_field_currency df doc if not currency currency frappe db get_default u'currency' if currency number_format frappe db get_value u'Currency' currency u'number_format' cache True if not number_format number_format frappe db get_default u'number_format' or u'# ### ##' decimal_str comma_str precision get_number_format_info number_format else precision cint frappe db get_default u'float_precision' or 3 return precision
def get_python_exec ver try return PYEXECS[ver]except KeyError raise ValueError 'Version%snotsupported/recognized' % ver
def unify_walk a b U if a __class__ b __class__ return Falseelif a b return Uelse return False
def unify_walk a b U if a __class__ b __class__ return Falseelif a b return Uelse return False
def unify_walk a b U if a __class__ b __class__ return Falseelif a b return Uelse return False
def hostgroup_list **connection_args conn_args _login **connection_args try if conn_args method 'hostgroup get'params {'output' 'extend'}ret _query method params conn_args['url'] conn_args['auth'] return ret['result']else raise KeyErrorexcept KeyError return False
def _getRecentTile layer coord format key layer coord format body use_by _recent_tiles['hash'] get key None 0 if body is None return Noneif time < use_by logging debug 'TileStache Core _addRecentTile foundtileinrecenttiles %s' key return bodytry del _recent_tiles['hash'][key]except KeyError passreturn None
def find_globals g tree for child in tree if hasattr child 'body' and isinstance child body list find_globals g child body elif isinstance child ast FunctionDef ast ClassDef g add child name continueelif isinstance child ast Assign try g add child targets[0] id except IndexError AttributeError passelif isinstance child ast Import g add child names[0] name elif isinstance child ast ImportFrom for name in child names g_name name asname or name name if g_name '*' continueg add g_name
def find_globals g tree for child in tree if hasattr child 'body' and isinstance child body list find_globals g child body elif isinstance child ast FunctionDef ast ClassDef g add child name continueelif isinstance child ast Assign try g add child targets[0] id except IndexError AttributeError passelif isinstance child ast Import g add child names[0] name elif isinstance child ast ImportFrom for name in child names g_name name asname or name name if g_name '*' continueg add g_name
def asset_get_current_log asset_id table current s3db asset_logquery table asset_id asset_id & table cancel False & table deleted False asset_log current db query select table id table status table datetime table cond table person_id table organisation_id table site_id orderby ~ table datetime limitby 0 1 first if asset_log return Storage datetime asset_log datetime person_id asset_log person_id cond int asset_log cond or 0 status int asset_log status or 0 organisation_id asset_log organisation_id site_id asset_log site_id else return Storage
def markFailed epObj log_str u''try with epObj lock quality Quality splitCompositeStatus epObj status [1]epObj status Quality compositeStatus FAILED quality epObj saveToDB except EpisodeNotFoundException as e logger log u'Unabletogetepisode pleasesetitsstatusmanually ' + ex e logger WARNING return log_str
@task@timeddef i18n_validate_transifex_config home path '~' expanduser config home / ' transifexrc' if not config isfile or config getsize 0 msg colorize 'red' 'CannotconnecttoTransifex configfileismissingorempty {config}\nSeehttp //help transifex com/features/client/#transifexrc\n' format config config sys stderr write msg sys exit 1
@given u'arunhavingmixedtextcontent' def given_a_run_having_mixed_text_content context r_xml u'<w r%s>\n<w t>abc</w t>\n<w tab/>\n<w t>def</w t>\n<w cr/>\n<w t>ghi</w t>\n<w drawing/>\n<w br/>\n<w t>jkl</w t>\n</w r>' % nsdecls u'w' r parse_xml r_xml context run Run r None
def get_current_request_hostname hostname Nonerequest get_current_request if request hostname request META get 'HTTP_HOST' return hostname
def get_current_request_hostname hostname Nonerequest get_current_request if request hostname request META get 'HTTP_HOST' return hostname
def storage_service_key bucket file_name key_name '{}/{}' format settings VIDEO_UPLOAD_PIPELINE get 'ROOT_PATH' '' file_name return s3 key Key bucket key_name
def determine_version api discovery make_service 'discovery' 'v1' response discovery apis list name api preferred True execute if not response get 'items' raise ValueError 'UnknownAPI"{0}" ' format api return response['items'][0]['version']
def getSelectedRadioPlugin names radioPlugins for radioPlugin in radioPlugins if radioPlugin value return radioPluginfor name in names for radioPlugin in radioPlugins if radioPlugin name name radioPlugin value Truereturn radioPluginprint 'thisshouldneverhappen nogetSelectedRadioPlugininsettings'print namesreturn radioPlugin[0]
def set_shutdown_hook hook if hook is not None and not callable hook raise TypeError 'hookmustbecallable got%s' % hook __class__ global __shutdown_hookwith __shutdown_mutex old_hook __shutdown_hook__shutdown_hook hookreturn old_hook
def check_modify_host_locking host update_data locked update_data get 'locked' None if locked is not None if locked and host locked raise model_logic ValidationError {'locked' 'Hostalreadylockedby%son%s ' % host locked_by host lock_time } if not locked and not host locked raise model_logic ValidationError {'locked' 'Hostalreadyunlocked '}
def _create_and_add_option option global _current_option_current_option Option type_ params _expand_one_key_dictionary option _current_option type type__create_and_add_parameters params _current_statement add_child _current_option
def get_hash path form 'sha256' chunk_size 65536 return salt utils get_hash os path expanduser path form chunk_size
def get_hash path form 'sha256' chunk_size 65536 return salt utils get_hash os path expanduser path form chunk_size
def get_hash path form 'sha256' chunk_size 65536 return salt utils get_hash os path expanduser path form chunk_size
def get_hash path form 'sha256' chunk_size 65536 return salt utils get_hash os path expanduser path form chunk_size
def get_hash path form 'sha256' chunk_size 65536 return salt utils get_hash os path expanduser path form chunk_size
def get_quotas_tenant profile None conn _auth profile return conn get_quotas_tenant
def get_quotas_tenant profile None conn _auth profile return conn get_quotas_tenant
def get_quotas_tenant profile None conn _auth profile return conn get_quotas_tenant
@pytest mark skipif not lb_enabled reason 'LoadBalancerdisabled' def test_if_minuteman_routes_to_named_vip dcos_api_session origin_app origin_uuid get_test_app origin_app['portDefinitions'][0]['labels'] {'VIP_0' 'foo 5000'}with dcos_api_session marathon deploy_and_cleanup origin_app proxy_app proxy_uuid get_test_app with dcos_api_session marathon deploy_and_cleanup proxy_app as service_points cmd '/opt/mesosphere/bin/curl-s-f-m5http //foo marathon l4lb thisdcos directory 5000/ping'ensure_routable cmd service_points
def make_NQueens_CSP n vars list range n domains list range n neighbors {}for v in vars neighbors[v] vars[ ]neighbors[v] remove v return NQueensCSP vars vars domains defaultdict lambda domains neighbors neighbors binary_constraint queens_constraint
def find_prototypes code prots []lines code split '\n' for line in lines m re match '\\s*' + re_func_prot line if m is not None rtype name args m groups [ 3]if args 'void' or args strip '' args []else args [tuple arg strip split '' for arg in args split ' ' ]prots append name args rtype return prots
def find_prototypes code prots []lines code split '\n' for line in lines m re match '\\s*' + re_func_prot line if m is not None rtype name args m groups [ 3]if args 'void' or args strip '' args []else args [tuple arg strip split '' for arg in args split ' ' ]prots append name args rtype return prots
@cli command 'open' @click option '-i' '--image' 'images' type click Path multiple True help 'Theimagefiletoopen ' @generatordef open_cmd images for image in images try click echo 'Opening"%s"' % image if image '-' img Image open click get_binary_stdin img filename '-'else img Image open image yield img except Exception as e click echo 'Couldnotopenimage"%s" %s' % image e err True
def get_default_page_content_type return ContentType objects get_for_model Page
def lead_text top_elem num_words 10 pat re compile u'\\s+' flags re UNICODE words []def get_text x attr u'text' ans getattr x attr if ans words extend filter None pat split ans stack [ top_elem u'text' ]while stack and len words < num_words elem attr stack pop get_text elem attr if attr u'text' if elem is not top_elem stack append elem u'tail' stack extend reversed list c u'text' for c in elem iterchildren u'*' return u'' join words[ num_words]
def testparagraph testpara paragraph 'paratext' style 'BodyText' assert testpara tag '{http //schemas openxmlformats org/wordprocessingml/2006/main}p' pass
def clean text tags ALLOWED_TAGS attributes ALLOWED_ATTRIBUTES styles ALLOWED_STYLES strip False strip_comments True if not text return u''text force_unicode text if text startswith u'< --' text u'' + text class s BleachSanitizer allowed_elements tagsallowed_attributes attributesallowed_css_properties stylesstrip_disallowed_elements stripstrip_html_comments strip_commentsparser html5lib HTMLParser tokenizer s return _render parser parseFragment text strip
def _is_user_profile_visible self user None try if hasattr self u'is_private' is_private self is_privateelse is_private self get_profile is_privatereturn user and user self or user is_staff or not is_private except Profile DoesNotExist return True
def runtime_hooks return [join curdir 'pyi_rth_kivy py' ]
def _get_processor_decline_html params payment_support_email configuration_helpers get_value 'payment_support_email' settings PAYMENT_SUPPORT_EMAIL return _format_error_html _ 'Sorry Ourpaymentprocessordidnotacceptyourpayment Thedecisiontheyreturnedwas{decision} andthereasonwas{reason} Youwerenotcharged Pleasetryadifferentformofpayment Contactuswithpayment-relatedquestionsat{email} ' format decision '<spanclass "decision">{decision}</span>' format decision params['decision'] reason '<spanclass "reason">{reason_code} {reason_msg}</span>' format reason_code params['reason_code'] reason_msg REASONCODE_MAP get params['reason_code'] email payment_support_email
def _fix_osmesa_gl_lib_if_testing test_name os getenv '_VISPY_TESTING_APP' None if test_name 'osmesa' from util osmesa_gl import fix_osmesa_gl_libfix_osmesa_gl_lib
def new key return PKCS115_SigScheme key
def seam_carve img energy_map mode num border 1 force_copy True utils assert_nD img 2 3 image util img_as_float img force_copy energy_map util img_as_float energy_map force_copy if image ndim 2 image image[ np newaxis]if mode 'horizontal' image np transpose image 1 0 2 image np ascontiguousarray image out _seam_carve_v image energy_map num border if mode 'horizontal' out np transpose out 1 0 2 return np squeeze out
def seam_carve img energy_map mode num border 1 force_copy True utils assert_nD img 2 3 image util img_as_float img force_copy energy_map util img_as_float energy_map force_copy if image ndim 2 image image[ np newaxis]if mode 'horizontal' image np transpose image 1 0 2 image np ascontiguousarray image out _seam_carve_v image energy_map num border if mode 'horizontal' out np transpose out 1 0 2 return np squeeze out
def new_state trans tool invalid False state galaxy tools DefaultToolState state inputs {}if invalid return statetry return tool new_state trans except Exception as e log debug 'Failedtobuildtoolstatefortool"%s"usingstandardmethod willtrytofallbackoncustommethod %s' tool id e inputs tool inputs_by_page[0]context ExpressionContext state inputs parent None for input in inputs values try state inputs[input name] input get_initial_value trans context except state inputs[input name] []return state
def kaiser_beta a if a > 50 beta 0 1102 * a - 8 7 elif a > 21 beta 0 5842 * a - 21 ** 0 4 + 0 07886 * a - 21 else beta 0 0return beta
def push_on_stack_section args if len args 0 return ''parts ['self->deeplevel++ ']for idx in xrange len args parts append 'self->arguments_stack[self->deeplevel "arg%d"] self->arg%d \n DCTB self->arg%d arg%d ' % idx idx idx idx return '\n DCTB ' join parts
def rand_text_alpha_upper length bad '' return rand_base length bad set upperAlpha
@content_type 'application/json' def json body charset 'utf-8' **kwargs return json_converter loads text body charset charset
def _resolve_id val return val if isinstance val six string_types else val id
def create_permission_grant role_db resource_uid resource_type permission_types permission_grant_db PermissionGrantDB resource_uid resource_uid resource_type resource_type permission_types permission_types permission_grant_db PermissionGrant add_or_update permission_grant_db role_db update push__permission_grants str permission_grant_db id return permission_grant_db
def test_ast_good_import_from can_compile u' import[x[y]] '
def report_messages_stats sect stats _ if not stats['by_msg'] raise utils EmptyReport in_order sorted [ value msg_id for msg_id value in six iteritems stats['by_msg'] if not msg_id startswith 'I' ] in_order reverse lines 'messageid' 'occurrences' for value msg_id in in_order lines + msg_id str value sect append ureports Table children lines cols 2 rheaders 1
@task task ignore_result True def store_friends user friends converter_class get_class_for 'user_conversion' logger info 'celeryisstoring%sfriends' % len friends converter_class _store_friends user friends return friends
@task task ignore_result True def store_friends user friends converter_class get_class_for 'user_conversion' logger info 'celeryisstoring%sfriends' % len friends converter_class _store_friends user friends return friends
@task task ignore_result True def store_friends user friends converter_class get_class_for 'user_conversion' logger info 'celeryisstoring%sfriends' % len friends converter_class _store_friends user friends return friends
@task task ignore_result True def store_friends user friends converter_class get_class_for 'user_conversion' logger info 'celeryisstoring%sfriends' % len friends converter_class _store_friends user friends return friends
def get_collection_rights collection_id strict True model collection_models CollectionRightsModel get collection_id strict strict if model is None return Nonereturn _get_activity_rights_from_model model feconf ACTIVITY_TYPE_COLLECTION
def get_dataset_directory dataset_name create_directory True path os path join _dataset_root dataset_name if create_directory try os makedirs path except OSError passreturn path
def hostgroup_delete hostgroupids **connection_args conn_args _login **connection_args try if conn_args method 'hostgroup delete'if not isinstance hostgroupids list params [hostgroupids]else params hostgroupidsret _query method params conn_args['url'] conn_args['auth'] return ret['result']['groupids']else raise KeyErrorexcept KeyError return ret
def findCertainShow showList indexerid if indexerid is None or showList is None or len showList 0 return Noneindexer_ids [indexerid] if not isinstance indexerid list else indexerid results [show for show in showList if show indexerid in indexer_ids ]if not results return Noneif len results 1 return results[0]raise MultipleShowObjectsException
def tenant_create name description None enabled True profile None **connection_args kstone auth profile **connection_args new getattr kstone _TENANTS None create name description enabled return tenant_get new id profile profile **connection_args
def minute_to_session column close_locs data out if column 'open' _minute_to_session_open close_locs data out elif column 'high' _minute_to_session_high close_locs data out elif column 'low' _minute_to_session_low close_locs data out elif column 'close' _minute_to_session_close close_locs data out elif column 'volume' _minute_to_session_volume close_locs data out return out
def minute_to_session column close_locs data out if column 'open' _minute_to_session_open close_locs data out elif column 'high' _minute_to_session_high close_locs data out elif column 'low' _minute_to_session_low close_locs data out elif column 'close' _minute_to_session_close close_locs data out elif column 'volume' _minute_to_session_volume close_locs data out return out
def make_default_headers n return tuple agate utils letter_name i for i in range n
def dic trace model None model modelcontext model mean_deviance -2 * np mean [model logp pt for pt in trace] free_rv_means {rv name trace[rv name] mean axis 0 for rv in model free_RVs}deviance_at_mean -2 * model logp free_rv_means return 2 * mean_deviance - deviance_at_mean
def testWithNoKwargs io launchHubServer keyboard io devices keyboardprint print '**PRESSAKEYTOCONTINUE ' while not keyboard getEvents io wait 0 25 print 'AKeyboardEventwasDetected exitingTest ' io quit
def post_order_list node filter_func no_filter l stack [] [] poped index 0 0 while node if filter_func node if node children and not poped stack append node index index 0node node children[0]else l append node index + 1try node stack[ -1 ][0] children[index]except IndexError node Noneelse node Nonepoped 0if node is None and stack node index stack pop poped 1return l
def post_order_list node filter_func no_filter l stack [] [] poped index 0 0 while node if filter_func node if node children and not poped stack append node index index 0node node children[0]else l append node index + 1try node stack[ -1 ][0] children[index]except IndexError node Noneelse node Nonepoped 0if node is None and stack node index stack pop poped 1return l
@task default True def test args None tests '' if args else 'tests' default_args '-sv--with-doctest--nologcapture--with-color%s' % tests default_args + '' + args if args else '' nose core run_exit argv [''] + default_args split
def deprecated_call func None *args **kwargs if not func return WarningsChecker expected_warning DeprecationWarning categories []def warn_explicit message category *args **kwargs categories append category old_warn_explicit message category *args **kwargs def warn message category None *args **kwargs if isinstance message Warning categories append message __class__ else categories append category old_warn message category *args **kwargs old_warn warnings warnold_warn_explicit warnings warn_explicitwarnings warn_explicit warn_explicitwarnings warn warntry ret func *args **kwargs finally warnings warn_explicit old_warn_explicitwarnings warn old_warndeprecation_categories DeprecationWarning PendingDeprecationWarning if not any issubclass c deprecation_categories for c in categories __tracebackhide__ Trueraise AssertionError '%rdidnotproduceDeprecationWarning' % func return ret
def deprecated_call func None *args **kwargs if not func return WarningsChecker expected_warning DeprecationWarning categories []def warn_explicit message category *args **kwargs categories append category old_warn_explicit message category *args **kwargs def warn message category None *args **kwargs if isinstance message Warning categories append message __class__ else categories append category old_warn message category *args **kwargs old_warn warnings warnold_warn_explicit warnings warn_explicitwarnings warn_explicit warn_explicitwarnings warn warntry ret func *args **kwargs finally warnings warn_explicit old_warn_explicitwarnings warn old_warndeprecation_categories DeprecationWarning PendingDeprecationWarning if not any issubclass c deprecation_categories for c in categories __tracebackhide__ Trueraise AssertionError '%rdidnotproduceDeprecationWarning' % func return ret
def is_hash_empty context builder h empty ir Constant h type EMPTY return builder icmp_unsigned ' ' h empty
def raises_msg exc msg func *args **kwds try func *args **kwds except exc as err assert msg in str err '"%s"notin"%s"' % msg err else raise AssertionError '%sdidnotraise%s' % func __name__ _excstr exc
def raises_msg exc msg func *args **kwds try func *args **kwds except exc as err assert msg in str err '"%s"notin"%s"' % msg err else raise AssertionError '%sdidnotraise%s' % func __name__ _excstr exc
def raises_msg exc msg func *args **kwds try func *args **kwds except exc as err assert msg in str err '"%s"notin"%s"' % msg err else raise AssertionError '%sdidnotraise%s' % func __name__ _excstr exc
def randompass import randomimport stringrandom seed lower '' join random choice string ascii_lowercase for x in range 6 upper '' join random choice string ascii_uppercase for x in range 6 number '' join random choice string digits for x in range 6 punct '' join random choice string punctuation for x in range 6 p lower + upper + number + punct return '' join random sample p len p
def randompass import randomimport stringrandom seed lower '' join random choice string ascii_lowercase for x in range 6 upper '' join random choice string ascii_uppercase for x in range 6 number '' join random choice string digits for x in range 6 punct '' join random choice string punctuation for x in range 6 p lower + upper + number + punct return '' join random sample p len p
def randompass import randomimport stringrandom seed lower '' join random choice string ascii_lowercase for x in range 6 upper '' join random choice string ascii_uppercase for x in range 6 number '' join random choice string digits for x in range 6 punct '' join random choice string punctuation for x in range 6 p lower + upper + number + punct return '' join random sample p len p
def config_option_update context data_dict return {'success' False}
def get_limit return _limit[0]
def construct tuple obj RSAobj if len tuple not in [2 3 5 6] raise error 'argumentforconstruct wronglength'for i in range len tuple field obj keydata[i]setattr obj field tuple[i] if len tuple > 5 if obj p > obj q obj p obj q obj q obj p if len tuple 5 obj u pubkey inverse obj p obj q return obj
def construct tuple obj RSAobj if len tuple not in [2 3 5 6] raise error 'argumentforconstruct wronglength'for i in range len tuple field obj keydata[i]setattr obj field tuple[i] if len tuple > 5 if obj p > obj q obj p obj q obj q obj p if len tuple 5 obj u pubkey inverse obj p obj q return obj
def vm_disk_snapshot_create name kwargs None call None if call 'action' raise SaltCloudSystemExit 'Thevm_disk_snapshot_createactionmustbecalledwith-aor--action ' if kwargs is None kwargs {}disk_id kwargs get 'disk_id' None description kwargs get 'description' None if disk_id is None or description is None raise SaltCloudSystemExit "Thevm_disk_snapshot_createfunctionrequiresa'disk_id'anda'description'tobeprovided " server user password _get_xml_rpc auth ' ' join [user password] vm_id int get_vm_id kwargs {'name' name} response server one vm disksnapshotcreate auth vm_id int disk_id description data {'action' 'vm disksnapshotcreate' 'created' response[0] 'snapshot_id' response[1] 'error_code' response[2]}return data
def _get_grain proxy name grains _retrieve_grains proxy if grains get 'result' False and grains get 'out' {} return grains get 'out' get name
def prep_case_insensitive value value re sub '\\s+' '' value strip lower return value
def to_str obj encoding 'utf-8' force_string False if force_string and not isinstance obj basestring try obj str obj except Exception obj unicode obj if isinstance obj basestring and isinstance obj unicode try obj obj encode encoding return objexcept UnicodeEncodeError for alt_encoding in ENCODINGS try obj obj encode alt_encoding return objexcept UnicodeEncodeError passtry return latinify obj '?' except Exception as err raise Exception "%s Error Unicodecouldnotencodeunicodestring'%s' %s toabytestring " % err obj encoding return obj
def show_chain *chains **kw backrefs kw pop 'backrefs' True chains [chain for chain in chains if chain]def in_chains x ids set map id itertools chain *chains return id x in ids max_depth max map len chains - 1 if backrefs show_backrefs [chain[ -1 ] for chain in chains] max_depth max_depth filter in_chains **kw else show_refs [chain[0] for chain in chains] max_depth max_depth filter in_chains **kw
def network_create_safe context values return IMPL network_create_safe context values
def _newer a b if not os path exists a return Falseif not os path exists b return Truereturn os path getmtime a > os path getmtime b
def simple_moving_average iterable k 10 a iterable if isinstance iterable list else list iterable for m in xrange len a i m - k j m + k + 1 w a[max 0 i j] yield float sum w / len w or 1
def setup_platform hass config add_devices discovery_info None if discovery_info is None return_LOGGER debug 'Settingupnestthermostat' temp_unit hass config units temperature_unitadd_devices [NestThermostat structure device temp_unit for structure device in hass data[DATA_NEST] thermostats ] True
def compare_torrents torrent_1 torrent_2 files1 [files for files in torrent_1['metainfo'] get_files_with_length if files[1] > 1024 * 1024 ]files2 [files for files in torrent_2['metainfo'] get_files_with_length if files[1] > 1024 * 1024 ]if len files1 len files2 for ft1 in files1 for ft2 in files2 if ft1[1] ft2[1] or levenshtein_dist ft1[0] ft2[0] > SIMILARITY_TRESHOLD return Falsereturn Truereturn False
@with_devicedef which name which_cmd '\nIFS \nBINARY %s\nP $PATH \nforpathin"${P[@]}" doif[-e"$path/$BINARY"] thenecho"$path/$BINARY" \nbreak\nfi\ndone\n' % name which_cmd which_cmd strip return process ['sh' '-c' which_cmd] recvall strip
def get_users channel 14 **kwargs with _IpmiCommand **kwargs as c return c get_users channel
def posixpath_normpath pathname backslash '\\'pathname2 os path normpath pathname or ' ' if backslash in pathname2 pathname2 pathname2 replace backslash '/' return pathname2
def build_instance_groups parsed_instance_groups instance_groups []for instance_group in parsed_instance_groups ig_config {}keys instance_group keys if 'Name' in keys ig_config['Name'] instance_group['Name']else ig_config['Name'] instance_group['InstanceGroupType']ig_config['InstanceType'] instance_group['InstanceType']ig_config['InstanceCount'] instance_group['InstanceCount']ig_config['InstanceRole'] instance_group['InstanceGroupType'] upper if 'BidPrice' in keys ig_config['BidPrice'] instance_group['BidPrice']ig_config['Market'] constants SPOTelse ig_config['Market'] constants ON_DEMANDif 'EbsConfiguration' in keys ig_config['EbsConfiguration'] instance_group['EbsConfiguration']if 'AutoScalingPolicy' in keys ig_config['AutoScalingPolicy'] instance_group['AutoScalingPolicy']instance_groups append ig_config return instance_groups
def build_instance_groups parsed_instance_groups instance_groups []for instance_group in parsed_instance_groups ig_config {}keys instance_group keys if 'Name' in keys ig_config['Name'] instance_group['Name']else ig_config['Name'] instance_group['InstanceGroupType']ig_config['InstanceType'] instance_group['InstanceType']ig_config['InstanceCount'] instance_group['InstanceCount']ig_config['InstanceRole'] instance_group['InstanceGroupType'] upper if 'BidPrice' in keys ig_config['BidPrice'] instance_group['BidPrice']ig_config['Market'] constants SPOTelse ig_config['Market'] constants ON_DEMANDif 'EbsConfiguration' in keys ig_config['EbsConfiguration'] instance_group['EbsConfiguration']if 'AutoScalingPolicy' in keys ig_config['AutoScalingPolicy'] instance_group['AutoScalingPolicy']instance_groups append ig_config return instance_groups
def __virtual__ if salt utils is_windows global _genrepo _update_git_repos_genrepo salt utils namespaced_function _genrepo globals _update_git_repos salt utils namespaced_function _update_git_repos globals return __virtualname__return False 'ThismoduleonlyworksonWindows '
@skip 'silverlight' def test_event_lifetime def keep_alive o passdef test_runner import _weakrefglobal calledcalled 0a IronPythonTest Events def foo global calledcalled + 1foo abc aa InstanceTest + fooa CallInstance AreEqual called 1 ret_val _weakref ref foo keep_alive foo import gcfor i in xrange 10 gc collect a CallInstance AreEqual called 2 return ret_valfunc_ref test_runner import gcfor i in xrange 10 gc collect Assert not hasattr func_ref 'abc' AreEqual func_ref None AreEqual called 2
@skip 'silverlight' def test_event_lifetime def keep_alive o passdef test_runner import _weakrefglobal calledcalled 0a IronPythonTest Events def foo global calledcalled + 1foo abc aa InstanceTest + fooa CallInstance AreEqual called 1 ret_val _weakref ref foo keep_alive foo import gcfor i in xrange 10 gc collect a CallInstance AreEqual called 2 return ret_valfunc_ref test_runner import gcfor i in xrange 10 gc collect Assert not hasattr func_ref 'abc' AreEqual func_ref None AreEqual called 2
@skip 'silverlight' def test_event_lifetime def keep_alive o passdef test_runner import _weakrefglobal calledcalled 0a IronPythonTest Events def foo global calledcalled + 1foo abc aa InstanceTest + fooa CallInstance AreEqual called 1 ret_val _weakref ref foo keep_alive foo import gcfor i in xrange 10 gc collect a CallInstance AreEqual called 2 return ret_valfunc_ref test_runner import gcfor i in xrange 10 gc collect Assert not hasattr func_ref 'abc' AreEqual func_ref None AreEqual called 2
@skip 'silverlight' def test_event_lifetime def keep_alive o passdef test_runner import _weakrefglobal calledcalled 0a IronPythonTest Events def foo global calledcalled + 1foo abc aa InstanceTest + fooa CallInstance AreEqual called 1 ret_val _weakref ref foo keep_alive foo import gcfor i in xrange 10 gc collect a CallInstance AreEqual called 2 return ret_valfunc_ref test_runner import gcfor i in xrange 10 gc collect Assert not hasattr func_ref 'abc' AreEqual func_ref None AreEqual called 2
def hmac_signature string shared_secret challenge_hmac if six PY3 msg salt utils to_bytes string key salt utils to_bytes shared_secret challenge salt utils to_bytes challenge_hmac else msg stringkey shared_secretchallenge challenge_hmachmac_hash hmac new key msg hashlib sha256 valid_hmac base64 b64encode hmac_hash digest return valid_hmac challenge
def retry func max_attempts return _call_with_retry max_attempts func
def retry func max_attempts return _call_with_retry max_attempts func
@csrf_exemptdef ssl_login request if not settings FEATURES['AUTH_USE_CERTIFICATES'] return HttpResponseForbidden cert ssl_get_cert_from_request request if not cert return student views index request _user email fullname _ssl_dn_extract_info cert redirect_to get_next_url_for_login_page request retfun functools partial redirect redirect_to return _external_login_or_signup request external_id email external_domain 'ssl MIT' credentials cert email email fullname fullname retfun retfun
def format_playlist playlist show_url True out '\x02{}\x02' format playlist['title'] if playlist['description'] out + ' "{}"' format formatting truncate playlist['description'] if playlist['genre'] out + '-\x02{}\x02' format playlist['genre'] out + '-by\x02{}\x02' format playlist['user']['username'] if not playlist['tracks'] out + '-Noitems'else out + '-{}items ' format len playlist['tracks'] seconds round int playlist['duration'] / 1000 out + '{}' format timeformat format_time seconds simple True if show_url out + '-{}' format web try_shorten playlist['permalink_url'] return out
def _get_configuration_from_db shop configuration {}for conf_item in ConfigurationItem objects filter shop shop configuration[conf_item key] conf_item valuereturn configuration
def generateHubSequences nCoinc 10 hubs [2 6] seqLength [5 6 7] nSeq 100 coincList range nCoinc for hub in hubs coincList remove hub seqList []for i in xrange nSeq length random choice seqLength - 1 seq random sample coincList length seq insert length // 2 random choice hubs seqList append seq return seqList
def open_repo name temp_dir None if temp_dir is None temp_dir tempfile mkdtemp repo_dir os path join os path dirname __file__ 'data' 'repos' name temp_repo_dir os path join temp_dir name shutil copytree repo_dir temp_repo_dir symlinks True return Repo temp_repo_dir
def _namespace_to_ord namespace n 0for i c in enumerate namespace n + _LEX_DISTANCE[ MAX_NAMESPACE_LENGTH - i - 1 ] * NAMESPACE_CHARACTERS index c + 1 return n
def make_transparent_proxy global_conf force_host None force_scheme 'http' return TransparentProxy force_host force_host force_scheme force_scheme
def modClearRefs s titlesRefs namesRefs charactersRefs s modClearTitleRefs s {} {} {} s modClearCharacterRefs s {} {} {} return modClearNameRefs s {} {} {}
def _compile_func body body u'def{0} \n{1}' format FUNC_NAME body replace '\n' '\n' code compile body 'inline' 'exec' env {}eval code env return env[FUNC_NAME]
@frappe whitelist def get_default_address_template return u'{{address_line1}}<br>{%ifaddress_line2%}{{address_line2}}<br>{%endif-%}{{city}}<br>\n{%ifstate%}{{state}}<br>{%endif-%}\n{%ifpincode%}{{pincode}}<br>{%endif-%}\n{{country}}<br>\n{%ifphone%}' + _ u'Phone' + u' {{phone}}<br>{%endif-%}\n{%iffax%}' + _ u'Fax' + u' {{fax}}<br>{%endif-%}\n{%ifemail_id%}' + _ u'Email' + u' {{email_id}}<br>{%endif-%}'
def clear_inputhook pyos_inputhook_ptr ctypes c_void_p in_dll ctypes pythonapi 'PyOS_InputHook' pyos_inputhook_ptr value ctypes c_void_p None valueallow_CTRL_C
def clear_inputhook pyos_inputhook_ptr ctypes c_void_p in_dll ctypes pythonapi 'PyOS_InputHook' pyos_inputhook_ptr value ctypes c_void_p None valueallow_CTRL_C
def _get_initializers initializers fields result {}for f in fields if isinstance initializers dict if f in initializers result[f] _convert_to_initializer initializers[f] else result[f] _convert_to_initializer initializers return result
def explore_account c while True printfolderflags {}data c list_folders for flags delimiter name in data folderflags[name] flagsfor name in sorted folderflags keys print '%-30s%s' % name '' join folderflags[name] printreply raw_input 'Typeafoldername or"q"toquit ' strip if reply lower startswith 'q' breakif reply in folderflags explore_folder c reply else print 'Error nofoldernamed' repr reply
def test_search_any_case result list search_packages_info ['PIP'] assert len result 1 assert 'pip' result[0]['name']
def create_python27_start_cmd app_name login_ip port load_balancer_host xmpp_ip db_location DATASTORE_PATHcmd ['/usr/bin/python2' constants APPSCALE_HOME + '/AppServer/dev_appserver py' '--port' + str port '--admin_port' + str port + 10000 '--login_server' + login_ip '--skip_sdk_update_check' '--nginx_host' + str load_balancer_host '--require_indexes' '--enable_sendmail' '--xmpp_path' + xmpp_ip '--php_executable_path ' + str PHP_CGI_LOCATION '--uaserver_path' + db_location + ' ' + str constants UA_SERVER_PORT '--datastore_path' + db_location + ' ' + str constants DB_SERVER_PORT '/var/apps/' + app_name + '/app' '--host' + appscale_info get_private_ip ]if app_name in TRUSTED_APPS cmd extend [TRUSTED_FLAG] return '' join cmd
def duplication_divergence_graph n p seed None if p > 1 or p < 0 msg 'NetworkXErrorp {0}isnotin[0 1] ' format p raise nx NetworkXError msg if n < 2 msg 'nmustbegreaterthanorequalto2'raise nx NetworkXError msg if seed is not None random seed seed G nx Graph G name 'duplication_divergence_graph {} {} ' format n p G add_edge 0 1 i 2while i < n random_node random choice list G G add_node i flag Falsefor nbr in G neighbors random_node if random random < p G add_edge i nbr flag Trueif not flag G remove_node i else i + 1return G
def duplication_divergence_graph n p seed None if p > 1 or p < 0 msg 'NetworkXErrorp {0}isnotin[0 1] ' format p raise nx NetworkXError msg if n < 2 msg 'nmustbegreaterthanorequalto2'raise nx NetworkXError msg if seed is not None random seed seed G nx Graph G name 'duplication_divergence_graph {} {} ' format n p G add_edge 0 1 i 2while i < n random_node random choice list G G add_node i flag Falsefor nbr in G neighbors random_node if random random < p G add_edge i nbr flag Trueif not flag G remove_node i else i + 1return G
def files_with_suffix base_path suffix if os path isfile base_path if base_path endswith suffix yield base_path else for root _ files in os walk base_path for filename in files if filename endswith suffix yield os path join root filename
def SequenceToImageAndDiff images image_diff_list []image_seq tf unstack images axis 1 for size in [32 64 128 256] resized_images [tf image resize_images i [size size] for i in image_seq]diffs []for i in xrange 0 len resized_images - 1 diffs append resized_images[ i + 1 ] - resized_images[i] image_diff_list append tf concat 0 resized_images[ -1 ] tf concat 0 diffs return image_diff_list
def SequenceToImageAndDiff images image_diff_list []image_seq tf unstack images axis 1 for size in [32 64 128 256] resized_images [tf image resize_images i [size size] for i in image_seq]diffs []for i in xrange 0 len resized_images - 1 diffs append resized_images[ i + 1 ] - resized_images[i] image_diff_list append tf concat 0 resized_images[ -1 ] tf concat 0 diffs return image_diff_list
def release_vlanid vlan_id LOG debug _ 'release_vlanid called' session db get_session try vlanid session query network_models_v2 VlanID filter_by vlan_id vlan_id one vlanid['vlan_used'] Falsesession merge vlanid session flush return vlanid['vlan_used']except exc NoResultFound raise c_exc VlanIDNotFound vlan_id vlan_id return
def release_vlanid vlan_id LOG debug _ 'release_vlanid called' session db get_session try vlanid session query network_models_v2 VlanID filter_by vlan_id vlan_id one vlanid['vlan_used'] Falsesession merge vlanid session flush return vlanid['vlan_used']except exc NoResultFound raise c_exc VlanIDNotFound vlan_id vlan_id return
@pytest mark parametrize 'parallel' [True False] def test_many_rows parallel read_basic text 'ABC\n'for i in range 500 text + '' join [str i for i in range 3 ] text + '\n'table read_basic text parallel parallel expected Table [ [0] * 500 [1] * 500 [2] * 500 ] names 'A' 'B' 'C' assert_table_equal table expected
def _ls_emr_bootstrap_stderr_logs fs log_dir_stream action_num None node_id None matches _ls_logs fs log_dir_stream _match_emr_bootstrap_stderr_path action_num None node_id None return sorted matches key lambda m - m['action_num'] m['node_id']
def skipUnlessGISLookup *gis_lookups def decorator test_func @wraps test_func def skip_wrapper *args **kwargs if any key not in connection ops gis_operators for key in gis_lookups raise unittest SkipTest "Databasedoesn'tsupportallthelookups %s" % ' ' join gis_lookups return test_func *args **kwargs return skip_wrapperreturn decorator
def task_enable_updates_testing distribution raise DistributionNotSupported distribution distribution
def _get_outerhtml html_node html_string lxml html tostring html_node return re sub '[^>]*$' '' html_string count 1
def test_pushdpopd xonsh_builtins xonsh_builtins __xonsh_env__ Env CDPATH PARENT PWD HERE dirstack cd [PARENT] owd os getcwd assert owd casefold xonsh_builtins __xonsh_env__['PWD'] casefold dirstack pushd [HERE] wd os getcwd assert wd casefold HERE casefold dirstack popd [] assert owd casefold os getcwd casefold 'popdreturnedcwdtoexpecteddir'
def test_pushdpopd xonsh_builtins xonsh_builtins __xonsh_env__ Env CDPATH PARENT PWD HERE dirstack cd [PARENT] owd os getcwd assert owd casefold xonsh_builtins __xonsh_env__['PWD'] casefold dirstack pushd [HERE] wd os getcwd assert wd casefold HERE casefold dirstack popd [] assert owd casefold os getcwd casefold 'popdreturnedcwdtoexpecteddir'
@contextmanagerdef temp_style style_name settings None if not settings settings DUMMY_SETTINGStemp_file u'%s %s' % style_name STYLE_EXTENSION tempdir tempfile mkdtemp with open os path join tempdir temp_file u'w' as f for k v in six iteritems settings f write u'%s %s' % k v USER_LIBRARY_PATHS append tempdir style reload_library try yield finally shutil rmtree tempdir style reload_library
def clear if not isatty sys stdout returnif WIN os system 'cls' else sys stdout write '\x1b[2J\x1b[1 1H'
def register_assert_rewrite *names for name in names if not isinstance name str msg 'expectedmodulenamesas*args got{0}instead'raise TypeError msg format repr names for hook in sys meta_path if isinstance hook rewrite AssertionRewritingHook importhook hookbreakelse importhook DummyRewriteHook importhook mark_rewrite *names
def register_assert_rewrite *names for name in names if not isinstance name str msg 'expectedmodulenamesas*args got{0}instead'raise TypeError msg format repr names for hook in sys meta_path if isinstance hook rewrite AssertionRewritingHook importhook hookbreakelse importhook DummyRewriteHook importhook mark_rewrite *names
def downgrade version db_version repo_path get_migrate_repo_path sql_connection CONF sql_connectionLOG info _ 'Downgradingdatabasetoversion%s' % version return versioning_api downgrade sql_connection repo_path version
def make_conditional response last_modified None etag None max_age 0 response cache_control must_revalidate Trueresponse cache_control max_age max_ageif last_modified response last_modified last_modifiedif etag response set_etag etag return response make_conditional request httprequest
def make_conditional response last_modified None etag None max_age 0 response cache_control must_revalidate Trueresponse cache_control max_age max_ageif last_modified response last_modified last_modifiedif etag response set_etag etag return response make_conditional request httprequest
def _get_msupdate_status obj_sm win32com client Dispatch 'Microsoft Update ServiceManager' col_services obj_sm Servicesfor service in col_services if service name 'MicrosoftUpdate' return Truereturn False
def Time hour minute second return dateconverter Time hour minute second
def load_class alias try return CLASS_CACHE[alias]except KeyError passfor loader in CLASS_LOADERS klass loader alias if klass is None continueif isinstance klass python class_types return register_class klass alias elif isinstance klass ClassAlias CLASS_CACHE[klass alias] klassCLASS_CACHE[klass klass] klassreturn klassraise TypeError 'ExpectingclassobjectorClassAliasfromloader' mod_class alias split ' ' if mod_class module ' ' join mod_class[ -1 ] klass mod_class[ -1 ]try module util get_module module except ImportError AttributeError passelse klass getattr module klass if isinstance klass python class_types return register_class klass alias elif isinstance klass ClassAlias CLASS_CACHE[klass alias] klassCLASS_CACHE[klass klass] klassreturn klass klasselse raise TypeError 'ExpectingclasstypeorClassAliasfromloader' raise UnknownClassAlias 'Unknownaliasfor%r' % alias
def ensure_home_directory fs user userprofile get_profile user if userprofile is not None and userprofile home_directory fs do_as_user user username fs create_home_dir userprofile home_directory else LOG warn 'Notcreatinghomedirectoryof%sashisprofileisempty' % user
def difftool_run files selection selected_group if not files returns selection selection model main model difftool_launch_with_head files bool s staged model head
def output_server session_id None url 'default' app_path '/' deprecated 0 12 3 'bokeh io output_server ' '\nbokeh clientsessionsasdescribedathttp //bokeh pydata org/en/latest/docs/user_guide/server html#connecting-with-bokeh-client"\n' from client import DEFAULT_SESSION_IDif session_id is None session_id DEFAULT_SESSION_ID_state output_server session_id session_id url url app_path app_path
def output_server session_id None url 'default' app_path '/' deprecated 0 12 3 'bokeh io output_server ' '\nbokeh clientsessionsasdescribedathttp //bokeh pydata org/en/latest/docs/user_guide/server html#connecting-with-bokeh-client"\n' from client import DEFAULT_SESSION_IDif session_id is None session_id DEFAULT_SESSION_ID_state output_server session_id session_id url url app_path app_path
def _end_of_set_index string start_index length len string closing_index start_indexif closing_index < length and string[closing_index] ' ' closing_index + 1if closing_index < length closing_index + 1while closing_index < length and string[closing_index] ']' closing_index + 1return closing_index
def _prep_acl_for_compare ACL ret deepcopy ACL ret['Owner'] _normalize_user ret['Owner'] for item in ret get 'Grants' item['Grantee'] _normalize_user item get 'Grantee' return ret
def get_pending_update vname ' Default 'key 'SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\WindowsUpdate\\AutoUpdate\\RebootRequired'reg_ret __salt__['reg read_value'] 'HKLM' key vname if reg_ret['success'] log debug 'Foundkey %s' key return Trueelse log debug 'Unabletoaccesskey %s' key return False
def get_color color data_colors data_colors if isinstance color str if color in data_colors return data_colors[color]else raise ValueError 'Colorname%sinprefsnotrecognized' % color else name coords colorif isinstance coords str colorspace 'rgb'else colorspace 'hsv'return Color name coords colorspace
def get_color color data_colors data_colors if isinstance color str if color in data_colors return data_colors[color]else raise ValueError 'Colorname%sinprefsnotrecognized' % color else name coords colorif isinstance coords str colorspace 'rgb'else colorspace 'hsv'return Color name coords colorspace
def remove name path cmd [_get_cmd '--remove' name path]out __salt__['cmd run_all'] cmd python_shell False if out['retcode'] > 0 return out['stderr']return out['stdout']
def remove name path cmd [_get_cmd '--remove' name path]out __salt__['cmd run_all'] cmd python_shell False if out['retcode'] > 0 return out['stderr']return out['stdout']
def list_formatter view values return u' ' join text_type v for v in values
def libvlc_vlm_get_media_instance_chapter p_instance psz_name i_instance f _Cfunctions get 'libvlc_vlm_get_media_instance_chapter' None or _Cfunction 'libvlc_vlm_get_media_instance_chapter' 1 1 1 None ctypes c_int Instance ctypes c_char_p ctypes c_int return f p_instance psz_name i_instance
def _branch results results list results ret Result for result in results[ -1 ] ret + resultret + result expr_as_stmt for result in results[ -1 ] ret + resultreturn ret
def setup_platform hass config add_devices discovery_info None dev_id config get CONF_ID devname config get CONF_NAME sensor_class config get CONF_SENSOR_CLASS add_devices [EnOceanBinarySensor dev_id devname sensor_class ]
def _add_current_user_id graph user if graph graph current_user_id Noneif user is_authenticated profile try_get_profile user facebook_id get_user_attribute user profile 'facebook_id' if facebook_id graph current_user_id facebook_id
def writeDjangoObject obj encoder None s obj pkif s is None encoder writeObject obj returndjango_objects getDjangoObjects encoder context kls obj __class__try referenced_object django_objects getClassKey kls s except KeyError referenced_object objdjango_objects addClassKey kls s obj encoder writeObject referenced_object
def writeDjangoObject obj encoder None s obj pkif s is None encoder writeObject obj returndjango_objects getDjangoObjects encoder context kls obj __class__try referenced_object django_objects getClassKey kls s except KeyError referenced_object objdjango_objects addClassKey kls s obj encoder writeObject referenced_object
@register simple_tag def crispy_addon field append '' prepend '' form_show_labels True if field context Context {'field' field 'form_show_errors' True 'form_show_labels' form_show_labels} template loader get_template '%s/layout/prepended_appended_text html' % get_template_pack context['crispy_prepended_text'] prependcontext['crispy_appended_text'] appendif not prepend and not append raise TypeError 'Expectedaprependand/orappendargument' context context flatten return template render context
def get_fontsize numrows thresholds [25 50 75 100 125]sizes [5 4 3 2 1 5 1]i 0while numrows > thresholds[i] i + 1if i len thresholds breakreturn sizes[i]
def PearsonMedianSkewness xs median Median xs mean RawMoment xs 1 var CentralMoment xs 2 std math sqrt var gp 3 * mean - median / std return gp
def walk_python_files def _is_dir_ignored root d if d startswith u' ' return Truereturn os path join rel_root d in IGNORED_DIRS for abs_root dirnames filenames in os walk PROJECT_ROOT rel_root os path relpath abs_root PROJECT_ROOT if rel_root u' ' rel_root u''dirnames[ ] [d for d in dirnames if not _is_dir_ignored rel_root d ]for filename in filenames if not filename endswith u' py' continueabs_name os path join abs_root filename rel_name os path join rel_root filename yield abs_name rel_name
def get_random_state global _random_statesdev cuda Device rs _random_states get dev id None if rs is None rs RandomState os getenv 'CHAINER_SEED' _random_states[dev id] rsreturn rs
def _ci arr ci from scipy import stats mean sigma arr mean 0 stats sem arr 0 return np asarray [stats t interval ci arr shape[0] loc mean_ scale sigma_ for mean_ sigma_ in zip mean sigma ] T
@pytest fixturedef webview qtbot webpage QtWebKitWidgets pytest importorskip 'PyQt5 QtWebKitWidgets' view QtWebKitWidgets QWebView qtbot add_widget view view page deleteLater view setPage webpage view resize 640 480 return view
def get_all_volumes volume_ids None filters None return_objs False region None key None keyid None profile None conn _get_conn region region key key keyid keyid profile profile try ret conn get_all_volumes volume_ids volume_ids filters filters return ret if return_objs else [r id for r in ret] except boto exception BotoServerError as e log error e return []
def setup_mainloop extension iteration_scheme None features [numpy array f dtype theano config floatX for f in [[1 2]] * 101 ]dataset IterableDataset dict features features data_stream DataStream dataset iteration_scheme iteration_scheme W shared_floatx [0 0] name 'W' x tensor vector 'features' cost tensor sum x - W ** 2 cost name 'cost'algorithm GradientDescent cost cost parameters [W] step_rule Scale 0 001 main_loop MainLoop model None data_stream data_stream algorithm algorithm extensions [FinishAfter after_n_epochs 1 extension] return main_loop
def extract_modes spans rangelen sorted [ x['to'] - x['from'] + 1 for x in spans] deflen sorted rangelen key rangelen count [ -1 ]reprs [str2fmt x['str'] for x in spans]deffmt sorted reprs key reprs count [ -1 ]return deflen deffmt
def _idnaText octets try import idnaexcept ImportError return octets decode 'idna' else return idna decode octets
@log_calldef metadef_tag_count context namespace_name namespace metadef_namespace_get context namespace_name _check_namespace_visibility context namespace namespace_name count 0for tag in DATA['metadef_tags'] if tag['namespace_id'] namespace['id'] count count + 1 return count
def _find_human_readable_labels synsets synset_to_human humans []for s in synsets assert s in synset_to_human 'Failedtofind %s' % s humans append synset_to_human[s] return humans
def get_wyr headers r requests get url API_URL headers headers data r json data['title'] data['title'] strip capitalize rstrip ' ? ' data['choicea'] data['choicea'] strip lower rstrip ' ? ' lstrip ' ' data['choiceb'] data['choiceb'] strip lower rstrip ' ? ' lstrip ' ' if data['tags'] data['tags'] data['tags'] lower split ' ' else data['tags'] []if data['nsfw'] data['tags'] append 'nsfw' return data
def fractional_matrix_power A t A _asarray_square A import scipy linalg _matfuncs_inv_ssqreturn scipy linalg _matfuncs_inv_ssq _fractional_matrix_power A t
def compile marker try return _cache[marker]except KeyError passif not marker strip def marker_fn environment None override None ''return Trueelse compiled_marker compile_marker parse_marker marker def marker_fn environment None override None 'overrideupdatesenvironment'if override is None override {}if environment is None environment default_environment environment update override return eval compiled_marker environment marker_fn __doc__ marker_cache[marker] marker_fnreturn _cache[marker]
def removeSVGFile svgFilePath if archive getEndsWithList svgFilePath ['_bottom svg' '_carve svg' '_chop svg' '_cleave svg' '_scale svg' '_vectorwrite svg'] os remove svgFilePath print 'removeGeneratedFilesdeleted' + svgFilePath
def sem a axis 0 ddof 1 nan_policy 'propagate' a axis _chk_asarray a axis contains_nan nan_policy _contains_nan a nan_policy if contains_nan and nan_policy 'omit' a ma masked_invalid a return mstats_basic sem a axis ddof n a shape[axis]s np std a axis axis ddof ddof / np sqrt n return s
def findOPLocalIdentifier service_element type_uris local_id_tags []if OPENID_1_1_TYPE in type_uris or OPENID_1_0_TYPE in type_uris local_id_tags append nsTag OPENID_1_0_NS 'Delegate' if OPENID_2_0_TYPE in type_uris local_id_tags append nsTag XRD_NS_2_0 'LocalID' local_id Nonefor local_id_tag in local_id_tags for local_id_element in service_element findall local_id_tag if local_id is None local_id local_id_element textelif local_id local_id_element text format 'Morethanone%rtagfoundinoneserviceelement'message format % local_id_tag raise DiscoveryFailure message None return local_id
def _preparse source f compose _replace_locals _replace_booleans _rewrite_assign assert callable f 'fmustbecallable'return tokenize untokenize lmap f tokenize_string source
def _preparse source f compose _replace_locals _replace_booleans _rewrite_assign assert callable f 'fmustbecallable'return tokenize untokenize lmap f tokenize_string source
@login_requireddef upload_manifest *args **kwargs return _upload_manifest *args **kwargs
def make_routine name expr argument_sequence None global_vars None language 'F95' code_gen get_code_generator language 'nothingElseMatters' return code_gen routine name expr argument_sequence global_vars
def track_from_id identifier timeout DEFAULT_ASYNC_TIMEOUT param_dict dict id identifier return _profile param_dict timeout
@contextmanagerdef temporary_folder tempdir mkdtemp try yield tempdir finally rmtree tempdir
def get_document name doc _document_registry get name None if not doc single_end name split ' ' [ -1 ]compound_end ' %s' % single_end possible_match [k for k in _document_registry keys if k endswith compound_end or k single_end ]if len possible_match 1 doc _document_registry get possible_match pop None if not doc raise NotRegistered '\n`%s`hasnotbeenregisteredinthedocumentregistry \nImportingthedocumentclassautomaticallyregistersit hasit\nbeenimported?\n' strip % name return doc
def generate_initial_profile_picture user_id user_email get_email_from_user_id user_id user_gravatar fetch_gravatar user_email update_profile_picture_data_url user_id user_gravatar
def generate_initial_profile_picture user_id user_email get_email_from_user_id user_id user_gravatar fetch_gravatar user_email update_profile_picture_data_url user_id user_gravatar
@depends HAS_PYVMOMI def update_host_datetime host username password protocol None port None host_names None service_instance salt utils vmware get_service_instance host host username username password password protocol protocol port port host_names _check_hosts service_instance host host_names ret {}for host_name in host_names host_ref _get_host_ref service_instance host host_name host_name date_time_manager _get_date_time_mgr host_ref try date_time_manager UpdateDateTime datetime datetime utcnow except vim fault HostConfigFault as err msg "'vsphere update_date_time'failedforhost{0} {1}" format host_name err log debug msg ret update {host_name {'Error' msg}} continueret update {host_name {'DatetimeUpdated' True}} return ret
def circmoment data p 1 0 centered False axis None weights None if centered phi circmean data axis weights else phi 0 0return _angle data p phi axis weights _length data p phi axis weights
def FindEndOfExpressionInLine line startpos depth startchar endchar for i in xrange startpos len line if line[i] startchar depth + 1elif line[i] endchar depth - 1if depth 0 return i + 1 0 return -1 depth
def default_request_ip_resolver request return real_ip request or x_forwarded_ip request or remote_addr_ip request
def default_request_ip_resolver request return real_ip request or x_forwarded_ip request or remote_addr_ip request
def generate_encryption_key key os urandom 32 encoded_key base64 b64encode key decode 'utf-8' print 'Base64encodedencryptionkey {}' format encoded_key
def generate_encryption_key key os urandom 32 encoded_key base64 b64encode key decode 'utf-8' print 'Base64encodedencryptionkey {}' format encoded_key
def auth nodes pcsuser 'hacluster' pcspasswd 'hacluster' extra_args None cmd ['pcs' 'cluster' 'auth']if pcsuser cmd + ['-u' pcsuser]if pcspasswd cmd + ['-p' pcspasswd]if isinstance extra_args list tuple cmd + extra_argscmd + nodesreturn __salt__['cmd run_all'] cmd output_loglevel 'trace' python_shell False
def auth nodes pcsuser 'hacluster' pcspasswd 'hacluster' extra_args None cmd ['pcs' 'cluster' 'auth']if pcsuser cmd + ['-u' pcsuser]if pcspasswd cmd + ['-p' pcspasswd]if isinstance extra_args list tuple cmd + extra_argscmd + nodesreturn __salt__['cmd run_all'] cmd output_loglevel 'trace' python_shell False
def pytest_configure config if config getoption 'gae_sdk' is not None set_up_gae_environment config getoption 'gae_sdk'
def pytest_configure config if config getoption 'gae_sdk' is not None set_up_gae_environment config getoption 'gae_sdk'
@must_have_addon SHORT_NAME 'node' @must_be_addon_authorizer SHORT_NAME def figshare_folder_list node_addon **kwargs return node_addon get_folders
@must_have_addon SHORT_NAME 'node' @must_be_addon_authorizer SHORT_NAME def figshare_folder_list node_addon **kwargs return node_addon get_folders
def unpack name dest None path None pack_format 'tar' compress 'bz2' if pack_format 'tar' _untar name dest path compress
@require_POST@ensure_csrf_cookie@cache_control no_cache True no_store True must_revalidate True @require_level 'staff' def get_grading_config request course_id course_id SlashSeparatedCourseKey from_deprecated_string course_id course get_course_with_access request user 'staff' course_id depth None grading_config_summary instructor_analytics basic dump_grading_context course response_payload {'course_id' course_id to_deprecated_string 'grading_config_summary' grading_config_summary}return JsonResponse response_payload
def test_destop from vispy gloo gl import gl2_test_function_names gl2 _test_constant_names gl2
def del_pid pidfile if os path exists pidfile os remove pidfile
def _valid_iface iface ifaces list_interfaces if iface in ifaces keys return Truereturn False
@auth s3_requires_membership 1 def setting return dict
def Channel n rv channels get n None if rv is None rv ChannelImpl n channels[n] rvreturn rv
def _render_report_form start_str end_str start_letter end_letter report_type total_count_error False date_fmt_error False context {'total_count_error' total_count_error 'date_fmt_error' date_fmt_error 'start_date' start_str 'end_date' end_str 'start_letter' start_letter 'end_letter' end_letter 'requested_report' report_type}return render_to_response 'shoppingcart/download_report html' context
def convert_systemd_target_to_runlevel target if target 'poweroff target' runlevel '0'elif target 'rescue target' runlevel 's'elif target 'multi-user target' runlevel '3'elif target 'graphical target' runlevel '5'elif target 'reboot target' runlevel '6'else raise ValueError 'unknowntarget%s' % target return runlevel
def convert_systemd_target_to_runlevel target if target 'poweroff target' runlevel '0'elif target 'rescue target' runlevel 's'elif target 'multi-user target' runlevel '3'elif target 'graphical target' runlevel '5'elif target 'reboot target' runlevel '6'else raise ValueError 'unknowntarget%s' % target return runlevel
def get_project_count bugs mysite search models Bug all_bugs all return bugs values u'project' distinct count
def _get_compute_api_class_name cell_type nova cells opts get_cell_type return CELL_TYPE_TO_CLS_NAME[cell_type]
def output data **kwargs try if 'output_indent' not in __opts__ return json dumps data default repr indent 4 indent __opts__ get 'output_indent' sort_keys Falseif indent is None indent Noneelif indent 'pretty' indent 4sort_keys Trueelif isinstance indent int if indent > 0 indent indentelse indent Nonereturn json dumps data default repr indent indent sort_keys sort_keys except UnicodeDecodeError as exc log error 'Unabletoserializeoutputtojson' return json dumps {'error' 'Unabletoserializeoutputtojson' 'message' str exc } except TypeError log debug 'AnerroroccurredwhileoutputtingJSON' exc_info True return json dumps {}
def _absolute_url_staticfile is_secure name url_path staticfiles_storage url name if urlparse urlparse url_path netloc return url_pathreturn _absolute_url is_secure url_path
def write_error_summary error fullpath request environ get 'FULLPATH' request path uid c user _id if c user_is_loggedin else '-' g log error 'E %sU %sFP %s' error uid fullpath
def _called_in_methods func klass methods if not isinstance func astroid Function return Falsefor method in methods try infered klass getattr method except astroid NotFoundError continuefor infer_method in infered for callfunc in infer_method nodes_of_class astroid CallFunc try bound next callfunc func infer except astroid InferenceError StopIteration continueif not isinstance bound astroid BoundMethod continuefunc_obj bound _proxiedif isinstance func_obj astroid UnboundMethod func_obj func_obj _proxiedif func_obj name func name return Truereturn False
def _called_in_methods func klass methods if not isinstance func astroid Function return Falsefor method in methods try infered klass getattr method except astroid NotFoundError continuefor infer_method in infered for callfunc in infer_method nodes_of_class astroid CallFunc try bound next callfunc func infer except astroid InferenceError StopIteration continueif not isinstance bound astroid BoundMethod continuefunc_obj bound _proxiedif isinstance func_obj astroid UnboundMethod func_obj func_obj _proxiedif func_obj name func name return Truereturn False
def version return _PIP_VERSION
def remove_file s for match in re finditer RE_P15 s m match group 0 caption m[ -2 ] split ' ' [ -1 ]s s replace m caption 1 return s
def pad data bytes_to_pad AES block_size - len data % AES block_size return data + bytes_to_pad * chr bytes_to_pad
def set_master service conn_type private 'y' unpriv 'y' chroot 'y' wakeup 'n' maxproc '100' command '' write_conf True path MASTER_CF conf_dict conf_list _parse_master path new_conf []dict_key '{0}{1}' format service conn_type new_line _format_master service conn_type private unpriv chroot wakeup maxproc command for line in conf_list if isinstance line dict if line['service'] service and line['conn_type'] conn_type new_conf append new_line else new_conf append _format_master **line else new_conf append line if dict_key not in conf_dict new_conf append new_line if write_conf _write_conf new_conf path return '\n' join new_conf
def new_unfollow self user_id user_name url_unfollow self url_unfollow % user_id try unfollow self s post url_unfollow if unfollow status_code 200 self unfollow_counter + 1log_string 'Unfollow %s#%i ' % user_name self unfollow_counter self write_log log_string return unfollowexcept self write_log 'Exeptonunfollow ' return False
def new_unfollow self user_id user_name url_unfollow self url_unfollow % user_id try unfollow self s post url_unfollow if unfollow status_code 200 self unfollow_counter + 1log_string 'Unfollow %s#%i ' % user_name self unfollow_counter self write_log log_string return unfollowexcept self write_log 'Exeptonunfollow ' return False
def publish_progress *args **kwargs import frappe asyncreturn frappe async publish_progress *args **kwargs
def expand_login_view login_view if login_view startswith 'https //' 'http //' '/' return login_viewelse return url_for login_view
def make_derivative_operator M z def doit C r z * C diff z + C * M r r applyfunc make_simp z return rreturn doit
def process_extract_samples samples_to_extract prefs {}if samples_to_extract samples samples_to_extract strip strip "'" split ' ' for j col in enumerate samples key str j prefs[key] {}prefs[key] colreturn prefs
def write_trans fname trans check_fname fname 'trans' '-trans fif' '-trans fif gz' fid start_file fname write_coord_trans fid trans end_file fid
def makeTextNotes replaceTxtNotes def _replacer s outS replaceTxtNotesif not isinstance s unicode str return sssplit s split ' ' 1 text ssplit[0]keysDict {}if text keysDict['text'] TrueoutS outS replace '% text s' text if len ssplit 2 keysDict['notes'] TrueoutS outS replace '% notes s' ssplit[1] else outS outS replace '% notes s' u'' def _excludeFalseConditionals matchobj if matchobj group 1 in keysDict return matchobj group 2 return u''while re_conditional search outS outS re_conditional sub _excludeFalseConditionals outS return outSreturn _replacer
@cache_page 3600 key_prefix 'js18n-%s' % get_language def javascript_catalog_all request domain 'djangojs' all_apps [x name for x in apps get_app_configs ]return javascript_catalog request domain all_apps
def get_admin_app_list parser token tokens token contents split if len tokens < 3 raise template TemplateSyntaxError "'%s'tagrequirestwoarguments" % tokens[0] if tokens[1] 'as' raise template TemplateSyntaxError "Firstargumentto'%s'tagmustbe'as'" % tokens[0] return AdminApplistNode tokens[2]
def _potential_after i input_string return i + 2 > len input_string or input_string[ i + 2 ] input_string[i] and input_string[ i + 1 ] not in seps
def make_sampling_table size sampling_factor 1e-05 gamma 0 577rank np array list range size rank[0] 1inv_fq rank * np log rank + gamma + 0 5 - 1 0 / 12 0 * rank f sampling_factor * inv_fq return np minimum 1 0 f / np sqrt f
def run_script scriptfile try f open scriptfile mode 'r' except Exception returnmpstate console writeln 'Runningscript%s' % scriptfile for line in f line line strip if line '' or line startswith '#' continueif line startswith '@' line line[1 ]else mpstate console writeln '->%s' % line process_stdin line f close
def pmonitor popens timeoutms 500 readline True readmax 1024 poller poll fdToHost {}for host popen in popens iteritems fd popen stdout fileno fdToHost[fd] hostpoller register fd POLLIN if not readline flags fcntl fd F_GETFL fcntl fd F_SETFL flags O_NONBLOCK while popens fds poller poll timeoutms if fds for fd event in fds host fdToHost[fd]popen popens[host]if event & POLLIN if readline line popen stdout readline else line popen stdout read readmax yield host line elif event & POLLHUP poller unregister fd del popens[host]else yield None ''
def pmonitor popens timeoutms 500 readline True readmax 1024 poller poll fdToHost {}for host popen in popens iteritems fd popen stdout fileno fdToHost[fd] hostpoller register fd POLLIN if not readline flags fcntl fd F_GETFL fcntl fd F_SETFL flags O_NONBLOCK while popens fds poller poll timeoutms if fds for fd event in fds host fdToHost[fd]popen popens[host]if event & POLLIN if readline line popen stdout readline else line popen stdout read readmax yield host line elif event & POLLHUP poller unregister fd del popens[host]else yield None ''
def check_print_compat return not os name 'nt' and version_check '5 3 0' operator lt
def check_print_compat return not os name 'nt' and version_check '5 3 0' operator lt
def hmc_move s_rng positions energy_fn stepsize n_steps initial_vel s_rng normal size positions shape final_pos final_vel simulate_dynamics initial_pos positions initial_vel initial_vel stepsize stepsize n_steps n_steps energy_fn energy_fn accept metropolis_hastings_accept energy_prev hamiltonian positions initial_vel energy_fn energy_next hamiltonian final_pos final_vel energy_fn s_rng s_rng return accept final_pos
def del_quote db nick msg query qtable update where qtable c chan 1 where qtable c nick nick lower where qtable c msg msg values deleted 1 db execute query db commit
def use_fsdev_lib fs_desc disk1_only reinit_disks global FSDEV_FS_DESCglobal FSDEV_RESTOREglobal FSDEV_DISK1_ONLYglobal FSDEV_PREP_CNTFSDEV_FS_DESC fs_descFSDEV_DISK1_ONLY disk1_onlyFSDEV_RESTORE reinit_disksFSDEV_PREP_CNT 0
def update_limits limits_dict limits get_limits limits update limits_dict update_site_config u'limits' limits validate False disable_users limits frappe local conf limits limits
def _make_grid_to_short_label dataset unique_values [sorted list frozenset column for column in dataset y[ 5] transpose ]category_index dataset label_name_to_index['category']unique_categories unique_values[category_index]category_to_name dataset label_to_value_funcs[category_index]if any category_to_name category 'blank' for category in unique_categories for d in range 1 len unique_values assert unique_values[d][0] -1 'unique_values %s' % str unique_values unique_values[d] unique_values[d][1 ]return unique_values
def _make_grid_to_short_label dataset unique_values [sorted list frozenset column for column in dataset y[ 5] transpose ]category_index dataset label_name_to_index['category']unique_categories unique_values[category_index]category_to_name dataset label_to_value_funcs[category_index]if any category_to_name category 'blank' for category in unique_categories for d in range 1 len unique_values assert unique_values[d][0] -1 'unique_values %s' % str unique_values unique_values[d] unique_values[d][1 ]return unique_values
def _make_grid_to_short_label dataset unique_values [sorted list frozenset column for column in dataset y[ 5] transpose ]category_index dataset label_name_to_index['category']unique_categories unique_values[category_index]category_to_name dataset label_to_value_funcs[category_index]if any category_to_name category 'blank' for category in unique_categories for d in range 1 len unique_values assert unique_values[d][0] -1 'unique_values %s' % str unique_values unique_values[d] unique_values[d][1 ]return unique_values
def single_source_dijkstra_path G source cutoff None weight 'weight' return multi_source_dijkstra_path G {source} cutoff cutoff weight weight
def cache_model model timeout None if hasattr model 'get_cached' returndef clear_cache sender instance *args **kwargs '\nClearsthecacheforthegiveninstance \n'delete_instance sender instance post_save connect clear_cache sender model weak False post_delete connect clear_cache sender model weak False @classmethoddef get cls pk using None '\nReturnsthemodelforthegivenprimarykey pk \n'if pk is None return Nonereturn get_instance cls pk timeout using model get_cached get
def cache_model model timeout None if hasattr model 'get_cached' returndef clear_cache sender instance *args **kwargs '\nClearsthecacheforthegiveninstance \n'delete_instance sender instance post_save connect clear_cache sender model weak False post_delete connect clear_cache sender model weak False @classmethoddef get cls pk using None '\nReturnsthemodelforthegivenprimarykey pk \n'if pk is None return Nonereturn get_instance cls pk timeout using model get_cached get
def cache_model model timeout None if hasattr model 'get_cached' returndef clear_cache sender instance *args **kwargs '\nClearsthecacheforthegiveninstance \n'delete_instance sender instance post_save connect clear_cache sender model weak False post_delete connect clear_cache sender model weak False @classmethoddef get cls pk using None '\nReturnsthemodelforthegivenprimarykey pk \n'if pk is None return Nonereturn get_instance cls pk timeout using model get_cached get
def cache_model model timeout None if hasattr model 'get_cached' returndef clear_cache sender instance *args **kwargs '\nClearsthecacheforthegiveninstance \n'delete_instance sender instance post_save connect clear_cache sender model weak False post_delete connect clear_cache sender model weak False @classmethoddef get cls pk using None '\nReturnsthemodelforthegivenprimarykey pk \n'if pk is None return Nonereturn get_instance cls pk timeout using model get_cached get
def main args None parser argparse ArgumentParser description 'LoadaYAMLfilewithoutperforminganytraining ' parser add_argument 'yaml_file' type argparse FileType 'r' help 'TheYAMLfiletoload ' parser add_argument '-N' '--no-instantiate' action 'store_const' default False const True help 'OnlyverifythattheYAMLparsescorrectlybutdonotattempttoinstantiatetheobjects ThismightbeusedasaquicksanitycheckifcheckingafilethatrequiresaGPUinanenvironmentthatlacksone e g aclusterheadnode ' args parser parse_args args args name args yaml_file nameinitialize if args no_instantiate yaml_load args yaml_file print 'Successfullyparsed%s butobjectsnotinstantiated ' % name else load args yaml_file print 'Successfullyparsedandloaded%s ' % name
def profileSP spClass spDim nRuns inDim [10000 1 1]colDim [spDim 1 1]sp spClass inputDimensions inDim columnDimensions colDim potentialRadius 3 potentialPct 0 5 globalInhibition False localAreaDensity -1 0 numActiveColumnsPerInhArea 3 stimulusThreshold 1 synPermInactiveDec 0 01 synPermActiveInc 0 1 synPermConnected 0 1 minPctOverlapDutyCycle 0 1 dutyCyclePeriod 10 boostStrength 10 0 seed 42 spVerbosity 0 dataDim inDimdataDim append nRuns data numpy random randint 0 2 dataDim astype 'float32' for i in xrange nRuns d data[ i]activeArray numpy zeros colDim sp compute d True activeArray
def profileSP spClass spDim nRuns inDim [10000 1 1]colDim [spDim 1 1]sp spClass inputDimensions inDim columnDimensions colDim potentialRadius 3 potentialPct 0 5 globalInhibition False localAreaDensity -1 0 numActiveColumnsPerInhArea 3 stimulusThreshold 1 synPermInactiveDec 0 01 synPermActiveInc 0 1 synPermConnected 0 1 minPctOverlapDutyCycle 0 1 dutyCyclePeriod 10 boostStrength 10 0 seed 42 spVerbosity 0 dataDim inDimdataDim append nRuns data numpy random randint 0 2 dataDim astype 'float32' for i in xrange nRuns d data[ i]activeArray numpy zeros colDim sp compute d True activeArray
@get '/scan/<taskid>/data' def scan_data taskid json_data_message list json_errors_message list if taskid not in DataStore tasks logger warning '[%s]InvalidtaskIDprovidedtoscan_data ' % taskid return jsonize {'success' False 'message' 'InvalidtaskID'} for status content_type value in DataStore current_db execute 'SELECTstatus content_type valueFROMdataWHEREtaskid ?ORDERBYidASC' taskid json_data_message append {'status' status 'type' content_type 'value' dejsonize value } for error in DataStore current_db execute 'SELECTerrorFROMerrorsWHEREtaskid ?ORDERBYidASC' taskid json_errors_message append error logger debug '[%s]Retrievedscandataanderrormessages' % taskid return jsonize {'success' True 'data' json_data_message 'error' json_errors_message}
@status 'Gettingthelistoffilesthathavebeenadded/changed' info lambda x n_files_str len x def changed_files if not os path isdir os path join SRCDIR ' hg' sys exit 'needacheckouttogetmodifiedfiles' cmd 'hgstatus--added--modified--no-status'if mq_patches_applied cmd + '--revqparent'with subprocess Popen cmd split stdout subprocess PIPE as st return [x decode rstrip for x in st stdout]
def _check_for_unavailable_sdk _config_vars cflags _config_vars get 'CFLAGS' '' m re search '-isysroot\\s+ \\S+ ' cflags if m is not None sdk m group 1 if not os path exists sdk for cv in _UNIVERSAL_CONFIG_VARS if cv in _config_vars and cv not in os environ flags _config_vars[cv]flags re sub '-isysroot\\s+\\S+ ? \\s $ ' '' flags _save_modified_value _config_vars cv flags return _config_vars
def matplotlib_fname if six PY2 cwd os getcwdu else cwd os getcwd fname os path join cwd u'matplotlibrc' if os path exists fname return fnameif u'MATPLOTLIBRC' in os environ path os environ[u'MATPLOTLIBRC']if os path exists path if os path isfile path return pathfname os path join path u'matplotlibrc' if os path exists fname return fnameconfigdir _get_configdir if os path exists configdir fname os path join configdir u'matplotlibrc' if os path exists fname return fnamepath get_data_path fname os path join path u'matplotlibrc' if not os path exists fname warnings warn u'Couldnotfindmatplotlibrc usingdefaults' return fname
def remove_property_value prop predicate removed_vals []removed_vals filter predicate prop propertyValue if len removed_vals len prop propertyValue prop parent removeProperty prop name else x prop propertyValue cssTextfor v in removed_vals x x replace v cssText u'' strip prop propertyValue cssText xreturn bool removed_vals
def remove_property_value prop predicate removed_vals []removed_vals filter predicate prop propertyValue if len removed_vals len prop propertyValue prop parent removeProperty prop name else x prop propertyValue cssTextfor v in removed_vals x x replace v cssText u'' strip prop propertyValue cssText xreturn bool removed_vals
def tar file dir expression '^ +$' filenames None exclude_content_from None tar tarfile TarFile file 'w' try if filenames is None filenames listdir dir expression add_dirs True exclude_content_from exclude_content_from for file in filenames tar add os path join dir file file False finally tar close
def md5sum_is_current src_file src_md5 get_md5sum src_file src_md5_file src_file + ' md5' if os path exists src_md5_file with open src_md5_file 'r' as file_checksum ref_md5 file_checksum read return src_md5 ref_md5 return False
def md5sum_is_current src_file src_md5 get_md5sum src_file src_md5_file src_file + ' md5' if os path exists src_md5_file with open src_md5_file 'r' as file_checksum ref_md5 file_checksum read return src_md5 ref_md5 return False
def create_command args zone create_zone args project_id args name args dns_name args description print 'Zone{}added ' format zone name
def test_random_sample_different_definitions a db from_sequence range 50 npartitions 5 assert list a random_sample 0 5 list a random_sample 0 5 assert a random_sample 0 5 name a random_sample 0 5 name
def test_random_sample_different_definitions a db from_sequence range 50 npartitions 5 assert list a random_sample 0 5 list a random_sample 0 5 assert a random_sample 0 5 name a random_sample 0 5 name
def test_random_sample_different_definitions a db from_sequence range 50 npartitions 5 assert list a random_sample 0 5 list a random_sample 0 5 assert a random_sample 0 5 name a random_sample 0 5 name
def colorize msg color if DONT_COLORIZE return msgelse return '{}{}{}' format COLORS[color] msg COLORS['endc']
def colorize msg color if DONT_COLORIZE return msgelse return '{}{}{}' format COLORS[color] msg COLORS['endc']
def ensure_dir_is_templated dirname if u'{{' in dirname and u'}}' in dirname return Trueelse raise NonTemplatedInputDirException
def is_python_interpreter filename real_filename os path realpath filename if not osp isfile real_filename or encoding is_text_file real_filename or not is_python_interpreter_valid_name filename return Falsetry proc run_program filename ['-h'] output to_text_string proc communicate [0] valid 'Optionsandarguments andcorrespondingenvironmentvariables 'if 'usage ' in output and valid in output return Trueelse return Falseexcept return False
@content_git_object_init connectdef git_sha_metadata content git_content if not content settings['GIT_SHA_METADATA'] returnif not git_content is_committed returncontent metadata['gitsha_newest'] str git_content get_newest_commit content metadata['gitsha_oldest'] str git_content get_oldest_commit
def __determine_before_str options now datetime datetime now today datetime datetime now year now month now day day_offset options agebefore_str today - datetime timedelta day_offset isoformat if day_offset is not None else None return before_str
def backup_create context values return IMPL backup_create context values
def version path with settings hide 'running' 'stdout' 'warnings' warn_only True res run os path join path 'bin/version sh' if res failed return Noneelse return _extract_tomcat_version res
def non_token_view_using_request_processor request context RequestContext request processors [csrf] template Template '' return HttpResponse template render context
def non_token_view_using_request_processor request context RequestContext request processors [csrf] template Template '' return HttpResponse template render context
def non_token_view_using_request_processor request context RequestContext request processors [csrf] template Template '' return HttpResponse template render context
def _get_activity_rights activity_type activity_id if activity_type feconf ACTIVITY_TYPE_EXPLORATION return get_exploration_rights activity_id strict False elif activity_type feconf ACTIVITY_TYPE_COLLECTION return get_collection_rights activity_id strict False else raise Exception 'Cannotgetactivityrightsforunknownactivitytype %s' % activity_type
def _get_activity_rights activity_type activity_id if activity_type feconf ACTIVITY_TYPE_EXPLORATION return get_exploration_rights activity_id strict False elif activity_type feconf ACTIVITY_TYPE_COLLECTION return get_collection_rights activity_id strict False else raise Exception 'Cannotgetactivityrightsforunknownactivitytype %s' % activity_type
def _get_activity_rights activity_type activity_id if activity_type feconf ACTIVITY_TYPE_EXPLORATION return get_exploration_rights activity_id strict False elif activity_type feconf ACTIVITY_TYPE_COLLECTION return get_collection_rights activity_id strict False else raise Exception 'Cannotgetactivityrightsforunknownactivitytype %s' % activity_type
def course_grading_policy course_key course _retrieve_course course_key return GradingPolicySerializer course raw_grader many True data
def _adapt_mismatch original matchee marker object if getattr original 'mismatched' marker is marker return mismatch matchee original describe original get_details return original
def _adapt_mismatch original matchee marker object if getattr original 'mismatched' marker is marker return mismatch matchee original describe original get_details return original
def _adapt_mismatch original matchee marker object if getattr original 'mismatched' marker is marker return mismatch matchee original describe original get_details return original
def _adapt_mismatch original matchee marker object if getattr original 'mismatched' marker is marker return mismatch matchee original describe original get_details return original
def sortLoopsInOrderOfArea isDescending loops loops sort key euclidean getAreaLoopAbsolute reverse isDescending
def sortLoopsInOrderOfArea isDescending loops loops sort key euclidean getAreaLoopAbsolute reverse isDescending
def _get_used_lun_id_counter mapping used_luns _get_used_lun_ids_for_mappings mapping used_lun_id_counter collections Counter used_luns return used_lun_id_counter
def _get_used_lun_id_counter mapping used_luns _get_used_lun_ids_for_mappings mapping used_lun_id_counter collections Counter used_luns return used_lun_id_counter
def _get_dev_port backend instance None port os environ get _get_dev_port_var backend instance None if port return int port else return None
def human_readable size sep '' divisor suffix 1 'B' for i candidate in enumerate 'B' 'KB' 'MB' 'GB' 'TB' 'PB' 'EB' if size < 1 << i + 1 * 10 divisor suffix 1 << i * 10 candidate breaksize str float size / divisor if size find ' ' > -1 size size[ size find ' ' + 2 ]if size endswith ' 0' size size[ -2 ]return size + sep + suffix
def delete_subnet subnet profile None conn _auth profile return conn delete_subnet subnet
def _delete_asset course_key asset_key_string if asset_key_string try asset_key AssetKey from_string asset_key_string except InvalidKeyError if '/' asset_key_string[0] asset_key_string asset_key_string[1 ]try asset_key AssetKey from_string asset_key_string except InvalidKeyError LOGGER info 'Incourse%r unabletoparseassetkey%r notattemptingtodeletesignatory ' course_key asset_key_string returnelse LOGGER info 'Incourse%r unabletoparseassetkey%r notattemptingtodeletesignatory ' course_key asset_key_string returntry delete_asset course_key asset_key except AssetNotFoundException pass
def _delete_asset course_key asset_key_string if asset_key_string try asset_key AssetKey from_string asset_key_string except InvalidKeyError if '/' asset_key_string[0] asset_key_string asset_key_string[1 ]try asset_key AssetKey from_string asset_key_string except InvalidKeyError LOGGER info 'Incourse%r unabletoparseassetkey%r notattemptingtodeletesignatory ' course_key asset_key_string returnelse LOGGER info 'Incourse%r unabletoparseassetkey%r notattemptingtodeletesignatory ' course_key asset_key_string returntry delete_asset course_key asset_key except AssetNotFoundException pass
def s3_auth_user_represent id row None if row return row emailelif not id return current messages['NONE']db current dbtable db auth_useruser db table id id select table email limitby 0 1 cache current s3db cache first try return user emailexcept return current messages UNKNOWN_OPT
@publicdef gcdex f g *gens **args options allowed_flags args ['auto' 'polys'] try F G opt parallel_poly_from_expr f g *gens **args except PolificationFailed as exc domain a b construct_domain exc exprs try s t h domain gcdex a b except NotImplementedError raise ComputationFailed 'gcdex' 2 exc else return domain to_sympy s domain to_sympy t domain to_sympy h s t h F gcdex G auto opt auto if not opt polys return s as_expr t as_expr h as_expr else return s t h
def _cast_to_array_dtype in1 in2 if numpy issubdtype in2 dtype numpy float in1 in1 real astype in2 dtype else in1 in1 astype in2 dtype return in1
@cachedef suggest query search_params {u'list' u'search' u'srinfo' u'suggestion' u'srprop' u''}search_params[u'srsearch'] queryraw_result _wiki_request search_params if raw_result[u'query'] get u'searchinfo' return raw_result[u'query'][u'searchinfo'][u'suggestion']return None
@docfillerdef sobel input axis -1 output None mode 'reflect' cval 0 0 input numpy asarray input axis _ni_support _check_axis axis input ndim output return_value _ni_support _get_output output input modes _ni_support _normalize_sequence mode input ndim correlate1d input [ -1 0 1] axis output modes[axis] cval 0 axes [ii for ii in range input ndim if ii axis ]for ii in axes correlate1d output [1 2 1] ii output modes[ii] cval 0 return return_value
def _take iter num out []for val in iter out append val num - 1if num < 0 breakreturn out
def install_cygwin name install_args None override_args False return install name source 'cygwin' install_args install_args override_args override_args
def get_page_args pages {}for arg in request args re_match re findall 'page_ * ' arg if re_match pages[re_match[0]] int request args get arg return pages
def _zpkbilinear z p k fs z atleast_1d z p atleast_1d p degree _relative_degree z p fs2 2 * fs z_z fs2 + z / fs2 - z p_z fs2 + p / fs2 - p z_z append z_z - ones degree k_z k * real prod fs2 - z / prod fs2 - p return z_z p_z k_z
def gencastshapes for n in range 32 yield [n] ndim randrange 4 6 minshape 1 if randrange 100 > 80 else 2 yield [randrange minshape 5 for _ in range ndim ] ndim randrange 2 4 minshape 1 if randrange 100 > 80 else 2 yield [randrange minshape 5 for _ in range ndim ]
def test_db_illegal_catalog db vos_catalog VOSDatabase create_empty db _catalogs[u'foo'] {u'foo' u'bar'}with pytest raises VOSError db vos_catalog VOSDatabase db _tree
def setup_platform hass config add_devices discovery_info None username config get CONF_USERNAME password config get CONF_PASSWORD try hydroquebec_data HydroquebecData username password hydroquebec_data update except requests exceptions HTTPError as error _LOGGER error error return Falsename config get CONF_NAME sensors []for variable in config[CONF_MONITORED_VARIABLES] sensors append HydroQuebecSensor hydroquebec_data variable name add_devices sensors
def _update_secret namespace name data apiserver_url url '{0}/api/v1/namespaces/{1}/secrets/{2}' format apiserver_url namespace name data [{'op' 'replace' 'path' '/data' 'value' data}]ret _kpatch url data if ret get 'status' 404 return "Node{0}doesn'texist" format url return ret
def _update_secret namespace name data apiserver_url url '{0}/api/v1/namespaces/{1}/secrets/{2}' format apiserver_url namespace name data [{'op' 'replace' 'path' '/data' 'value' data}]ret _kpatch url data if ret get 'status' 404 return "Node{0}doesn'texist" format url return ret
def gm_constrs t x_list p assert is_weight p w dyad_completion p tree decompose w d defaultdict lambda lu create_var t size d[w] tif len x_list < len w x_list + [t]assert len x_list len w for i p v in enumerate zip w x_list if p > 0 tmp [0] * len w tmp[i] 1d[tuple tmp ] vconstraints []for elem children in tree items if 1 not in elem constraints + [gm d[elem] d[children[0]] d[children[1]] ]return constraints
def gm_constrs t x_list p assert is_weight p w dyad_completion p tree decompose w d defaultdict lambda lu create_var t size d[w] tif len x_list < len w x_list + [t]assert len x_list len w for i p v in enumerate zip w x_list if p > 0 tmp [0] * len w tmp[i] 1d[tuple tmp ] vconstraints []for elem children in tree items if 1 not in elem constraints + [gm d[elem] d[children[0]] d[children[1]] ]return constraints
def _trim_doc_string text lines text replace '\r\n' '\n' split '\n' nlines [lines pop 0 ]if lines min_indent min [ len line - len line lstrip for line in lines] for line in lines nlines append line[min_indent ] return '\n' join nlines
def find_playlist_changes orig_tracks modified_tracks s_pairs get_id_pairs orig_tracks d_pairs get_id_pairs modified_tracks s_count Counter s_pairs d_count Counter d_pairs to_del s_count - d_count to_add d_count - s_count to_keep set s_count & d_count return to_del to_add to_keep
def print_results query_results page_token Nonewhile True rows total_rows page_token query_results fetch_data max_results 10 page_token page_token for row in rows print rowif not page_token break
@frappe whitelist def get module data get_data module out {u'data' data}return out
@frappe whitelist def get module data get_data module out {u'data' data}return out
@frappe whitelist def get module data get_data module out {u'data' data}return out
def bin_constructor func func argtypes [c_char_p c_size_t]func restype GEOM_PTRfunc errcheck check_geomreturn func
def _check_ldev ldev_info ldev existing_ref if ldev_info['sts'] NORMAL_STS msg utils output_log MSG INVALID_LDEV_FOR_MANAGE raise exception ManageExistingInvalidReference existing_ref existing_ref reason msg vol_attr set ldev_info['vol_attr'] if not ldev_info['vol_type'] startswith 'OPEN-V' or len vol_attr < 2 or not vol_attr issubset _PERMITTED_TYPES msg utils output_log MSG INVALID_LDEV_ATTR_FOR_MANAGE ldev ldev ldevtype utils NVOL_LDEV_TYPE raise exception ManageExistingInvalidReference existing_ref existing_ref reason msg if ldev_info['vol_size'] % utils GIGABYTE_PER_BLOCK_SIZE msg utils output_log MSG INVALID_LDEV_SIZE_FOR_MANAGE ldev ldev raise exception ManageExistingInvalidReference existing_ref existing_ref reason msg if ldev_info['num_port'] msg utils output_log MSG INVALID_LDEV_PORT_FOR_MANAGE ldev ldev raise exception ManageExistingInvalidReference existing_ref existing_ref reason msg
def _check_ldev ldev_info ldev existing_ref if ldev_info['sts'] NORMAL_STS msg utils output_log MSG INVALID_LDEV_FOR_MANAGE raise exception ManageExistingInvalidReference existing_ref existing_ref reason msg vol_attr set ldev_info['vol_attr'] if not ldev_info['vol_type'] startswith 'OPEN-V' or len vol_attr < 2 or not vol_attr issubset _PERMITTED_TYPES msg utils output_log MSG INVALID_LDEV_ATTR_FOR_MANAGE ldev ldev ldevtype utils NVOL_LDEV_TYPE raise exception ManageExistingInvalidReference existing_ref existing_ref reason msg if ldev_info['vol_size'] % utils GIGABYTE_PER_BLOCK_SIZE msg utils output_log MSG INVALID_LDEV_SIZE_FOR_MANAGE ldev ldev raise exception ManageExistingInvalidReference existing_ref existing_ref reason msg if ldev_info['num_port'] msg utils output_log MSG INVALID_LDEV_PORT_FOR_MANAGE ldev ldev raise exception ManageExistingInvalidReference existing_ref existing_ref reason msg
def send_draft_copy account draft custom_body recipient response_on_success encode draft response_on_success['body'] custom_bodyresponse_on_success APIEncoder jsonify response_on_success try sendmail_client get_sendmail_client account sendmail_client send_custom draft custom_body [recipient] except SendMailException as exc kwargs {}if exc failures kwargs['failures'] exc failuresif exc server_error kwargs['server_error'] exc server_errorreturn err exc http_code exc message **kwargs return response_on_success
def raise_if_offline func @functools wraps func def decorator *args **kwargs if context is_offline_mode raise RuntimeError _ '%scannotbecalledwhileinofflinemode' % func __name__ return func *args **kwargs return decorator
def _filter_crowd_proposals roidb crowd_thresh for ix entry in enumerate roidb overlaps entry['gt_overlaps'] toarray crowd_inds np where overlaps max axis 1 -1 [0]non_gt_inds np where entry['gt_classes'] 0 [0]if len crowd_inds 0 or len non_gt_inds 0 continueiscrowd [int True for _ in xrange len crowd_inds ]crowd_boxes ds_utils xyxy_to_xywh entry['boxes'][crowd_inds ] non_gt_boxes ds_utils xyxy_to_xywh entry['boxes'][non_gt_inds ] ious COCOmask iou non_gt_boxes crowd_boxes iscrowd bad_inds np where ious max axis 1 > crowd_thresh [0]overlaps[non_gt_inds[bad_inds] ] -1 roidb[ix]['gt_overlaps'] scipy sparse csr_matrix overlaps return roidb
def create_region name region cache create_region region name namereturn region
def getUniqueVertexes loops vertexDictionary {}uniqueVertexes []for loop in loops for vertexIndex vertex in enumerate loop vertexTuple vertex x vertex y vertex z if vertexTuple in vertexDictionary loop[vertexIndex] vertexDictionary[vertexTuple]else if vertex __class__ Vector3Index loop[vertexIndex] index len vertexDictionary else loop[vertexIndex] Vector3Index len vertexDictionary vertex x vertex y vertex z vertexDictionary[vertexTuple] loop[vertexIndex]uniqueVertexes append loop[vertexIndex] return uniqueVertexes
def lemmatize word lemma nltk corpus wordnet morphy word pos nltk corpus wordnet VERB if lemma is not None return lemmareturn word
def lemmatize word lemma nltk corpus wordnet morphy word pos nltk corpus wordnet VERB if lemma is not None return lemmareturn word
def unicode_dict _dict r {}for k v in iteritems _dict r[unicode_obj k ] unicode_obj v return r
def test_operator_type assert TPOTSelectKBest type 'Selector'
def test_operator_type assert TPOTSelectKBest type 'Selector'
def independent a b return not dependent a b
def independent a b return not dependent a b
def independent a b return not dependent a b
def independent a b return not dependent a b
@allow_cross_site_request@non_atomic_requestsdef render_csv request addon stats fields title None show_disclaimer None ts time strftime '%c%z' context {'addon' addon 'timestamp' ts 'title' title 'show_disclaimer' show_disclaimer}response render request 'stats/csv_header txt' context writer UnicodeCSVDictWriter response fields restval 0 extrasaction 'ignore' writer writeheader writer writerows stats fudge_headers response stats response['Content-Type'] 'text/csv charset utf-8'return response
def _in x y try return x isin y except AttributeError if is_list_like x try return y isin x except AttributeError passreturn x in y
def num_solutions user return Question objects filter solution__creator user count
def _format_state_result name result changes None comment '' if changes is None changes {'old' '' 'new' ''}return {'name' name 'result' result 'changes' changes 'comment' comment}
def build_network_settings **settings current_network_settings _parse_rh_config _RH_NETWORK_FILE opts _parse_network_settings settings current_network_settings try template JINJA get_template 'network jinja' except jinja2 exceptions TemplateNotFound log error 'Couldnotloadtemplatenetwork jinja' return ''network template render opts if settings['test'] return _read_temp network _write_file_network network _RH_NETWORK_FILE return _read_file _RH_NETWORK_FILE
def get_resampler_for_grouping groupby rule how None fill_method None limit None kind None **kwargs tg TimeGrouper freq rule **kwargs resampler tg _get_resampler groupby obj kind kind r resampler _get_resampler_for_grouping groupby groupby return _maybe_process_deprecations r how how fill_method fill_method limit limit
def cxUniformPartialyMatched ind1 ind2 indpb size min len ind1 len ind2 p1 p2 [0] * size [0] * size for i in xrange size p1[ind1[i]] ip2[ind2[i]] ifor i in xrange size if random random < indpb temp1 ind1[i]temp2 ind2[i] ind1[i] ind1[p1[temp2]] temp2 temp1 ind2[i] ind2[p2[temp1]] temp1 temp2 p1[temp1] p1[temp2] p1[temp2] p1[temp1] p2[temp1] p2[temp2] p2[temp2] p2[temp1] return ind1 ind2
def sublist_reverse lst a b while b > a lst[a] lst[b] lst[b] lst[a] b - 1a + 1
def getAllTransformedVertexes transformedVertexes xmlObject for archivableObject in xmlObject archivableObjects transformedVertexes + archivableObject getTransformedVertexes return transformedVertexes
def test_function_series2 class my_function2 Function def fdiff self argindex 1 return - sin self args[0] @classmethoddef eval cls arg arg sympify arg if arg 0 return sympify 1 assert my_function2 x series x 0 10 cos x series x 0 10
def get key if CONFIG if key in CONFIG opts return CONFIG opts[key]['value']elif key in CONFIG _opts return CONFIG _opts[key]['value']
def inet_aton text parts text split ' ' if len parts 4 raise dns exception SyntaxErrorfor part in parts if not part isdigit raise dns exception SyntaxErrorif len part > 1 and part[0] '0' raise dns exception SyntaxErrortry bytes [int part for part in parts]return struct pack 'BBBB' *bytes except raise dns exception SyntaxError
def unlock path zk_hosts None identifier None max_concurrency 1 ephemeral_lease False if zk_hosts is not None and path not in SEMAPHORE_MAP zk _get_zk_conn zk_hosts SEMAPHORE_MAP[path] _Semaphore zk path identifier max_leases max_concurrency ephemeral_lease ephemeral_lease if path in SEMAPHORE_MAP SEMAPHORE_MAP[path] release del SEMAPHORE_MAP[path]return Trueelse logging error 'Unabletofindleaseforpath{0}' format path return False
def table_extend tables keep_headers True from copy import deepcopyfor ii t in enumerate tables[ ] t deepcopy t if t[0] datatype 'header' t[0][0] data t titlet[0][0] _datatype Nonet[0][0] row t[0][1] rowif not keep_headers and ii > 0 for c in t[0][1 ] c data ''if ii 0 table_all telse r1 table_all[ -1 ]r1 add_format 'txt' row_dec_below '-' table_all extend t table_all title Nonereturn table_all
def ensure_ca_bundle_dir global ca_bundle_dirif not ca_bundle_dir ca_bundle_dir os path join sublime packages_path 'User' if not os path exists ca_bundle_dir os mkdir ca_bundle_dir
def fanout_cast context topic msg return _get_impl fanout_cast CONF context topic msg
def config_dict handle conf_mappings handler None selected_config get_config handle selected_config add_listener _SyncListener conf_mappings handler update return conf_mappings
def mod_init low try __salt__['portage_config enforce_nice_config'] except Exception return Falsereturn True
def get_variables_to_restore return tf get_collection VARIABLES_TO_RESTORE [ ]
@depends HAS_HDPARM def _hdparm args failhard True cmd 'hdparm{0}' format args result __salt__['cmd run_all'] cmd if result['retcode'] 0 msg '{0} {1}' format cmd result['stderr'] if failhard raise CommandExecutionError msg else log warning msg return result['stdout']
def test_rechunk_method old 5 2 3 * 4 new 3 3 3 1 * 4 a np random uniform 0 1 10000 reshape 10 * 4 x da from_array a chunks old x2 x rechunk chunks new assert x2 chunks new assert np all x2 compute a
def reset_sequence model sql connection ops sequence_reset_sql no_style [model] for cmd in sql connection cursor execute cmd
def reset_sequence model sql connection ops sequence_reset_sql no_style [model] for cmd in sql connection cursor execute cmd
@jwt_required def auth_jwt g user current_identity
def _wait_for_new_device base expected_size time_limit 60 start_time time time elapsed_time time time - start_time while elapsed_time < time_limit for device in list set FilePath '/sys/block' children - set base device_name device basename if device_name startswith 'sd' 'xvd' and _get_device_size device_name expected_size return FilePath '/dev' child device_name time sleep 0 1 elapsed_time time time - start_time new_devices list device basename for device in set FilePath '/sys/block' children - set base new_devices_size list _get_device_size device_name for device_name in new_devices NO_NEW_DEVICE_IN_OS new_devices new_devices new_devices_size new_devices_size expected_size expected_size time_limit time_limit write return None
def _wait_for_new_device base expected_size time_limit 60 start_time time time elapsed_time time time - start_time while elapsed_time < time_limit for device in list set FilePath '/sys/block' children - set base device_name device basename if device_name startswith 'sd' 'xvd' and _get_device_size device_name expected_size return FilePath '/dev' child device_name time sleep 0 1 elapsed_time time time - start_time new_devices list device basename for device in set FilePath '/sys/block' children - set base new_devices_size list _get_device_size device_name for device_name in new_devices NO_NEW_DEVICE_IN_OS new_devices new_devices new_devices_size new_devices_size expected_size expected_size time_limit time_limit write return None
def get_hostname server None version None instance None req servers_service_pb GetHostnameRequest if server req set_server server if version req set_version version if instance if not isinstance instance basestring long int raise TypeError "'instance'argmustbeoftypebasestring longorint " req set_instance '%s' % instance resp servers_service_pb GetHostnameResponse try apiproxy_stub_map MakeSyncCall 'servers' 'GetHostname' req resp except apiproxy_errors ApplicationError as e if e application_error servers_service_pb ServersServiceError INVALID_SERVER raise InvalidServerError elif e application_error servers_service_pb ServersServiceError INVALID_INSTANCES raise InvalidInstancesError else raise Error return resp hostname
def _write_incron_lines user lines if user 'system' ret {}ret['retcode'] _write_file _INCRON_SYSTEM_TAB 'salt' '' join lines return retelse path salt utils files mkstemp with salt utils fopen path 'w+' as fp_ fp_ writelines lines if __grains__['os_family'] 'Solaris' and user 'root' __salt__['cmd run'] 'chown{0}{1}' format user path python_shell False ret __salt__['cmd run_all'] _get_incron_cmdstr path runas user python_shell False os remove path return ret
def rst2md text pandoc subprocess Popen ['pandoc' '--from rst' '--to markdown' '--no-wrap'] stdin subprocess PIPE stdout subprocess PIPE stderr subprocess PIPE stdout _ pandoc communicate text encode 'utf-8' md stdout decode 'utf-8' strip return re sub '^-' '-' md flags re M
def rst2md text pandoc subprocess Popen ['pandoc' '--from rst' '--to markdown' '--no-wrap'] stdin subprocess PIPE stdout subprocess PIPE stderr subprocess PIPE stdout _ pandoc communicate text encode 'utf-8' md stdout decode 'utf-8' strip return re sub '^-' '-' md flags re M
def create_dummy_vm name service_instance vm_folder resource_pool datastore vm_name 'MARVEL-' + name datastore_path '[' + datastore + ']' + vm_name vmx_file vim vm FileInfo logDirectory None snapshotDirectory None suspendDirectory None vmPathName datastore_path config vim vm ConfigSpec name vm_name memoryMB 128 numCPUs 1 files vmx_file guestId 'dosGuest' version 'vmx-07' print 'CreatingVM{} ' format vm_name task vm_folder CreateVM_Task config config pool resource_pool tasks wait_for_tasks service_instance [task]
def _llvm_jit_code args expr signature callback_type if callback_type is None jit LLVMJitCode signature else jit LLVMJitCodeCallback signature jit _create_args args jit _create_function_base jit _create_param_dict args strmod jit _create_function expr if False print 'LLVMIR' print strmod fptr jit _compile_function strmod return fptr
def html_escape txt if any ch in txt for ch in _HTML_TABLE return '' join _HTML_TABLE get ch ch for ch in txt else return txt
def print_query results print 'QueryParameters ' query results get 'query' for key value in query iteritems print '%s %s' % key value print
def inthread func @wraps func def wrapped *a **kw return threads deferToThread func *a **kw return wrapped
def cachedir_index_del minion_id base None base init_cachedir base index_file os path join base 'index p' lock_file index_file if os path exists index_file with salt utils fopen index_file 'r' as fh_ index msgpack load fh_ else returnif minion_id in index del index[minion_id]with salt utils fopen index_file 'w' as fh_ msgpack dump index fh_ unlock_file index_file
def _createPluginDummy entrypath pluginContent real pluginModule entrypath createDirectory pkg entrypath child 'plugindummy' pkg createDirectory if real pkg child '__init__ py' setContent '' plugs pkg child 'plugins' plugs createDirectory if real plugs child '__init__ py' setContent pluginInitFile plugs child pluginModule + ' py' setContent pluginContent return plugs
def test_dynamically_emptied_directories tmpdir adir tmpdir join 'adir' ensure dir True bdir adir join 'bdir' ensure dir True some_file bdir join 'afile' some_file write '1234567890' base_dir adir strpath spec parts tar_partition partition base_dir tar_paths []for part in parts for tar_info in part rel_path os path relpath tar_info submitted_path base_dir tar_paths append rel_path assert 'bdir' in tar_paths
def test_dynamically_emptied_directories tmpdir adir tmpdir join 'adir' ensure dir True bdir adir join 'bdir' ensure dir True some_file bdir join 'afile' some_file write '1234567890' base_dir adir strpath spec parts tar_partition partition base_dir tar_paths []for part in parts for tar_info in part rel_path os path relpath tar_info submitted_path base_dir tar_paths append rel_path assert 'bdir' in tar_paths
def test_dynamically_emptied_directories tmpdir adir tmpdir join 'adir' ensure dir True bdir adir join 'bdir' ensure dir True some_file bdir join 'afile' some_file write '1234567890' base_dir adir strpath spec parts tar_partition partition base_dir tar_paths []for part in parts for tar_info in part rel_path os path relpath tar_info submitted_path base_dir tar_paths append rel_path assert 'bdir' in tar_paths
def test_dynamically_emptied_directories tmpdir adir tmpdir join 'adir' ensure dir True bdir adir join 'bdir' ensure dir True some_file bdir join 'afile' some_file write '1234567890' base_dir adir strpath spec parts tar_partition partition base_dir tar_paths []for part in parts for tar_info in part rel_path os path relpath tar_info submitted_path base_dir tar_paths append rel_path assert 'bdir' in tar_paths
def test_dynamically_emptied_directories tmpdir adir tmpdir join 'adir' ensure dir True bdir adir join 'bdir' ensure dir True some_file bdir join 'afile' some_file write '1234567890' base_dir adir strpath spec parts tar_partition partition base_dir tar_paths []for part in parts for tar_info in part rel_path os path relpath tar_info submitted_path base_dir tar_paths append rel_path assert 'bdir' in tar_paths
def one year week home away kind 'REG' started False infos _search_schedule year week home away kind started if not infos return Noneassert len infos 1 'Morethanonegamematchesthegivencriteria 'return nflgame game Game infos[0]['eid']
def one year week home away kind 'REG' started False infos _search_schedule year week home away kind started if not infos return Noneassert len infos 1 'Morethanonegamematchesthegivencriteria 'return nflgame game Game infos[0]['eid']
def test_add_item_redirect_update qtbot tmpdir url 'http //www example com/'hist_file tmpdir / 'filled-history' hist_file write '12345{}\n' format url hist history WebHistory hist_dir str tmpdir hist_name 'filled-history' list hist async_read with qtbot assertNotEmitted hist add_completion_item with qtbot waitSignal hist item_added hist add_url QUrl url redirect True atime 67890 entry history Entry url QUrl url redirect True atime 67890 title '' assert hist history_dict[url] entry
def test_add_item_redirect_update qtbot tmpdir url 'http //www example com/'hist_file tmpdir / 'filled-history' hist_file write '12345{}\n' format url hist history WebHistory hist_dir str tmpdir hist_name 'filled-history' list hist async_read with qtbot assertNotEmitted hist add_completion_item with qtbot waitSignal hist item_added hist add_url QUrl url redirect True atime 67890 entry history Entry url QUrl url redirect True atime 67890 title '' assert hist history_dict[url] entry
def isqref object return isinstance object tuple and len object 2 and isinstance object[0] basestring and isinstance object[1] basestring
@gen coroutinedef RemoveFollowers client obj_store user_id device_id request request['user_id'] user_id yield Activity VerifyActivityId client user_id device_id request['activity']['activity_id'] yield gen Task Operation CreateAndExecute client user_id device_id 'RemoveFollowersOperation Execute' request logging info 'REMOVEFOLLOWERS user %d device %d %dfollowers' % user_id device_id len request['remove_ids'] raise gen Return {}
def tgrep_tokenize tgrep_string parser _build_tgrep_parser False if isinstance tgrep_string binary_type tgrep_string tgrep_string decode return list parser parseString tgrep_string
@export_as_apidef instruments id_or_symbols if isinstance id_or_symbols six string_types return get_data_proxy instrument id_or_symbols return map get_data_proxy instrument id_or_symbols
@export_as_apidef instruments id_or_symbols if isinstance id_or_symbols six string_types return get_data_proxy instrument id_or_symbols return map get_data_proxy instrument id_or_symbols
def extract_live_config config plugins live_config config _sections['live_config'] copy del live_config['__name__']parsed ConfigValueParser live_config parsed add_spec Globals live_config_spec for plugin in plugins parsed add_spec plugin live_config return parsed
def extract_live_config config plugins live_config config _sections['live_config'] copy del live_config['__name__']parsed ConfigValueParser live_config parsed add_spec Globals live_config_spec for plugin in plugins parsed add_spec plugin live_config return parsed
def supports_firefox file_obj apps file_obj version apps all if not file_obj binary_components and not file_obj strict_compatibility return apps filter max__application__in SIGN_FOR_APPS max__version_int__gte version_int settings MIN_D2C_VERSION else return apps filter max__application__in [amo FIREFOX id amo ANDROID id] max__version_int__gte version_int settings MIN_NOT_D2C_VERSION
def get_redirects redirects_filename redirects {}print 'ParsingtheNTredirectfile' for l line in enumerate BZ2File redirects_filename split line split if len split 4 print 'ignoringmalformedline ' + line continueredirects[short_name split[0] ] short_name split[2] if l % 1000000 0 print '[%s]line %08d' % datetime now isoformat l print 'Computingthetransitiveclosureoftheredirectrelation' for l source in enumerate redirects keys transitive_target Nonetarget redirects[source]seen set [source] while True transitive_target targettarget redirects get target if target is None or target in seen breakseen add target redirects[source] transitive_targetif l % 1000000 0 print '[%s]line %08d' % datetime now isoformat l return redirects
def get_redirects redirects_filename redirects {}print 'ParsingtheNTredirectfile' for l line in enumerate BZ2File redirects_filename split line split if len split 4 print 'ignoringmalformedline ' + line continueredirects[short_name split[0] ] short_name split[2] if l % 1000000 0 print '[%s]line %08d' % datetime now isoformat l print 'Computingthetransitiveclosureoftheredirectrelation' for l source in enumerate redirects keys transitive_target Nonetarget redirects[source]seen set [source] while True transitive_target targettarget redirects get target if target is None or target in seen breakseen add target redirects[source] transitive_targetif l % 1000000 0 print '[%s]line %08d' % datetime now isoformat l return redirects
def _load_editor caller key caller db _multidesc_editkeymatch [ind for ind tup in enumerate caller db multidesc if tup[0] key ]if match return caller db multidesc[match[0]][1]return ''
def has_permission obj_name principal permission access_mode 'grant' obj_type 'file' exact True if access_mode lower not in ['grant' 'deny'] raise SaltInvocationError 'Invalid"access_mode"passed {0}' format access_mode access_mode access_mode lower dacl Dacl obj_name obj_type obj_type obj_type lower sid get_sid principal chk_flag dacl ace_perms[obj_type]['basic'] get permission lower dacl ace_perms[obj_type]['advanced'] get permission lower False if not chk_flag raise SaltInvocationError 'Invalid"permission"passed {0}' format permission cur_flag Nonefor i in range 0 dacl dacl GetAceCount ace dacl dacl GetAce i if ace[2] sid and dacl ace_type[ace[0][0]] access_mode cur_flag ace[1]if not cur_flag return Falseif exact return chk_flag cur_flag return cur_flag & chk_flag chk_flag
def render_openid_request request openid_request return_to trust_root None if trust_root is None trust_root getattr settings 'OPENID_TRUST_ROOT' request build_absolute_uri '/' if openid_request shouldSendRedirect redirect_url openid_request redirectURL trust_root return_to return HttpResponseRedirect redirect_url else form_html openid_request htmlMarkup trust_root return_to form_tag_attrs {'id' 'openid_message'} return HttpResponse form_html content_type 'text/html charset UTF-8'
@intercept_errors UserAPIInternalError ignore_errors [UserAPIRequestError] def set_user_preference requesting_user preference_key preference_value username None existing_user _get_authorized_user requesting_user username serializer create_user_preference_serializer existing_user preference_key preference_value validate_user_preference_serializer serializer preference_key preference_value try serializer save except Exception as error raise _create_preference_update_error preference_key preference_value error
def test_ecliptic_heliobary icrs ICRS 1 * u deg 2 * u deg distance 1 5 * R_sun bary icrs transform_to BarycentricTrueEcliptic helio icrs transform_to HeliocentricTrueEcliptic assert np abs bary distance - helio distance > 1 * u km helio_in_bary_frame bary realize_frame helio cartesian assert bary separation helio_in_bary_frame > 1 * u arcmin
@frappe whitelist def get_opening_accounts company accounts frappe db sql_list u"select\n DCTB DCTB DCTB namefromtabAccount\n DCTB DCTB where\n DCTB DCTB DCTB is_group 0and\n DCTB DCTB DCTB report_type 'BalanceSheet'and\n DCTB DCTB DCTB ifnull warehouse '' ''and\n DCTB DCTB DCTB company %s\n DCTB DCTB orderbynameasc" company return [{u'account' a u'balance' get_balance_on a } for a in accounts]
def worker_stop worker lbn profile 'default' return _worker_ctl worker lbn 's' profile
def worker_stop worker lbn profile 'default' return _worker_ctl worker lbn 's' profile
def run_network_interacting_from_args *a **kw return retry_effect_with_timeout run_from_args *a **kw timeout _TIMEOUT total_seconds
@cached_query SubredditQueryCache def get_spam_filtered_links sr_id return Link _query Link c sr_id sr_id Link c _spam True Link c verdict 'mod-removed' sort db_sort 'new'
def get_storage_profile_spec session storage_policy profile_id pbm get_profile_id_by_name session storage_policy if profile_id client_factory session vim client factorystorage_profile_spec client_factory create 'ns0 VirtualMachineDefinedProfileSpec' storage_profile_spec profileId profile_id uniqueIdreturn storage_profile_spec
def search if not auth is_logged_in or auth basic returnvalue request vars term or request vars q type get_vars get 'type' None if value if type items person_search value type else items person_search value item json dumps items response headers['Content-Type'] 'application/json'return itemreturn
def job_log_logger registry xml_parent data top XML SubElement xml_parent 'org jenkins ci plugins jobloglogger JobLogLoggerBuildWrapper' XML SubElement top 'suppressEmpty' text str data get 'suppress-empty' True lower
def job_log_logger registry xml_parent data top XML SubElement xml_parent 'org jenkins ci plugins jobloglogger JobLogLoggerBuildWrapper' XML SubElement top 'suppressEmpty' text str data get 'suppress-empty' True lower
def job_log_logger registry xml_parent data top XML SubElement xml_parent 'org jenkins ci plugins jobloglogger JobLogLoggerBuildWrapper' XML SubElement top 'suppressEmpty' text str data get 'suppress-empty' True lower
def job_log_logger registry xml_parent data top XML SubElement xml_parent 'org jenkins ci plugins jobloglogger JobLogLoggerBuildWrapper' XML SubElement top 'suppressEmpty' text str data get 'suppress-empty' True lower
def get_vlan_device_name src_dev vlan src_dev p_utils get_interface_name src_dev max_len n_const DEVICE_NAME_MAX_LEN - MAX_VLAN_POSTFIX_LEN return '%s %s' % src_dev vlan
def quota_usage_create context project_id resource in_use reserved until_refresh return IMPL quota_usage_create context project_id resource in_use reserved until_refresh
def _nose_tools_functions module _BUILDER string_build textwrap dedent '\nimportunittest\n\nclassTest unittest TestCase \npass\na Test \n' try case next module['a'] infer except astroid InferenceError returnfor method in case methods if method name startswith 'assert' and '_' not in method name pep8_name _pep8 method name yield pep8_name astroid BoundMethod method case
def __get_tags vm_ t config get_cloud_config_value 'tags' vm_ __opts__ default '[]' search_global False try tags literal_eval t except Exception tags Noneif not tags or not isinstance tags list tags Nonereturn tags
def make_percent_align_filter min_percent min_percent float min_percent * 100 def align_filter blast_result if float blast_result['%IDENTITY'] < min_percent return Falseelse return Truereturn align_filter
def make_percent_align_filter min_percent min_percent float min_percent * 100 def align_filter blast_result if float blast_result['%IDENTITY'] < min_percent return Falseelse return Truereturn align_filter
def compareSegmentLength endpoint otherEndpoint if endpoint segmentLength > otherEndpoint segmentLength return 1if endpoint segmentLength < otherEndpoint segmentLength return -1 return 0
def compareSegmentLength endpoint otherEndpoint if endpoint segmentLength > otherEndpoint segmentLength return 1if endpoint segmentLength < otherEndpoint segmentLength return -1 return 0
def iso_to_javascript_timestamp iso secs iso_to_unix_time iso return secs * 1000
def _make_ctf_comp_coils info coils logger info 'Settingupcompensationdata ' comp_num get_current_comp info if comp_num is None or comp_num 0 logger info 'Nocompensationset Nothingmoretodo ' return Nonen_comp_ch sum [ c['kind'] FIFF FIFFV_MEG_CH for c in info['chs']] logger info '%doutof%dchannelshavethecompensationset ' % n_comp_ch len coils compensator make_compensator info 0 comp_num True logger info 'Desiredcompensationdata %s found ' % comp_num logger info 'Allcompensationchannelsfound ' logger info 'Preselectorcreated ' logger info 'Compensationdatamatrixcreated ' logger info 'Postselectorcreated ' return compensator
def remove_dir_if_empty path ignore_metadata_caches False try os rmdir path except OSError as e if e errno errno ENOTEMPTY or len os listdir path > 0 if ignore_metadata_caches try found Falsefor x in os listdir path if x lower in {' ds_store' 'thumbs db'} found Truex os path join path x if os path isdir x import shutilshutil rmtree x else os remove x except Exception found Falseif found remove_dir_if_empty path returnraise
def remove_credits text textlines text split '\n' credits Nonefor i in 0 -1 if textlines and 'lyrics' in textlines[i] lower credits textlines pop i if credits text '\n' join textlines return text
def remove_credits text textlines text split '\n' credits Nonefor i in 0 -1 if textlines and 'lyrics' in textlines[i] lower credits textlines pop i if credits text '\n' join textlines return text
def show_image image return _sensehat load_image image
def show_image image return _sensehat load_image image
def get_recording_dirs data_dir filtered_recording_dirs []if is_pupil_rec_dir data_dir filtered_recording_dirs append data_dir for root dirs files in os walk data_dir filtered_recording_dirs + [os path join root d for d in dirs if not d startswith ' ' and is_pupil_rec_dir os path join root d ]logger debug 'FilteredRecordingDirs {}' format filtered_recording_dirs return filtered_recording_dirs
def get_recording_dirs data_dir filtered_recording_dirs []if is_pupil_rec_dir data_dir filtered_recording_dirs append data_dir for root dirs files in os walk data_dir filtered_recording_dirs + [os path join root d for d in dirs if not d startswith ' ' and is_pupil_rec_dir os path join root d ]logger debug 'FilteredRecordingDirs {}' format filtered_recording_dirs return filtered_recording_dirs
def get_recording_dirs data_dir filtered_recording_dirs []if is_pupil_rec_dir data_dir filtered_recording_dirs append data_dir for root dirs files in os walk data_dir filtered_recording_dirs + [os path join root d for d in dirs if not d startswith ' ' and is_pupil_rec_dir os path join root d ]logger debug 'FilteredRecordingDirs {}' format filtered_recording_dirs return filtered_recording_dirs
def FlagCxx11Features filename clean_lines linenum error line clean_lines elided[linenum]include Match '\\s*#\\s*include\\s+[<"] [^<"]+ [">]' line if include and include group 1 startswith 'tr1/' error filename linenum 'build/c++tr1' 5 'C++TR1headerssuchas<%s>areunapproved ' % include group 1 if include and include group 1 in 'cfenv' 'condition_variable' 'fenv h' 'future' 'mutex' 'thread' 'chrono' 'ratio' 'regex' 'system_error' error filename linenum 'build/c++11' 5 '<%s>isanunapprovedC++11header ' % include group 1 if Match '\\s*#' line and not Match '\\s*#\\s*define\\b' line returnfor top_name in 'alignment_of' 'aligned_union' if Search '\\bstd %s\\b' % top_name line error filename linenum 'build/c++11' 5 'std %sisanunapprovedC++11classorfunction Sendc-styleanexampleofwhereitwouldmakeyourcodemorereadable andtheymayletyouuseit ' % top_name
def _add_gradients_summaries grads_and_vars summaries []for grad var in grads_and_vars if grad is not None if isinstance grad tf IndexedSlices grad_values grad valueselse grad_values gradsummaries append tf histogram_summary var op name + ' gradient' grad_values summaries append tf histogram_summary var op name + ' gradient_norm' tf global_norm [grad_values] else tf logging info 'Var%shasnogradient' var op name return summaries
def create_address kwargs None call None if call 'function' raise SaltCloudSystemExit 'Thecreate_addressfunctionmustbecalledwith-for--function ' if not kwargs or 'name' not in kwargs log error 'Anamemustbespecifiedwhencreatinganaddress ' return Falseif 'region' not in kwargs log error 'Aregionmustbespecifiedfortheaddress ' return Falsename kwargs['name']ex_region kwargs['region']ex_address kwargs get 'address' None conn get_conn __utils__['cloud fire_event'] 'event' 'createaddress' 'salt/cloud/address/creating' args kwargs sock_dir __opts__['sock_dir'] transport __opts__['transport'] addy conn ex_create_address name ex_region ex_address __utils__['cloud fire_event'] 'event' 'createdaddress' 'salt/cloud/address/created' args kwargs sock_dir __opts__['sock_dir'] transport __opts__['transport'] log info 'CreatedGCEAddress' + name return _expand_address addy
def test_meta_path_before_builtins class MyException Exception passclass K def find_module self name path if name 'time' return selfreturn Nonedef load_module self name raise MyExceptionif 'time' in sys modules del sys modules['time']loader K sys meta_path append loader try import timeAssertUnreachable except MyException passsys meta_path remove loader import time
def project_jnap_opts T current Treturn {1 T 'JNAP-1 StrategicArea1 Governance' 2 T 'JNAP-2 StrategicArea2 Monitoring' 3 T 'JNAP-3 StrategicArea3 DisasterManagement' 4 T 'JNAP-4 StrategicArea4 RiskReductionandClimateChangeAdaptation' }
def showvariables **connection_args mod sys _getframe f_code co_namelog debug '{0}<--' format mod conn _connect **connection_args if conn is None return []rtnv __do_query_into_hash conn 'SHOWVARIABLES' conn close if len rtnv 0 rtnv append [] log debug '{0}-->{1}' format mod len rtnv[0] return rtnv
def add_entrance_exam_milestone course entrance_exam namespace_choices get_namespace_choices milestone_relationship_types get_milestone_relationship_types milestone_namespace generate_milestone_namespace namespace_choices get 'ENTRANCE_EXAM' course id milestone add_milestone {'name' 'TestMilestone' 'namespace' milestone_namespace 'description' 'TestingCoursewareEntranceExamChapter'} add_course_milestone unicode course id milestone_relationship_types['REQUIRES'] milestone add_course_content_milestone unicode course id unicode entrance_exam location milestone_relationship_types['FULFILLS'] milestone
def add_entrance_exam_milestone course entrance_exam namespace_choices get_namespace_choices milestone_relationship_types get_milestone_relationship_types milestone_namespace generate_milestone_namespace namespace_choices get 'ENTRANCE_EXAM' course id milestone add_milestone {'name' 'TestMilestone' 'namespace' milestone_namespace 'description' 'TestingCoursewareEntranceExamChapter'} add_course_milestone unicode course id milestone_relationship_types['REQUIRES'] milestone add_course_content_milestone unicode course id unicode entrance_exam location milestone_relationship_types['FULFILLS'] milestone
def _get_credit_course_requirement_xblocks course_key requirements []for category in CREDIT_REQUIREMENT_XBLOCK_CATEGORIES requirements extend [{'namespace' block get_credit_requirement_namespace 'name' block get_credit_requirement_name 'display_name' block get_credit_requirement_display_name 'start_date' block start 'criteria' {}} for block in _get_xblocks course_key category if _is_credit_requirement block ] return requirements
def _make_task_name cls addons None base_name ' ' join [cls __module__ cls __name__] extra ''if addons extra ' %s' % ' ' join [str a for a in addons] return base_name + extra
def group_update groupname user None host None port None maintenance_db None password None createdb None createroles None createuser None encrypted None inherit None login None superuser None replication None rolepassword None groups None runas None return _role_update groupname user user host host port port maintenance_db maintenance_db password password createdb createdb typ_ 'group' createroles createroles createuser createuser encrypted encrypted login login inherit inherit superuser superuser replication replication rolepassword rolepassword groups groups runas runas
@contextlib contextmanagerdef make_contiguous context builder sig args newtys []newargs []copies []for ty val in zip sig args args if not isinstance ty types Array or ty layout in 'CF' newty newval ty val else newty ty copy layout 'C' copysig signature newty ty newval array_copy context builder copysig val copies append newty newval newtys append newty newargs append newval yield signature sig return_type *newtys tuple newargs for ty val in copies context nrt decref builder ty val
@register simple_tagdef simple_one_default one two 'hi' return 'simple_one_default-Expectedresult %s %s' % one two
def find_all soup name None attrs None recursive True text None limit None **kwargs if text is None return soup find_all name attrs or {} recursive text limit **kwargs if isinstance text string_types text re compile re escape text re I tags soup find_all name attrs or {} recursive **kwargs rv []for tag in tags if match_text text tag rv append tag if limit is not None and len rv > limit breakreturn rv
def find_all soup name None attrs None recursive True text None limit None **kwargs if text is None return soup find_all name attrs or {} recursive text limit **kwargs if isinstance text string_types text re compile re escape text re I tags soup find_all name attrs or {} recursive **kwargs rv []for tag in tags if match_text text tag rv append tag if limit is not None and len rv > limit breakreturn rv
def find_all soup name None attrs None recursive True text None limit None **kwargs if text is None return soup find_all name attrs or {} recursive text limit **kwargs if isinstance text string_types text re compile re escape text re I tags soup find_all name attrs or {} recursive **kwargs rv []for tag in tags if match_text text tag rv append tag if limit is not None and len rv > limit breakreturn rv
def is_stable coefs verbose False A_var1 util comp_matrix coefs eigs np linalg eigvals A_var1 if verbose print 'EigenvaluesofVAR 1 rep' for val in np abs eigs print val return np abs eigs < 1 all
@csrf_exempt@add_p3p_headerdef lti_launch request course_id usage_id if not settings FEATURES['ENABLE_LTI_PROVIDER'] return HttpResponseForbidden params get_required_parameters request POST if not params return HttpResponseBadRequest params update get_optional_parameters request POST try lti_consumer LtiConsumer get_or_supplement params get 'tool_consumer_instance_guid' None params['oauth_consumer_key'] except LtiConsumer DoesNotExist return HttpResponseForbidden if not SignatureValidator lti_consumer verify request return HttpResponseForbidden try course_key usage_key parse_course_and_usage_keys course_id usage_id except InvalidKeyError log error 'Invalidcoursekey%sorusagekey%sfromrequest%s' course_id usage_id request raise Http404 params['course_key'] course_keyparams['usage_key'] usage_keyauthenticate_lti_user request params['user_id'] lti_consumer store_outcome_parameters params request user lti_consumer return render_courseware request params['usage_key']
def html_to_xhtml html try html html getroot except AttributeError passprefix '{%s}' % XHTML_NAMESPACE for el in html iter etree Element tag el tagif tag[0] '{' el tag prefix + tag
def html_to_xhtml html try html html getroot except AttributeError passprefix '{%s}' % XHTML_NAMESPACE for el in html iter etree Element tag el tagif tag[0] '{' el tag prefix + tag
def html_to_xhtml html try html html getroot except AttributeError passprefix '{%s}' % XHTML_NAMESPACE for el in html iter etree Element tag el tagif tag[0] '{' el tag prefix + tag
def absent name **kwargs ret {'name' name 'changes' {} 'result' False 'comment' ''}current_state __salt__['sysrc get'] name name **kwargs if current_state is None ret['result'] Trueret['comment'] '"{0}"isalreadyabsent ' format name return retif __opts__['test'] is True ret['comment'] '"{0}"willberemoved ' format name ret['changes'] {'old' current_state 'new' '"{0}"willberemoved ' format name }ret['result'] Nonereturn retnew_state __salt__['sysrc remove'] name name **kwargs ret['comment'] '"{0}"wasremoved ' format name ret['changes'] {'old' current_state 'new' new_state}ret['result'] Truereturn ret
def _log_runtime parameter proc_location start_time runtime time time - start_time log debug 'proccall %s %s runtime %0 4f ' % parameter proc_location runtime
def _httpResponseToMessage response server_url response_message Message fromKVForm response body if response status 400 raise ServerError fromMessage response_message elif response status not in 200 206 fmt 'badstatuscodefromserver%s %s'error_message fmt % server_url response status raise fetchers HTTPFetchingError error_message return response_message
def get_required_parameters dictionary additional_params None params {}additional_params additional_params or [] for key in REQUIRED_PARAMETERS + additional_params if key not in dictionary return Noneparams[key] dictionary[key]return params
def get_required_parameters dictionary additional_params None params {}additional_params additional_params or [] for key in REQUIRED_PARAMETERS + additional_params if key not in dictionary return Noneparams[key] dictionary[key]return params
def assert_trade_protocol event assert_datasource_protocol event assert event type DATASOURCE_TYPE TRADE assert isinstance event price numbers Real assert isinstance event volume numbers Integral assert isinstance event dt datetime
def check_csv option opt value if isinstance value list tuple return valuetry return splitstrip value except ValueError raise OptionValueError 'option%s invalidcsvvalue %r' % opt value
def OAuthAuthorizeTokenCGI method parameters outfile oauth_callback GetFirst parameters _OAUTH_CALLBACK_PARAM '' if method 'GET' outfile write 'Status 200\r\n' outfile write 'Content-Type text/html\r\n' outfile write '\r\n' outfile write RenderTokenApprovalTemplate oauth_callback elif method 'POST' if oauth_callback outfile write 'Status 302RedirectingtocallbackURL\r\n' outfile write 'Location %s\r\n' % oauth_callback outfile write '\r\n' else outfile write 'Status 200\r\n' outfile write 'Content-Type text/html\r\n' outfile write '\r\n' outfile write RenderTokenApprovedTemplate else outfile write 'Status 400Unsupportedmethod\r\n'
@pytest fixturedef objects empty_history stream _data user_data tabhistory serialize ITEMS qtutils deserialize_stream stream empty_history return Objects history empty_history user_data user_data
def quotify qtype word terminate if qtype qq return qq + word replace qq '\\"' + terminate and qq or '' elif qtype q return q + word replace q "\\'" + terminate and q or '' else return re sub ' [\\"\\\'\\t\\n\\r] ' '\\\\\\1' word
@cacheitdef _simplify_delta expr from sympy solvers import solveif isinstance expr KroneckerDelta try slns solve expr args[0] - expr args[1] dict True if slns and len slns 1 return Mul *[KroneckerDelta * key value for key value in slns[0] items ] except NotImplementedError passreturn expr
@cacheitdef _simplify_delta expr from sympy solvers import solveif isinstance expr KroneckerDelta try slns solve expr args[0] - expr args[1] dict True if slns and len slns 1 return Mul *[KroneckerDelta * key value for key value in slns[0] items ] except NotImplementedError passreturn expr
def getProfileCopyright profile try if not isinstance profile ImageCmsProfile profile ImageCmsProfile profile return profile profile product_copyright + '\n' except AttributeError IOError TypeError ValueError as v raise PyCMSError v
def xl_col_to_name col_num col_abs False col_num + 1col_str ''col_abs '$' if col_abs else '' while col_num remainder col_num % 26 if remainder 0 remainder 26col_letter chr ord 'A' + remainder - 1 col_str col_letter + col_str col_num int col_num - 1 / 26 return col_abs + col_str
def init retVal Falseif haveBitsDLL try retVal _bits bitsInit except Exception logging error 'bits init barfed ' return retVal
def init retVal Falseif haveBitsDLL try retVal _bits bitsInit except Exception logging error 'bits init barfed ' return retVal
def init retVal Falseif haveBitsDLL try retVal _bits bitsInit except Exception logging error 'bits init barfed ' return retVal
def init retVal Falseif haveBitsDLL try retVal _bits bitsInit except Exception logging error 'bits init barfed ' return retVal
def init retVal Falseif haveBitsDLL try retVal _bits bitsInit except Exception logging error 'bits init barfed ' return retVal
def init retVal Falseif haveBitsDLL try retVal _bits bitsInit except Exception logging error 'bits init barfed ' return retVal
def getAreaVector3LoopAbsolute loop return getAreaLoopAbsolute getComplexPath loop
def set_access_token access_token global _access_token_access_token access_token
def _hide_frame ax ax get_yticks ax xaxis set_ticks [] ax yaxis set_ticks [] ax set_frame_on False
@contextlib contextmanagerdef override_config name value old_value getattr config name setattr config name value try yield finally setattr config name old_value
@pytest fixturedef _pytest request return PytestArg request
@pytest fixturedef _pytest request return PytestArg request
@pytest fixturedef _pytest request return PytestArg request
@pytest fixturedef _pytest request return PytestArg request
@pytest fixturedef _pytest request return PytestArg request
def setup_platform hass config add_devices discovery_info None if discovery_info is None returngateways hass data get mysensors MYSENSORS_GATEWAYS if not gateways returnfor gateway in gateways pres gateway const Presentationset_req gateway const SetReqmap_sv_types {pres S_COVER [set_req V_DIMMER set_req V_LIGHT]}if float gateway protocol_version > 1 5 map_sv_types update {pres S_COVER [set_req V_PERCENTAGE set_req V_STATUS]} devices {}gateway platform_callbacks append mysensors pf_callback_factory map_sv_types devices MySensorsCover add_devices
def get_email_preferences user_id email_preferences_model user_models UserEmailPreferencesModel get user_id strict False if email_preferences_model is None return user_domain UserGlobalPrefs create_default_prefs else return user_domain UserGlobalPrefs email_preferences_model site_updates email_preferences_model editor_role_notifications email_preferences_model feedback_message_notifications email_preferences_model subscription_notifications
def find_audio_period aclip t_min 0 1 t_max 2 t_res 0 01 chunksize int t_res * aclip fps chunk_duration 1 0 * chunksize / aclip fps v np array [ c ** 2 sum for c in aclip iter_chunks chunksize ] v v - v mean corrs np correlate v v mode 'full' [ - len v ]corrs[ int t_min / chunk_duration ] 0corrs[int t_max / chunk_duration ] 0return chunk_duration * np argmax corrs
@mock patch 'ckanext datastore db _get_fields' def test_upsert_with_insert_method_and_invalid_data mock_get_fields_function mock_connection mock Mock mock_connection execute side_effect sqlalchemy exc DataError 'statement' 'params' 'orig' connection_invalidated False context {'connection' mock_connection}data_dict {'fields' [{'id' 'value' 'type' 'numeric'}] 'records' [{'value' 0} {'value' 1} {'value' 2} {'value' 3} {'value' ''} {'value' 5} {'value' 6} {'value' 7}] 'method' 'insert' 'resource_id' 'fake-resource-id'}mock_get_fields_function return_value data_dict['fields']nose tools assert_raises db InvalidDataError db upsert_data context data_dict
@mock patch 'ckanext datastore db _get_fields' def test_upsert_with_insert_method_and_invalid_data mock_get_fields_function mock_connection mock Mock mock_connection execute side_effect sqlalchemy exc DataError 'statement' 'params' 'orig' connection_invalidated False context {'connection' mock_connection}data_dict {'fields' [{'id' 'value' 'type' 'numeric'}] 'records' [{'value' 0} {'value' 1} {'value' 2} {'value' 3} {'value' ''} {'value' 5} {'value' 6} {'value' 7}] 'method' 'insert' 'resource_id' 'fake-resource-id'}mock_get_fields_function return_value data_dict['fields']nose tools assert_raises db InvalidDataError db upsert_data context data_dict
@pytest fixturedef nobody from django contrib auth import get_user_modelreturn get_user_model objects get_nobody_user
def virtual_interface_list provider names **kwargs client _get_client return client extra_action provider provider names names action 'virtual_interface_list' **kwargs
def virtual_interface_list provider names **kwargs client _get_client return client extra_action provider provider names names action 'virtual_interface_list' **kwargs
def abstractMethod raise NotImplementedError 'Methodnotimplemented '
@expose '/about' def about request return render_template 'about html'
def inputbox text x y w h from plasma lib ui inlineed import InlineEded InlineEd 0 0 0 text 0 [] ed screen curses newwin h w y x ret ed start_view ed screen if not ret return ''return ed text
def find_failing_tests result_images source entries []for root dirs files in os walk result_images for fname in files basename ext os path splitext fname if basename endswith u'-failed-diff' path os path join root fname entry Entry path result_images source entries append entry entries sort key lambda x x name return entries
def find_failing_tests result_images source entries []for root dirs files in os walk result_images for fname in files basename ext os path splitext fname if basename endswith u'-failed-diff' path os path join root fname entry Entry path result_images source entries append entry entries sort key lambda x x name return entries
def parse_boolean value if value in [u'true' u'1'] return Trueelif value in [u'false' u'0'] return Falseelse raise ValueError u"expected'true'or'false' got'%s'" % value
@pytest mark skipif 'notHAS_YAML' def test_write_simple t simple_table out StringIO t write out format 'ascii ecsv' assert out getvalue splitlines SIMPLE_LINES
def create_metadata name ext_version None schema None user None host None port None maintenance_db None password None runas None installed_ext get_installed_extension name user user host host port port maintenance_db maintenance_db password password runas runas ret [_EXTENSION_NOT_INSTALLED]if installed_ext ret [_EXTENSION_INSTALLED]if ext_version is not None and _pg_is_older_ext_ver installed_ext get 'extversion' ext_version ext_version ret append _EXTENSION_TO_UPGRADE if schema is not None and installed_ext get 'extrelocatable' 'f' 't' and installed_ext get 'schema_name' schema schema ret append _EXTENSION_TO_MOVE return ret
def get_featured_activity_summary_dicts language_codes activity_references activity_services get_featured_activity_references exploration_ids collection_ids activity_services split_by_type activity_references exp_summary_dicts get_displayable_exp_summary_dicts_matching_ids exploration_ids col_summary_dicts get_displayable_collection_summary_dicts_matching_ids collection_ids summary_dicts_by_id {feconf ACTIVITY_TYPE_EXPLORATION {summary_dict['id'] summary_dict for summary_dict in exp_summary_dicts} feconf ACTIVITY_TYPE_COLLECTION {summary_dict['id'] summary_dict for summary_dict in col_summary_dicts}}featured_summary_dicts []for reference in activity_references if reference id in summary_dicts_by_id[reference type] summary_dict summary_dicts_by_id[reference type][reference id]if summary_dict and summary_dict['language_code'] in language_codes featured_summary_dicts append summary_dict return featured_summary_dicts
def test_thread_delete db gmail_account generic_thread add_fake_thread db session gmail_account namespace id generic_message add_fake_message db session gmail_account namespace id generic_thread assert db session query Thread filter Thread id generic_thread id all [generic_thread] assert db session query Message filter Message id generic_message id all [generic_message] db session delete generic_thread db session commit assert db session query Thread filter Thread id generic_thread id all [] assert db session query Message filter Message id generic_message id all []
def test_thread_delete db gmail_account generic_thread add_fake_thread db session gmail_account namespace id generic_message add_fake_message db session gmail_account namespace id generic_thread assert db session query Thread filter Thread id generic_thread id all [generic_thread] assert db session query Message filter Message id generic_message id all [generic_message] db session delete generic_thread db session commit assert db session query Thread filter Thread id generic_thread id all [] assert db session query Message filter Message id generic_message id all []
def proportions_chisquare_pairscontrol count nobs value None multitest_method 'hs' alternative 'two-sided' if value is not None or not alternative in ['two-sided' '2s'] raise NotImplementedErrorall_pairs [ 0 k for k in range 1 len count ]pvals [proportions_chisquare count[list pair ] nobs[list pair ] [1] for pair in all_pairs]return AllPairsResults pvals all_pairs multitest_method multitest_method
def proportions_chisquare_pairscontrol count nobs value None multitest_method 'hs' alternative 'two-sided' if value is not None or not alternative in ['two-sided' '2s'] raise NotImplementedErrorall_pairs [ 0 k for k in range 1 len count ]pvals [proportions_chisquare count[list pair ] nobs[list pair ] [1] for pair in all_pairs]return AllPairsResults pvals all_pairs multitest_method multitest_method
def main if len sys argv > 1 writeOutput '' join sys argv[1 ] else settings startMainLoopFromConstructor getNewRepository
def _iframe src width 650 height 365 content None link None html '<iframewidth "%s"height "%s"src "%s"frameborder "0"allowfullscreen></iframe>' % width height src if not content return htmlif link content '<ahref "%s">%s</a>' % link content return '<figure>%s<figcaption>%s</figcaption></figure>' % html content
def regions from boto cloudtrail layer1 import CloudTrailConnectionreturn get_regions 'cloudtrail' connection_cls CloudTrailConnection
def wait_for_file filename interval 10 test_mode False while not exists filename if test_mode raise RuntimeWarningsleep interval
def _get_creator_counts query count page creator_counts query facet_counts ['creator_id']['terms']total len creator_counts creator_counts creator_counts[ page - 1 * count page * count ]user_ids [x['term'] for x in creator_counts]results UserMappingType search filter id__in user_ids values_dict 'id' 'username' 'display_name' 'avatar' 'twitter_usernames' 'last_contribution_date' [ count]results UserMappingType reshape results user_lookup {}for r in results lcd r get 'last_contribution_date' None if lcd delta datetime now - lcd r['days_since_last_activity'] delta dayselse r['days_since_last_activity'] Noneuser_lookup[r['id']] rfor item in creator_counts item['user'] user_lookup get item['term'] None return [item for item in creator_counts if item['user'] is not None ] total
@slow_test@testing requires_testing_datadef test_make_forward_solution fwd_py make_forward_solution fname_raw fname_trans fname_src fname_bem mindist 5 0 eeg True meg True assert_true isinstance fwd_py Forward fwd read_forward_solution fname_meeg assert_true isinstance fwd Forward _compare_forwards fwd fwd_py 366 1494 meg_rtol 0 001
@task@use_masterdef update_supported_locales_single id latest False **kw from mkt webapps models import Webapptry app Webapp objects get pk id except Webapp DoesNotExist log info u'[Webapp %s]Didnotfindwebapptoupdatesupportedlocales ' % id returntry if app update_supported_locales latest latest log info u'[Webapp %s]Updatedsupportedlocales ' % app id except Exception log info u'[Webapp%s]Updatingsupportedlocalesfailed ' % app id exc_info True
def send_login_instructions user token generate_login_token user login_link url_for_security 'token_login' token token _external True send_mail config_value 'EMAIL_SUBJECT_PASSWORDLESS' user email 'login_instructions' user user login_link login_link login_instructions_sent send app _get_current_object user user login_token token
def CAN_ASSIGN_OWNER article user return _is_staff_for_article article user
def CAN_ASSIGN_OWNER article user return _is_staff_for_article article user
def main cisco_file 'cisco_ipsec txt'cisco_cfg CiscoConfParse cisco_file crypto_maps cisco_cfg find_objects_wo_child parentspec 'cryptomapCRYPTO' childspec 'AES' print '\nCryptomapsnotusingAES 'for entry in crypto_maps for child in entry children if 'transform' in child text match re search 'settransform-set * $' child text encryption match group 1 print '{0}>>>{1}' format entry text strip encryption print
def main cisco_file 'cisco_ipsec txt'cisco_cfg CiscoConfParse cisco_file crypto_maps cisco_cfg find_objects_wo_child parentspec 'cryptomapCRYPTO' childspec 'AES' print '\nCryptomapsnotusingAES 'for entry in crypto_maps for child in entry children if 'transform' in child text match re search 'settransform-set * $' child text encryption match group 1 print '{0}>>>{1}' format entry text strip encryption print
def main cisco_file 'cisco_ipsec txt'cisco_cfg CiscoConfParse cisco_file crypto_maps cisco_cfg find_objects_wo_child parentspec 'cryptomapCRYPTO' childspec 'AES' print '\nCryptomapsnotusingAES 'for entry in crypto_maps for child in entry children if 'transform' in child text match re search 'settransform-set * $' child text encryption match group 1 print '{0}>>>{1}' format entry text strip encryption print
def main cisco_file 'cisco_ipsec txt'cisco_cfg CiscoConfParse cisco_file crypto_maps cisco_cfg find_objects_wo_child parentspec 'cryptomapCRYPTO' childspec 'AES' print '\nCryptomapsnotusingAES 'for entry in crypto_maps for child in entry children if 'transform' in child text match re search 'settransform-set * $' child text encryption match group 1 print '{0}>>>{1}' format entry text strip encryption print
def main cisco_file 'cisco_ipsec txt'cisco_cfg CiscoConfParse cisco_file crypto_maps cisco_cfg find_objects_wo_child parentspec 'cryptomapCRYPTO' childspec 'AES' print '\nCryptomapsnotusingAES 'for entry in crypto_maps for child in entry children if 'transform' in child text match re search 'settransform-set * $' child text encryption match group 1 print '{0}>>>{1}' format entry text strip encryption print
def main cisco_file 'cisco_ipsec txt'cisco_cfg CiscoConfParse cisco_file crypto_maps cisco_cfg find_objects_wo_child parentspec 'cryptomapCRYPTO' childspec 'AES' print '\nCryptomapsnotusingAES 'for entry in crypto_maps for child in entry children if 'transform' in child text match re search 'settransform-set * $' child text encryption match group 1 print '{0}>>>{1}' format entry text strip encryption print
def desktop_lockdown name user None disable_application_handlers None disable_command_line None disable_lock_screen None disable_log_out None disable_print_setup None disable_printing None disable_save_to_disk None disable_user_switching None user_administration_disabled None **kwargs gnome_kwargs {'user' user 'schema' 'org gnome desktop lockdown'}preferences ['disable_application_handlers' 'disable_command_line' 'disable_lock_screen' 'disable_log_out' 'disable_print_setup' 'disable_printing' 'disable_save_to_disk' 'disable_user_switching' 'user_administration_disabled']preferences_hash {}for pref in preferences if pref in locals and locals [pref] is not None key re sub '_' '-' pref preferences_hash[key] locals [pref]return _do name gnome_kwargs preferences_hash
def closest_feasible individual feasible_ind numpy array individual feasible_ind numpy maximum MIN_BOUND feasible_ind feasible_ind numpy minimum MAX_BOUND feasible_ind return feasible_ind
def hex2str hexnum intsize 4 if not isinstance hexnum six string_types nbits intsize * 8 hexnum '0x%x' % hexnum + 1 << nbits % 1 << nbits s hexnum[2 ]if len s % 2 0 s '0' + s result codecs decode s 'hex' [ -1 ]return result
def test_unpickle_gpuarray_as_numpy_ndarray_flag0 oldflag config experimental unpickle_gpu_on_cpuconfig experimental unpickle_gpu_on_cpu Falsetry testfile_dir os path dirname os path realpath __file__ fname 'GpuArray pkl'with open os path join testfile_dir fname 'rb' as fp if PY3 u CompatUnpickler fp encoding 'latin1' else u CompatUnpickler fp mat u load assert isinstance mat pygpu gpuarray GpuArray assert numpy asarray mat [0] -42 0 finally config experimental unpickle_gpu_on_cpu oldflag
def calc_angle v1 v2 v3 v1 v1 - v2 v3 v3 - v2 return v1 angle v3
def calc_angle v1 v2 v3 v1 v1 - v2 v3 v3 - v2 return v1 angle v3
def calc_angle v1 v2 v3 v1 v1 - v2 v3 v3 - v2 return v1 angle v3
def getmodebandnames mode return ImageMode getmode mode bands
def scan_video path if not os path exists path raise ValueError 'Pathdoesnotexist' if not path endswith VIDEO_EXTENSIONS raise ValueError '%risnotavalidvideoextension' % os path splitext path [1] dirpath filename os path split path logger info 'Scanningvideo%rin%r' filename dirpath video Video fromguess path guessit path video size os path getsize path if video size > 10485760 logger debug 'Sizeis%d' video size video hashes['opensubtitles'] hash_opensubtitles path video hashes['shooter'] hash_shooter path video hashes['thesubdb'] hash_thesubdb path video hashes['napiprojekt'] hash_napiprojekt path logger debug 'Computedhashes%r' video hashes else logger warning 'Sizeislowerthan10MB hashesnotcomputed' return video
def set_weights net model_file with open model_file as f print 'Loadpretrainedweightsfrom%s ' % model_file model pickle load f print 'Settheweights 'lasagne layers set_all_param_values net model trainable True
def set_weights net model_file with open model_file as f print 'Loadpretrainedweightsfrom%s ' % model_file model pickle load f print 'Settheweights 'lasagne layers set_all_param_values net model trainable True
def find_media_source url while url[0] '/' url url[1 ]d os path dirnamen os path normpathj os path joinf os path isfileskins n j d d __file__ 'skins' try media os path join skins settings OSQA_DEFAULT_SKIN url assert f media use_skin settings OSQA_DEFAULT_SKINexcept try media j skins 'default' url assert f media use_skin 'default'except media j skins 'common' url try assert f media use_skin 'common'except logging error 'couldnotfindmediafor%s' % url use_skin ''return Nonereturn use_skin + '/' + url
def find_media_source url while url[0] '/' url url[1 ]d os path dirnamen os path normpathj os path joinf os path isfileskins n j d d __file__ 'skins' try media os path join skins settings OSQA_DEFAULT_SKIN url assert f media use_skin settings OSQA_DEFAULT_SKINexcept try media j skins 'default' url assert f media use_skin 'default'except media j skins 'common' url try assert f media use_skin 'common'except logging error 'couldnotfindmediafor%s' % url use_skin ''return Nonereturn use_skin + '/' + url
def find_media_source url while url[0] '/' url url[1 ]d os path dirnamen os path normpathj os path joinf os path isfileskins n j d d __file__ 'skins' try media os path join skins settings OSQA_DEFAULT_SKIN url assert f media use_skin settings OSQA_DEFAULT_SKINexcept try media j skins 'default' url assert f media use_skin 'default'except media j skins 'common' url try assert f media use_skin 'common'except logging error 'couldnotfindmediafor%s' % url use_skin ''return Nonereturn use_skin + '/' + url
def get_signals signums return dict s signal getsignal s for s in signums
def purge_deleted db_api purge_deleted CONF command age CONF command granularity CONF command project_id CONF command batch_size
def get_close_matches word possibilities n 3 cutoff 0 6 if not n > 0 raise ValueError 'nmustbe>0 %r' % n if not 0 0 < cutoff < 1 0 raise ValueError 'cutoffmustbein[0 0 1 0] %r' % cutoff result []s SequenceMatcher s set_seq2 word for x in possibilities s set_seq1 x if s real_quick_ratio > cutoff and s quick_ratio > cutoff and s ratio > cutoff result append s ratio x result heapq nlargest n result return [x for score x in result]
def get_close_matches word possibilities n 3 cutoff 0 6 if not n > 0 raise ValueError 'nmustbe>0 %r' % n if not 0 0 < cutoff < 1 0 raise ValueError 'cutoffmustbein[0 0 1 0] %r' % cutoff result []s SequenceMatcher s set_seq2 word for x in possibilities s set_seq1 x if s real_quick_ratio > cutoff and s quick_ratio > cutoff and s ratio > cutoff result append s ratio x result heapq nlargest n result return [x for score x in result]
def read_packet sock timeout None sock settimeout timeout dlen data None None try if os name 'nt' datalen sock recv SZ dlen struct unpack 'l' datalen data ''while len data < dlen data + sock recv dlen else datalen temp_fail_retry socket error sock recv SZ socket MSG_WAITALL if len datalen SZ dlen struct unpack 'l' datalen data temp_fail_retry socket error sock recv dlen socket MSG_WAITALL except socket timeout raiseexcept socket error data Nonefinally sock settimeout None if data is not None try return pickle loads data except Exception if DEBUG_EDITOR traceback print_exc file STDERR return
def simple_load_icon module index as_data False size ICON_SIZE try large_icons small_icons win32gui ExtractIconEx module index 10 except pywintypes error as err if err winerror winerror ERROR_FILE_NOT_FOUND raiseprints u'File%rdoesnotexist cannotloadicon' % module returnicons large_icons + small_icons try if icons must_use_qt pixmap copy_to_size QtWin fromHICON icons[0] size size if as_data return pixmap_to_data pixmap return QIcon pixmap finally tuple map win32gui DestroyIcon icons
def simple_load_icon module index as_data False size ICON_SIZE try large_icons small_icons win32gui ExtractIconEx module index 10 except pywintypes error as err if err winerror winerror ERROR_FILE_NOT_FOUND raiseprints u'File%rdoesnotexist cannotloadicon' % module returnicons large_icons + small_icons try if icons must_use_qt pixmap copy_to_size QtWin fromHICON icons[0] size size if as_data return pixmap_to_data pixmap return QIcon pixmap finally tuple map win32gui DestroyIcon icons
def multi_constructor_pkl loader tag_suffix node global additional_environif tag_suffix '' and tag_suffix u'' raise AssertionError 'Expectedtag_suffixtobe""butitis"' + tag_suffix + '" Putspacebetween pkl andthefilename ' mapping loader construct_yaml_str node obj serial load preprocess mapping additional_environ proxy Proxy callable do_not_recurse positionals keywords {'value' obj} yaml_src yaml serialize node return proxy
def subset_dict dict_ keys subset partition_dict dict_ keys [0]return subset
def notification_sample sample def wrap cls if not getattr cls 'samples' None cls samples [sample]else cls samples append sample return clsreturn wrap
def notification_sample sample def wrap cls if not getattr cls 'samples' None cls samples [sample]else cls samples append sample return clsreturn wrap
def pr_get_ancestors pe_id s3db current s3dbatable s3db pr_affiliationrtable s3db pr_rolequery atable deleted True & atable role_id rtable id & atable pe_id pe_id & rtable deleted True & rtable role_type OU roles current db query select rtable id rtable pe_id rtable path rtable role_type paths []append paths appendfor role in roles path S3MultiPath [role pe_id] if role path is None ppath pr_role_rebuild_path role else ppath S3MultiPath role path path extend role pe_id ppath cut pe_id append path ancestors S3MultiPath all_nodes paths return ancestors
def _find_matching_rule module secgroup remotegroup protocol module params['protocol']remote_ip_prefix module params['remote_ip_prefix']ethertype module params['ethertype']direction module params['direction']remote_group_id remotegroup['id']for rule in secgroup['security_group_rules'] if protocol rule['protocol'] and remote_ip_prefix rule['remote_ip_prefix'] and ethertype rule['ethertype'] and direction rule['direction'] and remote_group_id rule['remote_group_id'] and _ports_match protocol module params['port_range_min'] module params['port_range_max'] rule['port_range_min'] rule['port_range_max'] return rulereturn None
def test_composite_unit_get_format_name unit1 u Unit u'nrad/s' unit2 u Unit u'Hz 1/2 ' assert str u CompositeUnit 1 [unit1 unit2] [1 -1 ] u'nrad/ Hz 1/2 s '
def error_for response klass error_classes get response status_code if klass is None if 400 < response status_code < 500 klass ClientErrorif 500 < response status_code < 600 klass ServerErrorreturn klass response
def delete overlay ret list old_overlays list_local cmd 'layman--quietness 0--delete{0}' format overlay delete_attempt __salt__['cmd run_all'] cmd python_shell False if delete_attempt['retcode'] 0 raise salt exceptions CommandExecutionError delete_attempt['stdout'] new_overlays list_local if len new_overlays 0 srcline 'source/var/lib/layman/make conf'makeconf _get_makeconf if __salt__['file contains'] makeconf 'layman' __salt__['file sed'] makeconf srcline '' ret [overlay for overlay in old_overlays if overlay not in new_overlays ]return ret
def place_order creator **kwargs if 'shipping_method' not in kwargs kwargs['shipping_method'] Free shipping_charge kwargs['shipping_method'] calculate kwargs['basket'] kwargs['total'] calculators OrderTotalCalculator calculate basket kwargs['basket'] shipping_charge shipping_charge kwargs['shipping_charge'] shipping_chargereturn creator place_order **kwargs
def yaml_dump data stream None dumper yaml Dumper **kwargs class Dumper dumper u'Customdumper 'if not PY3 Dumper add_representer unicode lambda dumper value dumper represent_scalar u'tag yaml org 2002 str' value return yaml dump data stream Dumper **kwargs
def yaml_dump data stream None dumper yaml Dumper **kwargs class Dumper dumper u'Customdumper 'if not PY3 Dumper add_representer unicode lambda dumper value dumper represent_scalar u'tag yaml org 2002 str' value return yaml dump data stream Dumper **kwargs
def _build_exp_freq_mat exp_freq_table exp_freq_mat ExpectedFrequencyMatrix alphabet exp_freq_table alphabet build_later 1 for i in exp_freq_mat if i[0] i[1] exp_freq_mat[i] exp_freq_table[i[0]] ** 2 else exp_freq_mat[i] 2 0 * exp_freq_table[i[0]] * exp_freq_table[i[1]] return exp_freq_mat
def generate_signed_url credentials resource expiration api_access_endpoint '' method 'GET' content_md5 None content_type None response_type None response_disposition None generation None expiration _get_expiration_seconds expiration string_to_sign '\n' join [method content_md5 or '' content_type or '' str expiration resource] query_params _get_signed_query_params credentials expiration string_to_sign if response_type is not None query_params['response-content-type'] response_typeif response_disposition is not None query_params['response-content-disposition'] response_dispositionif generation is not None query_params['generation'] generationreturn '{endpoint}{resource}?{querystring}' format endpoint api_access_endpoint resource resource querystring urlencode query_params
def generate_signed_url credentials resource expiration api_access_endpoint '' method 'GET' content_md5 None content_type None response_type None response_disposition None generation None expiration _get_expiration_seconds expiration string_to_sign '\n' join [method content_md5 or '' content_type or '' str expiration resource] query_params _get_signed_query_params credentials expiration string_to_sign if response_type is not None query_params['response-content-type'] response_typeif response_disposition is not None query_params['response-content-disposition'] response_dispositionif generation is not None query_params['generation'] generationreturn '{endpoint}{resource}?{querystring}' format endpoint api_access_endpoint resource resource querystring urlencode query_params
def _to_serializables obj if isinstance obj PRecord result dict obj result[_CLASS_MARKER] obj __class__ __name__return resultelif isinstance obj PClass result obj _to_dict result[_CLASS_MARKER] obj __class__ __name__return resultelif isinstance obj PMap return {_CLASS_MARKER u'PMap' u'values' dict obj items }elif isinstance obj PSet PVector set return list obj elif isinstance obj FilePath return {_CLASS_MARKER u'FilePath' u'path' obj path decode 'utf-8' }elif isinstance obj UUID return {_CLASS_MARKER u'UUID' 'hex' unicode obj }elif isinstance obj datetime if obj tzinfo is None raise ValueError 'Datetimewithoutatimezone {}' format obj return {_CLASS_MARKER u'datetime' 'seconds' timegm obj utctimetuple }return obj
def sha512_digest instr if six PY3 b salt utils to_bytes instr return hashlib sha512 b hexdigest return hashlib sha512 instr hexdigest
def _decode_pa_dict d retval cdict for k v in d items if isinstance k tuple for subk in k retval[subk] vfor k v in d items if not isinstance k tuple retval[k] vreturn retval
def get_hostname with settings hide 'running' 'stdout' return run 'hostname--fqdn'
def get_processtree_pids pid include_parent True parents get_all_processes_pids all_pids parents keys pids set [pid] while True pids_new pids copy for _pid in all_pids if parents[_pid] in pids pids_new add _pid if pids_new pids breakpids pids_new copy if not include_parent pids remove pid return list pids
def _TestGenerateAccessToken action tester device_dict auth_info_dict user_cookie None use_short_token True response_dict _GenerateAccessToken action tester device_dict auth_info_dict user_cookie use_short_token use_short_token identity_type value Identity SplitKey auth_info_dict['identity'] expected_digits 4 if use_short_token or identity_type 'Phone' else 9 assert response_dict['token_digits'] expected_digits response_dictidentity tester _RunAsync Identity Query tester validator client auth_info_dict['identity'] None tester validator ValidateUpdateDBObject Identity key auth_info_dict['identity'] authority 'Viewfinder' user_id identity user_id access_token identity access_token expires identity expires return identity
def _TestGenerateAccessToken action tester device_dict auth_info_dict user_cookie None use_short_token True response_dict _GenerateAccessToken action tester device_dict auth_info_dict user_cookie use_short_token use_short_token identity_type value Identity SplitKey auth_info_dict['identity'] expected_digits 4 if use_short_token or identity_type 'Phone' else 9 assert response_dict['token_digits'] expected_digits response_dictidentity tester _RunAsync Identity Query tester validator client auth_info_dict['identity'] None tester validator ValidateUpdateDBObject Identity key auth_info_dict['identity'] authority 'Viewfinder' user_id identity user_id access_token identity access_token expires identity expires return identity
def GetXcodeArchsDefault global XCODE_ARCHS_DEFAULT_CACHEif XCODE_ARCHS_DEFAULT_CACHE return XCODE_ARCHS_DEFAULT_CACHE xcode_version _ XcodeVersion if xcode_version < '0500' XCODE_ARCHS_DEFAULT_CACHE XcodeArchsDefault '$ ARCHS_STANDARD ' XcodeArchsVariableMapping ['i386'] XcodeArchsVariableMapping ['i386'] XcodeArchsVariableMapping ['armv7'] elif xcode_version < '0510' XCODE_ARCHS_DEFAULT_CACHE XcodeArchsDefault '$ ARCHS_STANDARD_INCLUDING_64_BIT ' XcodeArchsVariableMapping ['x86_64'] ['x86_64'] XcodeArchsVariableMapping ['i386'] ['i386' 'x86_64'] XcodeArchsVariableMapping ['armv7' 'armv7s'] ['armv7' 'armv7s' 'arm64'] else XCODE_ARCHS_DEFAULT_CACHE XcodeArchsDefault '$ ARCHS_STANDARD ' XcodeArchsVariableMapping ['x86_64'] ['x86_64'] XcodeArchsVariableMapping ['i386' 'x86_64'] ['i386' 'x86_64'] XcodeArchsVariableMapping ['armv7' 'armv7s' 'arm64'] ['armv7' 'armv7s' 'arm64'] return XCODE_ARCHS_DEFAULT_CACHE
def GetXcodeArchsDefault global XCODE_ARCHS_DEFAULT_CACHEif XCODE_ARCHS_DEFAULT_CACHE return XCODE_ARCHS_DEFAULT_CACHE xcode_version _ XcodeVersion if xcode_version < '0500' XCODE_ARCHS_DEFAULT_CACHE XcodeArchsDefault '$ ARCHS_STANDARD ' XcodeArchsVariableMapping ['i386'] XcodeArchsVariableMapping ['i386'] XcodeArchsVariableMapping ['armv7'] elif xcode_version < '0510' XCODE_ARCHS_DEFAULT_CACHE XcodeArchsDefault '$ ARCHS_STANDARD_INCLUDING_64_BIT ' XcodeArchsVariableMapping ['x86_64'] ['x86_64'] XcodeArchsVariableMapping ['i386'] ['i386' 'x86_64'] XcodeArchsVariableMapping ['armv7' 'armv7s'] ['armv7' 'armv7s' 'arm64'] else XCODE_ARCHS_DEFAULT_CACHE XcodeArchsDefault '$ ARCHS_STANDARD ' XcodeArchsVariableMapping ['x86_64'] ['x86_64'] XcodeArchsVariableMapping ['i386' 'x86_64'] ['i386' 'x86_64'] XcodeArchsVariableMapping ['armv7' 'armv7s' 'arm64'] ['armv7' 'armv7s' 'arm64'] return XCODE_ARCHS_DEFAULT_CACHE
@dodef update_repo package_directory target_bucket target_key source_repo packages flocker_version distribution package_directory createDirectory package_type distribution package_type yield Effect DownloadS3KeyRecursively source_bucket target_bucket source_prefix target_key target_path package_directory filter_extensions ' ' + package_type value downloaded_packages yield Effect DownloadPackagesFromRepository source_repo source_repo target_path package_directory packages packages flocker_version flocker_version distribution distribution new_metadata yield Effect CreateRepo repository_path package_directory distribution distribution yield Effect UploadToS3Recursively source_path package_directory target_bucket target_bucket target_key target_key files downloaded_packages new_metadata
@dodef update_repo package_directory target_bucket target_key source_repo packages flocker_version distribution package_directory createDirectory package_type distribution package_type yield Effect DownloadS3KeyRecursively source_bucket target_bucket source_prefix target_key target_path package_directory filter_extensions ' ' + package_type value downloaded_packages yield Effect DownloadPackagesFromRepository source_repo source_repo target_path package_directory packages packages flocker_version flocker_version distribution distribution new_metadata yield Effect CreateRepo repository_path package_directory distribution distribution yield Effect UploadToS3Recursively source_path package_directory target_bucket target_bucket target_key target_key files downloaded_packages new_metadata
def test_install_package_with_prefix script data prefix_path script scratch_path / 'prefix' result script pip 'install' '--prefix' prefix_path '-f' data find_links '--no-binary' 'simple' '--no-index' 'simple 1 0' if hasattr sys 'pypy_version_info' path script scratch / 'prefix' else path script scratch / 'prefix' / 'lib' / 'python{0}' format pyversion install_path path / 'site-packages' / 'simple-1 0-py{0} egg-info' format pyversion assert install_path in result files_created str result
@pytest fixture scope 'function' def clean_system request user_config_path os path expanduser '~/ cookiecutterrc' user_config_path_backup os path expanduser '~/ cookiecutterrc backup' if os path exists user_config_path user_config_found Trueshutil copy user_config_path user_config_path_backup os remove user_config_path else user_config_found Falsecookiecutters_dir os path expanduser '~/ cookiecutters' cookiecutters_dir_backup os path expanduser '~/ cookiecutters backup' cookiecutters_dir_found backup_dir cookiecutters_dir cookiecutters_dir_backup cookiecutter_replay_dir os path expanduser '~/ cookiecutter_replay' cookiecutter_replay_dir_backup os path expanduser '~/ cookiecutter_replay backup' cookiecutter_replay_dir_found backup_dir cookiecutter_replay_dir cookiecutter_replay_dir_backup def restore_backup if user_config_found and os path exists user_config_path_backup shutil copy user_config_path_backup user_config_path os remove user_config_path_backup restore_backup_dir cookiecutters_dir cookiecutters_dir_backup cookiecutters_dir_found restore_backup_dir cookiecutter_replay_dir cookiecutter_replay_dir_backup cookiecutter_replay_dir_found request addfinalizer restore_backup
def _s3_cleanup glob_path time_old dry_run False **runner_kwargs runner EMRJobRunner **runner_kwargs log info 'Deletingallfilesin%sthatareolderthan%s' % glob_path time_old for path in runner fs ls glob_path bucket_name key_name parse_s3_uri path bucket runner fs get_bucket bucket_name for key in bucket list key_name last_modified iso8601_to_datetime key last_modified age datetime utcnow - last_modified if age > time_old log info 'Deleting%s is%sold' % key name age if not dry_run key delete
def check_write_to_datafiles con warning critical perf_data warning warning or 20 critical critical or 40 try data get_server_status con writes data['dur']['writeToDataFilesMB']message 'Writetodatafiles % 2fMB' % writes message + performance_data perf_data [ '% 2f' % writes 'write_to_data_files' warning critical ] return check_levels writes warning critical message except Exception as e return exit_with_general_critical e
def _check_scale scale if np isscalar scale and scale < 0 raise ValueError 'scalemustbepositive not%s' % scale
def _assert_matching_drivers if CONF database slave_connection '' returnnormal sqlalchemy engine url make_url CONF database connection slave sqlalchemy engine url make_url CONF database slave_connection assert normal drivername slave drivername
def _assert_matching_drivers if CONF database slave_connection '' returnnormal sqlalchemy engine url make_url CONF database connection slave sqlalchemy engine url make_url CONF database slave_connection assert normal drivername slave drivername
def _assert_matching_drivers if CONF database slave_connection '' returnnormal sqlalchemy engine url make_url CONF database connection slave sqlalchemy engine url make_url CONF database slave_connection assert normal drivername slave drivername
def question title msg default True yes QtWidgets QMessageBox Yesno QtWidgets QMessageBox Nobuttons yes no if default default yeselse default noparent active_window MessageBox QtWidgets QMessageBoxresult MessageBox question parent title msg buttons default return result QtWidgets QMessageBox Yes
def loadProfile filename allMachines False global settingsListprofileParser ConfigParser ConfigParser try profileParser read filename except ConfigParser ParsingError returnif allMachines n 0while profileParser has_section 'profile_%d' % n for set in settingsList if set isPreference continuesection 'profile_%d' % n if set isAlteration section 'alterations_%d' % n if profileParser has_option section set getName set setValue unicode profileParser get section set getName 'utf-8' 'replace' n n + 1else for set in settingsList if set isPreference continuesection 'profile'if set isAlteration section 'alterations'if profileParser has_option section set getName set setValue unicode profileParser get section set getName 'utf-8' 'replace' if getProfileSetting 'retraction_combing' '1' putProfileSetting 'retraction_combing' 'All'
def disable name stop False **kwargs if not enabled name return Falsesvc_realpath _get_svc_path name [0]down_file os path join svc_realpath 'down' if stop stop name if not os path exists down_file try salt utils fopen down_file 'w' close except IOError log error 'Unabletocreatefile{0}' format down_file return Falsereturn True
def disable name stop False **kwargs if not enabled name return Falsesvc_realpath _get_svc_path name [0]down_file os path join svc_realpath 'down' if stop stop name if not os path exists down_file try salt utils fopen down_file 'w' close except IOError log error 'Unabletocreatefile{0}' format down_file return Falsereturn True
def disable name stop False **kwargs if not enabled name return Falsesvc_realpath _get_svc_path name [0]down_file os path join svc_realpath 'down' if stop stop name if not os path exists down_file try salt utils fopen down_file 'w' close except IOError log error 'Unabletocreatefile{0}' format down_file return Falsereturn True
def test_scalar_info c time Time '2000 001' cinfo c info out None assert cinfo['n_bad'] 0 assert 'length' not in cinfo
def _Same tool name setting_type _Renamed tool name name setting_type
def unregister func if hasattr atexit 'unregister' atexit unregister func else handler_entries [e for e in _exithandlers if e[0] func ]for e in handler_entries _exithandlers remove e
def unregister func if hasattr atexit 'unregister' atexit unregister func else handler_entries [e for e in _exithandlers if e[0] func ]for e in handler_entries _exithandlers remove e
def unregister func if hasattr atexit 'unregister' atexit unregister func else handler_entries [e for e in _exithandlers if e[0] func ]for e in handler_entries _exithandlers remove e
@commands u'getsafeforwork' u'getsfw' @example u' getsfw[channel]' def get_channel_sfw bot trigger channel trigger group 2 if not channel channel trigger senderif channel is_nick return bot say u' getsfwwithnochannelparamisonlypermittedinchannels' channel channel strip sfw bot db get_channel_value channel u'sfw' if sfw bot say u'%sisflaggedasSFW' % channel else bot say u'%sisflaggedasNSFW' % channel
def get_master_status **connection_args mod sys _getframe f_code co_namelog debug '{0}<--' format mod conn _connect **connection_args if conn is None return []rtnv __do_query_into_hash conn 'SHOWMASTERSTATUS' conn close if len rtnv 0 rtnv append [] log debug '{0}-->{1}' format mod len rtnv[0] return rtnv[0]
def SetPeSubsystem fd console True current_pos fd tell fd seek 60 header_offset struct unpack '<I' fd read 4 [0]subsystem_offset header_offset + 92 fd seek subsystem_offset if console fd write '\x03' else fd write '\x02' fd seek current_pos
def create_filter predicate_param predicate_factory modifier param _extract_modifier predicate_param predicates map predicate_factory param split u' ' def filt x return modifier any map lambda pred pred x predicates return filt
@register inclusion_tag 'filebrowser/include/_response html' takes_context True def query_string context add None remove None add string_to_dict add remove string_to_list remove params context['query'] copy response get_query_string params add remove return {'response' response}
def validate_domain value if not re search u'^[a-zA-Z0-9-\\ ]+$' value raise ValidationError u'"{}"containsunexpectedcharacters' format value
def get_var_endog y lags trend 'c' has_constant 'skip' nobs len y Z np array [y[ t - lags t][ -1 ] ravel for t in range lags nobs ] if trend 'nc' Z tsa add_trend Z prepend True trend trend has_constant has_constant return Z
@contextmanagerdef bake_in_temp_dir cookies *args **kwargs result cookies bake *args **kwargs try yield result finally rmtree str result project
@contextmanagerdef bake_in_temp_dir cookies *args **kwargs result cookies bake *args **kwargs try yield result finally rmtree str result project
def random_integers random_state size None low 0 high 1 ndim None dtype 'int64' low tensor as_tensor_variable low high tensor as_tensor_variable high ndim size bcast _infer_ndim_bcast ndim size low high op RandomFunction random_integers_helper tensor TensorType dtype dtype broadcastable bcast return op random_state size low high
def random_integers random_state size None low 0 high 1 ndim None dtype 'int64' low tensor as_tensor_variable low high tensor as_tensor_variable high ndim size bcast _infer_ndim_bcast ndim size low high op RandomFunction random_integers_helper tensor TensorType dtype dtype broadcastable bcast return op random_state size low high
def file_access_rights filename rights check_above False if os path exists filename return os access filename rights elif check_above return os access os path dirname os path abspath filename rights else return False
def get_default_ca_certs if not hasattr get_default_ca_certs '_path' for path in '/etc/pki/ca-trust/extracted/openssl/ca-bundle trust crt' '/etc/ssl/certs' '/etc/ssl/certificates' if os path exists path get_default_ca_certs _path pathbreakelse get_default_ca_certs _path Nonereturn get_default_ca_certs _path
def activate csr_file certificate_id web_server_type approver_email None http_dc_validation False **kwargs return __get_certificates 'namecheap ssl activate' 'SSLActivateResult' csr_file certificate_id web_server_type approver_email http_dc_validation kwargs
def activate csr_file certificate_id web_server_type approver_email None http_dc_validation False **kwargs return __get_certificates 'namecheap ssl activate' 'SSLActivateResult' csr_file certificate_id web_server_type approver_email http_dc_validation kwargs
def default_listener col_attr default @event listens_for col_attr 'init_scalar' retval True propagate True def init_scalar target value dict_ if default is_callable value default arg None elif default is_scalar value default argelse raise NotImplementedError "Can'tinvokepre-defaultforaSQL-levelcolumndefault" dict_[col_attr key] valuereturn value
def get_dataset name split_name dataset_dir file_pattern None reader None if name not in datasets_map raise ValueError 'Nameofdatasetunknown%s' % name return datasets_map[name] get_split split_name dataset_dir file_pattern reader
def _check_window_params data window_length if window_length < 1 raise WindowLengthNotPositive window_length window_length if window_length > data shape[0] raise WindowLengthTooLong nrows data shape[0] window_length window_length
def _check_window_params data window_length if window_length < 1 raise WindowLengthNotPositive window_length window_length if window_length > data shape[0] raise WindowLengthTooLong nrows data shape[0] window_length window_length
def test_multiple_subordinate_steps_are_run @step 'Iruntwosubordinatesteps' def two_subordinate_steps step step behave_as '\nWhenIrunthefirstsub-step\nAndIrunthesecondsub-step\n' global first_ranglobal second_ranfirst_ran Falsesecond_ran False@step 'Irunthefirstsub-step$' def increment step global first_ranfirst_ran True@step 'Irunthesecondsub-step' def increment_twice step global second_ransecond_ran Truerunnable_step Step from_string 'GivenIruntwosubordinatesteps' runnable_step run True assert_equals first_ran second_ran True True del first_randel second_ran
def first_diff a b i -1 for i in xrange 0 len a if a[i] b[1] return iif i 255 return i
def make_friedman1 n_samples 100 n_features 10 noise 0 0 random_state None if n_features < 5 raise ValueError 'n_featuresmustbeatleastfive ' generator check_random_state random_state X generator rand n_samples n_features y 10 * np sin np pi * X[ 0] * X[ 1] + 20 * X[ 2] - 0 5 ** 2 + 10 * X[ 3] + 5 * X[ 4] + noise * generator randn n_samples return X y
def find_dir p x 'x'while x and not os path exists p p x os path split p return p
def find_dir p x 'x'while x and not os path exists p p x os path split p return p
def _FindRuleTriggerFiles rule sources return rule get 'rule_sources' []
def apply_features feature_func toks labeled None if labeled is None labeled toks and isinstance toks[0] tuple list if labeled def lazy_func labeled_token return feature_func labeled_token[0] labeled_token[1] return LazyMap lazy_func toks else return LazyMap feature_func toks
def apply_features feature_func toks labeled None if labeled is None labeled toks and isinstance toks[0] tuple list if labeled def lazy_func labeled_token return feature_func labeled_token[0] labeled_token[1] return LazyMap lazy_func toks else return LazyMap feature_func toks
def remove feature remove_payload False restart False mgmt_tools ''if salt utils version_cmp __grains__['osversion'] '6 2' > 0 mgmt_tools '-IncludeManagementTools'rmv ''if remove_payload rmv '-Remove'rst ''if restart rst '-Restart'cmd 'Remove-WindowsFeature-Name{0}{1}{2}{3}-ErrorActionSilentlyContinue-WarningActionSilentlyContinue' format _cmd_quote feature mgmt_tools rmv rst out _pshell_json cmd if out['FeatureResult'] return {'ExitCode' out['ExitCode'] 'DisplayName' out['FeatureResult'][0]['DisplayName'] 'RestartNeeded' out['FeatureResult'][0]['RestartNeeded'] 'Success' out['Success']}else return {'ExitCode' out['ExitCode'] 'DisplayName' '{0} notinstalled ' format feature 'RestartNeeded' False 'Success' out['Success']}
def pwnstallerGenerateUtilsH methodSubs code '#include"launch h"\n'code + 'voidinit_launcher void \n'code + 'intget_thisfile char*%s constchar*%s \n' % helpers randomString helpers randomString code + 'intCreateActContext char*%s char*%s \n' % helpers randomString helpers randomString code + 'voidReleaseActContext void \n'code + 'intget_thisfilew LPWSTR%s \n' % helpers randomString code + 'voidget_homepath char*%s constchar*%s \n' % helpers randomString helpers randomString code + 'voidget_archivefile char*%s constchar*%s \n' % helpers randomString helpers randomString code + 'intset_environment constARCHIVE_STATUS*%s \n' % helpers randomString code + 'intspawn LPWSTR%s \n' % helpers randomString for m in methodSubs code code replace m[0] m[1] return code
def expand_ipaddress_pattern string family if family not in [4 6] raise Exception 'InvalidIPaddressfamily {}' format family if family 4 regex IP4_EXPANSION_PATTERNbase 10else regex IP6_EXPANSION_PATTERNbase 16 lead pattern remnant re split regex string maxsplit 1 x y pattern split '-' for i in range int x base int y base + 1 if re search regex remnant for string in expand_ipaddress_pattern remnant family yield '' join [lead format i 'x' if family 6 else 'd' string] else yield '' join [lead format i 'x' if family 6 else 'd' remnant]
def filter_by_latest_downloadable_changeset_revision_that_has_missing_tool_test_components trans repository repository_metadata get_latest_downloadable_repository_metadata_if_it_includes_tools trans repository if repository_metadata is not None and repository_metadata missing_test_components return repository_metadata changeset_revisionreturn None
def quit global qdb listener connif qdb sys settrace None qdb Noneif conn conn close conn Noneif listener listener close listener None
def to_bytes obj b TMemoryBuffer p TBinaryProtocol b obj write p return b getvalue
def deconstructible *args **kwargs path kwargs pop 'path' None def decorator klass def __new__ cls *args **kwargs obj super klass cls __new__ cls obj _constructor_args args kwargs return objdef deconstruct obj '\nReturnsa3-tupleofclassimportpath positionalarguments \nandkeywordarguments \n'if path module_name _ name path rpartition ' ' else module_name obj __module__name obj __class__ __name__module import_module module_name if not hasattr module name raise ValueError 'Couldnotfindobject%sin%s \nPleasenotethatyoucannotserializethingslikeinnerclasses Pleasemovetheobjectintothemainmodulebodytousemigrations \nFormoreinformation seehttps //docs djangoproject com/en/%s/topics/migrations/#serializing-values' % name module_name get_docs_version return path or '%s %s' % obj __class__ __module__ name obj _constructor_args[0] obj _constructor_args[1] klass __new__ staticmethod __new__ klass deconstruct deconstructreturn klassif not args return decoratorreturn decorator *args **kwargs
@mock_ec2def test_igw_detach_wrong_vpc conn boto connect_vpc u'the_key' u'the_secret' igw conn create_internet_gateway vpc1 conn create_vpc VPC_CIDR vpc2 conn create_vpc VPC_CIDR conn attach_internet_gateway igw id vpc1 id with assert_raises EC2ResponseError as cm conn detach_internet_gateway igw id vpc2 id cm exception code should equal u'Gateway NotAttached' cm exception status should equal 400 cm exception request_id should_not be none
def django_auth_setup global DJANGO_AUTH_CLASSif DJANGO_AUTH_CLASS is not None returnif '^model' in __opts__['external_auth']['django'] django_model_fullname __opts__['external_auth']['django']['^model']django_model_name django_model_fullname split ' ' [ -1 ]django_module_name ' ' join django_model_fullname split ' ' [0 -1 ] __import__ django_module_name globals locals 'SaltExternalAuthModel' DJANGO_AUTH_CLASS_str 'django_auth_module {0}' format django_model_name DJANGO_AUTH_CLASS eval DJANGO_AUTH_CLASS_str if django VERSION > 1 7 django setup
def _make_menu_item menu_item title **kw _menu_items config['routes named_routes']if menu_item not in _menu_items raise Exception 'menuitem`%s`cannotbefound' % menu_item item copy copy _menu_items[menu_item] item update kw active _link_active item needed item pop 'needed' for need in needed if need not in kw raise Exception 'menuitem`%s`needparameter`%s`' % menu_item need link _link_to title menu_item suppress_active_class True **item if active return literal '<liclass "active">' + link + literal '</li>' return literal '<li>' + link + literal '</li>'
def intToBytes integer length None hexString '%x' % integer if length is None n len hexString else n length * 2 return binascii unhexlify hexString zfill n + n & 1
def coregistration tabbed False split True scene_width 500 inst None subject None subjects_dir None guess_mri_subject None _check_mayavi_version from _backend import _check_backend_check_backend from _coreg_gui import CoregFrame _make_viewview _make_view tabbed split scene_width gui CoregFrame inst subject subjects_dir guess_mri_subject gui configure_traits view view return gui
def deterministicPool worker doer createMemoryWorker return DeterministicThreadPool Team LockWorker Lock local lambda worker lambda None doer
def kulsinski u v u _validate_vector u v _validate_vector v n float len u nff nft ntf ntt _nbool_correspond_all u v return ntf + nft - ntt + n / ntf + nft + n
def delval filename keyword *args **kwargs if 'do_not_scale_image_data' not in kwargs kwargs['do_not_scale_image_data'] Trueclosed fileobj_closed filename hdulist extidx _getext filename 'update' *args **kwargs try del hdulist[extidx] header[keyword]finally hdulist _close closed closed
def get_swap transformed_list if not all isinstance device DriverSwapBlockDevice or 'swap_size' in device for device in transformed_list return Nonetry return transformed_list pop except IndexError return None
def getSectionNumber header if not header return Nonereturn domhelpers gatherTextNodes header childNodes[0]
def toplevel func synctaskletfunc synctasklet func @utils wrapping func def add_context_wrapper *args **kwds __ndb_debug__ utils func_info func _state clear_all_pending ctx make_default_context try set_context ctx return synctaskletfunc *args **kwds finally set_context None ctx flush check_success eventloop run return add_context_wrapper
def toplevel func synctaskletfunc synctasklet func @utils wrapping func def add_context_wrapper *args **kwds __ndb_debug__ utils func_info func _state clear_all_pending ctx make_default_context try set_context ctx return synctaskletfunc *args **kwds finally set_context None ctx flush check_success eventloop run return add_context_wrapper
def _window_too_small return Window TokenListControl static [ Token WindowTooSmall u'Windowtoosmall ' ]
def _window_too_small return Window TokenListControl static [ Token WindowTooSmall u'Windowtoosmall ' ]
def getCircleIntersectionLoops circleIntersections circleIntersectionLoops []for circleIntersection in circleIntersections if not circleIntersection steppedOn circleIntersectionLoop [circleIntersection]circleIntersectionLoops append circleIntersectionLoop addCircleIntersectionLoop circleIntersectionLoop circleIntersections return circleIntersectionLoops
def getCircleIntersectionLoops circleIntersections circleIntersectionLoops []for circleIntersection in circleIntersections if not circleIntersection steppedOn circleIntersectionLoop [circleIntersection]circleIntersectionLoops append circleIntersectionLoop addCircleIntersectionLoop circleIntersectionLoop circleIntersections return circleIntersectionLoops
def _enable_privilege privilege_name return _change_privilege_state privilege_name True
def delivery_pipeline registry xml_parent data pipeline XML SubElement xml_parent 'se diabol jenkins pipeline PipelineProperty' pipeline set 'plugin' 'delivery-pipeline-plugin' mapping [ 'stage' 'stageName' '' 'task' 'taskName' '' 'description' 'descriptionTemplate' '' ]helpers convert_mapping_to_xml pipeline data mapping fail_required True
def delivery_pipeline registry xml_parent data pipeline XML SubElement xml_parent 'se diabol jenkins pipeline PipelineProperty' pipeline set 'plugin' 'delivery-pipeline-plugin' mapping [ 'stage' 'stageName' '' 'task' 'taskName' '' 'description' 'descriptionTemplate' '' ]helpers convert_mapping_to_xml pipeline data mapping fail_required True
def get_custom_metric client project_id custom_metric_type request client projects metricDescriptors list name project_id filter 'metric type starts_with "{}" ' format custom_metric_type response request execute print 'ListCustomMetricsresponse 'pprint pprint response try return response['metricDescriptors']except KeyError return None
def get_scanner hass config info config[DOMAIN]host info get CONF_HOST username info get CONF_USERNAME password info get CONF_PASSWORD port info get CONF_PORT scanner NetgearDeviceScanner host username password port return scanner if scanner success_init else None
def _contains_hidden_files n for sub in n name sub nameif len name > 1 and name startswith ' ' return Truereturn False
def ASSIGNJS **kargs from gluon serializers import jsons ''for key value in kargs items s + 'var%s %s \n' % key json value return XML s
def ASSIGNJS **kargs from gluon serializers import jsons ''for key value in kargs items s + 'var%s %s \n' % key json value return XML s
def course_and_time_based_filename_generator course_id base_name return u'{course_prefix}_{base_name}_{timestamp_str}' format course_prefix course_filename_prefix_generator course_id base_name get_valid_filename base_name timestamp_str datetime now UTC strftime '%Y-%m-%d-%H%M%S'
def _auto_create_specific_service_command_generator run utils run command_generator _command_generators[get_name_of_init run ]command_list [c for c in COMMANDS if c not in ['list' 'set_target'] ]return _ServiceCommandGenerator command_generator command_list
def _auto_create_specific_service_command_generator run utils run command_generator _command_generators[get_name_of_init run ]command_list [c for c in COMMANDS if c not in ['list' 'set_target'] ]return _ServiceCommandGenerator command_generator command_list
def _auto_create_specific_service_command_generator run utils run command_generator _command_generators[get_name_of_init run ]command_list [c for c in COMMANDS if c not in ['list' 'set_target'] ]return _ServiceCommandGenerator command_generator command_list
def local_diff_branch remote os getenv LOCAL_REMOTE_ENV branch os getenv LOCAL_BRANCH_ENV if remote is not None and branch is not None return '%s/%s' % remote branch
def local_diff_branch remote os getenv LOCAL_REMOTE_ENV branch os getenv LOCAL_BRANCH_ENV if remote is not None and branch is not None return '%s/%s' % remote branch
@require_POST@login_requireddef unwatch_ready request product None if request LANGUAGE_CODE settings WIKI_DEFAULT_LANGUAGE raise Http404kwargs {}if product is not None kwargs['product'] productReadyRevisionEvent stop_notifying request user **kwargs return HttpResponse
@require_POST@login_requireddef unwatch_ready request product None if request LANGUAGE_CODE settings WIKI_DEFAULT_LANGUAGE raise Http404kwargs {}if product is not None kwargs['product'] productReadyRevisionEvent stop_notifying request user **kwargs return HttpResponse
def get_previous_sle args for_update False args[u'name'] args get u'sle' None or u'' sle get_stock_ledger_entries args u'< ' u'desc' u'limit1' for_update for_update return sle and sle[0] or {}
def get_previous_sle args for_update False args[u'name'] args get u'sle' None or u'' sle get_stock_ledger_entries args u'< ' u'desc' u'limit1' for_update for_update return sle and sle[0] or {}
def package_relationship_delete context data_dict model context['model']user context['user'] id id2 rel _get_or_bust data_dict ['subject' 'object' 'type'] pkg1 model Package get id pkg2 model Package get id2 if not pkg1 raise NotFound 'Subjectpackage%rwasnotfound ' % id if not pkg2 return NotFound 'Objectpackage%rwasnotfound ' % id2 existing_rels pkg1 get_relationships_with pkg2 rel if not existing_rels raise NotFoundrelationship existing_rels[0]revisioned_details 'PackageRelationship %s%s%s' % id rel id2 context['relationship'] relationship_check_access 'package_relationship_delete' context data_dict rev model repo new_revision rev author userrev message _ u'RESTAPI Delete%s' % revisioned_details relationship delete model repo commit
def test_valid_css styles ['color' 'float']eq_ '<pstyle "float left ">foo</p>' clean '<pstyle "float left color ">foo</p>' styles styles eq_ '<pstyle "">foo</p>' clean '<pstyle "color float left ">foo</p>' styles styles
def test_valid_css styles ['color' 'float']eq_ '<pstyle "float left ">foo</p>' clean '<pstyle "float left color ">foo</p>' styles styles eq_ '<pstyle "">foo</p>' clean '<pstyle "color float left ">foo</p>' styles styles
def phrase_extraction srctext trgtext alignment max_phrase_length 0 srctext srctext split trgtext trgtext split srclen len srctext trglen len trgtext f_aligned [j for _ j in alignment]max_phrase_length max_phrase_length or max srclen trglen bp set for e_start in range srclen max_idx min srclen e_start + max_phrase_length for e_end in range e_start max_idx f_start f_end trglen - 1 -1 for e f in alignment if e_start < e < e_end f_start min f f_start f_end max f f_end phrases extract f_start f_end e_start e_end alignment f_aligned srctext trgtext srclen trglen max_phrase_length if phrases bp update phrases return bp
def phrase_extraction srctext trgtext alignment max_phrase_length 0 srctext srctext split trgtext trgtext split srclen len srctext trglen len trgtext f_aligned [j for _ j in alignment]max_phrase_length max_phrase_length or max srclen trglen bp set for e_start in range srclen max_idx min srclen e_start + max_phrase_length for e_end in range e_start max_idx f_start f_end trglen - 1 -1 for e f in alignment if e_start < e < e_end f_start min f f_start f_end max f f_end phrases extract f_start f_end e_start e_end alignment f_aligned srctext trgtext srclen trglen max_phrase_length if phrases bp update phrases return bp
def setup_platform hass config add_entities discovery_info None import pyflichost config get CONF_HOST port config get CONF_PORT discovery config get CONF_DISCOVERY try client pyflic FlicClient host port except ConnectionRefusedError _LOGGER error 'Failedtoconnecttoflicserver ' returndef new_button_callback address 'Setupnewlyverifiedbuttonasdeviceinhomeassistant 'setup_button hass config add_entities client address client on_new_verified_button new_button_callbackif discovery start_scanning config add_entities client hass bus listen_once EVENT_HOMEASSISTANT_STOP lambda event client close threading Thread target client handle_events start def get_info_callback items 'Addentitiesforalreadyverifiedbuttons 'addresses items['bd_addr_of_verified_buttons'] or [] for address in addresses setup_button hass config add_entities client address client get_info get_info_callback
def doctest_skip_parser func lines func __doc__ split '\n' new_lines []for line in lines match SKIP_RE match line if match is None new_lines append line continue code space expr match groups try if eval expr func __globals__ code code + space + '#doctest +SKIP' except AttributeError if eval expr func __init__ __globals__ code code + space + '#doctest +SKIP' new_lines append code func __doc__ '\n' join new_lines return func
def doctest_skip_parser func lines func __doc__ split '\n' new_lines []for line in lines match SKIP_RE match line if match is None new_lines append line continue code space expr match groups try if eval expr func __globals__ code code + space + '#doctest +SKIP' except AttributeError if eval expr func __init__ __globals__ code code + space + '#doctest +SKIP' new_lines append code func __doc__ '\n' join new_lines return func
def _format_issue issue ret {'id' issue get 'id' 'issue_number' issue get 'number' 'state' issue get 'state' 'title' issue get 'title' 'user' issue get 'user' get 'login' 'html_url' issue get 'html_url' }assignee issue get 'assignee' if assignee assignee assignee get 'login' labels issue get 'labels' label_names []for label in labels label_names append label get 'name' milestone issue get 'milestone' if milestone milestone milestone get 'title' ret['assignee'] assigneeret['labels'] label_namesret['milestone'] milestonereturn ret
def _format_issue issue ret {'id' issue get 'id' 'issue_number' issue get 'number' 'state' issue get 'state' 'title' issue get 'title' 'user' issue get 'user' get 'login' 'html_url' issue get 'html_url' }assignee issue get 'assignee' if assignee assignee assignee get 'login' labels issue get 'labels' label_names []for label in labels label_names append label get 'name' milestone issue get 'milestone' if milestone milestone milestone get 'title' ret['assignee'] assigneeret['labels'] label_namesret['milestone'] milestonereturn ret
def services *args def decorator f services ['compute' 'image' 'baremetal' 'volume' 'network' 'identity' 'object_storage']for service in args if service not in services raise exceptions InvalidServiceTag '%sisnotavalidservice' % service attr type list args f @functools wraps f def wrapper self *func_args **func_kwargs service_list get_service_list for service in args if not service_list[service] msg 'Skippedbecausethe%sserviceisnotavailable' % service raise testtools TestCase skipException msg return f self *func_args **func_kwargs return wrapperreturn decorator
def services *args def decorator f services ['compute' 'image' 'baremetal' 'volume' 'network' 'identity' 'object_storage']for service in args if service not in services raise exceptions InvalidServiceTag '%sisnotavalidservice' % service attr type list args f @functools wraps f def wrapper self *func_args **func_kwargs service_list get_service_list for service in args if not service_list[service] msg 'Skippedbecausethe%sserviceisnotavailable' % service raise testtools TestCase skipException msg return f self *func_args **func_kwargs return wrapperreturn decorator
def join_css_classes class_list return '' join sorted str val for val in class_list if val
def cors_tool req_head cherrypy request headersresp_head cherrypy response headersresp_head['Access-Control-Allow-Origin'] req_head get 'Origin' '*' resp_head['Access-Control-Expose-Headers'] 'GET POST'resp_head['Access-Control-Allow-Credentials'] 'true'if cherrypy request method 'OPTIONS' cherrypy serving request handler cors_handler
def cors_tool req_head cherrypy request headersresp_head cherrypy response headersresp_head['Access-Control-Allow-Origin'] req_head get 'Origin' '*' resp_head['Access-Control-Expose-Headers'] 'GET POST'resp_head['Access-Control-Allow-Credentials'] 'true'if cherrypy request method 'OPTIONS' cherrypy serving request handler cors_handler
def garbagecollect func def inner *args **kwargs result func *args **kwargs gc collect return resultreturn inner
def garbagecollect func def inner *args **kwargs result func *args **kwargs gc collect return resultreturn inner
def get_plugin plugin if plugin in _PLUGINS_SERVICE return _PLUGINS_SERVICE[plugin]
@with_sitldef test_227 connpath vehicle connect connpath wait_ready True def assert_commands count vehicle commands download vehicle commands wait_ready assert_equals len vehicle commands count assert_commands 0 vehicle commands add Command 0 0 0 mavutil mavlink MAV_FRAME_GLOBAL_RELATIVE_ALT mavutil mavlink MAV_CMD_NAV_WAYPOINT 0 0 0 0 0 0 10 10 10 vehicle flush assert_commands 1
def require_support_permission func @wraps func def inner request *args **kwargs if has_access request user 'support' 'global' return func request *args **kwargs else return HttpResponseForbidden return login_required inner
def require_support_permission func @wraps func def inner request *args **kwargs if has_access request user 'support' 'global' return func request *args **kwargs else return HttpResponseForbidden return login_required inner
def require_support_permission func @wraps func def inner request *args **kwargs if has_access request user 'support' 'global' return func request *args **kwargs else return HttpResponseForbidden return login_required inner
def event_source_mapping_absent name EventSourceArn FunctionName region None key None keyid None profile None ret {'name' None 'result' True 'comment' '' 'changes' {}}desc __salt__['boto_lambda describe_event_source_mapping'] EventSourceArn EventSourceArn FunctionName FunctionName region region key key keyid keyid profile profile if 'error' in desc ret['result'] Falseret['comment'] 'Failedtodeleteeventsourcemapping {0} ' format desc['error']['message'] return retif not desc get 'event_source_mapping' ret['comment'] 'Eventsourcemappingdoesnotexist 'return retret['name'] desc['event_source_mapping']['UUID']if __opts__['test'] ret['comment'] 'Eventsourcemappingissettoberemoved 'ret['result'] Nonereturn retr __salt__['boto_lambda delete_event_source_mapping'] EventSourceArn EventSourceArn FunctionName FunctionName region region key key keyid keyid profile profile if not r['deleted'] ret['result'] Falseret['comment'] 'Failedtodeleteeventsourcemapping {0} ' format r['error']['message'] return retret['changes']['old'] descret['changes']['new'] {'event_source_mapping' None}ret['comment'] 'Eventsourcemappingdeleted 'return ret
def test_ast_valid_let can_compile u' let[ab] ' can_compile u' let[a1] ' can_compile u' let[a1bNone] '
def test_ast_valid_let can_compile u' let[ab] ' can_compile u' let[a1] ' can_compile u' let[a1bNone] '
def tight_layout pad 1 08 h_pad None w_pad None rect None fig gcf fig tight_layout pad pad h_pad h_pad w_pad w_pad rect rect
def wrap_socket sock keyfile None certfile None server_side None cert_reqs None ssl_version None ca_certs None do_handshake_on_connect None suppress_ragged_eofs None for arg in ['cert_reqs' 'ssl_version' 'ca_certs' 'do_handshake_on_connect' 'suppress_ragged_eofs'] if locals [arg] is not None raise TypeError 'Touseargument%rinstallsslpackage http //pypi python org/pypi/ssl' % arg return ssl sock keyfile keyfile certfile certfile
def Arcsin name a 0 b 1 return rv name ArcsinDistribution a b
def unarchive archive_path dest if tarfile is_tarfile archive_path with contextlib closing tarfile open archive_path 'r' as archive archive extractall dest elif is_zipfile archive_path with contextlib closing ZipFile archive_path 'r' as archive for name in archive namelist dest_path os path join dest *name split '/' dirname filename os path split dest_path if dirname and not os path exists dirname os makedirs dirname if filename with open dest_path 'wb' as dest_file dest_file write archive read name else raise IOError 'Unknownarchivetype %s' % archive_path
def unarchive archive_path dest if tarfile is_tarfile archive_path with contextlib closing tarfile open archive_path 'r' as archive archive extractall dest elif is_zipfile archive_path with contextlib closing ZipFile archive_path 'r' as archive for name in archive namelist dest_path os path join dest *name split '/' dirname filename os path split dest_path if dirname and not os path exists dirname os makedirs dirname if filename with open dest_path 'wb' as dest_file dest_file write archive read name else raise IOError 'Unknownarchivetype %s' % archive_path
def get_references file_name encoding 'utf-8' text ''if file_name is not None if os path exists file_name try with codecs open file_name 'r' encoding encoding as f text f read except print traceback format_exc else print 'Couldnotfindreferencefile%s ' file_name return text
def does_not_modify_data_dict validator def call_and_assert key data errors context None if context is None context {}original_data copy deepcopy data original_errors copy deepcopy errors original_context copy deepcopy context result validator key data errors context context assert data original_data 'Shouldnotmodifydatadictwhencalledwithkey {key} data {data} errors {errors} context {context}' format key key data original_data errors original_errors context original_context return resultreturn call_and_assert
def test_cache_deactivated_private_browsing config_stub tmpdir config_stub data {'storage' {'cache-size' 1024} 'general' {'private-browsing' True}}disk_cache cache DiskCache str tmpdir metadata QNetworkCacheMetaData metadata setUrl QUrl 'http //www example com/' assert metadata isValid assert disk_cache prepare metadata is None
def round_rectangle size radius fill width height sizerectangle Image new u'L' size 255 corner round_corner radius 255 rectangle paste corner 0 0 rectangle paste corner rotate 90 0 height - radius rectangle paste corner rotate 180 width - radius height - radius rectangle paste corner rotate 270 width - radius 0 return rectangle
def _has_access_descriptor user action descriptor course_key None def can_load "\nNOTE Thisdoesnotcheckthatthestudentisenrolledinthecourse\nthatcontainsthismodule Wemayormaynotwanttoallownon-enrolled\nstudentstoseemodules Ifnot viewsshouldcheckthecourse sowe\ndon'thavetohittheenrollmentstableoneverymoduleload \n"if not _has_group_access descriptor user course_key return ACCESS_DENIEDif _has_staff_access_to_descriptor user descriptor course_key return ACCESS_GRANTEDreturn _visible_to_nonstaff_users descriptor and _can_access_descriptor_with_milestones user descriptor course_key and _has_detached_class_tag descriptor or _can_access_descriptor_with_start_date user descriptor course_key checkers {'load' can_load 'staff' lambda _has_staff_access_to_descriptor user descriptor course_key 'instructor' lambda _has_instructor_access_to_descriptor user descriptor course_key }return _dispatch checkers action user descriptor
def wait_for func timeout first 0 0 step 1 0 text None start_time time time end_time time time + timeout time sleep first while time time < end_time if text logging debug '%s %fsecs ' text time time - start_time output func if output return outputtime sleep step return None
def typed_ordered_dict key_type value_type default return lambda setting OrderedDict key_type StringConverter key value_type StringConverter value if value '' else default for key value in OrderedDict setting items
def typed_ordered_dict key_type value_type default return lambda setting OrderedDict key_type StringConverter key value_type StringConverter value if value '' else default for key value in OrderedDict setting items
def compute_wcs key challenge assert type key in [six text_type six binary_type] assert type challenge in [six text_type six binary_type] if type key six text_type key key encode 'utf8' if type challenge six text_type challenge challenge encode 'utf8' sig hmac new key challenge hashlib sha256 digest return binascii b2a_base64 sig strip
def clear_compatversion_cache_on_delete sender instance **kw try if not instance addon type amo ADDON_EXTENSION returnexcept ObjectDoesNotExist returnif not kw get 'raw' instance addon invalidate_d2c_versions
def get_unique content_type object_pk name None request None ip None user_agent None user None if request if request user is_authenticated user request userip user_agent Noneelse user Noneip request META get 'REMOTE_ADDR' '' user_agent request META get 'HTTP_USER_AGENT' '' [ 255]hash_text '\n' join unicode x encode 'utf-8' for x in content_type pk object_pk name or '' ip user_agent user and user pk or 'None' unique_hash hashlib md5 hash_text hexdigest return user ip user_agent unique_hash
def present dbname name owner None db_user None db_password None db_host None db_port None ret {'dbname' dbname 'name' name 'changes' {} 'result' True 'comment' 'Schema{0}isalreadypresentindatabase{1}' format name dbname }db_args {'db_user' db_user 'db_password' db_password 'db_host' db_host 'db_port' db_port}schema_attr __salt__['postgres schema_get'] dbname name **db_args cret Noneif schema_attr is None cret __salt__['postgres schema_create'] dbname name owner owner **db_args else msg 'Schema{0}alreadyexistsindatabase{1}'cret Noneif cret msg 'Schema{0}hasbeencreatedindatabase{1}'ret['result'] Trueret['changes'][name] 'Present'elif cret is not None msg 'Failedtocreateschema{0}indatabase{1}'ret['result'] Falseelse msg 'Schema{0}alreadyexistsindatabase{1}'ret['result'] Trueret['comment'] msg format name dbname return ret
def getChanges request options None payload json loads request args['payload'][0] repo_url '%s%s' % payload['canon_url'] payload['repository']['absolute_url'] project request args get 'project' [''] [0]changes []for commit in payload['commits'] changes append {'author' commit['raw_author'] 'files' [f['file'] for f in commit['files']] 'comments' commit['message'] 'revision' commit['raw_node'] 'when_timestamp' dateparse commit['utctimestamp'] 'branch' commit['branch'] 'revlink' '%scommits/%s' % repo_url commit['raw_node'] 'repository' repo_url 'project' project} log msg 'Newrevision %s' % commit['node'] log msg 'Received%schangesfrombitbucket' % len changes return changes payload['repository']['scm']
def getChanges request options None payload json loads request args['payload'][0] repo_url '%s%s' % payload['canon_url'] payload['repository']['absolute_url'] project request args get 'project' [''] [0]changes []for commit in payload['commits'] changes append {'author' commit['raw_author'] 'files' [f['file'] for f in commit['files']] 'comments' commit['message'] 'revision' commit['raw_node'] 'when_timestamp' dateparse commit['utctimestamp'] 'branch' commit['branch'] 'revlink' '%scommits/%s' % repo_url commit['raw_node'] 'repository' repo_url 'project' project} log msg 'Newrevision %s' % commit['node'] log msg 'Received%schangesfrombitbucket' % len changes return changes payload['repository']['scm']
def getChanges request options None payload json loads request args['payload'][0] repo_url '%s%s' % payload['canon_url'] payload['repository']['absolute_url'] project request args get 'project' [''] [0]changes []for commit in payload['commits'] changes append {'author' commit['raw_author'] 'files' [f['file'] for f in commit['files']] 'comments' commit['message'] 'revision' commit['raw_node'] 'when_timestamp' dateparse commit['utctimestamp'] 'branch' commit['branch'] 'revlink' '%scommits/%s' % repo_url commit['raw_node'] 'repository' repo_url 'project' project} log msg 'Newrevision %s' % commit['node'] log msg 'Received%schangesfrombitbucket' % len changes return changes payload['repository']['scm']
@gating_enabled default [] def get_gated_content course user if _has_access_to_course user 'staff' course id return []else return [m['content_id'] for m in find_gating_milestones course id None 'requires' {'id' user id} ]
def check_dataset_edition_permission authorize_get False def inner view_func def decorate request *args **kwargs dataset kwargs get 'dataset' if dataset is not None and not authorize_get and request method 'GET' Job objects can_edit_or_exception request dataset coordinator return view_func request *args **kwargs return wraps view_func decorate return inner
def check_dataset_edition_permission authorize_get False def inner view_func def decorate request *args **kwargs dataset kwargs get 'dataset' if dataset is not None and not authorize_get and request method 'GET' Job objects can_edit_or_exception request dataset coordinator return view_func request *args **kwargs return wraps view_func decorate return inner
@error context_awaredef lv_revert_with_snapshot vg_name lv_name lv_snapshot_name lv_snapshot_size error context 'Revertingtosnapshotandtakinganewone' logging info lv_revert vg_name lv_name lv_snapshot_name lv_take_snapshot vg_name lv_name lv_snapshot_name lv_snapshot_size
@error context_awaredef lv_revert_with_snapshot vg_name lv_name lv_snapshot_name lv_snapshot_size error context 'Revertingtosnapshotandtakinganewone' logging info lv_revert vg_name lv_name lv_snapshot_name lv_take_snapshot vg_name lv_name lv_snapshot_name lv_snapshot_size
@error context_awaredef lv_revert_with_snapshot vg_name lv_name lv_snapshot_name lv_snapshot_size error context 'Revertingtosnapshotandtakinganewone' logging info lv_revert vg_name lv_name lv_snapshot_name lv_take_snapshot vg_name lv_name lv_snapshot_name lv_snapshot_size
@register filterdef unflag_url obj slug content_type ContentType objects get_for_model obj return reverse 'unflag' kwargs {'slug' slug 'app_label' content_type app_label 'model' content_type model 'object_id' obj pk}
def prespi_create_container_table self conn conn executescript "\nCREATETABLEcontainer \nROWIDINTEGERPRIMARYKEYAUTOINCREMENT \nnameTEXT \nput_timestampTEXT \ndelete_timestampTEXT \nobject_countINTEGER \nbytes_usedINTEGER \ndeletedINTEGERDEFAULT0\n \n\nCREATEINDEXix_container_deleted_nameON\ncontainer deleted name \n\nCREATETRIGGERcontainer_insertAFTERINSERTONcontainer\nBEGIN\nUPDATEaccount_stat\nSETcontainer_count container_count+ 1-new deleted \nobject_count object_count+new object_count \nbytes_used bytes_used+new bytes_used \nhash chexor hash new name \nnew put_timestamp '-' \nnew delete_timestamp '-' \nnew object_count '-' new bytes_used \nEND \n\nCREATETRIGGERcontainer_updateBEFOREUPDATEONcontainer\nBEGIN\nSELECTRAISE FAIL 'UPDATEnotallowed DELETEandINSERT' \nEND \n\n\nCREATETRIGGERcontainer_deleteAFTERDELETEONcontainer\nBEGIN\nUPDATEaccount_stat\nSETcontainer_count container_count- 1-old deleted \nobject_count object_count-old object_count \nbytes_used bytes_used-old bytes_used \nhash chexor hash old name \nold put_timestamp '-' \nold delete_timestamp '-' \nold object_count '-' old bytes_used \nEND \n"
def MERGE_SMALL writer segments from whoosh filedb filereading import SegmentReadernewsegments []sorted_segment_list sorted s doc_count_all s for s in segments total_docs 0for i count seg in enumerate sorted_segment_list if count > 0 total_docs + countif total_docs < fib i + 5 reader SegmentReader writer storage writer schema seg writer add_reader reader reader close else newsegments append seg return newsegments
def build_standard_config module doctype_info if not frappe db get_value u'ModuleDef' module frappe throw _ u'ModuleNotFound' data []add_section data _ u'Documents' u'fafa-star' [d for d in doctype_info if d document_type in u'Document' u'Transaction' ] add_section data _ u'Setup' u'fafa-cog' [d for d in doctype_info if d document_type in u'Master' u'Setup' u'' ] add_section data _ u'StandardReports' u'fafa-list' get_report_list module is_standard u'Yes' return data
def _untested_error where raise RuntimeError 'Unknown%sfailure' % where
def make_long_description readme_path README_PATHreadme_md strip_html_comments read readme_path history_md strip_html_comments read HISTORY_PATH license_md 'License\n \n\n' + read LICENSE_PATH sections [readme_md history_md license_md]md_description '\n\n' join sections md_ext os path splitext readme_path [1]md_description_path make_temp_path RST_DESCRIPTION_PATH new_ext md_ext write md_description md_description_path rst_temp_path make_temp_path RST_DESCRIPTION_PATH long_description convert_md_to_rst md_path md_description_path rst_temp_path rst_temp_path return '\n' join [RST_LONG_DESCRIPTION_INTRO long_description]
def get_task_logger name if name in RESERVED_LOGGER_NAMES raise RuntimeError u'Loggername{0 r}isreserved ' format name return _using_logger_parent task_logger get_logger name
def _merge_prefix deque size if len deque 1 and len deque[0] < size returnprefix []remaining sizewhile deque and remaining > 0 chunk deque popleft if len chunk > remaining deque appendleft chunk[remaining ] chunk chunk[ remaining]prefix append chunk remaining - len chunk if prefix deque appendleft type prefix[0] join prefix if not deque deque appendleft ''
def _item_to_changes iterator resource return Changes from_api_repr resource iterator zone
def set_sync value return set_var 'SYNC' value
def coerce_put_post request if request method 'PUT' if hasattr request '_post' del request _postdel request _filestry request method 'POST'request _load_post_and_files request method 'PUT'except AttributeError request META['REQUEST_METHOD'] 'POST'request _load_post_and_files request META['REQUEST_METHOD'] 'PUT'request PUT request POST
def coerce_put_post request if request method 'PUT' if hasattr request '_post' del request _postdel request _filestry request method 'POST'request _load_post_and_files request method 'PUT'except AttributeError request META['REQUEST_METHOD'] 'POST'request _load_post_and_files request META['REQUEST_METHOD'] 'PUT'request PUT request POST
def coerce_put_post request if request method 'PUT' if hasattr request '_post' del request _postdel request _filestry request method 'POST'request _load_post_and_files request method 'PUT'except AttributeError request META['REQUEST_METHOD'] 'POST'request _load_post_and_files request META['REQUEST_METHOD'] 'PUT'request PUT request POST
@bdd then bdd parsers parse 'Thesessionshouldlooklike \n{expected}' def compare_session request quteproc expected quteproc compare_session expected
def GetQueryNodeTextUnicode node if node getType QueryParser VALUE and len node children > 2 return u'' join c getText for c in node children[1 ] elif node getType QueryParser VALUE return Nonereturn node getText
def delete_peers *peers **options test options pop 'test' False commit options pop 'commit' True return __salt__['net load_template'] 'delete_ntp_peers' peers peers test test commit commit
def get_scale x scales [20 50 100 200 400 600 800 1000]for scale in scales if x < scale return scalereturn x
def return_probe_from_definition df types args df['args']retval_type df['retval_type']printf_specifier type_description retval_type types ['printf_specifier']template Template RETURN_PROBE_TEMPLATE mapping {'__LIBRARY__' df get 'library' '' '__NAME__' df['api'] '__ARGS_FORMAT_STRING__' arguments_format_string args types '__RETVAL_FORMAT_SPECIFIER__' printf_specifier '__ARGUMENTS__' arguments_section args types '__RETVAL__' retval_section retval_type types '__ARGUMENTS_POP_FROM_STACK__' pop_from_stack_section args }return template substitute mapping
def normalize_slice s start stop step s start s stop s step if start is None start 0if step is None step 1if start < 0 or step < 0 or stop is not None and stop < 0 raise NotImplementedError return slice start stop step
def regexp2pattern string if type string is REGEXP_T flags string flagsstring string patternif string startswith '^' string string[1 ]else string ' *' + string if string endswith '$' string string[ -1 ]else string + ' *'return string flags else return re escape string 0
def regexp2pattern string if type string is REGEXP_T flags string flagsstring string patternif string startswith '^' string string[1 ]else string ' *' + string if string endswith '$' string string[ -1 ]else string + ' *'return string flags else return re escape string 0
def get_output_ids ids_bcs_added_field corrected_bc num_errors added_field max_bc_errors 1 5 enum_val 1 bc_corrected_flag Noneif added_field is None curr_added_field ''else curr_added_field added_fieldif corrected_bc is None curr_bc ''else curr_bc corrected_bclog_id ''if num_errors > max_bc_errors sample_id 'Unassigned_%d' % enum_val bc_corrected_flag 'not_corrected'else try base_sample_id ids_bcs_added_field[ curr_bc curr_added_field ]sample_id '%s_%d' % base_sample_id enum_val if corrected_bc log_id + '%s' % corrected_bc if corrected_bc and added_field log_id + ' 'if added_field log_id + '%s' % added_field if log_id log_id + ' 'log_id + '%s' % base_sample_id if num_errors > 0 bc_corrected_flag 'corrected'except KeyError sample_id 'Unassigned_%d' % enum_val return sample_id log_id bc_corrected_flag
def create_tcp_socket module type_ module SOCK_STREAMif hasattr module 'SOCK_CLOEXEC' type_ module SOCK_CLOEXECsock module socket module AF_INET type_ _set_default_tcpsock_options module sock return sock
def popular_tags_for_model model count 10 content_type ContentType objects get_for_model model return Tag objects filter taggit_taggeditem_items__content_type content_type annotate item_count Count u'taggit_taggeditem_items' order_by u'-item_count' [ count]
def fix_local_scheme home_dir symlink True try import sysconfigexcept ImportError passelse if sysconfig _get_default_scheme 'posix_local' local_path os path join home_dir 'local' if not os path exists local_path os mkdir local_path for subdir_name in os listdir home_dir if subdir_name 'local' continuecopyfile os path abspath os path join home_dir subdir_name os path join local_path subdir_name symlink
def fix_local_scheme home_dir symlink True try import sysconfigexcept ImportError passelse if sysconfig _get_default_scheme 'posix_local' local_path os path join home_dir 'local' if not os path exists local_path os mkdir local_path for subdir_name in os listdir home_dir if subdir_name 'local' continuecopyfile os path abspath os path join home_dir subdir_name os path join local_path subdir_name symlink
def fix_local_scheme home_dir symlink True try import sysconfigexcept ImportError passelse if sysconfig _get_default_scheme 'posix_local' local_path os path join home_dir 'local' if not os path exists local_path os mkdir local_path for subdir_name in os listdir home_dir if subdir_name 'local' continuecopyfile os path abspath os path join home_dir subdir_name os path join local_path subdir_name symlink
def fix_local_scheme home_dir symlink True try import sysconfigexcept ImportError passelse if sysconfig _get_default_scheme 'posix_local' local_path os path join home_dir 'local' if not os path exists local_path os mkdir local_path for subdir_name in os listdir home_dir if subdir_name 'local' continuecopyfile os path abspath os path join home_dir subdir_name os path join local_path subdir_name symlink
def add_checks actions if FETCH in actions actions setdefault CHECK_FETCH [True] if EXTRACT in actions actions setdefault CHECK_EXTRACT [True]
@taskdef post_index new_index old_index alias index_name settings _print 'Optimizing updatingsettingsandaliases ' alias ES indices optimize index new_index ES indices put_settings index new_index body settings actions [{'add' {'index' new_index 'alias' alias}}]if old_index actions append {'remove' {'index' old_index 'alias' alias}} ES indices update_aliases body dict actions actions _print 'Unflaggingthedatabase ' alias Reindexing unflag_reindexing alias alias _print 'Removingindex{index} ' format index old_index alias if old_index and ES indices exists index old_index ES indices delete index old_index alias_output ''for indexer in INDEXERS alias ES_INDEXES[indexer get_mapping_type_name ]alias_output + unicode ES indices get_aliases index alias + '\n' _print 'Reindexationdone Currentaliasesconfiguration {output}\n' format output alias_output alias
def parse_value s if not s return REMOVE_THIS_KEYelif s[0] in u'"{[0123456789-' or s in u'null' u'true' u'false' return json loads s else return s
def parse_value s if not s return REMOVE_THIS_KEYelif s[0] in u'"{[0123456789-' or s in u'null' u'true' u'false' return json loads s else return s
def _getPersistentRSAKey location keySize 4096 location parent makedirs ignoreExistingDirectory True if not location exists privateKey rsa generate_private_key public_exponent 65537 key_size keySize backend default_backend pem privateKey private_bytes encoding serialization Encoding PEM format serialization PrivateFormat TraditionalOpenSSL encryption_algorithm serialization NoEncryption location setContent pem with location open 'rb' as keyFile privateKey serialization load_pem_private_key keyFile read password None backend default_backend return Key privateKey
def _numeric n return isinstance n salt ext six integer_types + float
def Int2AP num val a2p [] 'ABCDEFGHIJKLMNOP' num int abs num while num num mod divmod num 16 val insert 0 a2p[mod] return '' join val
def get_color_dict l **kwargs cs ColorSpiral **kwargs colors cs get_colors len l dict {}for item in l dict[item] next colors return dict
def relabel_nodes G mapping copy True if not hasattr mapping '__getitem__' m dict n mapping n for n in G else m mappingif copy return _relabel_copy G m else return _relabel_inplace G m
def expand_db_html html for_editor False def replace_a_tag m attrs extract_attrs m group 1 if u'linktype' not in attrs return m group 0 handler get_link_handler attrs[u'linktype'] return handler expand_db_attributes attrs for_editor def replace_embed_tag m attrs extract_attrs m group 1 handler get_embed_handler attrs[u'embedtype'] return handler expand_db_attributes attrs for_editor html FIND_A_TAG sub replace_a_tag html html FIND_EMBED_TAG sub replace_embed_tag html return html
def wait_on_app port retries math ceil START_APP_TIMEOUT / BACKOFF_TIME private_ip appscale_info get_private_ip url 'http //' + private_ip + ' ' + str port + FETCH_PATH while retries > 0 try opener urllib2 build_opener NoRedirection response opener open url if response code HTTP_OK logging warning '{}returned{} Headers {}' format url response code response headers headers return Trueexcept IOError retries - 1time sleep BACKOFF_TIME logging error 'Applicationdidnotcomeupon{}after{}seconds' format url START_APP_TIMEOUT return False
def evaluate_marker text extra None try marker packaging markers Marker text return marker evaluate except packaging markers InvalidMarker as e raise SyntaxError e
def _to_trace_component_db component if not isinstance component basestring dict print type component raise ValueError 'Expectedcomponenttobestrordict' object_id component if isinstance component basestring else component['id'] ref component get 'ref' '' if isinstance component dict else '' caused_by component get 'caused_by' {} if isinstance component dict else {} return TraceComponentDB object_id object_id ref ref caused_by caused_by
def test_bad_setv cant_compile u' setvif*1 ' cant_compile u' setv ab [12] '
def test_bad_setv cant_compile u' setvif*1 ' cant_compile u' setv ab [12] '
def test_bad_setv cant_compile u' setvif*1 ' cant_compile u' setv ab [12] '
def vhosts cmd '{0}-S' format _detect_os ret {}namevhost ''out __salt__['cmd run'] cmd for line in out splitlines if not line continuecomps line split if 'isaNameVirtualHost' in line namevhost comps[0]ret[namevhost] {}else if comps[0] 'default' ret[namevhost]['default'] {}ret[namevhost]['default']['vhost'] comps[2]ret[namevhost]['default']['conf'] re sub '\\ \\ ' '' comps[3] if comps[0] 'port' ret[namevhost][comps[3]] {}ret[namevhost][comps[3]]['vhost'] comps[3]ret[namevhost][comps[3]]['conf'] re sub '\\ \\ ' '' comps[4] ret[namevhost][comps[3]]['port'] comps[1]return ret
def query sql_query vars None processed False if vars is None vars {}db_cursor ctx db cursor if not processed sql_query vars reparam sql_query vars ctx db_execute db_cursor sql_query vars if db_cursor description names [x[0] for x in db_cursor description]def iterwrapper row db_cursor fetchone while row yield Storage dict zip names row row db_cursor fetchone out iterbetter iterwrapper out __len__ lambda int db_cursor rowcount out list lambda [Storage dict zip names x for x in db_cursor fetchall ] else out db_cursor rowcountif not ctx db_transaction ctx db commit return out
def identification_field_factory label error_required return forms CharField label label widget forms TextInput attrs attrs_dict max_length 75 error_messages {u'required' error_required}
def identification_field_factory label error_required return forms CharField label label widget forms TextInput attrs attrs_dict max_length 75 error_messages {u'required' error_required}
def avgFoundAfter decreasingTargetValues listsOfActualValues batchSize 1 useMedian False from scipy import sumnumLists len listsOfActualValues longest max list map len listsOfActualValues res [[0] for _ in range numLists ]for tval in decreasingTargetValues for li l in enumerate listsOfActualValues lres res[li]found Falsefor i in range lres[ -1 ] len l if l[i] < tval lres append i found Truebreakif not found lres append longest tmp array res if useMedian resx median tmp axis 0 [1 ]else resx sum tmp axis 0 [1 ] / float numLists return resx * batchSize
def stop_container container_id try subprocess check_output 'dockerstop%s' % container_id shell True except subprocess CalledProcessError as cpe print 'w3afcontainerfailedtostop "%s"' % cpe sys exit 1
def test_can_create_all_resources session create_session for service_name in session get_available_resources yield _test_create_resource session service_name
def __generate_crc16_table result []for byte in range 256 crc 0for _ in range 8 if byte ^ crc & 1 crc crc >> 1 ^ 40961 else crc >> 1byte >> 1result append crc return result
@world absorbdef wait_for_visible css_selector index 0 timeout GLOBAL_WAIT_FOR_TIMEOUT wait_for func lambda _ css_visible css_selector index timeout timeout timeout_msg 'Timedoutwaitingfor{}tobevisible ' format css_selector
def _find_head_bem subject subjects_dir high_res False fnames _high_res_head_fnames if high_res else _head_fnames for fname in fnames path fname format subjects_dir subjects_dir subject subject if os path exists path return path
def connect_to_autoscale region None return _create_client ep_name 'autoscale' region region
def eventlog request event 0 if not test_user_authenticated request return login request next '/cobbler_web/eventlog/%s' % str event expired True event_info remote get_events if event not in event_info return HttpResponse 'eventnotfound' data event_info[event]eventname data[0]eventtime data[1]eventstate data[2]eventlog remote get_event_log event t get_template 'eventlog tmpl' vars {'eventlog' eventlog 'eventname' eventname 'eventstate' eventstate 'eventid' event 'eventtime' eventtime 'version' remote extended_version request session['token'] ['version'] 'username' username}html t render RequestContext request vars return HttpResponse html
def isValidHMAC hmac1 hmac2 key assert len hmac1 len hmac2 doubleHmac1 mycrypto HMAC_SHA256_128 key hmac1 doubleHmac2 mycrypto HMAC_SHA256_128 key hmac2 if doubleHmac1 doubleHmac2 return Falselog debug 'ThecomputedHMACisvalid ' return True
def build_output_stream args env request response output_options req_h OUT_REQ_HEAD in output_options req_b OUT_REQ_BODY in output_options resp_h OUT_RESP_HEAD in output_options resp_b OUT_RESP_BODY in output_options req req_h or req_b resp resp_h or resp_b output []Stream get_stream_type env args if req output append Stream msg HTTPRequest request with_headers req_h with_body req_b if req_b and resp output append ['\n\n'] if resp output append Stream msg HTTPResponse response with_headers resp_h with_body resp_b if env stdout_isatty and resp_b output append ['\n\n'] return chain *output
def build_output_stream args env request response output_options req_h OUT_REQ_HEAD in output_options req_b OUT_REQ_BODY in output_options resp_h OUT_RESP_HEAD in output_options resp_b OUT_RESP_BODY in output_options req req_h or req_b resp resp_h or resp_b output []Stream get_stream_type env args if req output append Stream msg HTTPRequest request with_headers req_h with_body req_b if req_b and resp output append ['\n\n'] if resp output append Stream msg HTTPResponse response with_headers resp_h with_body resp_b if env stdout_isatty and resp_b output append ['\n\n'] return chain *output
def convert_search search if not search search ''else search search replace '*' '%' replace '' '%' if search and search startswith '^' search search replace '^' '' search + '%'elif search and search endswith '$' search search replace '$' '' search '%' + search else search '%' + search + '%' return search
def convert_search search if not search search ''else search search replace '*' '%' replace '' '%' if search and search startswith '^' search search replace '^' '' search + '%'elif search and search endswith '$' search search replace '$' '' search '%' + search else search '%' + search + '%' return search
def inverse data impulse_response None filter_params {} max_gain 2 predefined_filter None assert_nD data 2 'data' if predefined_filter is None filt LPIFilter2D impulse_response **filter_params else filt predefined_filter F G filt _prepare data _min_limit F F 1 / F mask np abs F > max_gain F[mask] np sign F[mask] * max_gain return _centre np abs ifftshift np dual ifftn G * F data shape
def inverse data impulse_response None filter_params {} max_gain 2 predefined_filter None assert_nD data 2 'data' if predefined_filter is None filt LPIFilter2D impulse_response **filter_params else filt predefined_filter F G filt _prepare data _min_limit F F 1 / F mask np abs F > max_gain F[mask] np sign F[mask] * max_gain return _centre np abs ifftshift np dual ifftn G * F data shape
def _get_filename filename if os path isabs filename basedir os path abspath os path join os path dirname __file__ ' ' ' ' common_path os path commonprefix [basedir filename] if common_path filename filename[len common_path ] lstrip '/' return filename
def rax_find_server module rax_module server cs rax_module cloudserverstry UUID server server cs servers get server except ValueError servers cs servers list search_opts dict name '^%s$' % server if not servers module fail_json msg 'NoServerwasmatchedbyname tryusingtheServerIDinstead' if len servers > 1 module fail_json msg 'Multipleserversmatchedbyname tryusingtheServerIDinstead' server servers[0]return server
def nearest_intersection line0 line1 Pa Pb intersection_dist nearest_intersection_points line0 line1 if Pa is not None nPoint Pa - Pb return Pb + nPoint * 0 5 intersection_dist else return None None
def register_type cffi_type numba_type tm _type_map tm[cffi_type] numba_type
def bptrs a return pycuda gpuarray arange a ptr a ptr + a shape[0] * a strides[0] a strides[0] dtype cublas ctypes c_void_p
@receiver post_save sender CourseUserGroup def _cohort_added sender **kwargs instance kwargs['instance']if kwargs['created'] and instance group_type CourseUserGroup COHORT tracker emit 'edx cohort created' {'cohort_id' instance id 'cohort_name' instance name}
@receiver post_save sender CourseUserGroup def _cohort_added sender **kwargs instance kwargs['instance']if kwargs['created'] and instance group_type CourseUserGroup COHORT tracker emit 'edx cohort created' {'cohort_id' instance id 'cohort_name' instance name}
def warn msg html False write msg 'WARN' html
def warn msg html False write msg 'WARN' html
def generate_adjlist G delimiter '' directed G is_directed seen set for s nbrs in G adjacency line make_str s + delimiter for t data in nbrs items if not directed and t in seen continueif G is_multigraph for d in data values line + make_str t + delimiter else line + make_str t + delimiter if not directed seen add s yield line[ - len delimiter ]
def download_signed request uuid **kwargs extension get_object_or_404 Extension objects without_deleted public uuid uuid version get_object_or_404 extension versions without_deleted public pk kwargs['version_id'] log info 'Downloadingpublicadd-on %sversion%sfrom%s' % extension pk version pk version signed_file_path return _download request extension version version signed_file_path
def relevant_issues issues after logging info 'findingrelevantissuesafter{} ' format after seen set for issue in issues if relevent_issue issue after and issue['title'] not in seen seen add issue['title'] yield issue
def get_value_from_system_metadata instance key type default value instance system_metadata get key default try return type value except ValueError LOG warning _LW 'Metadatavalue% value sfor% key sisnotoftype% type s Usingdefaultvalue% default s ' {'value' value 'key' key 'type' type 'default' default} instance instance return default
def remove_samples path RE_SAMPLE re compile sample_match re I for root _dirs files in os walk path for file_ in files if RE_SAMPLE search file_ path os path join root file_ try logging info 'Removingunwantedsamplefile%s' path os remove path except logging error T 'Removing%sfailed' clip_path path logging info 'Traceback ' exc_info True
def get_random_id from random import randomfrom time import timetry from hashlib import sha1 as shaexcept ImportError import shasha sha newreturn sha '%s %s' % random time hexdigest
def _bc_adjust_shape_strides context builder shapes strides target_shape bc_shapes []bc_strides []zero context get_constant types uintp 0 one context get_constant types uintp 1 mismatch [builder icmp_signed ' ' tar old for tar old in zip target_shape shapes ]src_is_one [builder icmp_signed ' ' old one for old in shapes]preds [builder and_ x y for x y in zip mismatch src_is_one ]bc_shapes [builder select p tar old for p tar old in zip preds target_shape shapes ]bc_strides [builder select p zero old for p old in zip preds strides ]return bc_shapes bc_strides
def create_container_with_mbytes_and_specific_cpus name mbytes cpus None root SUPER_ROOT io {} move_in True timeout 0 need_mem_containers if not container_exists root raise error AutotestError 'Parentcontainer"%s"doesnotexist' % root if cpus is None cpus get_cpus root else cpus set cpus if not cpus raise error AutotestError 'Creatingcontainerwithnocpus' name os path join root name if os path exists full_path name raise error AutotestError 'Container%salreadyexists' % name create_container_directly name mbytes cpus set_io_controls name **io if move_in move_self_into_container name return name
def create_container_with_mbytes_and_specific_cpus name mbytes cpus None root SUPER_ROOT io {} move_in True timeout 0 need_mem_containers if not container_exists root raise error AutotestError 'Parentcontainer"%s"doesnotexist' % root if cpus is None cpus get_cpus root else cpus set cpus if not cpus raise error AutotestError 'Creatingcontainerwithnocpus' name os path join root name if os path exists full_path name raise error AutotestError 'Container%salreadyexists' % name create_container_directly name mbytes cpus set_io_controls name **io if move_in move_self_into_container name return name
def create_container_with_mbytes_and_specific_cpus name mbytes cpus None root SUPER_ROOT io {} move_in True timeout 0 need_mem_containers if not container_exists root raise error AutotestError 'Parentcontainer"%s"doesnotexist' % root if cpus is None cpus get_cpus root else cpus set cpus if not cpus raise error AutotestError 'Creatingcontainerwithnocpus' name os path join root name if os path exists full_path name raise error AutotestError 'Container%salreadyexists' % name create_container_directly name mbytes cpus set_io_controls name **io if move_in move_self_into_container name return name
def get_review_by_repository_id_changeset_revision_user_id app repository_id changeset_revision user_id sa_session app model context currentreturn sa_session query app model RepositoryReview filter and_ app model RepositoryReview repository_id app security decode_id repository_id app model RepositoryReview changeset_revision changeset_revision app model RepositoryReview user_id app security decode_id user_id first
@_refresh_mine_cache@_api_version 1 12 @_ensure_existsdef unpause name orig_state state name if orig_state 'stopped' return {'result' False 'state' {'old' orig_state 'new' orig_state} 'comment' "Container'{0}'isstopped cannotunpause" format name }return _change_state name 'unpause' 'running'
def StudentT name nu return rv name StudentTDistribution nu
def marshal data fields envelope None def make cls if isinstance cls type return cls return clsif isinstance data list tuple return OrderedDict [ envelope [marshal d fields for d in data] ] if envelope else [marshal d fields for d in data] items k marshal data v if isinstance v dict else make v output k data for k v in fields items return OrderedDict [ envelope OrderedDict items ] if envelope else OrderedDict items
def marshal data fields envelope None def make cls if isinstance cls type return cls return clsif isinstance data list tuple return OrderedDict [ envelope [marshal d fields for d in data] ] if envelope else [marshal d fields for d in data] items k marshal data v if isinstance v dict else make v output k data for k v in fields items return OrderedDict [ envelope OrderedDict items ] if envelope else OrderedDict items
def calculator counter_type def set_calculator func _counter_type_calculators[counter_type] funcreturn funcreturn set_calculator
def test_cache_deactivated_update_metadata config_stub tmpdir config_stub data {'storage' {'cache-size' 1024} 'general' {'private-browsing' True}}url 'http //qutebrowser org'disk_cache cache DiskCache str tmpdir metadata QNetworkCacheMetaData metadata setUrl QUrl url assert metadata isValid assert disk_cache updateMetaData metadata is None
def create_lrouter cluster tenant_id display_name nexthop tags [{'tag' tenant_id 'scope' 'os_tid'}]display_name _check_and_truncate_name display_name lrouter_obj {'display_name' display_name 'tags' tags 'routing_config' {'default_route_next_hop' {'gateway_ip_address' nexthop 'type' 'RouterNextHop'} 'type' 'SingleDefaultRouteImplicitRoutingConfig'} 'type' 'LogicalRouterConfig'}try return json loads do_single_request HTTP_POST _build_uri_path LROUTER_RESOURCE json dumps lrouter_obj cluster cluster except NvpApiClient NvpApiException LOG exception _ 'AnexceptionoccuredwhilecommunicatingwiththeNVPcontrollerforcluster %s' cluster name raise
def create_lrouter cluster tenant_id display_name nexthop tags [{'tag' tenant_id 'scope' 'os_tid'}]display_name _check_and_truncate_name display_name lrouter_obj {'display_name' display_name 'tags' tags 'routing_config' {'default_route_next_hop' {'gateway_ip_address' nexthop 'type' 'RouterNextHop'} 'type' 'SingleDefaultRouteImplicitRoutingConfig'} 'type' 'LogicalRouterConfig'}try return json loads do_single_request HTTP_POST _build_uri_path LROUTER_RESOURCE json dumps lrouter_obj cluster cluster except NvpApiClient NvpApiException LOG exception _ 'AnexceptionoccuredwhilecommunicatingwiththeNVPcontrollerforcluster %s' cluster name raise
def log *args for v in args sys stderr write str v sys stderr write '\n'
def add_namespace_to_cmd cmd namespace None return ['ip' 'netns' 'exec' namespace] + cmd if namespace else cmd
def generate_fake_facilities names 'WilsonElementary' facilities []for name in names found_facilities Facility objects filter name name if found_facilities facility found_facilities[0]logging info "Retrievedfacility'%s'" % name else facility Facility name name facility save logging info "Createdfacility'%s'" % name facilities append facility return facilities
def _normalize_html data try data lxml html tostring lxml html fromstring data encoding 'unicode' except passreturn '< DOCTYPEhtml>\n' + data
def _normalize_html data try data lxml html tostring lxml html fromstring data encoding 'unicode' except passreturn '< DOCTYPEhtml>\n' + data
@receiver COURSE_CERT_AWARDED sender GeneratedCertificate def create_course_group_badge sender user course_key status **kwargs course_group_check user course_key
@receiver COURSE_CERT_AWARDED sender GeneratedCertificate def create_course_group_badge sender user course_key status **kwargs course_group_check user course_key
def partition_node node host nodeport 27017idx node rfind ' ' if idx -1 host port node[ idx] int node[ idx + 1 ] if host startswith '[' host host[1 -1 ]return host port
def partition_node node host nodeport 27017idx node rfind ' ' if idx -1 host port node[ idx] int node[ idx + 1 ] if host startswith '[' host host[1 -1 ]return host port
def _formatRouteBody data schema_store baseSubstitutions {u'DOMAIN' u'example com' u'NODE_0' u'cf0f0346-17b2-4812-beca-1434997d6c3f' u'NODE_1' u'7ec3c4eb-6b1c-43da-8015-a163f7d15244'}for line in data['description'] yield line if 'input' in data for line in _formatActualSchema data['input_schema'] '+RequestJSONSchema' schema_store yield line if 'output' in data for line in _formatActualSchema data['output_schema'] '+ResponseJSONSchema' schema_store yield line for example in data['examples'] substitutions baseSubstitutions copy for line in _formatExample example substitutions yield line if 'input' in data for line in _formatSchema data['input'] True yield line if 'output' in data for line in _formatSchema data['output'] False yield line
def movavg x n w np empty n dtype float w[ ] 1 0 / n return np convolve x w mode u'valid'
def apply_adjustments rgba adjustments if not adjustments return rgbafor adjustment in adjustments name args adjustment[0] adjustment[1 ] if name 'threshold' rgba apply_threshold_adjustment rgba *args elif name 'curves' rgba apply_curves_adjustment rgba *args elif name 'curves2' rgba apply_curves2_adjustment rgba *args else raise KnownUnknown 'Unrecognizedcompositeadjustment "%s"withargs%s' % name repr args return rgba
def naughty_strings filepath FILEPATH strings []with open filepath 'r' as f strings f readlines strings [x strip u'\n' for x in strings]strings [x for x in strings if x and not x startswith u'#' ]strings insert 0 u'' return strings
def verifyThirdPartyFile url checksum fname name os path basename fname if os path exists fname print 'Usinglocalcopyof%s' % name else print 'Didnotfindlocalcopyof%s' % name print 'Downloading%s' % name downloadURL url fname print 'Archivefor%sstoredas%s' % name fname if os system 'MD5 $ opensslmd5%s test"${MD5##* }" "%s"' % shellQuote fname checksum fatal 'MD5checksummismatchforfile%s' % fname
def underscore value return value replace u' ' u'_'
@register assignment_tag takes_context True def assignment_params_and_context context arg return 'assignment_params_and_context-Expectedresult contextvalue %s %s' % context['value'] arg
def _invert f_x y x domain S Complexes x sympify x if not x is_Symbol raise ValueError 'xmustbeasymbol' f_x sympify f_x if not f_x has x raise ValueError "Inverseofconstantfunctiondoesn'texist" y sympify y if y has x raise ValueError 'yshouldbeindependentofx' if domain is_subset S Reals x s _invert_real f_x FiniteSet y x else x s _invert_complex f_x FiniteSet y x return x s intersection domain if isinstance s FiniteSet else s
def get_suite import doctestimport sysreturn doctest DocTestSuite sys modules[__name__]
def link target link_to assert isinstance target str assert os path exists target assert isinstance link_to str abs_path os path dirname os path abspath link_to if not os path isdir abs_path os makedirs abs_path chmod target os symlink target link_to
def create_populated_cluster ctxt num_services num_down_svcs 0 **values up_time timeutils utcnow down_time up_time - datetime timedelta seconds CONF service_down_time + 1 cluster create_cluster ctxt **values svcs [db service_create ctxt {'cluster_name' cluster name 'host' 'host' + str i 'updated_at' down_time if i < num_down_svcs else up_time } for i in range num_services ]return cluster svcs
def external_login_email_get form ResendConfirmationForm request form session get_session if not session is_external_first_login raise HTTPError http UNAUTHORIZED external_id_provider session data['auth_user_external_id_provider']return {'form' form 'external_id_provider' external_id_provider}
def external_login_email_get form ResendConfirmationForm request form session get_session if not session is_external_first_login raise HTTPError http UNAUTHORIZED external_id_provider session data['auth_user_external_id_provider']return {'form' form 'external_id_provider' external_id_provider}
def addGradientListToDocstring def dec fn if fn __doc__ is not None fn __doc__ fn __doc__ + str Gradients keys strip '[' strip ']' return fnreturn dec
def symmath_check_simple expect ans adict {} symtab None extra_options None options {'__MATRIX__' False '__ABC__' False '__LOWER__' False}if extra_options options update extra_options for op in options if op in expect expect expect replace op '' options[op] Trueexpect expect replace '__OR__' '__or__' if options['__LOWER__'] expect expect lower ans ans lower try ret check expect ans matrix options['__MATRIX__'] abcsym options['__ABC__'] symtab symtab except Exception as err return {'ok' False 'msg' 'Error%s<br/>Failedinevaluatingcheck %s %s ' % err expect ans }return ret
def symmath_check_simple expect ans adict {} symtab None extra_options None options {'__MATRIX__' False '__ABC__' False '__LOWER__' False}if extra_options options update extra_options for op in options if op in expect expect expect replace op '' options[op] Trueexpect expect replace '__OR__' '__or__' if options['__LOWER__'] expect expect lower ans ans lower try ret check expect ans matrix options['__MATRIX__'] abcsym options['__ABC__'] symtab symtab except Exception as err return {'ok' False 'msg' 'Error%s<br/>Failedinevaluatingcheck %s %s ' % err expect ans }return ret
def ancestors node results []try current node parent except AttributeError return resultswhile current results append current current current parent return results
def get_tests_from_fs parent_dir control_pattern add_noncompliant False tests {}profilers Falseif 'client/profilers' in parent_dir profilers Truefor dir in [parent_dir] files recursive_walk dir control_pattern for file in files if '__init__ py' in file or ' svn' in file continueif not profilers if not add_noncompliant try found_test control_data parse_control file raise_warnings True tests[file] found_testexcept control_data ControlVariableException as e logging warn 'Skipping%s\n%s' file e except Exception as e logging error 'Bad%s\n%s' file e else found_test control_data parse_control file tests[file] found_testelse tests[file] compiler parseFile file docreturn tests
def get_tests_from_fs parent_dir control_pattern add_noncompliant False tests {}profilers Falseif 'client/profilers' in parent_dir profilers Truefor dir in [parent_dir] files recursive_walk dir control_pattern for file in files if '__init__ py' in file or ' svn' in file continueif not profilers if not add_noncompliant try found_test control_data parse_control file raise_warnings True tests[file] found_testexcept control_data ControlVariableException as e logging warn 'Skipping%s\n%s' file e except Exception as e logging error 'Bad%s\n%s' file e else found_test control_data parse_control file tests[file] found_testelse tests[file] compiler parseFile file docreturn tests
def getFolders directory archive makeDirectory directory directoryListing []try directoryListing os listdir directory except OSError print 'Skeinforgecannotlistthedirectory 'print directoryprint 'sogiveitread/writepermissionforthatdirectory 'folders []for fileName in directoryListing if os path isdir os path join directory fileName folders append fileName return folders
def matchSetContribution match_set target_set index contribution 0 0for t in target_set match - float 'inf' id -1 for i m in enumerate match_set v matchStrength m t if v > match match vid iif id index contribution + matchreturn contribution / len target_set
def Erlang name k l return rv name GammaDistribution k 1 / l
def css_gradient gradient stop finalStop gradient start gradient finalStop x1 y1 x2 y2 stop x stop y finalStop x finalStop y stops gradient stops stops '\n' join 'stop {0 f}{1}' format stop color name for stop color in stops return 'qlineargradient \nx1 {x1} y1 {y1} x2 {x1} y2 {y2} \n{stops} ' format x1 x1 y1 y1 x2 x2 y2 y2 stops stops
@pytest fixture scope 'session' def unicode_encode_err return UnicodeEncodeError 'ascii' '' 0 2 'fakeexception'
def test_events_are_condensed api_client message ts int time time + 22 cursor get_cursor api_client ts message_id api_client get_data '/messages/' [0]['id']message_path '/messages/{}' format message_id api_client put_data message_path {'unread' True} api_client put_data message_path {'unread' False} api_client put_data message_path {'unread' True} sync_data api_client get_data '/delta?cursor {}' format cursor deltas sync_data['deltas']message_deltas [d for d in deltas if d['object'] 'message' ]assert len message_deltas 1 delta message_deltas[0]assert delta['object'] 'message' and delta['event'] 'modify' assert delta['attributes']['unread'] is True
def test_events_are_condensed api_client message ts int time time + 22 cursor get_cursor api_client ts message_id api_client get_data '/messages/' [0]['id']message_path '/messages/{}' format message_id api_client put_data message_path {'unread' True} api_client put_data message_path {'unread' False} api_client put_data message_path {'unread' True} sync_data api_client get_data '/delta?cursor {}' format cursor deltas sync_data['deltas']message_deltas [d for d in deltas if d['object'] 'message' ]assert len message_deltas 1 delta message_deltas[0]assert delta['object'] 'message' and delta['event'] 'modify' assert delta['attributes']['unread'] is True
def has_override_value val_name return BACKEND has_override_value val_name
def trigger_emails settings frappe get_doc u'DailyWorkSummarySettings' for d in settings companies if frappe utils nowtime split u' ' [0] d send_emails_at split u' ' [0] emails get_employee_emails d company if emails daily_work_summary frappe get_doc dict doctype u'DailyWorkSummary' company d company insert daily_work_summary send_mails settings emails
def trigger_emails settings frappe get_doc u'DailyWorkSummarySettings' for d in settings companies if frappe utils nowtime split u' ' [0] d send_emails_at split u' ' [0] emails get_employee_emails d company if emails daily_work_summary frappe get_doc dict doctype u'DailyWorkSummary' company d company insert daily_work_summary send_mails settings emails
def test_alnum arr for element in arr if not element isalnum return Falsereturn True
@only_cidef generate_requirements_txt failed_deps req_file file REQUIREMENTS_TXT 'w' if failed_deps for pkg in failed_deps if pkg is_git req_file write '%s\n' % pkg git_src else req_file write '%s %s\n' % pkg package_name pkg package_version req_file close return REQUIREMENTS_TXT
def put_response_to_local_cache url _our_resp without_content False if parse method 'GET' or _our_resp status_code 200 returndbgprint 'PuttingCache ' url 'without_content ' without_content if without_content our_resp copy copy _our_resp our_resp response Noneobj_size 0else our_resp _our_respobj_size len parse remote_response content last_modified parse remote_response headers get 'Last-Modified' None cache put_obj url our_resp expires get_expire_from_mime parse mime obj_size obj_size last_modified last_modified info_dict {'without_content' without_content 'last_modified' last_modified}
@register inclusion_tag 'utilities/render_custom_fields html' def render_custom_fields form return {'form' form}
@register inclusion_tag 'utilities/render_custom_fields html' def render_custom_fields form return {'form' form}
def test_suggestions keyhint key_config_stub key_config_stub set_bindings_for 'normal' OrderedDict [ 'aa' 'cmd-aa' 'ab' 'cmd-ab' 'aba' 'cmd-aba' 'abb' 'cmd-abb' 'xd' 'cmd-xd' 'xe' 'cmd-xe' ] keyhint update_keyhint 'normal' 'a' assert keyhint text expected_text 'a' 'yellow' 'a' 'cmd-aa' 'a' 'yellow' 'b' 'cmd-ab' 'a' 'yellow' 'ba' 'cmd-aba' 'a' 'yellow' 'bb' 'cmd-abb'
def setUnjellyableFactoryForClass classname copyFactory global unjellyableFactoryRegistryclassname _maybeClass classname unjellyableFactoryRegistry[classname] copyFactoryglobalSecurity allowTypes classname
def setUnjellyableFactoryForClass classname copyFactory global unjellyableFactoryRegistryclassname _maybeClass classname unjellyableFactoryRegistry[classname] copyFactoryglobalSecurity allowTypes classname
def _convert_minutes_seconds timeout in_seconds False return timeout if in_seconds else timeout * 60
def rot_axis3 theta ct cos theta st sin theta lil ct st 0 - st ct 0 0 0 1 return Matrix lil
def _CheckStatus status if status code search_service_pb SearchServiceError OK if status code in _ERROR_MAP raise _ERROR_MAP[status code ] status error_detail else raise InternalError status error_detail
def _CheckStatus status if status code search_service_pb SearchServiceError OK if status code in _ERROR_MAP raise _ERROR_MAP[status code ] status error_detail else raise InternalError status error_detail
def _get_source_sum source_hash file_path saltenv ret dict schemes u'salt' u'http' u'https' u'ftp' u'swift' u's3' u'file' invalid_hash_msg u"Sourcehash'{0}'formatisinvalid Itmustbeintheformat<hashtype> <hash>" format source_hash source_hash str source_hash source_hash_scheme _urlparse source_hash schemeif source_hash_scheme in schemes cached_hash_file __salt__[u'cp cache_file'] source_hash saltenv if not cached_hash_file raise CommandExecutionError u'Sourcehashfile{0}notfound' format source_hash ret __salt__[u'file extract_hash'] cached_hash_file u'' file_path if ret is None raise SaltInvocationError invalid_hash_msg else items source_hash split u' ' 1 if len items 2 invalid_hash_msg u'{0} oritmustbeasupportedprotocol {1}' format invalid_hash_msg u' ' join schemes raise SaltInvocationError invalid_hash_msg ret[u'hash_type'] ret[u'hsum'] [item strip lower for item in items]return ret
def redefined_by_decorator node if node decorators for decorator in node decorators nodes if isinstance decorator astroid Getattr and getattr decorator expr 'name' None node name return Truereturn False
def rewrite_exception data exc_data data get 'sentry interfaces Exception' if not exc_data return Falserv Falsefor exc in exc_data['values'] for processor in six itervalues error_processors try if processor try_process exc rv Truebreakexcept Exception as e logger error 'Failedtorunprocessor"%s" %s' processor vendor e exc_info True return rv
def _valid_method_call_check_resources resource method for name in resource keys _valid_method_call_check_resource name method
def is_callable obj if not callable obj raise ValueError 'Valuemustbeacallable' return True
def _compute_mne_loc coil_loc loc np zeros 12 if np linalg norm coil_loc['inner_coil'] 0 and np linalg norm coil_loc['outer_coil'] 0 return locloc[0 3] coil_loc['inner_coil'] / 39 370078 z_axis coil_loc['outer_coil'] - coil_loc['inner_coil'] R rotation3d_align_z_axis z_axis loc[3 13] R T reshape 9 return loc
def event_return events with _get_serv events commit True as cur for event in events tag event get 'tag' '' data event get 'data' '' sql 'INSERTINTO`salt_events` `tag` `data` `master_id` \nVALUES %s %s %s 'cur execute sql tag json dumps data __opts__['id']
def parse_with_bindops sentence grammar None trace 0 if not grammar grammar 'grammars/book_grammars/storage fcfg'parser load_parser grammar trace trace chart_class InstantiateVarsChart tokens sentence split return list parser parse tokens
def _reset_config app user factories Sysadmin env {'REMOTE_USER' user['name'] encode 'ascii' }app post url url_for controller 'admin' action 'reset_config' extra_environ env
def task_id_eq task_id body message return body[u'id'] task_id
def Enabled return not GlobalProcess IsDefault
def Enabled return not GlobalProcess IsDefault
def make_camera cam_type *args **kwargs cam_types {None BaseCamera}for camType in BaseCamera PanZoomCamera PerspectiveCamera TurntableCamera FlyCamera ArcballCamera cam_types[camType __name__[ -6 ] lower ] camTypetry return cam_types[cam_type] *args **kwargs except KeyError raise KeyError 'Unknowncameratype"%s" Optionsare %s' % cam_type cam_types keys
def make_camera cam_type *args **kwargs cam_types {None BaseCamera}for camType in BaseCamera PanZoomCamera PerspectiveCamera TurntableCamera FlyCamera ArcballCamera cam_types[camType __name__[ -6 ] lower ] camTypetry return cam_types[cam_type] *args **kwargs except KeyError raise KeyError 'Unknowncameratype"%s" Optionsare %s' % cam_type cam_types keys
def make_camera cam_type *args **kwargs cam_types {None BaseCamera}for camType in BaseCamera PanZoomCamera PerspectiveCamera TurntableCamera FlyCamera ArcballCamera cam_types[camType __name__[ -6 ] lower ] camTypetry return cam_types[cam_type] *args **kwargs except KeyError raise KeyError 'Unknowncameratype"%s" Optionsare %s' % cam_type cam_types keys
@pytest fixturedef import_fake monkeypatch fake ImportFake monkeypatch setattr 'builtins __import__' fake fake_import monkeypatch setattr 'qutebrowser utils version importlib import_module' fake fake_importlib_import return fake
def shared_dataset data_xy borrow True data_x data_y data_xyshared_x theano shared np asarray data_x dtype theano config floatX borrow borrow shared_y theano shared np asarray data_y dtype theano config floatX borrow borrow return shared_x T cast shared_y 'int32'
def shared_dataset data_xy borrow True data_x data_y data_xyshared_x theano shared np asarray data_x dtype theano config floatX borrow borrow shared_y theano shared np asarray data_y dtype theano config floatX borrow borrow return shared_x T cast shared_y 'int32'
def shared_dataset data_xy borrow True data_x data_y data_xyshared_x theano shared np asarray data_x dtype theano config floatX borrow borrow shared_y theano shared np asarray data_y dtype theano config floatX borrow borrow return shared_x T cast shared_y 'int32'
def shared_dataset data_xy borrow True data_x data_y data_xyshared_x theano shared np asarray data_x dtype theano config floatX borrow borrow shared_y theano shared np asarray data_y dtype theano config floatX borrow borrow return shared_x T cast shared_y 'int32'
def read handle record Nonetry line next handle record Record __read_names record line line next handle __read_threshold record line line next handle __read_lengths record line line next handle __read_profilewidth record line line next handle __read_scores record line except StopIteration if not record raise ValueError 'Norecordfoundinhandle' else raise ValueError 'Unexpectedendofstream ' for line in handle if not line strip continue__read_query_alignment record line try line next handle __read_positive_alignment record line line next handle __read_hit_alignment record line except StopIteration raise ValueError 'Unexpectedendofstream ' return record
def read handle record Nonetry line next handle record Record __read_names record line line next handle __read_threshold record line line next handle __read_lengths record line line next handle __read_profilewidth record line line next handle __read_scores record line except StopIteration if not record raise ValueError 'Norecordfoundinhandle' else raise ValueError 'Unexpectedendofstream ' for line in handle if not line strip continue__read_query_alignment record line try line next handle __read_positive_alignment record line line next handle __read_hit_alignment record line except StopIteration raise ValueError 'Unexpectedendofstream ' return record
def gemset_create ruby gemset runas None return _rvm_do ruby ['rvm' 'gemset' 'create' gemset] runas runas
def setup_platform hass config add_devices discovery_info None devs []for device in INSTEON devices if device DeviceCategory 'SwitchedLightingControl' devs append InsteonToggleDevice device if device DeviceCategory 'DimmableLightingControl' devs append InsteonToggleDevice device add_devices devs
def is_binary value binary_chars None if binary_chars is None binary_chars BINARY_CHARSfor binary_char in binary_chars if binary_char in value return Truereturn False
def PropertyTypeName value if value __class__ in _PROPERTY_MEANINGS meaning _PROPERTY_MEANINGS[value __class__]name entity_pb Property _Meaning_NAMES[meaning]return name lower replace '_' ' ' elif isinstance value basestring return 'string'elif isinstance value users User return 'user'elif isinstance value long return 'int'elif value is None return 'null'else return typename value lower
def get_user username None if username user models User objects get login username else user models User current_user return user
def edns_to_text flags return _to_text flags _edns_by_value _edns_flags_order
def a_product x y z 1 return x * y * z
def edxnotes cls if 'edxnotes' in sys modules from edxnotes decorators import edxnotes as notesreturn notes cls else return cls
def edxnotes cls if 'edxnotes' in sys modules from edxnotes decorators import edxnotes as notesreturn notes cls else return cls
def CollectObjectIDs ids obj if id obj in ids returnids add id obj if isinstance obj list tuple set frozenset for e in obj CollectObjectIDs ids e elif isinstance obj dict for k v in obj items CollectObjectIDs ids k CollectObjectIDs ids v return len ids
def CollectObjectIDs ids obj if id obj in ids returnids add id obj if isinstance obj list tuple set frozenset for e in obj CollectObjectIDs ids e elif isinstance obj dict for k v in obj items CollectObjectIDs ids k CollectObjectIDs ids v return len ids
def _compute_n_patches i_h i_w p_h p_w max_patches None n_h i_h - p_h + 1 n_w i_w - p_w + 1 all_patches n_h * n_w if max_patches if isinstance max_patches numbers Integral and max_patches < all_patches return max_patcheselif isinstance max_patches numbers Real and 0 < max_patches < 1 return int max_patches * all_patches else raise ValueError 'Invalidvalueformax_patches %r' % max_patches else return all_patches
def get_stay_open return stay_open
def s3_role_required T current Tgtable current auth settings table_grouprepresent S3Represent lookup 'auth_group' fields ['role'] f S3ReusableField 'role_required' gtable sortby 'role' requires IS_EMPTY_OR IS_ONE_OF current db 'auth_group id' represent zero T 'Public' represent represent label T 'RoleRequired' comment DIV _class 'tooltip' _title '%s %s' % T 'RoleRequired' T 'Ifthisrecordshouldberestrictedthenselectwhichroleisrequiredtoaccesstherecordhere ' ondelete 'RESTRICT' return f
def get_program_types user None catalog_integration CatalogIntegration current if catalog_integration enabled user _get_service_user user catalog_integration service_username if not user return []api create_catalog_api_client user catalog_integration cache_key '{base} program_types' format base catalog_integration CACHE_KEY return get_edx_api_data catalog_integration user 'program_types' cache_key cache_key if catalog_integration is_cache_enabled else None api api else return []
def ThrottleRate throttle_dict max_count time_period now GetCurrentTimestamp if not throttle_dict or now > throttle_dict['start_time'] + time_period throttle_dict {'start_time' now 'count' 0}else throttle_dict {'start_time' throttle_dict['start_time'] 'count' throttle_dict['count']}if throttle_dict['count'] > max_count return throttle_dict True throttle_dict['count'] + 1return throttle_dict False
@pytest mark parametrize 'env_id' test_envs def test_smoke env_id gym undo_logger_setup logging getLogger setLevel logging INFO env gym make env_id env wrappers Unvectorize env if os environ get 'FORCE_LATEST_UNIVERSE_DOCKER_RUNTIMES' configure_with_latest_docker_runtime_tag env else env configure remotes 1 env reset _rollout env timestep_limit 60 * 30
@pytest mark parametrize 'env_id' test_envs def test_smoke env_id gym undo_logger_setup logging getLogger setLevel logging INFO env gym make env_id env wrappers Unvectorize env if os environ get 'FORCE_LATEST_UNIVERSE_DOCKER_RUNTIMES' configure_with_latest_docker_runtime_tag env else env configure remotes 1 env reset _rollout env timestep_limit 60 * 30
def test_oldclass_newclass_construction class nc object passclass oc passnewType type oc __new__ type oc 'foo' nc oc {} AreEqual type newType type
def _detect_django_path module_path result []for parent in traverse_parents module_path with common ignored IOError with open parent + os path sep + 'manage py' debug dbg 'Founddjangopath %s' module_path result append parent return result
def badges_enabled return settings FEATURES get 'ENABLE_OPENBADGES' False
@core_helperdef resource_view_is_iframed resource_view view_plugin datapreview get_view_plugin resource_view['view_type'] return view_plugin info get 'iframed' True
@core_helperdef resource_view_is_iframed resource_view view_plugin datapreview get_view_plugin resource_view['view_type'] return view_plugin info get 'iframed' True
def parseBoolValue value fail_on_errors True preserve_none False if not isinstance value string_type if preserve_none and value is None return valuereturn bool value elif preserve_none and value lower u'none' return Noneelif value lower in u'true' u'yes' u'y' u'on' u'1' return Trueelif value lower in u'false' u'no' u'n' u'off' u'0' u'none' return Falseelif fail_on_errors raise ValueError u'Cannotparseboolvalue %r' % value
def setup_platform hass config add_devices discovery_info None data SenseHatData dev []for variable in config[CONF_DISPLAY_OPTIONS] dev append SenseHatSensor data variable add_devices dev
def resize_quota_delta context new_flavor old_flavor sense compare def _quota_delta resource return sense * new_flavor[resource] - old_flavor[resource] deltas {}if compare * _quota_delta 'vcpus' > 0 deltas['cores'] _quota_delta 'vcpus' if compare * _quota_delta 'memory_mb' > 0 deltas['ram'] _quota_delta 'memory_mb' return deltas
def df2idf docfreq totaldocs log_base 2 0 add 0 0 return add + math log 1 0 * totaldocs / docfreq log_base
def df2idf docfreq totaldocs log_base 2 0 add 0 0 return add + math log 1 0 * totaldocs / docfreq log_base
def mapping_get index doc_type hosts None profile None es _get_instance hosts profile try ret es indices get_mapping index index doc_type doc_type return retexcept elasticsearch exceptions NotFoundError return Nonereturn None
def sorting_score start advertised_start announcement announcement start now sorting_dates start advertised_start announcement scale 300 0if announcement days now - announcement daysscore - exp - days / scale else days now - start daysscore exp days / scale return score
def sorting_score start advertised_start announcement announcement start now sorting_dates start advertised_start announcement scale 300 0if announcement days now - announcement daysscore - exp - days / scale else days now - start daysscore exp days / scale return score
def applyFilter normalized_uri xrd_data flt None flt mkFilter flt et parseXRDS xrd_data endpoints []for service_element in iterServices et endpoints extend flt getServiceEndpoints normalized_uri service_element return endpoints
def create_digest_traverser cloudtrail_client s3_client_provider trail_arn trail_source_region None on_invalid None on_gap None on_missing None bucket None prefix None assert_cloudtrail_arn_is_valid trail_arn account_id get_account_id_from_arn trail_arn if bucket is None trail_info get_trail_by_arn cloudtrail_client trail_arn LOG debug 'Loadedtrailinfo %s' trail_info bucket trail_info['S3BucketName']prefix trail_info get 'S3KeyPrefix' None trail_region trail_arn split ' ' [3]trail_name trail_arn split '/' [ -1 ]digest_provider DigestProvider account_id account_id trail_name trail_name s3_client_provider s3_client_provider trail_source_region trail_source_region trail_home_region trail_region return DigestTraverser digest_provider digest_provider starting_bucket bucket starting_prefix prefix on_invalid on_invalid on_gap on_gap on_missing on_missing public_key_provider PublicKeyProvider cloudtrail_client
def _verify_query_segregation query auth_project None auth_project auth_project or rbac get_limited_to_project pecan request headers if not auth_project returnfor q in query if q field in 'project' 'project_id' and auth_project q value raise base ProjectNotAuthorized q value
def test_b_wien from import b_wienfrom import units as ut 5778 * u K w b_wien / t to u nm assert round w value 502
def test_b_wien from import b_wienfrom import units as ut 5778 * u K w b_wien / t to u nm assert round w value 502
def ParseKeyFilteredQuery filters orders remaining_filters []key_range ValueRange key_prop datastore_types KEY_SPECIAL_PROPERTYfor f in filters op f op if not f property_size 1 and f property 0 name key_prop and not op datastore_pb Query_Filter IN or op datastore_pb Query_Filter EXISTS remaining_filters append f continueval f property 0 value Check val has_referencevalue '__key__kindmustbecomparedtoakey' limit datastore_types FromReferenceProperty val key_range Update op limit remaining_orders []for o in orders if not o direction datastore_pb Query_Order ASCENDING and o property datastore_types KEY_SPECIAL_PROPERTY remaining_orders append o else breakCheck not remaining_filters 'Onlycomparisonfilterson' + key_prop + 'supported' Check not remaining_orders 'Onlyascendingorderon' + key_prop + 'supported' return key_range
def ParseKeyFilteredQuery filters orders remaining_filters []key_range ValueRange key_prop datastore_types KEY_SPECIAL_PROPERTYfor f in filters op f op if not f property_size 1 and f property 0 name key_prop and not op datastore_pb Query_Filter IN or op datastore_pb Query_Filter EXISTS remaining_filters append f continueval f property 0 value Check val has_referencevalue '__key__kindmustbecomparedtoakey' limit datastore_types FromReferenceProperty val key_range Update op limit remaining_orders []for o in orders if not o direction datastore_pb Query_Order ASCENDING and o property datastore_types KEY_SPECIAL_PROPERTY remaining_orders append o else breakCheck not remaining_filters 'Onlycomparisonfilterson' + key_prop + 'supported' Check not remaining_orders 'Onlyascendingorderon' + key_prop + 'supported' return key_range
def get_object_info env app path None swift_source None version account container obj split_path path or env['PATH_INFO'] 4 4 True info _get_object_info app env account container obj swift_source swift_source if info info deepcopy info else info headers_to_object_info {} 0 for field in 'length' if info get field is None info[field] 0else info[field] int info[field] return info
def modelform_factory model form ModelForm fields None exclude None formfield_callback None widgets None attrs {u'model' model}if fields is not None attrs[u'fields'] fieldsif exclude is not None attrs[u'exclude'] excludeif widgets is not None attrs[u'widgets'] widgetsparent object if hasattr form u'Meta' parent form Meta object Meta type str u'Meta' parent attrs class_name model __name__ + str u'Form' form_class_attrs {u'Meta' Meta u'formfield_callback' formfield_callback}return type form class_name form form_class_attrs
def modelform_factory model form ModelForm fields None exclude None formfield_callback None widgets None attrs {u'model' model}if fields is not None attrs[u'fields'] fieldsif exclude is not None attrs[u'exclude'] excludeif widgets is not None attrs[u'widgets'] widgetsparent object if hasattr form u'Meta' parent form Meta object Meta type str u'Meta' parent attrs class_name model __name__ + str u'Form' form_class_attrs {u'Meta' Meta u'formfield_callback' formfield_callback}return type form class_name form form_class_attrs
def Record name object import gencacheobject gencache EnsureDispatch object module sys modules[object __class__ __module__]package gencache GetModuleForTypelib module CLSID module LCID module MajorVersion module MinorVersion try struct_guid package RecordMap[name]except KeyError raise ValueError "Thestructure'%s'isnotdefinedinmodule'%s'" % name package return pythoncom GetRecordFromGuids module CLSID module MajorVersion module MinorVersion module LCID struct_guid
def test_main _test_function_names gl _test_constant_names gl
def text_dialog text title parent qtutils active_window label QtWidgets QLabel parent label setFont qtutils diff_font label setText text label setTextInteractionFlags Qt NoTextInteraction widget QtWidgets QDialog parent widget setWindowModality Qt WindowModal widget setWindowTitle title layout qtutils hbox defs margin defs spacing label widget setLayout layout qtutils add_action widget N_ u'Close' widget accept Qt Key_Question Qt Key_Enter Qt Key_Return widget show return widget
def text_dialog text title parent qtutils active_window label QtWidgets QLabel parent label setFont qtutils diff_font label setText text label setTextInteractionFlags Qt NoTextInteraction widget QtWidgets QDialog parent widget setWindowModality Qt WindowModal widget setWindowTitle title layout qtutils hbox defs margin defs spacing label widget setLayout layout qtutils add_action widget N_ u'Close' widget accept Qt Key_Question Qt Key_Enter Qt Key_Return widget show return widget
def addbroadcast x *axes rval Rebroadcast *[ axis True for axis in axes] x return theano tensor opt apply_rebroadcast_opt rval
def get_sorcery_ver module cmd_sorcery '%s--version' % SORCERY['sorcery'] rc stdout stderr module run_command cmd_sorcery if rc 0 or not stdout module fail_json msg 'unabletogetSorceryversion' return stdout strip
def split s keep False lexer ShellLexer s lexer keep keeptokens list lexer if not tokens return []out []spaces ''log shlexer vdebug '{ r}->{ r}' format s tokens for t in tokens if t isspace spaces + telse out append spaces + t spaces ''if spaces out append spaces return out
def loop active timeout None use_poll False errCount 0if timeout is None timeout Utils DEFAULT_SLEEP_TIMEpoll asyncore pollif use_poll and asyncore poll2 and hasattr asyncore select 'poll' logSys debug 'Serverlistener select usespoll' timeout float timeout / 1000 poll asyncore poll2while active try poll timeout if errCount errCount - 1except Exception as e if not active breakerrCount + 1if errCount < 20 if e args[0] in errno ENOTCONN errno EBADF logSys info 'Serverconnectionwasclosed %s' str e else logSys error 'Serverconnectionwasclosed %s' str e elif errCount 20 logSys info 'Toomanyerrors-stoploggingconnectionerrors' logSys exception e
def loop active timeout None use_poll False errCount 0if timeout is None timeout Utils DEFAULT_SLEEP_TIMEpoll asyncore pollif use_poll and asyncore poll2 and hasattr asyncore select 'poll' logSys debug 'Serverlistener select usespoll' timeout float timeout / 1000 poll asyncore poll2while active try poll timeout if errCount errCount - 1except Exception as e if not active breakerrCount + 1if errCount < 20 if e args[0] in errno ENOTCONN errno EBADF logSys info 'Serverconnectionwasclosed %s' str e else logSys error 'Serverconnectionwasclosed %s' str e elif errCount 20 logSys info 'Toomanyerrors-stoploggingconnectionerrors' logSys exception e
def _get_aliases context data_dict res_id data_dict['resource_id']alias_sql sqlalchemy text u'SELECTnameFROM"_table_metadata"WHEREalias_of id' results context['connection'] execute alias_sql id res_id fetchall return [x[0] for x in results]
def egquery **keywds cgi 'https //eutils ncbi nlm nih gov/entrez/eutils/egquery fcgi'variables {}variables update keywds return _open cgi variables
def egquery **keywds cgi 'https //eutils ncbi nlm nih gov/entrez/eutils/egquery fcgi'variables {}variables update keywds return _open cgi variables
@open_file 1 mode 'wt' def write_sparse6 G path nodes None header True path write generate_sparse6 G nodes nodes header header path write '\n'
def handle_nan value if isinstance value float if value float 'inf' return '<inf>'if value float '-inf' return '<-inf>'if value value return '<nan>'return value
def _get_chance_level scorer y_train if scorer __name__ 'accuracy_score' chance np max [np mean y_train c for c in np unique y_train ] elif scorer __name__ 'roc_auc_score' chance 0 5else chance np nanwarn 'Cannotfindchancelevelfrom%s specifychancelevel' % scorer __name__ return chance
def secure_cookie return request environ['wsgi url_scheme'] 'https'
def secure_cookie return request environ['wsgi url_scheme'] 'https'
def json_to_dict x if x find 'callback' > -1 pos_lb x find '{' pos_rb x find '}' x x[pos_lb pos_rb + 1 ]try if type x str x x decode 'utf-8' return json loads x encoding 'utf-8' except return x
def json_to_dict x if x find 'callback' > -1 pos_lb x find '{' pos_rb x find '}' x x[pos_lb pos_rb + 1 ]try if type x str x x decode 'utf-8' return json loads x encoding 'utf-8' except return x
def json_to_dict x if x find 'callback' > -1 pos_lb x find '{' pos_rb x find '}' x x[pos_lb pos_rb + 1 ]try if type x str x x decode 'utf-8' return json loads x encoding 'utf-8' except return x
def json_to_dict x if x find 'callback' > -1 pos_lb x find '{' pos_rb x find '}' x x[pos_lb pos_rb + 1 ]try if type x str x x decode 'utf-8' return json loads x encoding 'utf-8' except return x
def color_style if not supports_color style no_style else DJANGO_COLORS os environ get 'DJANGO_COLORS' '' color_settings termcolors parse_color_setting DJANGO_COLORS if color_settings class dummy passstyle dummy for role in termcolors PALETTES[termcolors NOCOLOR_PALETTE] format color_settings get role {} setattr style role termcolors make_style **format style ERROR_OUTPUT style ERRORelse style no_style return style
def error_msg text msg colorize 'Error ' + str text 'red'
def reset_notifier global _notifier_notifier None
def provider name def wrapper cls def wrapped init def __wrapped_init__ self *args **kwargs 'Initializethewrappedobjectandaddittotheregistry 'init self *args **kwargs REGISTRY[name] selfreturn __wrapped_init__cls __init__ wrapped cls __init__ return clsreturn wrapper
def print_tag tag decode outstream sys stdout outstream write 'Tagger ' + decode tag tagger + '\n' outstream write 'Date ' + decode tag tag_time + '\n' outstream write '\n' outstream write decode tag message + '\n' outstream write '\n'
def waic trace model None n_eff False pointwise False model modelcontext model log_py log_post_trace trace model lppd_i logsumexp log_py axis 0 b 1 0 / log_py shape[0] vars_lpd np var log_py axis 0 if np any vars_lpd > 0 4 warnings warn 'Foroneormoresamplestheposteriorvarianceofthe\nlogpredictivedensitiesexceeds0 4 Thiscouldbeindicationof\nWAICstartingtofailseehttp //arxiv org/abs/1507 04544fordetails\n' waic_i -2 * lppd_i - vars_lpd waic_se np sqrt len waic_i * np var waic_i waic np sum waic_i p_waic np sum vars_lpd if n_eff return waic waic_se p_waic elif pointwise return waic waic_se waic_i p_waic else return waic waic_se
def get_meta module cf container src dest c _get_container module cf container objs Noneif src and dest module fail_json msg 'Error ambiguousinstructions filestobedeletedhavebeenspecifiedonbothsrcanddestargs' elif dest objs destelse objs srcif objs objs objs split ' ' objs map str strip objs else objs c get_object_names results dict for obj in objs try meta c get_object obj get_metadata except Exception as e module fail_json msg e message else results[obj] dict for k v in meta items meta_key k split META_PREFIX [ -1 ]results[obj][meta_key] vEXIT_DICT['container'] c nameif results EXIT_DICT['meta_results'] resultsEXIT_DICT['success'] Truemodule exit_json **EXIT_DICT
def OpenDocumentChart doc OpenDocument 'application/vnd oasis opendocument chart' doc chart Chart doc body addElement doc chart return doc
def test_basic_call_on_method_registering_without_decorator_coroutine class API object def __init__ self hug call self hello_world_method @asyncio coroutinedef hello_world_method self return 'HelloWorld 'api_instance API assert loop run_until_complete api_instance hello_world_method 'HelloWorld ' assert hug test get api '/hello_world_method' data 'HelloWorld '
def get_array_section_has_problem course_id course modulestore get_course course_id depth 4 b_section_has_problem [False] * len course get_children i 0for section in course get_children for subsection in section get_children for unit in subsection get_children for child in unit get_children if child location category 'problem' b_section_has_problem[i] Truebreakif b_section_has_problem[i] breakif b_section_has_problem[i] breaki + 1return b_section_has_problem
def set_output_volume volume cmd 'osascript-e"setvolumeoutputvolume{0}"' format volume call __salt__['cmd run_all'] cmd output_loglevel 'debug' python_shell False _check_cmd call return get_output_volume
def usergroup_create name **connection_args conn_args _login **connection_args try if conn_args method 'usergroup create'params {'name' name}params _params_extend params **connection_args ret _query method params conn_args['url'] conn_args['auth'] return ret['result']['usrgrpids']else raise KeyErrorexcept KeyError return ret
def __virtual__ if 'rbac profile_list' in __salt__ and 'user list_users' in __salt__ and __grains__['kernel'] 'SunOS' return Trueelse return False '{0}statemodulecanonlybeloadedonSolaris' format __virtualname__
def _clear_actions name location '\\' if name not in list_tasks location return '{0}notfoundin{1}' format name location pythoncom CoInitialize task_service win32com client Dispatch 'Schedule Service' task_service Connect task_folder task_service GetFolder location task_definition task_folder GetTask name Definitionactions task_definition Actionsactions Clear return _save_task_definition name name task_folder task_folder task_definition task_definition user_name task_definition Principal UserID password None logon_type task_definition Principal LogonType
def test_shared_cudandarray a cuda shared_constructor cuda CudaNdarray zeros 2 3 assert isinstance a type tcn CudaNdarrayType
def test_preserve_refs text u'laphilologiem&#x00e8 neaupire'assert strip_tags text u'laphilologiem\xe8neaupire' text u'laphilologiem&#232 neaupire'assert strip_tags text u'laphilologiem\xe8neaupire' text u'veer&amp wander'assert strip_tags text 'veer&wander'
def binary_partitions n from math import ceil logpow int 2 ** ceil log n 2 sum 0partition []while pow if sum + pow < n partition append pow sum + powpow >> 1last_num len partition - 1 - n & 1 while last_num > 0 yield partition if partition[last_num] 2 partition[last_num] 1partition append 1 last_num - 1continuepartition append 1 partition[last_num] >> 1x partition[ last_num + 1 ] partition[last_num]last_num + 1while x > 1 if x < len partition - last_num - 1 del partition[ - x + 1 ]last_num + 1partition[last_num] xelse x >> 1 yield [1] * n
def weAreFrozen return hasattr sys 'frozen'
def results_extractor train_obj return DD
def _PPIGuessPayloadClass p **kargs if len p > 4 t pfh_len struct unpack '<HH' p[ 4] cls getPPIType t 'default' pfh_len + 4out cls p[ pfh_len] **kargs if out payload out payload conf raw_layer out payload load if len p > pfh_len out payload payload conf padding_layer p[pfh_len ] elif len p > pfh_len out payload conf padding_layer p[pfh_len ] else out conf raw_layer p **kargs return out
def _PPIGuessPayloadClass p **kargs if len p > 4 t pfh_len struct unpack '<HH' p[ 4] cls getPPIType t 'default' pfh_len + 4out cls p[ pfh_len] **kargs if out payload out payload conf raw_layer out payload load if len p > pfh_len out payload payload conf padding_layer p[pfh_len ] elif len p > pfh_len out payload conf padding_layer p[pfh_len ] else out conf raw_layer p **kargs return out
def test_album_info *args **kwargs track_info TrackInfo title u'newtitle' track_id u'trackid' index 0 album_info AlbumInfo artist u'artist' album u'album' tracks [track_info] album_id u'albumid' artist_id u'artistid' return iter [album_info]
def _normalizeargs sequence output None if output is None output []cls sequence __class__if InterfaceClass in cls __mro__ or Implements in cls __mro__ output append sequence else for v in sequence _normalizeargs v output return output
def _normalizeargs sequence output None if output is None output []cls sequence __class__if InterfaceClass in cls __mro__ or Implements in cls __mro__ output append sequence else for v in sequence _normalizeargs v output return output
def delete name force False region None key None keyid None profile None conn _get_conn region region key key keyid keyid profile profile try conn delete_auto_scaling_group name force msg 'Deletedautoscalegroup{0} ' format name log info msg return Trueexcept boto exception BotoServerError as e log debug e msg 'Failedtodeleteautoscalegroup{0}' format name log error msg return False
def edit sheet open_with_editor path sheet
def fmt_highlights raw_value value unit if unit is None return valuehighlights highlight_string raw_value unit start_search 0for highlight in highlights htext escape force_text highlight[2] find_highlight value find htext start_search if find_highlight > 0 newpart HL_CHECK format htext next_part value[ find_highlight + len htext ]value value[ find_highlight] + newpart + next_part start_search find_highlight + len newpart return value
def find_missing input_list total len input_list * len input_list + 1 / 2 summed 0for element in input_list summed + elementmissing total - summed return missing
def divmod_by_constant builder val divisor assert divisor > 0 divisor val type divisor one val type 1 quot alloca_once builder val type with builder if_else is_neg_int builder val as if_neg if_pos with if_pos quot_val builder sdiv val divisor builder store quot_val quot with if_neg val_plus_one builder add val one quot_val builder sdiv val_plus_one divisor builder store builder sub quot_val one quot quot_val builder load quot rem_val builder sub val builder mul quot_val divisor return quot_val rem_val
def attach_issue_custom_attributes queryset as_field 'issue_custom_attributes_attr' model queryset modelsql '\nSELECTjson_agg \nrow_to_json custom_attributes_issuecustomattribute \nORDERBYcustom_attributes_issuecustomattribute order\n \nFROMcustom_attributes_issuecustomattribute\nWHEREcustom_attributes_issuecustomattribute project_id {tbl} id\n'sql sql format tbl model _meta db_table queryset queryset extra select {as_field sql} return queryset
def bellman_ford_predecessor_and_distance G source target None cutoff None weight 'weight' if source not in G raise nx NodeNotFound 'Node%sisnotfoundinthegraph' % source weight _weight_function G weight if any weight u v d < 0 for u v d in G selfloop_edges data True raise nx NetworkXUnbounded 'Negativecostcycledetected ' dist {source 0}pred {source [None]}if len G 1 return pred dist weight _weight_function G weight return pred _bellman_ford G [source] weight pred pred dist dist cutoff cutoff target target
def bellman_ford_predecessor_and_distance G source target None cutoff None weight 'weight' if source not in G raise nx NodeNotFound 'Node%sisnotfoundinthegraph' % source weight _weight_function G weight if any weight u v d < 0 for u v d in G selfloop_edges data True raise nx NetworkXUnbounded 'Negativecostcycledetected ' dist {source 0}pred {source [None]}if len G 1 return pred dist weight _weight_function G weight return pred _bellman_ford G [source] weight pred pred dist dist cutoff cutoff target target
@command 'save' def save_last if g last_opened open_save_view 'save' g last_opened else saveas ''if g model saveas g model[0] title[ 18] strip saveas re sub '[^-\\w]' '-' saveas re UNICODE post 0while g userpl get saveas post + 1saveas g model[0] title[ 18] strip + '-' + str post saveas saveas lstrip '0123456789' open_save_view 'save' saveas
@pytest fixturedef admin from django contrib auth import get_user_modelreturn get_user_model objects get username 'admin'
def libvlc_video_get_teletext p_mi f _Cfunctions get 'libvlc_video_get_teletext' None or _Cfunction 'libvlc_video_get_teletext' 1 None ctypes c_int MediaPlayer return f p_mi
def iterModules return theSystemPath iterModules
def iterModules return theSystemPath iterModules
def test_resize qtbot label TextBase qtbot add_widget label long_string 'Helloworld ' * 20 label setText long_string with qtbot waitExposed label label show text_1 label _elided_textlabel resize 20 50 text_2 label _elided_textassert text_1 text_2
def setup_platform hass config add_devices discovery_info None name config get CONF_NAME plex_user config get CONF_USERNAME plex_password config get CONF_PASSWORD plex_server config get CONF_SERVER plex_host config get CONF_HOST plex_port config get CONF_PORT plex_url 'http //{} {}' format plex_host plex_port add_devices [PlexSensor name plex_url plex_user plex_password plex_server ]
def mkFilter parts if parts is None parts [BasicServiceEndpoint]try parts list parts except TypeError return mkCompoundFilter [parts] else return mkCompoundFilter parts
def resolve obj path if path for name in path split ' ' obj getattr obj name None return obj
def dlimport_workdir basedir return tempfile mkdtemp dir basedir
def getInteriorOverhangRadians elementNode return math radians getInteriorOverhangAngle elementNode
def getFirstTranslatorFileNameUnmodified fileName if fileName '' return fileNameunmodified getGNUTranslatorFilesUnmodified if len unmodified 0 print 'Therearenounmodifiedgcodefilesinthisfolder 'return ''return unmodified[0]
def getFirstTranslatorFileNameUnmodified fileName if fileName '' return fileNameunmodified getGNUTranslatorFilesUnmodified if len unmodified 0 print 'Therearenounmodifiedgcodefilesinthisfolder 'return ''return unmodified[0]
def holdAcknowledge a TpPd pd 3 b MessageType mesType 25 packet a / b return packet
@pytest mark parametrize 'parallel' [True False] def test_rstrip_whitespace parallel read_basic text '1 2 DCTB 3\nA DCTB B C DCTB DCTB \n DCTB a b c\n'table read_basic text delimiter ' ' parallel parallel expected Table [['A' 'a'] ['B' 'b'] ['C' 'c']] names '1' '2' '3' assert_table_equal table expected
def filter_services services filtered_services services[ ]for service_name in services mod __import__ 'services ' + service_name globals globals locals locals fromlist ['Service'] level -1 service mod Serviceif service required_features is not None and bs4 builder_registry lookup *service required_features is None logger warning u'Service%snotavailable noneofavailablefeaturescouldbeused Oneof%rrequired' % service_name service required_features filtered_services remove service_name return filtered_services
@open_file 0 mode 'rb' def read_gpickle path return pickle load path
def functest_builder method func def do_test self method self func return do_test
def makescript filename compiler dirname os path split filename [0]if not os access dirname os X_OK os mkdir dirname 493 fp open filename 'w' fp write SCRIPT % compiler fp close os chmod filename 493 print 'fixapplepython23 Created' filename
def _get_external_workers worker worker_that_blocked_task collections defaultdict set get_work_response_history worker _get_work_response_historyfor get_work_response in get_work_response_history if get_work_response['task_id'] is None for running_task in get_work_response['running_tasks'] other_worker_id running_task['worker']other_task_id running_task['task_id']other_task worker _scheduled_tasks get other_task_id if other_worker_id worker _id or not other_task continueworker_that_blocked_task[other_worker_id] add other_task return worker_that_blocked_task
@pytest fixturedef it_tutorial_po po_directory settings italian_tutorial return _require_store italian_tutorial settings POOTLE_TRANSLATION_DIRECTORY 'tutorial po'
def FindRegisterPythonExe exeAlias searchPaths actualFileNames None import regutil string fname ok FindPythonExe exeAlias actualFileNames searchPaths if not ok regutil RegisterPythonExe fname exeAlias return fname
def db_init db conn_name global db_readyif db_ready count conn_name < 1 db execute 'createtableifnotexistsseen_user name time quote chan host primarykey name chan ' db commit db_ready append conn_name
def db_init db conn_name global db_readyif db_ready count conn_name < 1 db execute 'createtableifnotexistsseen_user name time quote chan host primarykey name chan ' db commit db_ready append conn_name
def _read_float32 f return np float32 struct unpack '>f' f read 4 [0]
@main command @click option '-b' '--bundle' default 'quantopian-quandl' metavar 'BUNDLE-NAME' show_default True help 'Thedatabundletoingest ' @click option '--assets-version' type int multiple True help 'Versionoftheassetsdbtowhichtodowngrade ' @click option '--show-progress/--no-show-progress' default True help 'Printprogressinformationtotheterminal ' def ingest bundle assets_version show_progress bundles_module ingest bundle os environ pd Timestamp utcnow assets_version show_progress
def generate_settings dev False context {'secret_key' generate_secret_key 'debug_flag' dev 'mail backend' 'console' if dev else 'smtp' }py load_config_template DEFAULT_SETTINGS_OVERRIDE 'default' % context yaml load_config_template DEFAULT_SETTINGS_CONF 'default' % context return py yaml
def create_gs_thumbnail layer overwrite False params {'layers' layer typename encode 'utf-8' 'format' 'image/png8' 'width' 200 'height' 150 'TIME' '-99999999999-01-01T00 00 00 0Z/99999999999-01-01T00 00 00 0Z'}if None not in layer bbox params['bbox'] layer bbox_stringp '&' join '%s %s' % item for item in params items thumbnail_remote_url ogc_server_settings PUBLIC_LOCATION + 'wms/reflect?' + p thumbnail_create_url ogc_server_settings LOCATION + 'wms/reflect?' + p create_thumbnail layer thumbnail_remote_url thumbnail_create_url ogc_client http_client overwrite overwrite
def findBiggest root if root right is None return rootelse return findBiggest root right
def compile_fnclex context codegen context codegen library codegen create_library 'kb982107' ir_mod '\ndefinevoid@fnclex {\ncallvoidasmsideeffect"fnclex" "" \nretvoid\n}\n'll initialize_native_asmparser library add_llvm_module ll parse_assembly ir_mod library finalize return library
def _single_spectrum_helper x mode Fs None window None pad_to None sides None if mode is None or mode u'psd' or mode u'default' raise ValueError u'_single_spectrum_helperdoesnotworkwith%smode' % mode if pad_to is None pad_to len x spec freqs _ _spectral_helper x x y None NFFT len x Fs Fs detrend_func detrend_none window window noverlap 0 pad_to pad_to sides sides scale_by_freq False mode mode if mode u'complex' spec spec realif spec ndim 2 and spec shape[1] 1 spec spec[ 0]return spec freqs
def _babel_locale locale return locale replace '-' '_'
def django_to_webob_request django_request return DjangoWebobRequest django_request
def django_to_webob_request django_request return DjangoWebobRequest django_request
@pytest fixturedef fake_keyconfig bindings dict BINDINGS fake_keyconfig mock Mock spec ['get_bindings_for'] fake_keyconfig get_bindings_for side_effect lambda s bindings[s] objreg register 'key-config' fake_keyconfig yield bindings objreg delete 'key-config'
def end_recording status firepython_set_extension_data None if firepython_set_extension_data is not None warnings warn 'Firepythonisnolongersupported' rec recorder_proxy get_for_current_request recorder_proxy clear_for_current_request if config DEBUG logging debug 'Clearedrecorder' if rec is not None try rec record_http_status status rec save finally memcache delete lock_key namespace config KEY_NAMESPACE
def end_recording status firepython_set_extension_data None if firepython_set_extension_data is not None warnings warn 'Firepythonisnolongersupported' rec recorder_proxy get_for_current_request recorder_proxy clear_for_current_request if config DEBUG logging debug 'Clearedrecorder' if rec is not None try rec record_http_status status rec save finally memcache delete lock_key namespace config KEY_NAMESPACE
def end_recording status firepython_set_extension_data None if firepython_set_extension_data is not None warnings warn 'Firepythonisnolongersupported' rec recorder_proxy get_for_current_request recorder_proxy clear_for_current_request if config DEBUG logging debug 'Clearedrecorder' if rec is not None try rec record_http_status status rec save finally memcache delete lock_key namespace config KEY_NAMESPACE
def clear_static_cache path fully_qualified_path os path join django conf settings WEB_ROOT path try new_temp_path tempfile mkdtemp dir django conf settings WEB_ROOT + '/' except OSError as e if e errno 28 shutil rmtree fully_qualified_path returntry os rename fully_qualified_path os path join new_temp_path 'old' except OSError as e if e errno 2 passelse raiseshutil rmtree new_temp_path
@taskdef build_docs with cd '/home/vagrant/repos/sympy' run 'mkdir-pdist' venv '/home/vagrant/docs-virtualenv'make_virtualenv venv dependencies ['sphinx 1 1 3' 'numpy' 'mpmath'] with virtualenv venv with cd '/home/vagrant/repos/sympy/doc' run 'makeclean' run 'makehtml-errors' run 'makeman' with cd '/home/vagrant/repos/sympy/doc/_build' run 'mvhtml{html-nozip}' format **tarball_formatter run 'zip-9lr{html}{html-nozip}' format **tarball_formatter run 'cp{html} / /dist/' format **tarball_formatter run 'makeclean' run 'makelatex' with cd '/home/vagrant/repos/sympy/doc/_build/latex' run 'make' run 'cp{pdf-orig} / / /dist/{pdf}' format **tarball_formatter
def use_wildcard_certificate bench_path ret from bench config common_site_config import get_configconfig get_config bench_path bench_path wildcard config get 'wildcard' if not wildcard returndomain wildcard['domain']ssl_certificate wildcard['ssl_certificate']ssl_certificate_key wildcard['ssl_certificate_key']if domain startswith '* ' domain domain[1 ]else domain ' ' + domain for site in ret if site get 'ssl_certificate' continueif site get 'domain' or site['name'] endswith domain site['ssl_certificate'] ssl_certificatesite['ssl_certificate_key'] ssl_certificate_keysite['wildcard'] 1
def simplify_mul tree neg inputs treeif isinstance inputs list s_inputs []for s_i in imap simplify_mul inputs if s_i[1] is None neg ^ s_i[0]else s_inputs append s_i if not s_inputs rval [neg None]elif len s_inputs 1 s_inputs[0][0] ^ negrval s_inputs[0]else rval [neg s_inputs]else rval treereturn rval
def start_interactive_mode result debuggers result debuggersdescrs result error_descrs + result fail_descrs if len debuggers 1 debuggers[0] start else while True testindex 0print 'Chooseatesttodebug ' print '\n' join [ ' DCTB %s %s' % i descr for i _ descr in enumerate descrs ] print "Type'exit' or^D toquit" print try todebug input 'Enteratestname ' if todebug strip lower 'exit' print breakelse try testindex int todebug debugger debuggers[descrs[testindex][0]]except ValueError IndexError print 'ERROR invalidtestnumber%r' % todebug else debugger start except EOFError KeyboardInterrupt print break
def set_default sld tld opts salt utils namecheap get_opts 'namecheap domains dns setDefault' opts['SLD'] sldopts['TLD'] tldresponse_xml salt utils namecheap post_request opts if response_xml is None return Falsednsresult response_xml getElementsByTagName 'DomainDNSSetDefaultResult' [0]return salt utils namecheap string_to_value dnsresult getAttribute 'Updated'
def _common_bytes blocks1 blocks2 if len blocks1 > len blocks2 blocks1 blocks2 blocks2 blocks1 score 0for block count1 in blocks1 items count2 blocks2 get block if count2 score + min count1 count2 return score
def _common_bytes blocks1 blocks2 if len blocks1 > len blocks2 blocks1 blocks2 blocks2 blocks1 score 0for block count1 in blocks1 items count2 blocks2 get block if count2 score + min count1 count2 return score
def _get_image_properties image immin immax np min image np max image imtype image dtype typetry lo hi dtypes dtype_range[imtype]except KeyError lo hi immin immax signed immin < 0 out_of_range_float np issubdtype image dtype np float and immin < lo or immax > hi low_data_range immin immax and is_low_contrast image unsupported_dtype image dtype not in dtypes _supported_types return ImageProperties signed out_of_range_float low_data_range unsupported_dtype
def runWithWarningsSuppressed suppressedWarnings f *args **kwargs with warnings catch_warnings for a kw in suppressedWarnings warnings filterwarnings *a **kw return f *args **kwargs
def _nmeaFloat degrees minutes return '%i%0 3f' % degrees minutes
def _nmeaFloat degrees minutes return '%i%0 3f' % degrees minutes
def _find_directive directives directive_name if not directives or isinstance directives str or len directives 0 return Noneif directives[0] directive_name return directivesmatches _find_directive line directive_name for line in directives return next m for m in matches if m is not None None
def test_scenario_has_name feature Feature from_string FEATURE1 assert isinstance feature Feature expect feature name to equal 'Rentmovies'
def safe_no_dnn_algo_bwd algo if algo raise RuntimeError 'Theoption`dnn conv algo_bwd`hasbeenremovedandshouldnotbeusedanymore Pleaseusetheoptions`dnn conv algo_bwd_filter`and`dnn conv algo_bwd_data`instead ' return True
def safe_no_dnn_algo_bwd algo if algo raise RuntimeError 'Theoption`dnn conv algo_bwd`hasbeenremovedandshouldnotbeusedanymore Pleaseusetheoptions`dnn conv algo_bwd_filter`and`dnn conv algo_bwd_data`instead ' return True
def safe_no_dnn_algo_bwd algo if algo raise RuntimeError 'Theoption`dnn conv algo_bwd`hasbeenremovedandshouldnotbeusedanymore Pleaseusetheoptions`dnn conv algo_bwd_filter`and`dnn conv algo_bwd_data`instead ' return True
def safe_no_dnn_algo_bwd algo if algo raise RuntimeError 'Theoption`dnn conv algo_bwd`hasbeenremovedandshouldnotbeusedanymore Pleaseusetheoptions`dnn conv algo_bwd_filter`and`dnn conv algo_bwd_data`instead ' return True
@frappe whitelist def delete doctype name frappe delete_doc doctype name
def setup_platform hass config add_devices discovery_info None add_devices [DemoClimate 'HeatPump' 68 TEMP_FAHRENHEIT None 77 'AutoLow' None None 'Auto' 'heat' None None None DemoClimate 'Hvac' 21 TEMP_CELSIUS True 22 'OnHigh' 67 54 'Off' 'cool' False None None DemoClimate 'Ecobee' None TEMP_CELSIUS None 23 'AutoLow' None None 'Auto' 'auto' None 24 21 ]
def init args if args backend 'webengine' from qutebrowser browser webengine import webenginesettingswebenginesettings init args else from qutebrowser browser webkit import webkitsettingswebkitsettings init args
def top_down_once rule fns basic_fns return do_one rule lambda expr sall top_down rule fns fns expr
def read_element_unicode stream size return _read stream size decode 'utf-8'
def __virtual__ if 'logrotate show_conf' in __salt__ return __virtualname__return False
def _SkipLengthDelimited buffer pos end size pos _DecodeVarint buffer pos pos + sizeif pos > end raise _DecodeError 'Truncatedmessage ' return pos
def _load_provider_feature feature providers features []for provider in providers mod __import__ provider fromlist [feature] features getattr mod feature features return features
def GetAllPosts posts [{'content' str row[1] 'time' str row[0] } for row in DB]posts sort key lambda row row['time'] reverse True return posts
def _get_account_policy name cmd 'pwpolicy-u{0}-getpolicy' format name try ret salt utils mac_utils execute_return_result cmd except CommandExecutionError as exc if 'Error user<{0}>notfound' format name in exc strerror raise CommandExecutionError 'Usernotfound {0}' format name raise CommandExecutionError 'Unknownerror {0}' format exc strerror try policy_list ret split '\n' [1] split '' policy_dict {}for policy in policy_list if ' ' in policy key value policy split ' ' policy_dict[key] valuereturn policy_dictexcept IndexError return {}
def split_stem sentence sentence re sub ' [a-z] [A-Z] ' u'\\1\\2' sentence return sentence split
def split_stem sentence sentence re sub ' [a-z] [A-Z] ' u'\\1\\2' sentence return sentence split
def _connect_socket host port start time time while True try return _try_connect host port except sc_exceptions SocketConnectionRefused if time time - start > constants SOCKET_CONNECT_TIMEOUT raise sc_exceptions ConnectionTimeoutException host host port port
def get_default_role request global DEFAULT_ROLEdefault getattr settings 'OPENSTACK_KEYSTONE_DEFAULT_ROLE' None if default and DEFAULT_ROLE is None try roles keystoneclient request admin True roles list except roles []exceptions handle request for role in roles if role id default or role name default DEFAULT_ROLE rolebreakreturn DEFAULT_ROLE
def _get_comment_callback comment_data thread_id parent_id def callback request _uri headers '\nSimulatethecommentcreationorupdateendpointasdescribedabove \n'response_data make_minimal_cs_comment comment_data response_data['thread_id'] thread_idresponse_data['parent_id'] parent_idfor key val_list in request parsed_body items val val_list[0]if key in ['anonymous' 'anonymous_to_peers' 'endorsed'] response_data[key] val 'True' else response_data[key] valreturn 200 headers json dumps response_data return callback
def _get_comment_callback comment_data thread_id parent_id def callback request _uri headers '\nSimulatethecommentcreationorupdateendpointasdescribedabove \n'response_data make_minimal_cs_comment comment_data response_data['thread_id'] thread_idresponse_data['parent_id'] parent_idfor key val_list in request parsed_body items val val_list[0]if key in ['anonymous' 'anonymous_to_peers' 'endorsed'] response_data[key] val 'True' else response_data[key] valreturn 200 headers json dumps response_data return callback
@dec onlyif lambda sys platform 'win32' or has_pywin32 'Thistestrunsonposixorinwin32withwin32apiinstalled' def test_find_cmd_fail nt assert_raises FindCmdError find_cmd 'asdfasdf'
@dec onlyif lambda sys platform 'win32' or has_pywin32 'Thistestrunsonposixorinwin32withwin32apiinstalled' def test_find_cmd_fail nt assert_raises FindCmdError find_cmd 'asdfasdf'
def time_to_epoch t if isinstance t int return telif isinstance t tuple or isinstance t time struct_time return int time mktime t elif hasattr t 'timetuple' return int time mktime t timetuple elif hasattr t 'strftime' return int t strftime '%s' elif isinstance t str or isinstance t unicode try if t startswith '+' return time time + int t[1 ] return int t except ValueError try return time strptime t except ValueError as ex debug 'Failedtoparsedatewithstrptime %s' ex passraise S3 Exceptions ParameterError "Unabletoconvert%rtoanepochtime Passanepochtime Try`date-d'now+1year'+%%s` shell ortime mktime Python " % t
def time_to_epoch t if isinstance t int return telif isinstance t tuple or isinstance t time struct_time return int time mktime t elif hasattr t 'timetuple' return int time mktime t timetuple elif hasattr t 'strftime' return int t strftime '%s' elif isinstance t str or isinstance t unicode try if t startswith '+' return time time + int t[1 ] return int t except ValueError try return time strptime t except ValueError as ex debug 'Failedtoparsedatewithstrptime %s' ex passraise S3 Exceptions ParameterError "Unabletoconvert%rtoanepochtime Passanepochtime Try`date-d'now+1year'+%%s` shell ortime mktime Python " % t
def time_to_epoch t if isinstance t int return telif isinstance t tuple or isinstance t time struct_time return int time mktime t elif hasattr t 'timetuple' return int time mktime t timetuple elif hasattr t 'strftime' return int t strftime '%s' elif isinstance t str or isinstance t unicode try if t startswith '+' return time time + int t[1 ] return int t except ValueError try return time strptime t except ValueError as ex debug 'Failedtoparsedatewithstrptime %s' ex passraise S3 Exceptions ParameterError "Unabletoconvert%rtoanepochtime Passanepochtime Try`date-d'now+1year'+%%s` shell ortime mktime Python " % t
@pytest fixturedef en_tutorial_ts english_tutorial ts_directory from pootle_format models import Formatenglish_tutorial project filetypes add Format objects get name 'ts' return store _require_store english_tutorial ts_directory 'tutorial ts'
def libvlc_media_player_release p_mi f _Cfunctions get 'libvlc_media_player_release' None or _Cfunction 'libvlc_media_player_release' 1 None None MediaPlayer return f p_mi
def libvlc_media_player_release p_mi f _Cfunctions get 'libvlc_media_player_release' None or _Cfunction 'libvlc_media_player_release' 1 None None MediaPlayer return f p_mi
def get_credit_provider_info request provider_id credit_provider CreditProvider get_credit_provider provider_id provider_id credit_provider_data {}if credit_provider credit_provider_data {'provider_id' credit_provider provider_id 'display_name' credit_provider display_name 'provider_url' credit_provider provider_url 'provider_status_url' credit_provider provider_status_url 'provider_description' credit_provider provider_description 'enable_integration' credit_provider enable_integration 'fulfillment_instructions' credit_provider fulfillment_instructions 'thumbnail_url' credit_provider thumbnail_url}return JsonResponse credit_provider_data
def get_credit_provider_info request provider_id credit_provider CreditProvider get_credit_provider provider_id provider_id credit_provider_data {}if credit_provider credit_provider_data {'provider_id' credit_provider provider_id 'display_name' credit_provider display_name 'provider_url' credit_provider provider_url 'provider_status_url' credit_provider provider_status_url 'provider_description' credit_provider provider_description 'enable_integration' credit_provider enable_integration 'fulfillment_instructions' credit_provider fulfillment_instructions 'thumbnail_url' credit_provider thumbnail_url}return JsonResponse credit_provider_data
def get__all__entries obj try words getattr obj '__all__' except return []return [cast_unicode_py2 w for w in words if isinstance w str ]
def join leftkey leftseq rightkey rightseq left_default no_default right_default no_default if not callable leftkey leftkey getter leftkey if not callable rightkey rightkey getter rightkey d groupby leftkey leftseq seen_keys set left_default_is_no_default left_default no_default for item in rightseq key rightkey item seen_keys add key try left_matches d[key]for match in left_matches yield match item except KeyError if not left_default_is_no_default yield left_default item if right_default no_default for key matches in d items if key not in seen_keys for match in matches yield match right_default
def get_cuda_memory if not _cuda_capable warn 'CUDAnotenabled returningzeroformemory' mem 0else from pycuda driver import mem_get_infomem mem_get_info [0]return sizeof_fmt mem
def delta_seconds before after delta after - before try return delta total_seconds except AttributeError return delta days * 24 * 3600 + delta seconds + float delta microseconds / 10 ** 6
def p_statement_assign t names[t[1]] t[3]
def _fill_and_one_pad_stride stride n try return 1 + _fill_shape stride n + 1 except TypeError raise base IncompatibleShapeError 'strideis{} {} mustbeeitheranintegeroraniterableofintegersofsize{}' format stride type stride n
def _fill_and_one_pad_stride stride n try return 1 + _fill_shape stride n + 1 except TypeError raise base IncompatibleShapeError 'strideis{} {} mustbeeitheranintegeroraniterableofintegersofsize{}' format stride type stride n
def _PutSigningKeyset io_loop secret _GetSecretsManager PutSecret secret json dumps secrets CreateSigningKeyset secret io_loop stop
def proxying_engine conn_cls DBAPIProxyConnection cursor_cls DBAPIProxyCursor def mock_conn return conn_cls config db cursor_cls return testing_engine options {'creator' mock_conn}
def proxying_engine conn_cls DBAPIProxyConnection cursor_cls DBAPIProxyCursor def mock_conn return conn_cls config db cursor_cls return testing_engine options {'creator' mock_conn}
def proxying_engine conn_cls DBAPIProxyConnection cursor_cls DBAPIProxyCursor def mock_conn return conn_cls config db cursor_cls return testing_engine options {'creator' mock_conn}
def check_phylip_reject_duplicate handle StringIO sequences [SeqRecord Seq 'AAAA' id 'longsequencename1' SeqRecord Seq 'AAAA' id 'longsequencename2' SeqRecord Seq 'AAAA' id 'other_sequence' ]alignment MultipleSeqAlignment sequences try AlignIO write alignment handle 'phylip' assert False 'DuplicateIDsaftertruncationarenotallowed 'except ValueError as e assert "Repeatedname'longsequen'" in str e
def clean_filename fn return re sub '[^0-9a-zA-Z]+' '-' fn
def clean_filename fn return re sub '[^0-9a-zA-Z]+' '-' fn
def pgcalc sa_session id dryrun False sql_calc 'SELECTCOALESCE SUM total_size 0 \nFROM SELECTDISTINCTON d id d total_size d id\nFROMhistory_dataset_associationhda\nJOINhistoryhONh id hda history_id\nJOINdatasetdONhda dataset_id d id\nWHEREh user_id id\nANDh purged false\nANDhda purged false\nANDd purged false\nANDd idNOTIN SELECTdataset_id\nFROMlibrary_dataset_dataset_association \n sizes'sql_update 'UPDATEgalaxy_user\nSETdisk_usage %s \nWHEREid id\nRETURNINGdisk_usage ' % sql_calc if dryrun r sa_session execute sql_calc {'id' id} else r sa_session execute sql_update {'id' id} return r fetchone [0]
def pgcalc sa_session id dryrun False sql_calc 'SELECTCOALESCE SUM total_size 0 \nFROM SELECTDISTINCTON d id d total_size d id\nFROMhistory_dataset_associationhda\nJOINhistoryhONh id hda history_id\nJOINdatasetdONhda dataset_id d id\nWHEREh user_id id\nANDh purged false\nANDhda purged false\nANDd purged false\nANDd idNOTIN SELECTdataset_id\nFROMlibrary_dataset_dataset_association \n sizes'sql_update 'UPDATEgalaxy_user\nSETdisk_usage %s \nWHEREid id\nRETURNINGdisk_usage ' % sql_calc if dryrun r sa_session execute sql_calc {'id' id} else r sa_session execute sql_update {'id' id} return r fetchone [0]
def random_distribution min -5 0 max 5 0 total_items 50 num_items random randrange 5 total_items all_info []for item in range num_items new_item random uniform min max all_info append new_item return all_info
def distro_release_info return _distro distro_release_info
def service_present name service_type description None profile None **connection_args ret {'name' name 'changes' {} 'result' True 'comment' 'Service"{0}"alreadyexists' format name }role __salt__['keystone service_get'] name name profile profile **connection_args if 'Error' not in role return retelse if __opts__ get 'test' ret['result'] Noneret['comment'] 'Service"{0}"willbeadded' format name return ret__salt__['keystone service_create'] name service_type description profile profile **connection_args ret['comment'] 'Service"{0}"hasbeenadded' format name ret['changes']['Service'] 'Created'return ret
def delete resource_obj resource_instance params dry_run decorator '[dryrun]' if dry_run else '' print '{decorator}DELETE{name}[{time}]FROM{params}' format decorator decorator name resource_instance get 'name' time determine_timestamp resource_instance params params if dry_run returnresource_obj delete **params execute
def _diff_replication_group current desired if current get 'AutomaticFailover' is not None current['AutomaticFailoverEnabled'] True if current['AutomaticFailover'] in 'enabled' 'enabling' else False modifiable {'AutomaticFailoverEnabled' 'AutomaticFailoverEnabled' 'AutoMinorVersionUpgrade' None 'CacheNodeType' None 'CacheParameterGroupName' None 'CacheSecurityGroupNames' None 'EngineVersion' None 'NotificationTopicArn' None 'NotificationTopicStatus' None 'PreferredMaintenanceWindow' None 'PrimaryClusterId' None 'ReplicationGroupDescription' 'Description' 'SecurityGroupIds' None 'SnapshotRetentionLimit' 'SnapshotRetentionLimit' 'SnapshottingClusterId' 'SnapshottingClusterId' 'SnapshotWindow' 'SnapshotWindow'}need_update {}for m o in modifiable items if m in desired if not o need_update[m] desired[m]elif m in current if current[m] desired[m] need_update[m] desired[m]return need_update
def _diff_replication_group current desired if current get 'AutomaticFailover' is not None current['AutomaticFailoverEnabled'] True if current['AutomaticFailover'] in 'enabled' 'enabling' else False modifiable {'AutomaticFailoverEnabled' 'AutomaticFailoverEnabled' 'AutoMinorVersionUpgrade' None 'CacheNodeType' None 'CacheParameterGroupName' None 'CacheSecurityGroupNames' None 'EngineVersion' None 'NotificationTopicArn' None 'NotificationTopicStatus' None 'PreferredMaintenanceWindow' None 'PrimaryClusterId' None 'ReplicationGroupDescription' 'Description' 'SecurityGroupIds' None 'SnapshotRetentionLimit' 'SnapshotRetentionLimit' 'SnapshottingClusterId' 'SnapshottingClusterId' 'SnapshotWindow' 'SnapshotWindow'}need_update {}for m o in modifiable items if m in desired if not o need_update[m] desired[m]elif m in current if current[m] desired[m] need_update[m] desired[m]return need_update
def _diff_replication_group current desired if current get 'AutomaticFailover' is not None current['AutomaticFailoverEnabled'] True if current['AutomaticFailover'] in 'enabled' 'enabling' else False modifiable {'AutomaticFailoverEnabled' 'AutomaticFailoverEnabled' 'AutoMinorVersionUpgrade' None 'CacheNodeType' None 'CacheParameterGroupName' None 'CacheSecurityGroupNames' None 'EngineVersion' None 'NotificationTopicArn' None 'NotificationTopicStatus' None 'PreferredMaintenanceWindow' None 'PrimaryClusterId' None 'ReplicationGroupDescription' 'Description' 'SecurityGroupIds' None 'SnapshotRetentionLimit' 'SnapshotRetentionLimit' 'SnapshottingClusterId' 'SnapshottingClusterId' 'SnapshotWindow' 'SnapshotWindow'}need_update {}for m o in modifiable items if m in desired if not o need_update[m] desired[m]elif m in current if current[m] desired[m] need_update[m] desired[m]return need_update
def radius_neighbors_graph X radius mode 'connectivity' metric 'minkowski' p 2 metric_params None include_self False n_jobs 1 if not isinstance X RadiusNeighborsMixin X NearestNeighbors radius radius metric metric p p metric_params metric_params n_jobs n_jobs fit X else _check_params X metric p metric_params query _query_include_self X include_self return X radius_neighbors_graph query radius mode
def is_public_ip ipstr addr ipstr split ' ' [0]addr int addr 16 byte1 addr & 255 byte2 addr >> 8 & 255 if byte1 in 10 0 127 return Falseif byte1 172 and byte2 > 16 return Falseif byte1 192 and byte2 168 return Falsereturn True
def webpack_asset path asset_paths asset_paths debug settings DEBUG_MODE if not asset_paths logger warn 'webpack-assets jsonhasnotyetbeengenerated Fallingbacktonon-cache-bustedassets' return pathif not debug key path replace base_static_path '' replace ' js' '' hash_path asset_paths[key]return os path join base_static_path hash_path else return path
def _unpick_search sort allowed_fields None total None sorts []split_sort sort split ' ' for part in split_sort split_part part strip split field split_part[0]if len split_part > 1 order split_part[1] lower else order 'asc'if allowed_fields if field not in allowed_fields raise ValidationError 'Cannotsortbyfield`%s`' % field if order not in ['asc' 'desc'] raise ValidationError 'Invalidsortdirection`%s`' % order sorts append field order if total and len sorts > total raise ValidationError 'Toomanysortcriteriaprovidedonly%sallowed' % total return sorts
def getWindowAnalyzeFileGivenText fileName gcodeText skein CommentSkein skein parseGcode gcodeText archive writeFileMessageEnd '_comment gcode' fileName skein output getvalue 'Thecommentedfileissavedas'
def matchStrengthNoNoise x y n return sum xi yi for xi yi ni in zip x y n if ni '#'
def _get_local_ip s socket socket socket AF_INET socket SOCK_DGRAM s connect 'google com' 80 ip s getsockname [0]s close return ip
def formatRARVersion field return '%u %u' % divmod field value 10
def formatRARVersion field return '%u %u' % divmod field value 10
def formatRARVersion field return '%u %u' % divmod field value 10
def ensure_minimal_setup package_manager if package_manager in 'dnf' 'yum' return run_network_interacting_from_args ['su' 'root' '-c' [package_manager '-y' 'install' 'sudo']] elif package_manager 'apt' return sequence [run_network_interacting_from_args ['su' 'root' '-c' ['apt-get' 'update']] run_network_interacting_from_args ['su' 'root' '-c' ['apt-get' '-y' 'install' 'sudo']] ] else raise UnsupportedDistribution
def show br None return _os_dispatch 'brshow' br
def het_white resid exog retres False x np asarray exog y np asarray resid if x ndim 1 raise ValueError 'xshouldhaveconstantandatleastonemorevariable' nobs nvars0 x shape i0 i1 np triu_indices nvars0 exog x[ i0] * x[ i1] nobs nvars exog shapeassert nvars nvars0 * nvars0 - 1 / 2 0 + nvars0 resols OLS y ** 2 exog fit fval resols fvaluefpval resols f_pvaluelm nobs * resols rsquared assert resols df_model np_matrix_rank exog - 1 lmpval stats chi2 sf lm resols df_model return lm lmpval fval fpval
def het_white resid exog retres False x np asarray exog y np asarray resid if x ndim 1 raise ValueError 'xshouldhaveconstantandatleastonemorevariable' nobs nvars0 x shape i0 i1 np triu_indices nvars0 exog x[ i0] * x[ i1] nobs nvars exog shapeassert nvars nvars0 * nvars0 - 1 / 2 0 + nvars0 resols OLS y ** 2 exog fit fval resols fvaluefpval resols f_pvaluelm nobs * resols rsquared assert resols df_model np_matrix_rank exog - 1 lmpval stats chi2 sf lm resols df_model return lm lmpval fval fpval
def checks_run_recently request ten_mins datetime utcnow replace tzinfo utc - timedelta minutes 10 most_recent StatusCheckResult objects filter time_complete__gte ten_mins if most_recent exists return HttpResponse 'Checksrunning' return HttpResponse 'Checksnotrunning'
def _get_ext_comm_subtype type_high return _ext_comm_subtypes_classes get type_high {}
def present name definition None if definition is None definition {}ret {'name' name 'changes' {} 'result' True 'comment' ''}index_template_exists __salt__['elasticsearch index_template_exists'] name name if not index_template_exists if __opts__['test'] ret['comment'] 'Indextemplate{0}willbecreated' format name ret['result'] Noneelse ret['result'] __salt__['elasticsearch index_template_create'] name name body definition if ret['result'] ret['comment'] 'Createdindextemplate{0}successfully' format name elif index_template_exists ret['comment'] 'Indextemplate{0}isalreadypresent' format name else ret['comment'] 'Failedtodeterminewhetherindextemplate{0}ispresent seeMinionlogformoreinformation' format name ret['result'] Falsereturn ret
@receiver models signals post_save sender VerificationStatus @receiver models signals post_delete sender VerificationStatus def invalidate_verification_status_cache sender instance **kwargs cache_key VerificationStatus cache_key_name instance user id unicode instance checkpoint course_id cache delete cache_key
def _srvmgr func as_json False command 'Import-ModuleWebAdministration 'if as_json command '{0}ConvertTo-Json-Compress-Depth4-InputObject@ {1} ' format command func else command '{0}{1}' format command func cmd_ret __salt__['cmd run_all'] command shell 'powershell' python_shell True if cmd_ret['retcode'] 0 _LOG error 'Unabletoexecutecommand %s\nError %s' command cmd_ret['stderr'] return cmd_ret
@app after_requestdef after response print 'andherewehavetheresponseobjectinstead ' response return response
@app after_requestdef after response print 'andherewehavetheresponseobjectinstead ' response return response
def set_lcd_filter filt library get_handle error FT_Library_SetLcdFilter library filt if error raise FT_Exception error
def tags_for cls model instance None **extra_filters kwargs extra_filters or {} if instance is not None kwargs update { '%s__content_object' % cls tag_relname instance} return cls tag_model objects filter **kwargs kwargs update { '%s__content_object__isnull' % cls tag_relname False} return cls tag_model objects filter **kwargs distinct
def tags_for cls model instance None **extra_filters kwargs extra_filters or {} if instance is not None kwargs update { '%s__content_object' % cls tag_relname instance} return cls tag_model objects filter **kwargs kwargs update { '%s__content_object__isnull' % cls tag_relname False} return cls tag_model objects filter **kwargs distinct
@event u'manager before_config_validate' def process_variables config manager env_params {u'block_start_string' u'^^disabled^^' u'block_end_string' u'^^disabled^^' u'variable_start_string' u'{?' u'variable_end_string' u'?}'}if u'variables' not in config or config get u'variables' is False returnenv Environment **env_params if isinstance config[u'variables'] bool log debug u'tryingtoloadvariablesfromDB' variables variables_from_db else log debug u'tryingtoloadvariablesfromfile' variables variables_from_file manager config_base config[u'variables'] log debug u'updatingDBwithvariablefilecontents' variables_to_db variables env globals variables_process config env return config
def trimLens lens minLen index 0for i in range len lens if lens[i] < minLen index + 1else breakreturn lens[index len lens ]
def generate_accepted_kwargs function *named_arguments if hasattr function '__code__' and takes_kwargs function function_takes_kwargs Truefunction_takes_arguments []else function_takes_kwargs Falsefunction_takes_arguments takes_arguments function *named_arguments def accepted_kwargs kwargs if function_takes_kwargs return kwargselif function_takes_arguments return {key value for key value in kwargs items if key in function_takes_arguments }else return {}return accepted_kwargs
def generate_accepted_kwargs function *named_arguments if hasattr function '__code__' and takes_kwargs function function_takes_kwargs Truefunction_takes_arguments []else function_takes_kwargs Falsefunction_takes_arguments takes_arguments function *named_arguments def accepted_kwargs kwargs if function_takes_kwargs return kwargselif function_takes_arguments return {key value for key value in kwargs items if key in function_takes_arguments }else return {}return accepted_kwargs
def drop_missing Y X None axis 1 Y np asarray Y if Y ndim 1 Y Y[ None]if X is not None X np array X if X ndim 1 X X[ None]keepidx np logical_and ~ np isnan Y any axis ~ np isnan X any axis return Y[keepidx] X[keepidx] else keepidx ~ np isnan Y any axis return Y[keepidx]
def drop_missing Y X None axis 1 Y np asarray Y if Y ndim 1 Y Y[ None]if X is not None X np array X if X ndim 1 X X[ None]keepidx np logical_and ~ np isnan Y any axis ~ np isnan X any axis return Y[keepidx] X[keepidx] else keepidx ~ np isnan Y any axis return Y[keepidx]
def drop_missing Y X None axis 1 Y np asarray Y if Y ndim 1 Y Y[ None]if X is not None X np array X if X ndim 1 X X[ None]keepidx np logical_and ~ np isnan Y any axis ~ np isnan X any axis return Y[keepidx] X[keepidx] else keepidx ~ np isnan Y any axis return Y[keepidx]
def daemonize_if opts if 'salt-call' in sys argv[0] returnif not opts get 'multiprocessing' True returnif sys platform startswith 'win' returndaemonize False
def daemonize_if opts if 'salt-call' in sys argv[0] returnif not opts get 'multiprocessing' True returnif sys platform startswith 'win' returndaemonize False
@login_requireddef display_person_edit_name_do request user request usernew_first request POST['first_name']new_last request POST['last_name']user first_name new_firstuser last_name new_lastuser save return HttpResponseRedirect '/people/%s' % urllib quote user username
@login_requireddef display_person_edit_name_do request user request usernew_first request POST['first_name']new_last request POST['last_name']user first_name new_firstuser last_name new_lastuser save return HttpResponseRedirect '/people/%s' % urllib quote user username
def UrnStringToHuntId urn if urn startswith AFF4_PREFIX urn urn[len AFF4_PREFIX ]components urn split '/' if len components 2 or components[0] 'hunts' raise ValueError 'InvalidhuntURN %s' urn return components[ -1 ]
def response controller arg return _response controller _normalize arg
def numpy_cupy_array_less err_msg '' verbose True name 'xp' type_check True accept_error False def check_func x y array assert_array_less x y err_msg verbose return _make_decorator check_func name type_check accept_error
def final_status statuses return sorted statuses key itemgetter 'updated_at' [ -1 ]
def generate_oauth_authorization_url token next None hd DEFAULT_DOMAIN hl None btmpl None auth_server OAUTH_AUTHORIZE_URL uri atom http_core Uri parse_uri auth_server uri query['oauth_token'] tokenuri query['hd'] hdif next is not None uri query['oauth_callback'] str next if hl is not None uri query['hl'] hlif btmpl is not None uri query['btmpl'] btmplreturn uri
def idle priority 0 hub get_hub watcher hub loop idle if priority watcher priority priorityhub wait watcher
def get_user_icons user user_perms UserPermissions user user_perms build_permissions from frappe boot import get_allowed_pagesallowed_pages get_allowed_pages icons []for icon in get_desktop_icons user add Trueif icon hidden_in_standard add Falseif not icon custom if icon module_name u'Learn' passelif icon type u'page' and icon link not in allowed_pages add Falseelif icon type u'module' and icon module_name not in user_perms allow_modules add Falseif add icons append icon return icons
@core_helperdef build_nav_main *args output ''for item in args menu_item title item[ 2]if len item 3 and not check_access item[2] continueoutput + _make_menu_item menu_item title return output
def get_dir_time_suffix dirfmt '%4d-%02d-%02d_%02d%02d%02d'now time localtime [0 6]dirname dirfmt % now return dirname
def raise_if_deadlock_error operational_error engine_name re _DEADLOCK_RE_DB get engine_name if re is None returnm re match operational_error message if not m returnraise exception DBDeadlock operational_error
def raise_if_deadlock_error operational_error engine_name re _DEADLOCK_RE_DB get engine_name if re is None returnm re match operational_error message if not m returnraise exception DBDeadlock operational_error
def build_instance Model data db obj Model **data if obj pk is None and hasattr Model 'natural_key' and hasattr Model _default_manager 'get_by_natural_key' natural_key obj natural_key try obj pk Model _default_manager db_manager db get_by_natural_key *natural_key pkexcept Model DoesNotExist passreturn obj
def include d e return d [f for f in glob glob '%s/%s' % d e if os path isfile f ]
def do_reverse value if isinstance value string_types return value[ -1 ]try return reversed value except TypeError try rv list value rv reverse return rvexcept TypeError raise FilterArgumentError 'argumentmustbeiterable'
def do_reverse value if isinstance value string_types return value[ -1 ]try return reversed value except TypeError try rv list value rv reverse return rvexcept TypeError raise FilterArgumentError 'argumentmustbeiterable'
def ParseTaskAgeLimit age_limit age_limit age_limit strip if not age_limit raise MalformedQueueConfiguration 'TaskAgeLimitmustnotbeempty ' unit age_limit[ -1 ]if unit not in 'smhd' raise MalformedQueueConfiguration 'TaskAge_Limitmustbeins seconds m minutes h hours ord days ' try number float age_limit[0 -1 ] if unit 's' return int number if unit 'm' return int number * 60 if unit 'h' return int number * 3600 if unit 'd' return int number * 86400 except ValueError raise MalformedQueueConfiguration 'TaskAge_Limit"%s"isinvalid ' % age_limit
def test_write_invalid_toplevel_meta_ipac table ascii get_reader Reader ascii Ipac data table read 't/no_data_ipac dat' data meta['blah'] 'extra'with catch_warnings AstropyWarning as ASwarn out StringIO data write out format 'ascii ipac' assert len ASwarn 1 assert 'werenotwritten' in str ASwarn[0] message
def test_write_invalid_toplevel_meta_ipac table ascii get_reader Reader ascii Ipac data table read 't/no_data_ipac dat' data meta['blah'] 'extra'with catch_warnings AstropyWarning as ASwarn out StringIO data write out format 'ascii ipac' assert len ASwarn 1 assert 'werenotwritten' in str ASwarn[0] message
def test_write_invalid_toplevel_meta_ipac table ascii get_reader Reader ascii Ipac data table read 't/no_data_ipac dat' data meta['blah'] 'extra'with catch_warnings AstropyWarning as ASwarn out StringIO data write out format 'ascii ipac' assert len ASwarn 1 assert 'werenotwritten' in str ASwarn[0] message
def check_negative_indices *nodes for node in nodes if node is None or not isinstance node constant_result _py_int_types and not isinstance node constant_result float continueif node constant_result < 0 warning node pos "theresultofusingnegativeindicesinsideofcodesectionsmarkedas'wraparound False'isundefined" level 1
def check_negative_indices *nodes for node in nodes if node is None or not isinstance node constant_result _py_int_types and not isinstance node constant_result float continueif node constant_result < 0 warning node pos "theresultofusingnegativeindicesinsideofcodesectionsmarkedas'wraparound False'isundefined" level 1
def check_negative_indices *nodes for node in nodes if node is None or not isinstance node constant_result _py_int_types and not isinstance node constant_result float continueif node constant_result < 0 warning node pos "theresultofusingnegativeindicesinsideofcodesectionsmarkedas'wraparound False'isundefined" level 1
def makedirs path if not path returnif os path exists path if not os path isdir path raise OSError u'Path{0}alreadyexistsandisnotafolder ' format path else returntry os makedirs path returnexcept Exception if os path isdir path returnraise
def find_best_blas_type arrays dtype None dtype _np dtype dtype prefer_fortran Falseif arrays dtypes [ar dtype for ar in arrays]dtype _np find_common_type dtypes try index dtypes index dtype except ValueError index 0if arrays[index] flags['FORTRAN'] prefer_fortran Trueprefix _type_conv get dtype char 'd' if dtype char 'G' dtype _np dtype 'D' elif dtype char not in 'fdFD' dtype _np dtype 'd' return prefix dtype prefer_fortran
def test_weights_iris clf1 LogisticRegression random_state 123 clf2 RandomForestClassifier random_state 123 clf3 GaussianNB eclf VotingClassifier estimators [ 'lr' clf1 'rf' clf2 'gnb' clf3 ] voting 'soft' weights [1 2 10] scores cross_val_score eclf X y cv 5 scoring 'accuracy' assert_almost_equal scores mean 0 93 decimal 2
@taskdef release options sdist options python_version '2 7'bdist_superpack options options python_version '3 4'bdist_superpack options write_release_and_log
def create_ikepolicy name profile None **kwargs conn _auth profile return conn create_ikepolicy name **kwargs
def directory path use_sudo False owner '' group '' mode '' func use_sudo and run_as_root or run if not is_dir path func 'mkdir-p"% path s"' % locals if owner and _owner path use_sudo owner or group and _group path use_sudo group func 'chown% owner s % group s"% path s"' % locals if mode and _mode path use_sudo mode func 'chmod% mode s"% path s"' % locals
def parse_query_part part query_classes {} prefixes {} default_class query SubstringQuery part part strip match PARSE_QUERY_PART_REGEX match part assert matchkey match group 1 term match group 2 replace '\\ ' ' ' for pre query_class in prefixes items if term startswith pre return key term[len pre ] query_class query_class query_classes get key default_class return key term query_class
def _get_app_revision environ None if environ is None environ os environif 'CURRENT_VERSION_ID' in environ return environ['CURRENT_VERSION_ID'] split ' ' [1]
def repeat_last_axis array count return as_strided array array shape + count array strides + 0
def get_language_name language_code try lang_dict get_current_babel_locale languagesexcept AttributeError ValueError return language_codefor option in language_code str language_code replace '-' '_' if option in lang_dict return lang_dict[option]return language_code
def get_language_name language_code try lang_dict get_current_babel_locale languagesexcept AttributeError ValueError return language_codefor option in language_code str language_code replace '-' '_' if option in lang_dict return lang_dict[option]return language_code
@skip_if_on_windows@pytest mark parametrize 'cmd fmt exp' [ 'pwd' None lambda os getcwd + '\n' 'echoWORKING' None 'WORKING\n' 'ls-f' lambda out out splitlines sort os listdir sort ] def test_single_command cmd fmt exp out err rtn run_xonsh cmd stderr sp DEVNULL if callable fmt out fmt out if callable exp exp exp assert out exp assert rtn 0
def test_merge_new merge_log_err packages {'pack_1' {'light' [{'platform' 'one'}]} 'pack_11' {'input_select' {'ib1' None}} 'pack_2' {'light' {'platform' 'one'} 'panel_custom' {'pan1' None} 'api' {}}}config {config_util CONF_CORE {config_util CONF_PACKAGES packages}}config_util merge_packages_config config packages assert merge_log_err call_count 0 assert 'api' in config assert len config 5 assert len config['light'] 2 assert len config['panel_custom'] 1
def get_token http service_account 'default' token_json get http 'instance/service-accounts/{0}/token' format service_account token_expiry client _UTCNOW + datetime timedelta seconds token_json['expires_in'] return token_json['access_token'] token_expiry
def get_token http service_account 'default' token_json get http 'instance/service-accounts/{0}/token' format service_account token_expiry client _UTCNOW + datetime timedelta seconds token_json['expires_in'] return token_json['access_token'] token_expiry
def claModelControlEnableTPLearningCb claModel assert isinstance claModel CLAModel claModel _getTPRegion setParameter 'learningMode' True return
def claModelControlEnableTPLearningCb claModel assert isinstance claModel CLAModel claModel _getTPRegion setParameter 'learningMode' True return
def get_mor_by_property service_instance object_type property_value property_name 'name' container_ref None object_list get_mors_with_properties service_instance object_type property_list [property_name] container_ref container_ref for obj in object_list obj_id str obj get 'object' '' strip '\'"' if obj[property_name] property_value or property_value obj_id return obj['object']return None
def get_mor_by_property service_instance object_type property_value property_name 'name' container_ref None object_list get_mors_with_properties service_instance object_type property_list [property_name] container_ref container_ref for obj in object_list obj_id str obj get 'object' '' strip '\'"' if obj[property_name] property_value or property_value obj_id return obj['object']return None
@treeio_login_required@handle_response_formatdef item_add_typed request type_id response_format 'html' item_type get_object_or_404 ItemType pk type_id if not request user profile has_permission item_type mode 'x' return user_denied request message "Youdon'thaveaccesstocreate" + unicode item_type response_format response_format if request POST if 'cancel' not in request POST form ItemForm request user profile item_type request POST files request FILES if form is_valid item form save request return HttpResponseRedirect reverse 'infrastructure_item_view' args [item id] else return HttpResponseRedirect reverse 'infrastructure_index' else form ItemForm request user profile item_type context _get_default_context request context update {'item_type' item_type 'form' form} return render_to_response 'infrastructure/item_add_typed' context context_instance RequestContext request response_format response_format
def ensure_sep sep s n 2 return s + sep * n - s count sep
@positional 1 def get_package_for_module module if isinstance module basestring try module sys modules[module]except KeyError return Nonetry return unicode module package except AttributeError if module __name__ '__main__' try file_name module __file__except AttributeError passelse base_name os path basename file_name split_name os path splitext base_name if len split_name 1 return unicode base_name else return u' ' join split_name[ -1 ] return unicode module __name__
def index_hydrate params container cli_type key value if 'IndexField' not in params params['IndexField'] {}if 'IndexFieldType' not in params['IndexField'] raise RuntimeError 'Youmustpassthe--typeoption ' _type params['IndexField']['IndexFieldType']_type '' join [i capitalize for i in _type split '-' ] if _type 'Latlon' _type 'LatLon'if key split SEP [ -1 ] 'DefaultValue' value DEFAULT_VALUE_TYPE_MAP get _type lambda x x value if _type + 'Options' not in params['IndexField'] params['IndexField'][ _type + 'Options' ] {}params['IndexField'][ _type + 'Options' ][key split SEP [ -1 ]] value
def index_hydrate params container cli_type key value if 'IndexField' not in params params['IndexField'] {}if 'IndexFieldType' not in params['IndexField'] raise RuntimeError 'Youmustpassthe--typeoption ' _type params['IndexField']['IndexFieldType']_type '' join [i capitalize for i in _type split '-' ] if _type 'Latlon' _type 'LatLon'if key split SEP [ -1 ] 'DefaultValue' value DEFAULT_VALUE_TYPE_MAP get _type lambda x x value if _type + 'Options' not in params['IndexField'] params['IndexField'][ _type + 'Options' ] {}params['IndexField'][ _type + 'Options' ][key split SEP [ -1 ]] value
def _is_nthpow_residue_bign a n m if primitive_root m is None for prime power in factorint m items if not _is_nthpow_residue_bign_prime_power a n prime power return Falsereturn Truef totient m k f // igcd f n return pow a k m 1
def generate_proto_go_source target source env source source[0]global proto_import_reimport_protos proto_import_re findall source get_text_contents parameters 'import_prefix %s/' % env['PROTOBUFGOPATH'] if import_protos proto_mappings []for proto in import_protos dir os path dirname proto name os path basename proto proto_mappings append 'M%s %s' % proto os path join dir name replace ' ' '_' parameters + ' %s' % ' ' join proto_mappings cmd '%s--proto_path --plugin protoc-gen-go %s-I %s-I %s--go_out %s %s%s' % env['PROTOC'] env['PROTOCGOPLUGIN'] env['PROTOBUFINCS'] os path dirname str source parameters env['BUILDDIR'] source return echospawn args [cmd] env os environ sh None cmd None escape None
def fill_stmt iterable fill_len fill_len + 1overflow Noneit iter iterable while True buffer_ []total_len 0if overflow buffer_ append overflow total_len + len overflow + 1 overflow Nonewhile total_len < fill_len try new_item it next buffer_ append new_item total_len + len new_item + 1 except StopIteration if buffer_ breakif overflow yield overflow returnif total_len > fill_len overflow buffer_ pop total_len - len overflow - 1 ret '' join buffer_ assert len ret < fill_len yield ret
def complement func return compose operator not_ func
def get_max_workspace_size return _max_workspace_size
def compute_precision tp fp precision tp / T maximum 1 0 tp + fp return precision
def fixup_internal_links config soups reverse_directory {}for d s in config[u'sources'] items reverse_directory[s] dfor name soup in soups items old_src_dir os path dirname config[u'sources'][name] for tag in soup find_all True if not u'href' in tag attrs continueold_rel_path tag[u'href'] split u'#' [0]old_dst os path normpath os path join old_src_dir old_rel_path if not old_dst in reverse_directory continuenew_dst reverse_directory[old_dst] + u' html' new_rel_path rel_href name new_dst tag[u'href'] tag[u'href'] replace old_rel_path new_rel_path 1
def setCacheCapacity capacity DEFAULT_CACHE_CAPACITY enableCache _entityCache setCapacity capacity
def _add_retry_host filter_properties host node retry filter_properties get 'retry' None if not retry returnhosts retry['hosts']hosts append [host node]
@verbosedef psd_multitaper inst fmin 0 fmax np inf tmin None tmax None bandwidth None adaptive False low_bias True normalization 'length' picks None proj False n_jobs 1 verbose None data sfreq _check_psd_data inst tmin tmax picks proj return _psd_multitaper data sfreq fmin fmin fmax fmax bandwidth bandwidth adaptive adaptive low_bias low_bias normalization normalization n_jobs n_jobs
def set_color_codes palette 'deep' if palette 'reset' colors [ 0 0 0 0 1 0 0 0 0 5 0 0 1 0 0 0 0 0 0 75 0 75 0 0 0 75 0 75 0 0 0 0 0 75 0 75 0 0 0 0 0 0 ]else colors SEABORN_PALETTES[palette] + [ 0 1 0 1 0 1 ] for code color in zip 'bgrmyck' colors rgb mpl colors colorConverter to_rgb color mpl colors colorConverter colors[code] rgbmpl colors colorConverter cache[code] rgb
def _check_header_magic_bytes file_obj file_obj seek 0 0 magic file_obj read 4 return magic 'PAR1'
def sentence_chrf reference hypothesis min_len 1 max_len 6 beta 3 0 return corpus_chrf [reference] [hypothesis] min_len max_len beta beta
def sentence_chrf reference hypothesis min_len 1 max_len 6 beta 3 0 return corpus_chrf [reference] [hypothesis] min_len max_len beta beta
def sentence_chrf reference hypothesis min_len 1 max_len 6 beta 3 0 return corpus_chrf [reference] [hypothesis] min_len max_len beta beta
def cliques_containing_node G nodes None cliques None if cliques is None cliques list find_cliques G if nodes is None nodes list G nodes if not isinstance nodes list v nodesvcliques [c for c in cliques if v in c ]else vcliques {}for v in nodes vcliques[v] [c for c in cliques if v in c ]return vcliques
@skip 'multiple_execute' def test_no_clr_attributes class x passfor stuff in [object int float bool str long complex dict set None NotImplemented Ellipsis type test_no_clr_attributes classmethod staticmethod frozenset property sys BaseException type zip slice buffer enumerate file range xrange type x type x ] for dir_stuff in dir stuff if dir_stuff[ 1] isalpha Assert dir_stuff[ 1] islower '%sshouldnotbeanattributeof%s' % dir_stuff str stuff
@skip 'multiple_execute' def test_no_clr_attributes class x passfor stuff in [object int float bool str long complex dict set None NotImplemented Ellipsis type test_no_clr_attributes classmethod staticmethod frozenset property sys BaseException type zip slice buffer enumerate file range xrange type x type x ] for dir_stuff in dir stuff if dir_stuff[ 1] isalpha Assert dir_stuff[ 1] islower '%sshouldnotbeanattributeof%s' % dir_stuff str stuff
def _norm_encoding encoding try return codecs lookup encoding nameexcept LookupError return encoding
def _norm_encoding encoding try return codecs lookup encoding nameexcept LookupError return encoding
def _norm_encoding encoding try return codecs lookup encoding nameexcept LookupError return encoding
def _norm_encoding encoding try return codecs lookup encoding nameexcept LookupError return encoding
def make_timestamp_range start end start_timestamp_op None end_timestamp_op None ts_range {}if start if start_timestamp_op 'gt' start_timestamp_op '$gt'else start_timestamp_op '$gte'ts_range[start_timestamp_op] startif end if end_timestamp_op 'le' end_timestamp_op '$lte'else end_timestamp_op '$lt'ts_range[end_timestamp_op] endreturn ts_range
def make_timestamp_range start end start_timestamp_op None end_timestamp_op None ts_range {}if start if start_timestamp_op 'gt' start_timestamp_op '$gt'else start_timestamp_op '$gte'ts_range[start_timestamp_op] startif end if end_timestamp_op 'le' end_timestamp_op '$lte'else end_timestamp_op '$lt'ts_range[end_timestamp_op] endreturn ts_range
def history_remove_completed logging info 'Scheduledremovalofallcompletedjobs' history_db HistoryDB history_db remove_completed history_db close del history_db
def sh_chebyu n monic False base sh_jacobi n 2 0 1 5 monic monic if monic return basefactor 4 ** n base _scale factor return base
def write_drawing worksheet if worksheet _charts or worksheet _images rel Relationship type 'drawing' target '' worksheet _rels append rel drawing Related drawing id 'rId%s' % len worksheet _rels return drawing to_tree 'drawing'
@pytest fixture params DISABLED_PROJECT_URL_PARAMS keys def dp_view_urls request view_types kwargs DISABLED_PROJECT_URL_PARAMS[request param] copy view_name kwargs pop 'view_name' view_name '%s-%s' % view_name view_types return reverse view_name kwargs kwargs
def _theano_leapfrog_integrator H q p **theano_kwargs epsilon tt dscalar 'epsilon' epsilon tag test_value 1n_steps tt iscalar 'n_steps' n_steps tag test_value 2 q_new p_new leapfrog H q p epsilon n_steps energy_new energy H q_new p_new f theano function [q p epsilon n_steps] [q_new p_new energy_new] **theano_kwargs f trust_input Truereturn f
def _theano_leapfrog_integrator H q p **theano_kwargs epsilon tt dscalar 'epsilon' epsilon tag test_value 1n_steps tt iscalar 'n_steps' n_steps tag test_value 2 q_new p_new leapfrog H q p epsilon n_steps energy_new energy H q_new p_new f theano function [q p epsilon n_steps] [q_new p_new energy_new] **theano_kwargs f trust_input Truereturn f
def export_users path_prefix '/' region None key None keyid None profile None conn _get_conn region region key key keyid keyid profile profile if not conn return Noneresults odict OrderedDict users get_all_users path_prefix region key keyid profile for user in users name user user_name_policies conn get_all_user_policies name max_items 100 _policies _policies list_user_policies_response list_user_policies_result policy_namespolicies {}for policy_name in _policies _policy conn get_user_policy name policy_name _policy json loads _unquote _policy get_user_policy_response get_user_policy_result policy_document policies[policy_name] _policyuser_sls []user_sls append {'name' name} user_sls append {'policies' policies} user_sls append {'path' user path} results[ 'manageuser' + name ] {'boto_iam user_present' user_sls}return _safe_dump results
def port_standard standard_annotations sel sample extractors new_annotations []for annotation in standard_annotations if not annotation get 'tagid' continueelement find_element annotation sel if element is None continueselector find_generalized_css_selector element sel if not selector continueannotation['accept_selectors'] [selector]annotation['selector'] selectorannotation['reject_selectors'] []annotation _add_annotation_data annotation sample extractors for _id data in annotation get 'data' {} items a copy deepcopy annotation a['id'] gen_predictable_id _id a['id'] a['data'] {gen_predictable_id a['id'] 1 data}new_annotations append a return new_annotations
def read_valuation s encoding None if encoding is not None s s decode encoding statements []for linenum line in enumerate s splitlines line line strip if line startswith u'#' or line u'' continuetry statements append _read_valuation_line line except ValueError raise ValueError u'Unabletoparseline%s %s' % linenum line return Valuation statements
def upgrade migrate_engine if migrate_engine name 'mysql' if not utils index_exists migrate_engine SYS_META_TABLE_NAME INDEX_NAME utils add_index migrate_engine SYS_META_TABLE_NAME INDEX_NAME INDEX_COLUMNS
def upgrade migrate_engine if migrate_engine name 'mysql' if not utils index_exists migrate_engine SYS_META_TABLE_NAME INDEX_NAME utils add_index migrate_engine SYS_META_TABLE_NAME INDEX_NAME INDEX_COLUMNS
def upgrade migrate_engine if migrate_engine name 'mysql' if not utils index_exists migrate_engine SYS_META_TABLE_NAME INDEX_NAME utils add_index migrate_engine SYS_META_TABLE_NAME INDEX_NAME INDEX_COLUMNS
def phabricator registry xml_parent data root XML SubElement xml_parent 'com uber jenkins phabricator PhabricatorNotifier' if 'comment-on-success' in data XML SubElement root 'commentOnSuccess' text str data get 'comment-on-success' lower if 'uberalls-enabled' in data XML SubElement root 'uberallsEnabled' text str data get 'uberalls-enabled' lower if 'comment-file' in data XML SubElement root 'commentFile' text data get 'comment-file' if 'comment-size' in data XML SubElement root 'commentSize' text str data get 'comment-size' if 'comment-with-console-link-on-failure' in data XML SubElement root 'commentWithConsoleLinkOnFailure' text str data get 'comment-with-console-link-on-failure' lower
def read_bin filepath try with open filepath 'rb' as bin_reader data bin_reader read return dataexcept Exception as e sys stderr write 'Error Unabletoreadfile' + filepath + ' ' + str e
def read_bin filepath try with open filepath 'rb' as bin_reader data bin_reader read return dataexcept Exception as e sys stderr write 'Error Unabletoreadfile' + filepath + ' ' + str e
def update repo_uri with Repo repo_uri as repo repo pull
def volume_attach name server_name device '/dev/xvdb' profile None timeout 300 conn _auth profile return conn volume_attach name server_name device timeout
def volume_attach name server_name device '/dev/xvdb' profile None timeout 300 conn _auth profile return conn volume_attach name server_name device timeout
def remove_credit_requirement_status username course_key req_namespace req_name req_to_remove CreditRequirement get_course_requirements course_key namespace req_namespace name req_name if not req_to_remove log error u'Couldnotremovecreditrequirementincourse"%s"withnamespace"%s"andname"%s"becausetherequirementdoesnotexist ' unicode course_key req_namespace req_name returnCreditRequirementStatus remove_requirement_status username req_to_remove
def find_ref_chain obj predicate max_depth 20 extra_ignore return _find_chain obj predicate gc get_referents max_depth max_depth extra_ignore extra_ignore [ -1 ]
def preBuild site pass
def get_mask_ipv6 bits if bits > 128 or bits < 0 raise ValueError 'Amaskcanonlybe0-128bits got%i' % bits elif bits 128 return FULL_IPv6_MASKmask_bin _get_binary 2 ** bits - 1 128 [ -1 ]groupings [mask_bin[ 16 * i 16 * i + 1 ] for i in range 8 ]return ' ' join [ '%04x' % int group 2 for group in groupings] upper
def _create_atom_id resource_path authority_name None date_string None if authority_name is None authority_name config get 'ckan feeds authority_name' '' strip if not authority_name site_url config get 'ckan site_url' '' strip authority_name urlparse urlparse site_url netlocif not authority_name log warning 'Noauthority_nameavailableforfeedgeneration Generatedfeedwillbeinvalid ' if date_string is None date_string config get 'ckan feeds date' '' if not date_string log warning 'Nodate_stringavailableforfeedgeneration Pleasesetthe"ckan feeds date"configvalue ' site_url config get 'ckan site_url' '' return '/' join [site_url resource_path] tagging_entity ' ' join [authority_name date_string] return ' ' join ['tag' tagging_entity resource_path]
def has_isoinfo return has_userland_tool 'isoinfo'
def has_isoinfo return has_userland_tool 'isoinfo'
def has_isoinfo return has_userland_tool 'isoinfo'
def has_isoinfo return has_userland_tool 'isoinfo'
def retrieve_config DEBUG Truenet_devices NetworkDevice objects all for a_device in net_devices if 'ssh' in a_device device_class if DEBUG print 'Retrievedeviceconfiguration {}{}\n' format a_device device_name a_device device_class ssh_connect SSHConnection a_device ssh_connect enable_mode output ssh_connect send_command 'showrun\n' file_name a_device device_name + ' txt' full_path CFGS_DIR + file_name if DEBUG print 'Writingconfigurationfiletofilesystem\n'with open full_path 'w' as f f write output
def get_outgoing_url url if not settings REDIRECT_URL return urlparsed_url urlparse url url_netloc parsed_url netlocif parsed_url scheme not in ['http' 'https'] return '/'if url_netloc urlparse settings REDIRECT_URL netloc or url_netloc in settings REDIRECT_URL_ALLOW_LIST return urlurl force_bytes jinja2 utils Markup url unescape sig hmac new settings REDIRECT_SECRET_KEY msg url digestmod hashlib sha256 hexdigest return '/' join [settings REDIRECT_URL rstrip '/' sig urllib quote url safe '/& ' ]
def open filename sc Shortcut sc load filename return sc
def xorg name ret {'name' name 'changes' {} 'result' None 'comment' ''}if __salt__['keyboard get_x'] name ret['result'] Trueret['comment'] 'XOrglayout{0}alreadyset' format name return retif __opts__['test'] ret['comment'] 'XOrglayout{0}needstobeset' format name return retif __salt__['keyboard set_x'] name ret['changes'] {'layout' name}ret['result'] Trueret['comment'] 'SetXOrgkeyboardlayout{0}' format name return retelse ret['result'] Falseret['comment'] 'FailedtosetXOrgkeyboardlayout'return ret
def _check_precisions precisions covariance_type n_components n_features precisions check_array precisions dtype [np float64 np float32] ensure_2d False allow_nd covariance_type 'full' precisions_shape {'full' n_components n_features n_features 'tied' n_features n_features 'diag' n_components n_features 'spherical' n_components }_check_shape precisions precisions_shape[covariance_type] '%sprecision' % covariance_type _check_precisions {'full' _check_precisions_full 'tied' _check_precision_matrix 'diag' _check_precision_positivity 'spherical' _check_precision_positivity}_check_precisions[covariance_type] precisions covariance_type return precisions
def replace_subcircuit circuit subcircuit replace None pos 0 if pos < 0 pos 0if isinstance circuit Mul circuit circuit argsif isinstance subcircuit Mul subcircuit subcircuit argsif isinstance replace Mul replace replace argselif replace is None replace loc find_subcircuit circuit subcircuit start pos if loc > -1 left circuit[0 loc]right circuit[ loc + len subcircuit len circuit ]circuit left + replace + right return circuit
def replace_subcircuit circuit subcircuit replace None pos 0 if pos < 0 pos 0if isinstance circuit Mul circuit circuit argsif isinstance subcircuit Mul subcircuit subcircuit argsif isinstance replace Mul replace replace argselif replace is None replace loc find_subcircuit circuit subcircuit start pos if loc > -1 left circuit[0 loc]right circuit[ loc + len subcircuit len circuit ]circuit left + replace + right return circuit
def dummy_sparse groups from scipy import sparseindptr np arange len groups + 1 data np ones len groups dtype np int8 indi sparse csr_matrix data g indptr return indi
def gce_from_configuration cluster_id project None zone None credentials None if project is None project get_machine_project if zone is None zone get_machine_zone gce_credentials gce_credentials_from_config credentials compute discovery build 'compute' 'v1' credentials gce_credentials return GCEBlockDeviceAPI _operations GCEOperations _compute compute _project unicode project _zone unicode zone _cluster_id unicode cluster_id
def gce_from_configuration cluster_id project None zone None credentials None if project is None project get_machine_project if zone is None zone get_machine_zone gce_credentials gce_credentials_from_config credentials compute discovery build 'compute' 'v1' credentials gce_credentials return GCEBlockDeviceAPI _operations GCEOperations _compute compute _project unicode project _zone unicode zone _cluster_id unicode cluster_id
def dir suffix '' prefix 'tmp' parent None return tempfile mkdtemp suffix prefix parent
def run_config path source None config_name None config_data None config_data_source None script_parameters None salt_env 'base' ret compile_config path path source source config_name config_name config_data config_data config_data_source config_data_source script_parameters script_parameters salt_env salt_env if ret get 'Exists' config_path os path dirname ret['FullName'] return apply_config config_path else return False
@require_POST@login_requireddef watch_forum request forum_slug forum get_object_or_404 Forum slug forum_slug if not forum allows_viewing_by request user raise Http404if request POST get 'watch' 'yes' NewThreadEvent notify request user forum statsd incr 'forums watches forum' else NewThreadEvent stop_notifying request user forum return HttpResponseRedirect reverse 'forums threads' args [forum_slug]
def destroy instances client _get_client info client destroy instances return info
def test_no_truncate_using_compare w wcs WCS naxis 3 w wcs crval [240 9303333333 50 212345678000 0]w wcs cdelt [0 001 0 001 100000000 0]w wcs ctype [u'RA---TAN' u'DEC--TAN' u'FREQ']w wcs set w2 wcs WCS w to_header w wcs compare w2 wcs
def conn_has_method conn method_name if method_name in dir conn return Truelog error "Method'{0}'notyetsupported " format method_name return False
def wipe device cmd 'wipefs-a{0}' format device try out __salt__['cmd run_all'] cmd python_shell False except subprocess CalledProcessError as err return Falseif out['retcode'] 0 return Trueelse log error 'Errorwipingdevice{0} {1}' format device out['stderr'] return False
def _gluster cmd return _gluster_ok _gluster_xml cmd
def _gluster cmd return _gluster_ok _gluster_xml cmd
def cleanup_sys_modules directories cleaned []for modname module in list sys modules items modfile getattr module '__file__' None if modfile for directory in directories if modfile startswith directory cleaned append modname del sys modules[modname]breakreturn cleaned
@handle_dashboard_error@require_POST@ensure_csrf_cookie@cache_control no_cache True no_store True must_revalidate True @require_level 'staff' @require_post_params 'url' def show_unit_extensions request course_id course get_course_by_id SlashSeparatedCourseKey from_deprecated_string course_id unit find_unit course request POST get 'url' return JsonResponse dump_module_extensions course unit
@handle_dashboard_error@require_POST@ensure_csrf_cookie@cache_control no_cache True no_store True must_revalidate True @require_level 'staff' @require_post_params 'url' def show_unit_extensions request course_id course get_course_by_id SlashSeparatedCourseKey from_deprecated_string course_id unit find_unit course request POST get 'url' return JsonResponse dump_module_extensions course unit
def strip_raw_ansi string parser ANSI_PARSER return parser strip_raw_codes string
def get_preferred_file_name_encoding return sys getfilesystemencoding or locale getpreferredencoding or u'utf-8'
@frappe whitelist def get_stock_balance item_code warehouse posting_date None posting_time None with_valuation_rate False from erpnext stock stock_ledger import get_previous_sleif not posting_date posting_date nowdate if not posting_time posting_time nowtime last_entry get_previous_sle {u'item_code' item_code u'warehouse' warehouse u'posting_date' posting_date u'posting_time' posting_time} if with_valuation_rate return last_entry qty_after_transaction last_entry valuation_rate if last_entry else 0 0 0 0 else return last_entry qty_after_transaction or 0 0
@frappe whitelist def get_stock_balance item_code warehouse posting_date None posting_time None with_valuation_rate False from erpnext stock stock_ledger import get_previous_sleif not posting_date posting_date nowdate if not posting_time posting_time nowtime last_entry get_previous_sle {u'item_code' item_code u'warehouse' warehouse u'posting_date' posting_date u'posting_time' posting_time} if with_valuation_rate return last_entry qty_after_transaction last_entry valuation_rate if last_entry else 0 0 0 0 else return last_entry qty_after_transaction or 0 0
@frappe whitelist def get_stock_balance item_code warehouse posting_date None posting_time None with_valuation_rate False from erpnext stock stock_ledger import get_previous_sleif not posting_date posting_date nowdate if not posting_time posting_time nowtime last_entry get_previous_sle {u'item_code' item_code u'warehouse' warehouse u'posting_date' posting_date u'posting_time' posting_time} if with_valuation_rate return last_entry qty_after_transaction last_entry valuation_rate if last_entry else 0 0 0 0 else return last_entry qty_after_transaction or 0 0
@click command u'show-pending-jobs' @click option u'--site' help u'sitename' @pass_contextdef show_pending_jobs context site None from frappe utils doctor import pending_jobs as _pending_jobsif not site site get_site context with frappe init_site site pending_jobs _pending_jobs site site return pending_jobs
def frozen *args raise nx NetworkXError "Frozengraphcan'tbemodified"
def get_top_state_rule_answers exploration_id state_name rule_str_list return get_top_state_rule_answers_multi [ exploration_id state_name ] rule_str_list [0]
def get_top_state_rule_answers exploration_id state_name rule_str_list return get_top_state_rule_answers_multi [ exploration_id state_name ] rule_str_list [0]
def get_soap_accessor db_ip appscale_info get_db_master_ip bindport constants UA_SERVER_PORTreturn SOAPpy SOAPProxy 'https //{0} {1}' format db_ip bindport
def _solve_explike_DE f x DE g k from sympy solvers import rsolvefor t in Add make_args DE coeff d t as_independent g if coeff free_symbols returnRE exp_re DE g k init {}for i in range len Add make_args RE if i f f diff x init[g k subs k i ] f limit x 0 sol rsolve RE g k init if sol return sol / factorial k S Zero S Zero
def shortest_path digr s nodes_explored set [s] nodes_unexplored DFS digr s nodes_unexplored remove s dist {s 0}node_heap []for n in nodes_unexplored min compute_min_dist digr n nodes_explored dist heapq heappush node_heap min n while len node_heap > 0 min_dist nearest_node heapq heappop node_heap dist[nearest_node] min_distnodes_explored add nearest_node nodes_unexplored remove nearest_node for v in digr neighbors nearest_node if v in nodes_unexplored for i in range len node_heap if node_heap[i][1] v node_heap[i] compute_min_dist digr v nodes_explored dist v heapq heapify node_heap return dist
def lookupNull name timeout None return getResolver lookupNull name timeout
def rm_env user name lst list_tab user ret 'absent'rm_ Nonefor ind in range len lst['env'] if name lst['env'][ind]['name'] rm_ indif rm_ is not None lst['env'] pop rm_ ret 'removed'comdat _write_cron_lines user _render_tab lst if comdat['retcode'] return comdat['stderr']return ret
def remove_datasource jboss_config name profile None log debug ' MODULEFUNCTION jboss7 remove_datasource name %s profile %s' name profile operation '/subsystem datasources/data-source {name} remove' format name name if profile is not None operation '/profile "{profile}"' format profile profile + operation return __salt__['jboss7_cli run_operation'] jboss_config operation fail_on_error False
def remove_interface_suffix interface return interface partition '@' [0]
def stop args [] sys_exit True try kill_pid get_pid [0] os unlink PID_FILE except NotRunning as e sys stderr write 'Alreadystopped Statuswas {000 s}\n' format status codes[e status_code] killed_with_force Falseif e status_code STATUS_NOT_RESPONDING sys stderr write 'Notresponding killingwithforce\n' try pid __ read_pid_file PID_FILE kill_pid pid killed_with_force Trueexcept ValueError sys stderr write 'CouldnotfindPIDin pidfile\n' except OSError sys stderr write 'Couldnotread pidfile\n' if not killed_with_force if sys_exit sys exit -1 returnsys stderr write 'kalitestopped\n' if sys_exit sys exit 0
def service_enable s_name **connection_args ret Trueservice _service_get s_name **connection_args if service is None return Falsenitro _connect **connection_args if nitro is None return Falsetry NSService enable nitro service except NSNitroError as error log debug 'netscalermoduleerror-NSService enable failed {0}' format error ret False_disconnect nitro return ret
def inject_field_overrides blocks course user OverrideFieldData provider_classes Nonefor block in blocks block _field_data OverrideFieldData wrap user course block _field_data
def inject_field_overrides blocks course user OverrideFieldData provider_classes Nonefor block in blocks block _field_data OverrideFieldData wrap user course block _field_data
def inject_field_overrides blocks course user OverrideFieldData provider_classes Nonefor block in blocks block _field_data OverrideFieldData wrap user course block _field_data
def overrides_disabled return bool _OVERRIDES_DISABLED disabled
def discover_static_files base_path sub_path '' js_files discover_files base_path sub_path sub_path ext ' js' trim_base_path True sources mocks specs sort_js_files js_files html_files discover_files base_path sub_path sub_path ext ' html' trim_base_path True p path join base_path sub_path _log sources 'JavaScriptsource' p _log mocks 'JavaScriptmock' p _log specs 'JavaScriptspec' p _log html_files 'HTMLtemplate' p return sources mocks specs html_files
def discover_static_files base_path sub_path '' js_files discover_files base_path sub_path sub_path ext ' js' trim_base_path True sources mocks specs sort_js_files js_files html_files discover_files base_path sub_path sub_path ext ' html' trim_base_path True p path join base_path sub_path _log sources 'JavaScriptsource' p _log mocks 'JavaScriptmock' p _log specs 'JavaScriptspec' p _log html_files 'HTMLtemplate' p return sources mocks specs html_files
def __virtual__ if not salt utils is_darwin return False 'mac_portsonlyavailableonMacOS' if not salt utils which 'port' return False 'mac_portsrequiresthe"port"binary' return __virtualname__
def _assert_shielding raw_sss erm_power shielding_factor meg 'mag' picks pick_types raw_sss info meg meg ref_meg False if isinstance erm_power BaseRaw picks_erm pick_types raw_sss info meg meg ref_meg False assert_allclose picks picks_erm erm_power np sqrt erm_power[picks_erm][0] ** 2 sum sss_power raw_sss[picks][0] ravel sss_power np sqrt np sum sss_power * sss_power factor erm_power / sss_power assert_true factor > shielding_factor 'Shieldingfactor%0 3f<%0 3f' % factor shielding_factor
def clean_savepoints using None get_connection using clean_savepoints
def clean_savepoints using None get_connection using clean_savepoints
def clean_savepoints using None get_connection using clean_savepoints
def clean_savepoints using None get_connection using clean_savepoints
def regions from boto glacier layer2 import Layer2return get_regions 'glacier' connection_cls Layer2
def iso_to_plotly_time_string iso_string if iso_string split '-' [ 3] is '00 00' or iso_string split '+' [0] is '00 00' raise Exception "Plotlywon'taccepttimestringswithtimezoneinfo \nAlltimestringsareassumedtobeinUTC " iso_string iso_string replace '-00 00' '' replace '+00 00' '' if iso_string endswith 'T00 00 00' return iso_string replace 'T00 00 00' '' else return iso_string replace 'T' ''
def iso_to_plotly_time_string iso_string if iso_string split '-' [ 3] is '00 00' or iso_string split '+' [0] is '00 00' raise Exception "Plotlywon'taccepttimestringswithtimezoneinfo \nAlltimestringsareassumedtobeinUTC " iso_string iso_string replace '-00 00' '' replace '+00 00' '' if iso_string endswith 'T00 00 00' return iso_string replace 'T00 00 00' '' else return iso_string replace 'T' ''
def fixup_for_packaged if exists join ROOT 'PKG-INFOvi' if '--build-js' in sys argv or '--install-js' in sys argv print SDIST_BUILD_WARNING if '--build-js' in sys argv sys argv remove '--build-js' if '--install-js' in sys argv sys argv remove '--install-js' if '--existing-js' not in sys argv sys argv append '--existing-js'
def shift_series s s0 s copy s0 s0 shift 1 s0 iloc[0] 0 0return s0
def get_backend_configuration backend_name config_stanzas CONF list_all_sections if backend_name not in config_stanzas msg _ 'Couldnotfindbackendstanza% backend_name sinconfiguration Availablestanzasare% stanzas s' params {'stanzas' config_stanzas 'backend_name' backend_name}raise exception ConfigNotFound message msg % params config configuration Configuration driver volume_opts config_group backend_name config append_config_values na_opts netapp_proxy_opts config append_config_values na_opts netapp_connection_opts config append_config_values na_opts netapp_transport_opts config append_config_values na_opts netapp_basicauth_opts config append_config_values na_opts netapp_provisioning_opts config append_config_values na_opts netapp_cluster_opts config append_config_values na_opts netapp_san_opts config append_config_values na_opts netapp_replication_opts return config
def make_lsq_full_matrix x y t k 3 x y t map np asarray x y t m x sizen t size - k - 1 A np zeros m n dtype np float_ for j in range m xval x[j]if xval t[k] left kelse left np searchsorted t xval - 1 bb _bspl evaluate_all_bspl t k xval left A[j left - k left + 1 ] bbB np dot A T A Y np dot A T y c sl solve B Y return c A Y
def nochange function function NO_CHANGE Truereturn function
def nochange function function NO_CHANGE Truereturn function
def nochange function function NO_CHANGE Truereturn function
def test_reason monkeypatch def has_reason ids force reason assert reason 'expiry' monkeypatch setattr SIGN_ADDONS has_reason call_command 'sign_addons' 123 reason 'expiry'
@sync_performerdef perform_update_s3_error_page dispatcher intent s3 boto connect_s3 bucket s3 get_bucket intent bucket config bucket get_website_configuration_obj new_error_key intent error_keyold_error_key config error_keyif old_error_key new_error_key return Noneelse config error_key new_error_keybucket set_website_configuration config return old_error_key
def cs_output func argtypes func argtypes argtypesfunc restype CS_PTRfunc errcheck check_cs_ptrreturn func
def vm_action name kwargs None call None if call 'action' raise SaltCloudSystemExit 'Thevm_actionfunctionmustbecalledwith-aor--action ' if kwargs is None kwargs {}action kwargs get 'action' None if action is None raise SaltCloudSystemExit "Thevm_actionfunctionmusthavean'action'provided " server user password _get_xml_rpc auth ' ' join [user password] vm_id int get_vm_id kwargs {'name' name} response server one vm action auth action vm_id data {'action' 'vm action ' + str action 'actioned' response[0] 'vm_id' response[1] 'error_code' response[2]}return data
def _orbit_transversal degree generators alpha pairs af False tr [ alpha list range degree ]used [False] * degree used[alpha] Truegens [x _array_form for x in generators]for x px in tr for gen in gens temp gen[x]if used[temp] False tr append temp _af_rmul gen px used[temp] Trueif pairs if not af tr [ x _af_new y for x y in tr]return trif af return [y for _ y in tr]return [_af_new y for _ y in tr]
def horcmgr_synchronized func @functools wraps func def wrap self *args **kwargs 'SynchronizeCCIoperationsperCCIinstance '@coordination synchronized self lock[args[0]] def func_locked *_args **_kwargs 'Executethewrappedfunctioninasynchronizedsection 'return func *_args **_kwargs return func_locked self *args **kwargs return wrap
def horcmgr_synchronized func @functools wraps func def wrap self *args **kwargs 'SynchronizeCCIoperationsperCCIinstance '@coordination synchronized self lock[args[0]] def func_locked *_args **_kwargs 'Executethewrappedfunctioninasynchronizedsection 'return func *_args **_kwargs return func_locked self *args **kwargs return wrap
def has_wildcard pattern match wildcard_check_pattern search pattern return match is not None
def get_attr cmd if u' ' in cmd method frappe get_attr cmd else method globals [cmd]frappe log u'method ' + cmd return method
def main description project_id day month year hours minutes source_bucket sink_bucket credentials GoogleCredentials get_application_default storagetransfer discovery build 'storagetransfer' 'v1' credentials credentials transfer_job {'description' description 'status' 'ENABLED' 'projectId' project_id 'schedule' {'scheduleStartDate' {'day' day 'month' month 'year' year} 'startTimeOfDay' {'hours' hours 'minutes' minutes}} 'transferSpec' {'gcsDataSource' {'bucketName' source_bucket} 'gcsDataSink' {'bucketName' sink_bucket} 'objectConditions' {'minTimeElapsedSinceLastModification' '2592000s'} 'transferOptions' {'deleteObjectsFromSourceAfterTransfer' 'true'}}}result storagetransfer transferJobs create body transfer_job execute print 'ReturnedtransferJob {}' format json dumps result indent 4
def proxied_attribute local_attr proxied_attr doc def fget self return getattr getattr self local_attr proxied_attr def fset self value setattr getattr self local_attr proxied_attr value def fdel self delattr getattr self local_attr proxied_attr return property fget fset fdel doc
def proxied_attribute local_attr proxied_attr doc def fget self return getattr getattr self local_attr proxied_attr def fset self value setattr getattr self local_attr proxied_attr value def fdel self delattr getattr self local_attr proxied_attr return property fget fset fdel doc
def _ComputeOTP secret t h hmac new base64 b32decode secret struct pack '>Q' t hashlib sha1 hash h digest offset struct unpack 'B' hash[ -1 ] [0] & 15 truncated_hash struct unpack '>I' hash[offset offset + 4 ] [0]truncated_hash & 2147483647truncated_hash % _VERIFY_MODULUSreturn truncated_hash
def main if len sys argv > 1 writeOutput '' join sys argv[1 ] else settings startMainLoopFromConstructor getNewRepository
def import_vul_csv_part2 job_id request post_vars jobif not job_id return "ErrorNoJobID'sprovided"output s3_rest_controller 'vulnerability' 'data' csv_stylesheet 'data xsl' totalRecords output[0]totalErrors output[1]totalIgnored output[2]from gluon serializers import json as jsonsresponse headers['Content-Type'] 'application/json'return jsons {'totalRecords' totalRecords 'totalErrors' totalErrors 'totalIgnored' totalIgnored}
def locate_profile profile 'default' from IPython core profiledir import ProfileDir ProfileDirErrortry pd ProfileDir find_profile_dir_by_name get_ipython_dir profile except ProfileDirError raise IOError "Couldn'tfindprofile%r" % profile return pd location
def _resolveIPv6 ip port return socket getaddrinfo ip port 0 0 0 _NUMERIC_ONLY [0][4]
def example_certificates_status course_key return ExampleCertificateSet latest_status course_key
def normalize pos size flip_y False width height sizex pos[0]y pos[1]x / float width y / float height if flip_y return x 1 - y return x y
def dummy_deepcopy *arg return None
def dummy_deepcopy *arg return None
def dummy_deepcopy *arg return None
def getVersionString version result '%s%s' % version package version short return result
def get_security_default from hadoop import clustercluster cluster get_cluster_conf_for_job_submission return cluster SECURITY_ENABLED get
def modeladmin_register modeladmin_class instance modeladmin_class instance register_with_wagtail
def _initialise bot BridgeInstance bot 'samplebridge' IncomingMessages
def setup_ssl_options config_dir src dest option_path os path join config_dir dest shutil copyfile src option_path return option_path
def fnmatch_all names patterns for name in names matches Falsefor pattern in patterns matches fnmatch fnmatch name pattern if matches breakif not matches return Falsereturn True
def GetModuleForCLSID clsid clsid_str str clsid try typelibCLSID lcid major minor clsidToTypelib[clsid_str]except KeyError return Nonetry mod GetModuleForTypelib typelibCLSID lcid major minor except ImportError mod Noneif mod is not None sub_mod mod CLSIDToPackageMap get clsid_str if sub_mod is None sub_mod mod VTablesToPackageMap get clsid_str if sub_mod is not None sub_mod_name mod __name__ + ' ' + sub_mod try __import__ sub_mod_name except ImportError info typelibCLSID lcid major minor if info in demandGeneratedTypeLibraries info demandGeneratedTypeLibraries[info]import makepymakepy GenerateChildFromTypeLibSpec sub_mod info mod sys modules[sub_mod_name]return mod
def _item_to_subscription_for_topic iterator subscription_path subscription_name subscription_name_from_path subscription_path iterator client project return Subscription subscription_name iterator topic
def load_key_bindings_for_prompt **kw kw setdefault u'enable_abort_and_exit_bindings' True kw setdefault u'enable_search' True kw setdefault u'enable_auto_suggest_bindings' True return load_key_bindings **kw
def setup_platform hass config add_devices discovery_info None hosts []if discovery_info and discovery_info in KNOWN_HOSTS returnif discovery_info is not None _LOGGER debug 'DiscoveredRoku %s' discovery_info[0] hosts append discovery_info[0] elif CONF_HOST in config hosts append config get CONF_HOST rokus []for host in hosts new_roku RokuDevice host if new_roku name is None _LOGGER error 'Unabletoinitializerokuat%s' host else rokus append RokuDevice host KNOWN_HOSTS append host add_devices rokus
def write_bem_solution fname bem _check_bem_size bem['surfs'] with start_file fname as fid start_block fid FIFF FIFFB_BEM write_int fid FIFF FIFF_BEM_COORD_FRAME bem['surfs'][0]['coord_frame'] _write_bem_surfaces_block fid bem['surfs'] if 'solution' in bem if bem['bem_method'] FIFF FWD_BEM_LINEAR_COLL raise RuntimeError 'Onlylinearcollocationsupported' write_int fid FIFF FIFF_BEM_APPROX FIFF FIFFV_BEM_APPROX_LINEAR write_float_matrix fid FIFF FIFF_BEM_POT_SOLUTION bem['solution'] end_block fid FIFF FIFFB_BEM end_file fid
def color_print *args **kwargs file kwargs get u'file' _get_stdout end kwargs get u'end' u'\n' write file writeif isatty file and conf use_color for i in range 0 len args 2 msg args[i]if i + 1 len args color u''else color args[ i + 1 ]if color msg _color_text msg color if six PY2 and isinstance msg bytes msg _decode_preferred_encoding msg write _write_with_fallback msg write file write end else for i in range 0 len args 2 msg args[i]if six PY2 and isinstance msg bytes msg _decode_preferred_encoding msg write msg write end
def color_print *args **kwargs file kwargs get u'file' _get_stdout end kwargs get u'end' u'\n' write file writeif isatty file and conf use_color for i in range 0 len args 2 msg args[i]if i + 1 len args color u''else color args[ i + 1 ]if color msg _color_text msg color if six PY2 and isinstance msg bytes msg _decode_preferred_encoding msg write _write_with_fallback msg write file write end else for i in range 0 len args 2 msg args[i]if six PY2 and isinstance msg bytes msg _decode_preferred_encoding msg write msg write end
def make_abstract_dist req_to_install if req_to_install editable return IsSDist req_to_install elif req_to_install link and req_to_install link is_wheel return IsWheel req_to_install else return IsSDist req_to_install
def strip_formatted_string str return re sub '\\s\\s+' '' str strip
def strip_formatted_string str return re sub '\\s\\s+' '' str strip
@require_chanmsg@require_privilege OP u'Youarenotachanneloperator ' @commands u'kick' @priority u'high' def kick bot trigger if bot privileges[trigger sender][bot nick] < HALFOP return bot reply u"I'mnotachanneloperator " text trigger group split argc len text if argc < 2 returnopt Identifier text[1] nick optchannel trigger senderreasonidx 2if not opt is_nick if argc < 3 returnnick text[2]channel optreasonidx 3reason u'' join text[reasonidx ] if nick bot config core nick bot write [u'KICK' channel nick] reason
def _ros_sort df observations censorship warn False censored sort_values df[df[censorship]] observations axis 0 uncensored sort_values df[ ~ df[censorship] ] observations axis 0 if censored[observations] max > uncensored[observations] max censored censored[ censored[observations] < uncensored[observations] max ]if warn msg 'Droppingcensoredobservationsgreaterthanthemaxuncensoredobservation 'warnings warn msg return censored append uncensored [[observations censorship]] reset_index drop True
def _read_byte f return np uint8 struct unpack '>B' f read 4 [ 1] [0]
def axapi_enabled_disabled flag if flag 'enabled' return 1else return 0
def axapi_enabled_disabled flag if flag 'enabled' return 1else return 0
def is_ a b msg None assert a is b msg or '%risnot%r' % a b
def get_instance_uuid_by_ec2_id context ec2_id return IMPL get_instance_uuid_by_ec2_id context ec2_id
def get_instance_uuid_by_ec2_id context ec2_id return IMPL get_instance_uuid_by_ec2_id context ec2_id
def _init_log global logorig_logger_cls logging getLoggerClass logging setLoggerClass AstropyLogger try log logging getLogger 'astropy' log _set_defaults finally logging setLoggerClass orig_logger_cls return log
def _init_log global logorig_logger_cls logging getLoggerClass logging setLoggerClass AstropyLogger try log logging getLogger 'astropy' log _set_defaults finally logging setLoggerClass orig_logger_cls return log
def _init_log global logorig_logger_cls logging getLoggerClass logging setLoggerClass AstropyLogger try log logging getLogger 'astropy' log _set_defaults finally logging setLoggerClass orig_logger_cls return log
def _get_eth_link vif ifc_num link_id vif get 'devname' if not link_id link_id 'interface%d' % ifc_num if vif get 'type' 'ethernet' nic_type 'phy'else nic_type vif get 'type' link {'id' link_id 'vif_id' vif['id'] 'type' nic_type 'mtu' vif['network']['meta'] get 'mtu' 'ethernet_mac_address' vif get 'address' }return link
def shlex_split s if not six PY2 or isinstance s bytes return shlex split s elif isinstance s six text_type bs s encode 'utf-8' return [c decode 'utf-8' for c in shlex split bs ]else raise TypeError u'shlex_splitcalledwithnon-string'
def _lombscargle_pdf z N normalization dH 1 dK 3 if dK - dH 2 raise NotImplementedError 'Degreesoffreedom 2' Nk N - dK if normalization 'psd' return np exp - z elif normalization 'standard' return 0 5 * Nk * 1 + z ** -0 5 * Nk - 1 elif normalization 'model' return 0 5 * Nk * 1 - z ** 0 5 * Nk - 1 elif normalization 'log' return 0 5 * Nk * np exp -0 5 * Nk * z else raise ValueError "normalization '{0}'isnotrecognized" format normalization
def _lombscargle_pdf z N normalization dH 1 dK 3 if dK - dH 2 raise NotImplementedError 'Degreesoffreedom 2' Nk N - dK if normalization 'psd' return np exp - z elif normalization 'standard' return 0 5 * Nk * 1 + z ** -0 5 * Nk - 1 elif normalization 'model' return 0 5 * Nk * 1 - z ** 0 5 * Nk - 1 elif normalization 'log' return 0 5 * Nk * np exp -0 5 * Nk * z else raise ValueError "normalization '{0}'isnotrecognized" format normalization
def find_documentation module_data start_line -1 mod_ast_tree ast parse module_data for child in mod_ast_tree body if isinstance child ast Assign for target in child targets if target id 'DOCUMENTATION' start_line child lineno - 1 breakreturn start_line
def arbitrary_element iterable if is_iterator iterable raise ValueError 'cannotreturnanarbitraryitemfromaniterator' return next iter iterable
@lower_constant types Record def constant_record context builder ty pyval lty ir ArrayType ir IntType 8 pyval nbytes val lty bytearray pyval tostring return cgutils alloca_once_value builder val
@functools lru_cache maxsize None def is_language_prefix_patterns_used urlconf for url_pattern in get_resolver urlconf url_patterns if isinstance url_pattern LocaleRegexURLResolver return True url_pattern prefix_default_language return False False
@functools lru_cache maxsize None def is_language_prefix_patterns_used urlconf for url_pattern in get_resolver urlconf url_patterns if isinstance url_pattern LocaleRegexURLResolver return True url_pattern prefix_default_language return False False
def enabled name ret {'name' name 'result' True 'comment' '' 'changes' {}}is_enabled __salt__['apache check_site_enabled'] name if not is_enabled if __opts__['test'] msg 'Apachesite{0}issettobeenabled ' format name ret['comment'] msgret['changes']['old'] Noneret['changes']['new'] nameret['result'] Nonereturn retstatus __salt__['apache a2ensite'] name ['Status']if isinstance status string_types and 'enabled' in status ret['result'] Trueret['changes']['old'] Noneret['changes']['new'] nameelse ret['result'] Falseret['comment'] 'Failedtoenable{0}Apachesite' format name if isinstance status string_types ret['comment'] ret['comment'] + ' {0} ' format status return retelse ret['comment'] '{0}alreadyenabled ' format name return ret
def jobconf_from_env variable default None name variable replace ' ' '_' if name in os environ return os environ[name]for var in _JOBCONF_MAP get variable {} values name var replace ' ' '_' if name in os environ return os environ[name]return default
def get_print_name node simple_form True name node fullnameif hasattr node u'_interface' pkglist node _interface __class__ __module__ split u' ' interface node _interface __class__ __name__destclass u''if len pkglist > 2 destclass u' %s' % pkglist[2] if simple_form name node fullname + destclass else name u' ' join [node fullname interface] + destclass if simple_form parts name split u' ' if len parts > 2 return u' ' join parts[1 ] + u' ' elif len parts 2 return parts[1]return name
def handle_sigusr1 signal_number stack_frame handlers logging getLoggerClass manager root handlersreopen_log_files handlers handlers
def handle_sigusr1 signal_number stack_frame handlers logging getLoggerClass manager root handlersreopen_log_files handlers handlers
def handle_sigusr1 signal_number stack_frame handlers logging getLoggerClass manager root handlersreopen_log_files handlers handlers
def handle_sigusr1 signal_number stack_frame handlers logging getLoggerClass manager root handlersreopen_log_files handlers handlers
def getPreferenceColour name colorString getPreference name return [ float int colorString[1 3] 16 / 255 float int colorString[3 5] 16 / 255 float int colorString[5 7] 16 / 255 1 0]
def getPreferenceColour name colorString getPreference name return [ float int colorString[1 3] 16 / 255 float int colorString[3 5] 16 / 255 float int colorString[5 7] 16 / 255 1 0]
def _find_address_range addresses it iter addresses first last next it for ip in it if ip _ip last _ip + 1 yield first last first iplast ip yield first last
def all_bases_valid seq valid_bases ['a' 'A' 'c' 'C' 'g' 'G' 't' 'T' 'N']for base in seq if base not in valid_bases return Falsereturn True
def get_brick var return get_annotation var Brick
def get_brick var return get_annotation var Brick
def test_ajd n_times n_channels 10 3 seed np random RandomState 0 diags 2 0 + 0 1 * seed randn n_times n_channels A 2 * seed rand n_channels n_channels - 1 A / np atleast_2d np sqrt np sum A ** 2 1 Tcovmats np empty n_times n_channels n_channels for i in range n_times covmats[i] np dot np dot A np diag diags[i] A T V D _ajd_pham covmats V_matlab [[ -3 507280775058041 -5 498189967306344 7 720624541198574] [0 69468901323461 0 775690358505945 -1 162043086446043 ] [ -0 592603135588066 -0 59899692569626 1 009550086271192]]assert_array_almost_equal V V_matlab
def test_ajd n_times n_channels 10 3 seed np random RandomState 0 diags 2 0 + 0 1 * seed randn n_times n_channels A 2 * seed rand n_channels n_channels - 1 A / np atleast_2d np sqrt np sum A ** 2 1 Tcovmats np empty n_times n_channels n_channels for i in range n_times covmats[i] np dot np dot A np diag diags[i] A T V D _ajd_pham covmats V_matlab [[ -3 507280775058041 -5 498189967306344 7 720624541198574] [0 69468901323461 0 775690358505945 -1 162043086446043 ] [ -0 592603135588066 -0 59899692569626 1 009550086271192]]assert_array_almost_equal V V_matlab
def dzip_exact *dicts if not same *map viewkeys dicts raise ValueError 'dictkeysnotallequal \n\n%s' % _format_unequal_keys dicts return {k tuple d[k] for d in dicts for k in dicts[0]}
def semanage_port_get_ports seport setype proto records seport get_all_by_type if setype proto in records return records[ setype proto ]else return []
def semanage_port_get_ports seport setype proto records seport get_all_by_type if setype proto in records return records[ setype proto ]else return []
def process_static_urls text replacement_function data_dir None def wrap_part_extraction match '\nUnwrapsamatchgroupforthecapturesspecifiedin_url_replace_regex\nandforwardthemonasfunctionarguments\n'original match group 0 prefix match group 'prefix' quote match group 'quote' rest match group 'rest' full_url prefix + rest starts_with_static_url full_url startswith unicode settings STATIC_URL starts_with_prefix full_url startswith XBLOCK_STATIC_RESOURCE_PREFIX contains_prefix XBLOCK_STATIC_RESOURCE_PREFIX in full_url if starts_with_prefix or starts_with_static_url and contains_prefix return originalreturn replacement_function original prefix quote rest return re sub _url_replace_regex u' ? {static_url} /static/ ? {data_dir} ' format static_url settings STATIC_URL data_dir data_dir wrap_part_extraction text
def process_static_urls text replacement_function data_dir None def wrap_part_extraction match '\nUnwrapsamatchgroupforthecapturesspecifiedin_url_replace_regex\nandforwardthemonasfunctionarguments\n'original match group 0 prefix match group 'prefix' quote match group 'quote' rest match group 'rest' full_url prefix + rest starts_with_static_url full_url startswith unicode settings STATIC_URL starts_with_prefix full_url startswith XBLOCK_STATIC_RESOURCE_PREFIX contains_prefix XBLOCK_STATIC_RESOURCE_PREFIX in full_url if starts_with_prefix or starts_with_static_url and contains_prefix return originalreturn replacement_function original prefix quote rest return re sub _url_replace_regex u' ? {static_url} /static/ ? {data_dir} ' format static_url settings STATIC_URL data_dir data_dir wrap_part_extraction text
def conv1d_sc input filters image_shape None filter_shape None border_mode 'valid' subsample 1 filter_flip True if border_mode not in 'valid' 0 0 raise RuntimeError 'Unsupportedborder_modeforconv1d_sc %s' % border_mode if image_shape is None image_shape_sc Noneelse image_shape_sc image_shape[0] 1 image_shape[1] image_shape[2] if filter_shape is None filter_shape_sc Noneelse filter_shape_sc filter_shape[0] 1 filter_shape[1] filter_shape[2] input_sc input dimshuffle 0 'x' 1 2 filters_sc filters dimshuffle 0 'x' 1 2 [ -1 ]conved T nnet conv2d input_sc filters_sc image_shape_sc filter_shape_sc subsample 1 subsample[0] filter_flip filter_flip return conved[ 0 ]
def standalone view_func def inner request *args **kwargs response view_func request *args **kwargs if isinstance response HttpResponse response standalone Truereturn responsereturn wraps view_func inner
def mkfs device fs_type _validate_device device if fs_type not in set ['ext2' 'fat32' 'fat16' 'linux-swap' 'reiserfs' 'hfs' 'hfs+' 'hfsx' 'NTFS' 'ufs'] raise CommandExecutionError 'Invalidfs_typepassedtopartition mkfs' if fs_type is 'linux-swap' mkfs_cmd 'mkswap'else mkfs_cmd 'mkfs {0}' format fs_type if not salt utils which mkfs_cmd return 'Error {0}isunavailable 'cmd '{0}{1}' format mkfs_cmd device out __salt__['cmd run'] cmd splitlines return out
def _load formula _mk_client paths []for ext in 'yaml' 'json' source_url salt utils url create formula + '/defaults ' + ext paths append source_url defaults_files __context__['cp fileclient'] cache_files paths for file_ in defaults_files if not file_ continuesuffix file_ rsplit ' ' 1 [ -1 ]if suffix 'yaml' loader yamlelif suffix 'json' loader jsonelse log debug 'Failedtodetermineloaderfor%r' file_ continueif os path exists file_ log debug 'Readingdefaultsfrom%r' file_ with salt utils fopen file_ as fhr defaults loader load fhr log debug 'Readdefaults%r' defaults return defaults or {}
def test_cons_correct can_compile u' consab '
def is_running container try infos _get_container_infos container return infos get 'State' {} get 'Running' except Exception return False
def is_running container try infos _get_container_infos container return infos get 'State' {} get 'Running' except Exception return False
def _FormatServiceHealthReport report assert report get 'status' 'ALERT' message ''sub_messages []alerts report get 'alerts' if len alerts > 1 message + ' %dAlerts ' % len alerts for a in alerts sub_message '' + a get 'description' a get 'name' if a get 'cluster' False sub_message + ' Cluster 'else sub_message + ' %dmachines ' % a get 'count' sub_messages append sub_message return message + ' ' join sub_messages
def call_blink *args **kwargs devices _get_lights pause kwargs get 'pause' 0 res dict for dev_id in 'id' not in kwargs and sorted devices keys or _get_devices kwargs state devices[str dev_id ]['state']['on']_set dev_id state and Const LAMP_OFF or Const LAMP_ON if pause time sleep pause res[dev_id] _set dev_id not state and Const LAMP_OFF or Const LAMP_ON return res
def test_should_support_both_meta_sequence_and_constructor_exclude class SequencedTable tables Table a tables Column b tables Column c tables Column class Meta sequence u'a' u' ' table SequencedTable [] exclude u'c' table as_html request
def test_should_support_both_meta_sequence_and_constructor_exclude class SequencedTable tables Table a tables Column b tables Column c tables Column class Meta sequence u'a' u' ' table SequencedTable [] exclude u'c' table as_html request
def service_get_by_host_and_topic context host topic return IMPL service_get_by_host_and_topic context host topic
def get_full_pci_id pci_id cmd "lspci-D awk'/%s/{print$1}'" % pci_id status full_id commands getstatusoutput cmd if status 0 return Nonereturn full_id
def ChiSquared name k return rv name ChiSquaredDistribution k
def follow_log module le_path logs name None logtype None followed_count 0for log in logs if query_log_status module le_path log continueif module check_mode module exit_json changed True cmd [le_path 'follow' log]if name cmd extend ['--name' name] if logtype cmd extend ['--type' logtype] rc out err module run_command '' join cmd if not query_log_status module le_path log module fail_json msg "failedtofollow'%s' %s" % log err strip followed_count + 1if followed_count > 0 module exit_json changed True msg 'followed%dlog s ' % followed_count module exit_json changed False msg 'logs s alreadyfollowed'
def make_extension name files *args **kwargs ext DelayedExtension name files *args **kwargs for dir in get_base_dirs include_dir os path join dir 'include' if os path exists include_dir ext include_dirs append include_dir for lib in 'lib' 'lib64' lib_dir os path join dir lib if os path exists lib_dir ext library_dirs append lib_dir ext include_dirs append ' ' return ext
def _is_unicode_combining u i ord u for r in _COMBINING_RANGES if r[0] < i < r[1] return Truereturn False
def _is_unicode_combining u i ord u for r in _COMBINING_RANGES if r[0] < i < r[1] return Truereturn False
def autolabel rectangles for rect in rectangles height rect get_height ax text rect get_x + rect get_width / 2 0 1 05 * height '% 4f' % height ha 'center' va 'bottom'
def set_seed x node_names list six iterkeys tf get_default_graph _nodes_by_name if len node_names > 0 and node_names ['keras_learning_phase'] raise RuntimeError 'Seedingisnotsupportedafterinitializingpartofthegraph Pleasemoveset_seedtothebeginningofyourcode ' np random seed x tf set_random_seed x
def _writeDocument newFilename clonedNode makeSureDirectoryExists newFilename f open newFilename 'w' f write clonedNode toxml 'utf-8' f close
@utils synchronized SERIAL_LOCK def acquire_port host start stop _get_port_range for port in six moves range start stop if host port in ALLOCATED_PORTS continuetry _verify_port host port ALLOCATED_PORTS add host port return portexcept exception SocketPortInUseException as e LOG warning e format_message raise exception SocketPortRangeExhaustedException host host
def setup_platform hass config add_devices discovery_info None add_devices [ZigBeeBinarySensor hass ZigBeeDigitalInConfig config ]
def set_remote_login enable state salt utils mac_utils validate_enabled enable cmd 'systemsetup-f-setremotelogin{0}' format state salt utils mac_utils execute_return_success cmd return salt utils mac_utils confirm_updated state get_remote_login normalize_ret True
def set_remote_login enable state salt utils mac_utils validate_enabled enable cmd 'systemsetup-f-setremotelogin{0}' format state salt utils mac_utils execute_return_success cmd return salt utils mac_utils confirm_updated state get_remote_login normalize_ret True
@contextmanagerdef _start_worker_thread app concurrency 1 pool u'solo' loglevel WORKER_LOGLEVEL logfile None WorkController TestWorkController **kwargs setup_app_for_worker app loglevel logfile assert u'celery ping' in app tasks with app connection as conn conn default_channel queue_declareworker WorkController app app concurrency concurrency hostname anon_nodename pool pool loglevel loglevel logfile logfile ready_callback None without_heartbeat True without_mingle True without_gossip True **kwargs t threading Thread target worker start t start worker ensure_started _set_task_join_will_block False yield worker from celery worker import statestate should_terminate 0t join 10 state should_terminate None
def volume_detach name profile None timeout 300 conn _auth profile return conn volume_detach name timeout
def volume_detach name profile None timeout 300 conn _auth profile return conn volume_detach name timeout
@pytest mark parametrize 'sep' [None 'sep'] def test_str_cat sep if sep is None expr t_str_cat name str_cat t_str_cat comment expected '\nSELECTaccounts2 name accounts2 comment\nASanon_1FROMaccounts2\n'else expr t_str_cat name str_cat t_str_cat comment sep sep expected '\nSELECTaccounts2 name name_1 accounts2 comment\nASanon_1FROMaccounts2\n'result str compute expr s_str_cat return_type 'native' assert normalize result normalize expected
def read config_file with io open config_file u'rb' as filehandle return filehandle read
def calculateNonce realm algorithm MD5 global SUPPORTED_ALGORITHM DIGEST_AUTH_ENCODERSassert algorithm in SUPPORTED_ALGORITHM try encoder DIGEST_AUTH_ENCODERS[algorithm]except KeyError raise NotImplementedError 'Thechosenalgorithm %s doesnothaveanimplementationyet' % algorithm return encoder '%d %s' % time time realm
def calculateNonce realm algorithm MD5 global SUPPORTED_ALGORITHM DIGEST_AUTH_ENCODERSassert algorithm in SUPPORTED_ALGORITHM try encoder DIGEST_AUTH_ENCODERS[algorithm]except KeyError raise NotImplementedError 'Thechosenalgorithm %s doesnothaveanimplementationyet' % algorithm return encoder '%d %s' % time time realm
def _get_decimal128 data position dummy0 dummy1 dummy2 end position + 16 return Decimal128 from_bid data[position end] end
def _get_label_flip labels label_vertidx src from label import label_sign_fliplabel_flip list for label vertidx in zip labels label_vertidx if label hemi 'both' raise ValueError 'BiHemiLabelnotsupportedwhenusingsign-flip' if vertidx is not None flip label_sign_flip label src [ None]else flip Nonelabel_flip append flip return label_flip
def version with settings hide 'running' 'stdout' 'warnings' warn_only True res run 'java-version' if res failed return Noneelse return _extract_jdk_version res
def cigame registry xml_parent data XML SubElement xml_parent 'hudson plugins cigame GamePublisher'
def cigame registry xml_parent data XML SubElement xml_parent 'hudson plugins cigame GamePublisher'
def cigame registry xml_parent data XML SubElement xml_parent 'hudson plugins cigame GamePublisher'
def p_boolean_sliding_window texts segmented_topics dictionary window_size top_ids _ret_top_ids segmented_topics window_id 0per_topic_postings {}token2id_dict dictionary token2iddef add_topic_posting top_ids window per_topic_postings window_id token2id_dict for word in window word_id token2id_dict[word]if word_id in top_ids if word_id in per_topic_postings per_topic_postings[word_id] add window_id else per_topic_postings[word_id] set [window_id] window_id + 1return window_id per_topic_postings for document in texts it iter document window tuple islice it window_size window_id per_topic_postings add_topic_posting top_ids window per_topic_postings window_id token2id_dict for elem in it window window[1 ] + elem window_id per_topic_postings add_topic_posting top_ids window per_topic_postings window_id token2id_dict return per_topic_postings window_id
def check_fill_values data assert_true data['a'] mask [False False] all assert_true data['a'] ['1' 'a'] all assert_true data['b'] mask [False True] all assert_true data['b'] [2 -999 ] all data['b'] mask Falseassert_true data['b'] [2 1] all
def remove_file source if os path isdir source shutil rmtree source elif os path isfile source or os path islink source os remove source
def warp_coords coord_map shape dtype np float64 shape safe_as_int shape rows cols shape[0] shape[1] coords_shape [len shape rows cols]if len shape 3 coords_shape append shape[2] coords np empty coords_shape dtype dtype tf_coords np indices cols rows dtype dtype reshape 2 -1 Ttf_coords coord_map tf_coords tf_coords tf_coords T reshape -1 cols rows swapaxes 1 2 _stackcopy coords[1 ] tf_coords[0 ] _stackcopy coords[0 ] tf_coords[1 ] if len shape 3 coords[2 ] range shape[2] return coords
def config_value key app None default None app app or current_app return get_config app get key upper default
def lfsr_autocorrelation L P k if not isinstance L list raise TypeError 'L %s mustbealist' % L P int P k int k L0 L[ P]L1 L0 + L0[ k] L2 [ -1 ** L1[i] to_int + L1[ i + k ] to_int for i in range P ]tot sum L2 return Rational tot P
def purge name delete_key True quiet False path None data _do_names name 'destroy' path path if data is False return dataif delete_key skey salt key Key __opts__ skey delete_key name if data is None returnif not quiet __jid_event__ fire_event {'data' data 'outputter' 'lxc_purge'} 'progress' return data
def templated_docstring **docs def decorator f f __doc__ format_docstring f __name__ f __doc__ docs return freturn decorator
def set_hostname new_hostname global hostnamehostname new_hostname
def selectDialect protocol dialect protocol _selectDialect dialect
def current_metadata items assert itemslikelies {}consensus {}fields ['artist' 'album' 'albumartist' 'year' 'disctotal' 'mb_albumid' 'label' 'catalognum' 'country' 'media' 'albumdisambig']for field in fields values [item[field] for item in items if item] likelies[field] freq plurality values consensus[field] freq len values if consensus['albumartist'] and likelies['albumartist'] likelies['artist'] likelies['albumartist']return likelies consensus
def loadJsonValueFromFile inputFilePath with open inputFilePath as fileObj value json load fileObj return value
def calculate_distance_between_colors color1 color2 return color1[0] + color2[0] / 2 color1[1] + color2[1] / 2 color1[2] + color2[2] / 2
def calculate_distance_between_colors color1 color2 return color1[0] + color2[0] / 2 color1[1] + color2[1] / 2 color1[2] + color2[2] / 2
def calculate_distance_between_colors color1 color2 return color1[0] + color2[0] / 2 color1[1] + color2[1] / 2 color1[2] + color2[2] / 2
@core_helperdef roles_translated return authz roles_trans
def set_locale cls request force None locales cls app config get 'locales' if not locales return Nonelocale forceif locale not in locales locale cls request get 'hl' None if locale not in locales locale cls request cookies get 'hl' None if locale not in locales locale get_locale_from_accept_header cls request if locale not in locales territory get_country_code request or 'ZZ' locale str Locale negotiate territory locales if locale not in locales locale i18n get_store default_localei18n get_i18n set_locale locale cls response set_cookie 'hl' locale max_age 15724800 return locale
@must_be_logged_indef personal_access_token_detail auth **kwargs _id kwargs get '_id' try record ApiOAuth2PersonalToken find_one Q '_id' 'eq' _id except NoResultsFound raise HTTPError http NOT_FOUND if record owner auth user raise HTTPError http FORBIDDEN if record is_active is False raise HTTPError http GONE token_detail_url api_v2_url 'tokens/{}/' format _id return {'token_list_url' '' 'token_detail_url' token_detail_url 'scope_options' get_available_scopes }
def add_permission FunctionName StatementId Action Principal SourceArn None SourceAccount None Qualifier None region None key None keyid None profile None try conn _get_conn region region key key keyid keyid profile profile kwargs {}for key in 'SourceArn' 'SourceAccount' 'Qualifier' if locals [key] is not None kwargs[key] str locals [key] conn add_permission FunctionName FunctionName StatementId StatementId Action Action Principal str Principal **kwargs return {'updated' True}except ClientError as e return {'updated' False 'error' salt utils boto3 get_error e }
def approve_files files_with_review_type for file_ review_type in files_with_review_type version file_ versionaddon version addonhelper ReviewHelper request None addon addon version file_ version helper set_data {'addon_files' [file_] 'comments' u'bulkapproval'} if review_type 'full' helper handler process_public log info u'File%s addon%s approved' file_ pk addon pk else log info u'File%s addon%s notapproved addonstatus %s filestatus %s' file_ pk addon pk addon status file_ status
def get_resume_recommendations user request final get_most_recent_incomplete_item user if final content get_content_item language request language channel getattr final 'channel' 'khan' content_id final get 'id' return [content] if content else [] else return []
def approx_point_little src dest xy src read 2 * 8 dest write '\x00\x00\x00' dest write xy[ -13 -8 ] dest write '\x00\x00\x00' dest write xy[ -5 ]
def trace_view request if request method upper 'TRACE' return HttpResponseNotAllowed 'TRACE' elif request body return HttpResponseBadRequest 'TRACErequestsMUSTNOTincludeanentity' else protocol request META['SERVER_PROTOCOL']t Template '{{method}}{{uri}}{{version}}' name 'TRACETemplate' c Context {'method' request method 'uri' request path 'version' protocol} return HttpResponse t render c
def trace_view request if request method upper 'TRACE' return HttpResponseNotAllowed 'TRACE' elif request body return HttpResponseBadRequest 'TRACErequestsMUSTNOTincludeanentity' else protocol request META['SERVER_PROTOCOL']t Template '{{method}}{{uri}}{{version}}' name 'TRACETemplate' c Context {'method' request method 'uri' request path 'version' protocol} return HttpResponse t render c
def get_from_url url import urllib2try return urllib2 urlopen url read except return None
def getBytesFromFile filename offset numBytes if not isinstance offset int or not isinstance numBytes int return -1 'Theoffsetandthenumberofbytesmustbeintegers' if os path exists filename fileSize os path getsize filename bytesFile open filename 'rb' bytesFile seek offset if offset + numBytes > fileSize bytes bytesFile read else bytes bytesFile read numBytes bytesFile close return 0 bytes else return -1 'Filedoesnotexist'
def eglob pattern directory ' ' pieces pathsplit pattern return __find_matches pieces directory
def eglob pattern directory ' ' pieces pathsplit pattern return __find_matches pieces directory
def eglob pattern directory ' ' pieces pathsplit pattern return __find_matches pieces directory
def _get_token username __opts__ get 'rallydev' {} get 'username' None password __opts__ get 'rallydev' {} get 'password' None path 'https //rally1 rallydev com/slm/webservice/v2 0/security/authorize'result salt utils http query path decode True decode_type 'json' text True status True username username password password cookies True persist_session True opts __opts__ if 'dict' not in result return Nonereturn result['dict']['OperationResult']['SecurityToken']
def _to_bytes msg return _to_bytes_impl msg
def pot_for_column cls column summary False if summary return 'flavor'elif column info get 'ripped' return 'ripped'elif column name endswith 'effect' return 'effects'else return 'misc'
def submit_detailed_enrollment_features_csv request course_key task_type 'detailed_enrollment_report'task_class enrollment_report_features_csvtask_input {}task_key ''return submit_task request task_type task_class course_key task_input task_key
def set_environment env settings environment env
def extract_lsq_problems problems OrderedDict for name problem_class in inspect getmembers sys modules[__name__] inspect isclass if name 'LSQBenchmarkProblem' and issubclass problem_class LSQBenchmarkProblem and hasattr problem_class 'INITIAL_GUESSES' for i x0 in enumerate problem_class INITIAL_GUESSES if len problem_class INITIAL_GUESSES > 1 key_name '{0}_{1}' format name i else key_name nameproblems[key_name] problem_class x0 return problems
def extract_lsq_problems problems OrderedDict for name problem_class in inspect getmembers sys modules[__name__] inspect isclass if name 'LSQBenchmarkProblem' and issubclass problem_class LSQBenchmarkProblem and hasattr problem_class 'INITIAL_GUESSES' for i x0 in enumerate problem_class INITIAL_GUESSES if len problem_class INITIAL_GUESSES > 1 key_name '{0}_{1}' format name i else key_name nameproblems[key_name] problem_class x0 return problems
def test_json_camelcase test_data {'under_score' 'values_can' 'be_converted' [{'to_camelcase' 'value'} 'wont_be_convert']}output hug output_format json_camelcase test_data decode 'utf8' assert 'underScore' in output assert 'values_can' in output assert 'beConverted' in output assert 'toCamelcase' in output assert 'value' in output assert 'wont_be_convert' in output
def setglobal name value global_dict local_dict get_twill_glocals global_dict[name] value
def setglobal name value global_dict local_dict get_twill_glocals global_dict[name] value
@require_GETdef ajax_status request if not request user is_authenticated raise PermissionDeniedqs UserPreference objects filter user request user key NOTIFICATION_PREF_KEY return HttpResponse json dumps {'status' len qs } content_type 'application/json'
def domain url match r_domain search url if match domain strip_www match group 1 else domain urlreturn domain lower
def match_all string trie matches []for i in range len string substr string[ i + 1 ]if not trie has_prefix substr breakif substr in trie matches append substr return matches
def match_all string trie matches []for i in range len string substr string[ i + 1 ]if not trie has_prefix substr breakif substr in trie matches append substr return matches
@testing requires_testing_datadef test_acqparser_averaging raw read_raw_fif fname_raw_elekta preload True acqp AcqParserFIF raw info for cat in acqp categories cond acqp get_condition raw cat eps Epochs raw baseline -0 05 0 **cond ev eps average ev_ref read_evokeds fname_ave_elekta cat['comment'] baseline -0 05 0 proj False ev_mag ev copy ev_mag pick_channels ['MEG0111'] ev_grad ev copy ev_grad pick_channels ['MEG2643' 'MEG1622'] ev_ref_mag ev_ref copy ev_ref_mag pick_channels ['MEG0111'] ev_ref_grad ev_ref copy ev_ref_grad pick_channels ['MEG2643' 'MEG1622'] assert_allclose ev_mag data ev_ref_mag data rtol 0 atol 1e-15 assert_allclose ev_grad data ev_ref_grad data rtol 0 atol 1e-13
def get_versions_string result []for version in get_versions_list result append '*%s%s' % version[0] version[2] return '\n' join result
def gf_trace_map a b c n f p K u gf_compose_mod a b f p K v bif n & 1 U gf_add a u p K V belse U aV cn >> 1while n u gf_add u gf_compose_mod u v f p K p K v gf_compose_mod v v f p K if n & 1 U gf_add U gf_compose_mod u V f p K p K V gf_compose_mod v V f p K n >> 1return gf_compose_mod a V f p K U
def gf_trace_map a b c n f p K u gf_compose_mod a b f p K v bif n & 1 U gf_add a u p K V belse U aV cn >> 1while n u gf_add u gf_compose_mod u v f p K p K v gf_compose_mod v v f p K if n & 1 U gf_add U gf_compose_mod u V f p K p K V gf_compose_mod v V f p K n >> 1return gf_compose_mod a V f p K U
def _set_custom_selection params chs params['fig_selection'] lasso selectionif len chs 0 returnlabels [l _text for l in params['fig_selection'] radio labels]inds np in1d params['raw'] ch_names chs params['selections']['Custom'] np where inds [0]_set_radio_button labels index 'Custom' params params
def _set_custom_selection params chs params['fig_selection'] lasso selectionif len chs 0 returnlabels [l _text for l in params['fig_selection'] radio labels]inds np in1d params['raw'] ch_names chs params['selections']['Custom'] np where inds [0]_set_radio_button labels index 'Custom' params params
def unindent_dict docdict can_dict {}for name dstr in docdict items can_dict[name] unindent_string dstr return can_dict
def unindent_dict docdict can_dict {}for name dstr in docdict items can_dict[name] unindent_string dstr return can_dict
def Laplace name mu b return rv name LaplaceDistribution mu b
@register simple_tag takes_context True def render_product context product if not product return ''names [ 'catalogue/partials/product/upc-%s html' % product upc 'catalogue/partials/product/class-%s html' % product get_product_class slug 'catalogue/partials/product html']template_ select_template names context['product'] productreturn template_ render context
@register simple_tag takes_context True def render_product context product if not product return ''names [ 'catalogue/partials/product/upc-%s html' % product upc 'catalogue/partials/product/class-%s html' % product get_product_class slug 'catalogue/partials/product html']template_ select_template names context['product'] productreturn template_ render context
def inbox if not auth s3_logged_in session error T 'RequiresLogin ' redirect URL c 'default' f 'user' args 'login' table s3db msg_messages3 filter table inbound True table inbound readable Falsetablename 'msg_message's3 crud_strings[tablename] Storage title_display T 'MessageDetails' title_list T 'InBox' label_list_button T 'ViewInBox' label_delete_button T 'DeleteMessage' msg_record_deleted T 'Messagedeleted' msg_list_empty T 'NoMessagescurrentlyinInBox' s3db configure tablename editable False insertable False list_fields ['id' 'date' 'channel_id' 'from_address' 'body'] return s3_rest_controller module 'message'
def get_application_call var return get_annotation var ApplicationCall
def get_application_call var return get_annotation var ApplicationCall
def test_column_params_should_be_preserved_under_inheritance class MyTableA MyTable u'\nhavinganempty`classMeta`shouldnotundotheexplicitdefinition\nofcolumnitem1inMyTable \n'class Meta MyTable Meta passclass MyTableB MyTable u'\nhavinganon-empty`classMeta`shouldnotundotheexplicitdefinition\nofcolumnitem1inMyTable \n'class Meta MyTable Meta per_page 22table MyTable MyModel objects all tableA MyTableA MyModel objects all tableB MyTableB MyModel objects all assert table columns[u'item1'] verbose_name u'Nicecolumnname' assert tableA columns[u'item1'] verbose_name u'Nicecolumnname' assert tableB columns[u'item1'] verbose_name u'Nicecolumnname'
@pytest fixture scope 'session' def dcos_launchpad dcos_api_session if 'AWS_STACK_NAME' not in os environ pytest skip 'MustuseaAWSCloudformationtoruntest' stack_name os environ['AWS_STACK_NAME']aws_region os environ['AWS_REGION']aws_access_key_id os environ['AWS_ACCESS_KEY_ID']aws_secret_access_key os environ['AWS_SECRET_ACCESS_KEY']bw test_util aws BotoWrapper aws_region aws_access_key_id aws_secret_access_key return test_util aws DcosCfSimple stack_name bw
@pytest fixture scope 'session' def dcos_launchpad dcos_api_session if 'AWS_STACK_NAME' not in os environ pytest skip 'MustuseaAWSCloudformationtoruntest' stack_name os environ['AWS_STACK_NAME']aws_region os environ['AWS_REGION']aws_access_key_id os environ['AWS_ACCESS_KEY_ID']aws_secret_access_key os environ['AWS_SECRET_ACCESS_KEY']bw test_util aws BotoWrapper aws_region aws_access_key_id aws_secret_access_key return test_util aws DcosCfSimple stack_name bw
def get_cache_dir subdir None cache_dir os environ get 'NEON_CACHE_DIR' if cache_dir is None cache_dir appdirs user_cache_dir 'neon' 'neon' if subdir subdir subdir if isinstance subdir list else [subdir] cache_dir os path join cache_dir *subdir if not os path exists cache_dir os makedirs cache_dir return cache_dir
def rand_alnum length 0 jibber '' join [letters digits] return '' join choice jibber for _ in xrange length or randint 10 30
def to_class_path cls return ' ' join [cls __module__ cls __name__]
def fake_pgettext translations def _pgettext context text return translations get context text text return _pgettext
def inv a return Inv a
def updating_writer a log debug 'updatingthecontext' context a[0]register 3slave_id 0address 16values context[slave_id] getValues register address count 5 values [ v + 1 for v in values]log debug 'newvalues ' + str values context[slave_id] setValues register address values
def getGeometryPath subName '' return getJoinedPath getFabmetheusUtilitiesPath 'geometry' subName
@publicdef together expr deep False def _together expr if isinstance expr Basic if expr is_Atom or expr is_Function and not deep return exprelif expr is_Add return gcd_terms list map _together Add make_args expr elif expr is_Pow base _together expr base if deep exp _together expr exp else exp expr expreturn expr __class__ base exp else return expr __class__ *[_together arg for arg in expr args] elif iterable expr return expr __class__ [_together ex for ex in expr] return exprreturn _together sympify expr
def get_preamble latex_preamble rcParams get u'pgf preamble' u'' if type latex_preamble list latex_preamble u'\n' join latex_preamble return latex_preamble
def coordinate_to_tuple coordinate col row coordinate_from_string coordinate return row _COL_STRING_CACHE[col]
def strftime fmt t None if not fmt return u''if t is None t time localtime if hasattr t 'timetuple' t t timetuple early_year t[0] < 1900 if early_year replacement 1900 if t[0] % 4 0 else 1901 fmt fmt replace '%Y' '_earlyyearhack##' t list t orig_year t[0]t[0] replacementans Noneif iswindows if isinstance fmt unicode fmt fmt encode 'mbcs' fmt fmt replace '%e' '%#d' ans plugins['winutil'][0] strftime fmt t else ans time strftime fmt t decode preferred_encoding 'replace' if early_year ans ans replace '_earlyyearhack##' str orig_year return ans
def strftime fmt t None if not fmt return u''if t is None t time localtime if hasattr t 'timetuple' t t timetuple early_year t[0] < 1900 if early_year replacement 1900 if t[0] % 4 0 else 1901 fmt fmt replace '%Y' '_earlyyearhack##' t list t orig_year t[0]t[0] replacementans Noneif iswindows if isinstance fmt unicode fmt fmt encode 'mbcs' fmt fmt replace '%e' '%#d' ans plugins['winutil'][0] strftime fmt t else ans time strftime fmt t decode preferred_encoding 'replace' if early_year ans ans replace '_earlyyearhack##' str orig_year return ans
def strftime fmt t None if not fmt return u''if t is None t time localtime if hasattr t 'timetuple' t t timetuple early_year t[0] < 1900 if early_year replacement 1900 if t[0] % 4 0 else 1901 fmt fmt replace '%Y' '_earlyyearhack##' t list t orig_year t[0]t[0] replacementans Noneif iswindows if isinstance fmt unicode fmt fmt encode 'mbcs' fmt fmt replace '%e' '%#d' ans plugins['winutil'][0] strftime fmt t else ans time strftime fmt t decode preferred_encoding 'replace' if early_year ans ans replace '_earlyyearhack##' str orig_year return ans
def clocktime_to_millisecond value return value // Gst MSECOND
def _offset_or_limit_clause_asint clause attrname if clause is None return Nonetry value clause _limit_offset_valueexcept AttributeError raise exc CompileError 'ThisSELECTstructuredoesnotuseasimpleintegervaluefor%s' % attrname else return util asint value
def optimize p gets set puts []prevpos Nonefor opcode arg pos in genops p if prevpos is not None puts append prevarg prevpos pos prevpos Noneif 'PUT' in opcode name prevarg prevpos arg pos elif 'GET' in opcode name gets add arg s []i 0for arg start stop in puts j stop if arg in gets else start s append p[i j] i stops append p[i ] return '' join s
def optimize p gets set puts []prevpos Nonefor opcode arg pos in genops p if prevpos is not None puts append prevarg prevpos pos prevpos Noneif 'PUT' in opcode name prevarg prevpos arg pos elif 'GET' in opcode name gets add arg s []i 0for arg start stop in puts j stop if arg in gets else start s append p[i j] i stops append p[i ] return '' join s
def optimize p gets set puts []prevpos Nonefor opcode arg pos in genops p if prevpos is not None puts append prevarg prevpos pos prevpos Noneif 'PUT' in opcode name prevarg prevpos arg pos elif 'GET' in opcode name gets add arg s []i 0for arg start stop in puts j stop if arg in gets else start s append p[i j] i stops append p[i ] return '' join s
def optimize p gets set puts []prevpos Nonefor opcode arg pos in genops p if prevpos is not None puts append prevarg prevpos pos prevpos Noneif 'PUT' in opcode name prevarg prevpos arg pos elif 'GET' in opcode name gets add arg s []i 0for arg start stop in puts j stop if arg in gets else start s append p[i j] i stops append p[i ] return '' join s
@pytest mark parametrize 'parallel' [True False] def test_commented_header parallel read_commented_header text '\n#ABC\n123\n456\n't1 read_commented_header text parallel parallel expected Table [[1 4] [2 5] [3 6]] names 'A' 'B' 'C' assert_table_equal t1 expected text '#firstcommentedline\n#secondcommentedline\n\n' + text t2 read_commented_header text header_start 2 data_start 0 parallel parallel assert_table_equal t2 expected t3 read_commented_header text header_start -1 data_start 0 parallel parallel assert_table_equal t3 expected text + '789't4 read_commented_header text header_start 2 data_start 2 parallel parallel expected Table [[7] [8] [9]] names 'A' 'B' 'C' assert_table_equal t4 expected with pytest raises ParameterError read_commented_header text header_start -1 data_start -1 parallel parallel
@pytest mark parametrize 'parallel' [True False] def test_commented_header parallel read_commented_header text '\n#ABC\n123\n456\n't1 read_commented_header text parallel parallel expected Table [[1 4] [2 5] [3 6]] names 'A' 'B' 'C' assert_table_equal t1 expected text '#firstcommentedline\n#secondcommentedline\n\n' + text t2 read_commented_header text header_start 2 data_start 0 parallel parallel assert_table_equal t2 expected t3 read_commented_header text header_start -1 data_start 0 parallel parallel assert_table_equal t3 expected text + '789't4 read_commented_header text header_start 2 data_start 2 parallel parallel expected Table [[7] [8] [9]] names 'A' 'B' 'C' assert_table_equal t4 expected with pytest raises ParameterError read_commented_header text header_start -1 data_start -1 parallel parallel
@pytest mark parametrize 'parallel' [True False] def test_commented_header parallel read_commented_header text '\n#ABC\n123\n456\n't1 read_commented_header text parallel parallel expected Table [[1 4] [2 5] [3 6]] names 'A' 'B' 'C' assert_table_equal t1 expected text '#firstcommentedline\n#secondcommentedline\n\n' + text t2 read_commented_header text header_start 2 data_start 0 parallel parallel assert_table_equal t2 expected t3 read_commented_header text header_start -1 data_start 0 parallel parallel assert_table_equal t3 expected text + '789't4 read_commented_header text header_start 2 data_start 2 parallel parallel expected Table [[7] [8] [9]] names 'A' 'B' 'C' assert_table_equal t4 expected with pytest raises ParameterError read_commented_header text header_start -1 data_start -1 parallel parallel
def scourLength length length SVGLength length return scourUnitlessLength length value + Unit str length units
def get_video_from_youtube_id youtube_id for channel language in available_content_databases video _get_video_from_youtube_id channel channel language language youtube_id youtube_id if video return video
def missing_node_cache prov_dir node_list provider opts cached_nodes []for node in os listdir prov_dir cached_nodes append os path splitext node [0] for node in cached_nodes if node not in node_list delete_minion_cachedir node provider opts if 'diff_cache_events' in opts and opts['diff_cache_events'] fire_event 'event' 'cachednodemissingfromprovider' 'salt/cloud/{0}/cache_node_missing' format node args {'missingnode' node} sock_dir opts get 'sock_dir' os path join __opts__['sock_dir'] 'master' transport opts get 'transport' 'zeromq'
def missing_node_cache prov_dir node_list provider opts cached_nodes []for node in os listdir prov_dir cached_nodes append os path splitext node [0] for node in cached_nodes if node not in node_list delete_minion_cachedir node provider opts if 'diff_cache_events' in opts and opts['diff_cache_events'] fire_event 'event' 'cachednodemissingfromprovider' 'salt/cloud/{0}/cache_node_missing' format node args {'missingnode' node} sock_dir opts get 'sock_dir' os path join __opts__['sock_dir'] 'master' transport opts get 'transport' 'zeromq'
def makedirs_ path user None group None mode None path os path expanduser path dirname os path normpath os path dirname path if os path isdir dirname msg "Directory'{0}'alreadyexists" format dirname log debug msg return msgif os path exists dirname msg "Thepath'{0}'alreadyexistsandisnotadirectory" format dirname log debug msg return msgdirectories_to_create []while True if os path isdir dirname breakdirectories_to_create append dirname current_dirname dirnamedirname os path dirname dirname if current_dirname dirname raise SaltInvocationError "Recursivecreationforpath'{0}'wouldresultinaninfiniteloop Pleaseuseanabsolutepath " format dirname directories_to_create reverse for directory_to_create in directories_to_create log debug 'Creatingdirectory %s' directory_to_create mkdir directory_to_create user user group group mode mode
def jarque_bera x x np asarray x n float x size if n 0 raise ValueError 'Atleastoneobservationisrequired ' mu x mean diffx x - mu skewness 1 / n * np sum diffx ** 3 / 1 / n * np sum diffx ** 2 ** 3 / 2 0 kurtosis 1 / n * np sum diffx ** 4 / 1 / n * np sum diffx ** 2 ** 2 jb_value n / 6 * skewness ** 2 + kurtosis - 3 ** 2 / 4 p 1 - distributions chi2 cdf jb_value 2 return jb_value p
def jarque_bera x x np asarray x n float x size if n 0 raise ValueError 'Atleastoneobservationisrequired ' mu x mean diffx x - mu skewness 1 / n * np sum diffx ** 3 / 1 / n * np sum diffx ** 2 ** 3 / 2 0 kurtosis 1 / n * np sum diffx ** 4 / 1 / n * np sum diffx ** 2 ** 2 jb_value n / 6 * skewness ** 2 + kurtosis - 3 ** 2 / 4 p 1 - distributions chi2 cdf jb_value 2 return jb_value p
def attach_is_fan queryset user as_field 'is_fan_attr' model queryset modelif user is None or user is_anonymous sql 'SELECTfalse'else sql "\nSELECTCOUNT likes_like id >0\nFROMlikes_like\nINNERJOINdjango_content_typeONlikes_like content_type_id django_content_type id\nWHEREdjango_content_type model 'project'AND\ndjango_content_type app_label 'projects'AND\nlikes_like user_id {user_id}AND\nlikes_like object_id {tbl} id\n"sql sql format tbl model _meta db_table user_id user id queryset queryset extra select {as_field sql} return queryset
def attach_is_fan queryset user as_field 'is_fan_attr' model queryset modelif user is None or user is_anonymous sql 'SELECTfalse'else sql "\nSELECTCOUNT likes_like id >0\nFROMlikes_like\nINNERJOINdjango_content_typeONlikes_like content_type_id django_content_type id\nWHEREdjango_content_type model 'project'AND\ndjango_content_type app_label 'projects'AND\nlikes_like user_id {user_id}AND\nlikes_like object_id {tbl} id\n"sql sql format tbl model _meta db_table user_id user id queryset queryset extra select {as_field sql} return queryset
def uninstall global _task_task stop _task None
def get_changes_for_svn_txn repo_path txn_id changes subproc_check_output ['svnlook' 'changed' repo_path '-t' txn_id] for line in StringIO changes yield line[0] line[4 -1 ]
def delete_api_deployment restApiId deploymentId region None key None keyid None profile None try conn _get_conn region region key key keyid keyid profile profile conn delete_deployment restApiId restApiId deploymentId deploymentId return {'deleted' True}except ClientError as e return {'deleted' False 'error' salt utils boto3 get_error e }
def delete_api_deployment restApiId deploymentId region None key None keyid None profile None try conn _get_conn region region key key keyid keyid profile profile conn delete_deployment restApiId restApiId deploymentId deploymentId return {'deleted' True}except ClientError as e return {'deleted' False 'error' salt utils boto3 get_error e }
@pytest fixturedef english from pootle_language models import Languagereturn Language objects get code 'en'
def getProcessOutputAndValue executable args env {} path None reactor None return _callProtocolWithDeferred _EverythingGetter executable args env path reactor
def cleanQuery query retVal queryfor sqlStatements in SQL_STATEMENTS values for sqlStatement in sqlStatements sqlStatementEsc sqlStatement replace ' ' '\\ ' queryMatch re search ' %s ' % sqlStatementEsc query re I if queryMatch and 'sys_exec' not in query retVal retVal replace queryMatch group 1 sqlStatement upper return retVal
def fix_error_editor self filename linenum column msg warnings warn '\n`fix_error_editor`ispendingdeprecationasofIPython5 0andwillberemoved\ninfutureversions Itappearstobeusedonlyforautomaticallyfixingsyntax\nerrorthathasbeenbrokenforafewyearsandhasthusbeenremoved Ifyou\nhappendtousethisfunctionandstillneeditpleasemakeyourvoiceheardon\nthemailinglistipython-dev@scipy org orontheGitHubIssuetracker \nhttps //github com/ipython/ipython/issues/9649' UserWarning def vim_quickfix_file t tempfile NamedTemporaryFile t write '%s %d %d %s\n' % filename linenum column msg t flush return tif os path basename self editor 'vim' self hooks editor filename linenum returnt vim_quickfix_file try if os system 'vim--cmd"seterrorformat %f %l %c %m"-q' + t name raise TryNext finally t close
def handle_cputime_metric common_props sample cputime_samples []wallclock_key u'--WALL--'for data in sample['value'] itervalues wall_time data[wallclock_key]for process value in data iteritems if process wallclock_key cputime_sample dict common_props cputime_sample['process'] processcputime_sample['value'] valuecputime_sample['wallclock'] wall_timecputime_samples append cputime_sample return cputime_samples
def report_nodes_not_run notrun if notrun logger info u'***********************************' for info in notrun logger error u'couldnotrunnode %s' % u' ' join info[u'node'] _hierarchy info[u'node'] _id logger info u'crashfile %s' % info[u'crashfile'] logger debug u'Thefollowingdependentnodeswerenotrun' for subnode in info[u'dependents'] logger debug subnode _id logger info u'***********************************' raise RuntimeError u'Workflowdidnotexecutecleanly Checklogfordetails'
def report_nodes_not_run notrun if notrun logger info u'***********************************' for info in notrun logger error u'couldnotrunnode %s' % u' ' join info[u'node'] _hierarchy info[u'node'] _id logger info u'crashfile %s' % info[u'crashfile'] logger debug u'Thefollowingdependentnodeswerenotrun' for subnode in info[u'dependents'] logger debug subnode _id logger info u'***********************************' raise RuntimeError u'Workflowdidnotexecutecleanly Checklogfordetails'
def report_nodes_not_run notrun if notrun logger info u'***********************************' for info in notrun logger error u'couldnotrunnode %s' % u' ' join info[u'node'] _hierarchy info[u'node'] _id logger info u'crashfile %s' % info[u'crashfile'] logger debug u'Thefollowingdependentnodeswerenotrun' for subnode in info[u'dependents'] logger debug subnode _id logger info u'***********************************' raise RuntimeError u'Workflowdidnotexecutecleanly Checklogfordetails'
def report_nodes_not_run notrun if notrun logger info u'***********************************' for info in notrun logger error u'couldnotrunnode %s' % u' ' join info[u'node'] _hierarchy info[u'node'] _id logger info u'crashfile %s' % info[u'crashfile'] logger debug u'Thefollowingdependentnodeswerenotrun' for subnode in info[u'dependents'] logger debug subnode _id logger info u'***********************************' raise RuntimeError u'Workflowdidnotexecutecleanly Checklogfordetails'
@pytest mark django_dbdef test_add_store_fs_by_store po_directory english fs_path '/some/fs/example_store po'project ProjectDBFactory source_language english language LanguageDBFactory tp TranslationProjectFactory project project language language store StoreDBFactory translation_project tp parent tp directory name 'example_store po' conf config get tp project __class__ instance tp project conf set_config 'pootle_fs fs_type' 'localfs' conf set_config 'pootle_fs fs_url' 'foo' store_fs StoreFS objects create store store path fs_path assert store_fs project project assert store_fs store store assert store_fs pootle_path store pootle_path assert store_fs path fs_path assert store_fs last_sync_hash is None assert store_fs last_sync_mtime is None assert store_fs last_sync_revision is None
def sdm_spoly f g O K phantom None if not f or not g return sdm_zero LM1 sdm_LM f LM2 sdm_LM g if LM1[0] LM2[0] return sdm_zero LM1 LM1[1 ]LM2 LM2[1 ]lcm monomial_lcm LM1 LM2 m1 monomial_div lcm LM1 m2 monomial_div lcm LM2 c K quo - sdm_LC f K sdm_LC g K r1 sdm_add sdm_mul_term f m1 K one O K sdm_mul_term g m2 c O K O K if phantom is None return r1r2 sdm_add sdm_mul_term phantom[0] m1 K one O K sdm_mul_term phantom[1] m2 c O K O K return r1 r2
def absent name region user None opts False ret {'name' name 'result' True 'comment' '' 'changes' {}}does_exist __salt__['aws_sqs queue_exists'] name region opts user if does_exist if __opts__['test'] ret['result'] Noneret['comment'] 'AWSSQSqueue{0}issettoberemoved' format name return retremoved __salt__['aws_sqs delete_queue'] name region opts user if removed['retcode'] 0 ret['changes']['removed'] removed['stdout']else ret['result'] Falseret['comment'] removed['stderr']else ret['comment'] u'{0}doesnotexistin{1}' format name region return ret
def info msg html False also_console False write msg 'INFO' html if also_console console msg
def pattern_matches pattern target if not isinstance pattern Pattern pattern _compile_pattern pattern return pattern matches target
def randrange n vmin vmax return vmax - vmin * np random rand n + vmin
def randrange n vmin vmax return vmax - vmin * np random rand n + vmin
def upgrade migrate_engine meta MetaData meta bind migrate_enginevolume_type_projects Table 'volume_type_projects' meta autoload True if migrate_engine name 'postgresql' sql 'ALTERTABLEvolume_type_projectsALTERCOLUMNdeleted' + 'TYPEINTEGERUSINGdeleted integer' migrate_engine execute sql else volume_type_projects c deleted alter Integer
def setup_logging log setup CONF 'keystone' logging captureWarnings True
def get_default_version server None req servers_service_pb GetDefaultVersionRequest if server req set_server server resp servers_service_pb GetDefaultVersionResponse try apiproxy_stub_map MakeSyncCall 'servers' 'GetDefaultVersion' req resp except apiproxy_errors ApplicationError as e if e application_error servers_service_pb ServersServiceError INVALID_SERVER raise InvalidServerError if e application_error servers_service_pb ServersServiceError INVALID_VERSION raise InvalidVersionError else raise Error return resp version
def _get_menu_class_for_instance menu_class instance attrs {'instance' instance}class_name menu_class __name__meta_class type menu_class return meta_class class_name menu_class attrs
def _get_menu_class_for_instance menu_class instance attrs {'instance' instance}class_name menu_class __name__meta_class type menu_class return meta_class class_name menu_class attrs
def _attachment_sequence attachments if len attachments 2 and isinstance attachments[0] basestring return attachments return attachments
def get_std_icon name size None if not name startswith 'SP_' name 'SP_' + name icon QWidget style standardIcon getattr QStyle name if size is None return iconelse return QIcon icon pixmap size size
def load stream Loader Loader loader Loader stream try r loader get_single_data return r loader haserrors finally loader dispose
def load stream Loader Loader loader Loader stream try r loader get_single_data return r loader haserrors finally loader dispose
@apply_to_maskdef even_size clip w h clip sizeif w % 2 0 and h % 2 0 return clipif w % 2 0 and h % 2 0 fl_image lambda a a[ -1 -1 ] elif w % 2 0 fl_image lambda a a[ -1 ] else fl_image lambda a a[ -1 ] return clip fl_image fl_image
def cloneHost individual clone individual __class__ individual clone fitness values individual fitness valuesreturn clone
def cloneHost individual clone individual __class__ individual clone fitness values individual fitness valuesreturn clone
def get_runnertype_by_name runnertype_name try runnertypes RunnerType query name runnertype_name except ValueError ValidationError as e LOG error 'Databaselookupforname "%s"resultedinexception %s' runnertype_name e raise StackStormDBObjectNotFoundError 'Unabletofindrunnertypewithname "%s"' % runnertype_name if not runnertypes raise StackStormDBObjectNotFoundError 'UnabletofindRunnerTypewithname "%s"' % runnertype_name if len runnertypes > 1 LOG warning 'MorethanoneRunnerTypereturnedfromDBlookupbyname Resultlistis %s' runnertypes return runnertypes[0]
def get_solidity if get_compiler_path is None return Nonereturn solc_wrapper
def get_solidity if get_compiler_path is None return Nonereturn solc_wrapper
def test_no_nans np random seed 1 image 0 5 + 1e-09 * np random normal size 20 20 template np ones 6 6 template[ 3 ] 0result match_template image template assert not np any np isnan result
def test_no_nans np random seed 1 image 0 5 + 1e-09 * np random normal size 20 20 template np ones 6 6 template[ 3 ] 0result match_template image template assert not np any np isnan result
def test_no_nans np random seed 1 image 0 5 + 1e-09 * np random normal size 20 20 template np ones 6 6 template[ 3 ] 0result match_template image template assert not np any np isnan result
def _native_set_to_python_list typ payload c nitems payload usedlistobj c pyapi list_new nitems ok cgutils is_not_null c builder listobj with c builder if_then ok likely True index cgutils alloca_once_value c builder ir Constant nitems type 0 with payload _iterate as loop i c builder load index item loop entry keyitemobj c box typ dtype item c pyapi list_setitem listobj i itemobj i c builder add i ir Constant i type 1 c builder store i index return ok listobj
def async_runner timeout async_factory _AsyncRunner make_factory timeout timeout total_seconds suppress_twisted_logging False store_twisted_logs False return retry_flaky async_factory
def find_host search_host inventory host_attribs inventory get search_host {} print json dumps host_attribs
def decodeGenerator params return GenericBatchGenerator
def decodeGenerator params return GenericBatchGenerator
def _find_appropriate_compiler _config_vars if 'CC' in os environ return _config_varscc oldcc _config_vars['CC'] split [0]if not _find_executable cc cc _find_build_tool 'clang' elif os path basename cc startswith 'gcc' data _read_output "'%s'--version" % cc replace "'" '\'"\'"\'' if data and 'llvm-gcc' in data cc _find_build_tool 'clang' if not cc raise SystemError 'Cannotlocateworkingcompiler' if cc oldcc for cv in _COMPILER_CONFIG_VARS if cv in _config_vars and cv not in os environ cv_split _config_vars[cv] split cv_split[0] cc if cv 'CXX' else cc + '++' _save_modified_value _config_vars cv '' join cv_split return _config_vars
def _pnio_rtc_guess_payload_class _pkt _underlayer None *args **kargs config pnio_get_config _underlayer if isinstance config list cur_index - len _pkt for index cls params in config if cur_index index return cls _pkt config params *args **kargs return PNIORealTimeIOxS _pkt *args **kargs else return PNIORealTimeRawData _pkt config {'length' len _pkt } *args **kargs
def _pnio_rtc_guess_payload_class _pkt _underlayer None *args **kargs config pnio_get_config _underlayer if isinstance config list cur_index - len _pkt for index cls params in config if cur_index index return cls _pkt config params *args **kargs return PNIORealTimeIOxS _pkt *args **kargs else return PNIORealTimeRawData _pkt config {'length' len _pkt } *args **kargs
def assign obj **kwargs obj __dict__ update kwargs
def remove_items headers condition removed {}keys filter condition headers removed update key headers pop key for key in keys return removed
def remove_items headers condition removed {}keys filter condition headers removed update key headers pop key for key in keys return removed
@receiver REGISTER_USER def email_marketing_register_user sender user None profile None **kwargs email_config EmailMarketingConfiguration current if not email_config enabled returnif user is_anonymous returnupdate_user delay _create_sailthru_user_vars user user profile user email site _get_current_site new_user True
def _dotrig a b return a func b func and a has TrigonometricFunction and b has TrigonometricFunction or a has HyperbolicFunction and b has HyperbolicFunction
def _dotrig a b return a func b func and a has TrigonometricFunction and b has TrigonometricFunction or a has HyperbolicFunction and b has HyperbolicFunction
def encode_table pieces table table table or {} length_index len pieces pieces append None tablesize 0for key value in table items tablesize + encode_short_string pieces key tablesize + encode_value pieces value pieces[length_index] struct pack '>I' tablesize return tablesize + 4
def encode_table pieces table table table or {} length_index len pieces pieces append None tablesize 0for key value in table items tablesize + encode_short_string pieces key tablesize + encode_value pieces value pieces[length_index] struct pack '>I' tablesize return tablesize + 4
def to_sql frame name con flavor None schema None if_exists 'fail' index True index_label None chunksize None dtype None if if_exists not in 'fail' 'replace' 'append' raise ValueError "'{0}'isnotvalidforif_exists" format if_exists pandas_sql pandasSQL_builder con schema schema flavor flavor if isinstance frame Series frame frame to_frame elif not isinstance frame DataFrame raise NotImplementedError "'frame'argumentshouldbeeitheraSeriesoraDataFrame" pandas_sql to_sql frame name if_exists if_exists index index index_label index_label schema schema chunksize chunksize dtype dtype
def assert_desired_datasets case deployer desired_manifestations local_datasets local_applications additional_node_config set expected_datasets leases Leases calculator RecordingCalculator NOTHING_TO_DO deployer deployer set calculator calculator cluster_configuration Deployment nodes {Node uuid deployer node_uuid hostname deployer hostname manifestations {manifestation dataset dataset_id manifestation for manifestation in desired_manifestations} } additional_node_config leases leases local_state BlockDeviceDeployerLocalState node_uuid deployer node_uuid hostname deployer hostname datasets {dataset dataset_id dataset for dataset in local_datasets} node_state nonmanifest_datasets local_state shared_state_changes cluster_state DeploymentState nodes {node_state set applications local_applications } nonmanifest_datasets nonmanifest_datasets datasets deployer calculate_changes configuration cluster_configuration cluster_state cluster_state local_state local_state case assertEqual {dataset dataset_id dataset for dataset in expected_datasets} calculator desired_datasets
@handle_dashboard_error@require_POST@ensure_csrf_cookie@cache_control no_cache True no_store True must_revalidate True @require_level 'staff' @require_post_params 'student' 'url' def reset_due_date request course_id course get_course_by_id SlashSeparatedCourseKey from_deprecated_string course_id student require_student_from_identifier request POST get 'student' unit find_unit course request POST get 'url' set_due_date_extension course unit student None if not getattr unit 'due' None return JsonResponse _ 'Successfullyremovedinvalidduedateextension unithasnoduedate ' original_due_date_str unit due strftime '%Y-%m-%d%H %M' return JsonResponse _ 'Successfullyresetduedateforstudent{0}for{1}to{2}' format student profile name _display_unit unit original_due_date_str
def get_nat_gateways client subnet_id None nat_gateway_id None states None check_mode False params dict err_msg ''gateways_retrieved Falseexisting_gateways list if not states states ['available' 'pending']if nat_gateway_id params['NatGatewayIds'] [nat_gateway_id]else params['Filter'] [{'Name' 'subnet-id' 'Values' [subnet_id]} {'Name' 'state' 'Values' states}]try if not check_mode gateways client describe_nat_gateways **params ['NatGateways']if gateways for gw in gateways existing_gateways append convert_to_lower gw gateways_retrieved Trueelse gateways_retrieved Trueif nat_gateway_id if DRY_RUN_GATEWAYS[0]['nat_gateway_id'] nat_gateway_id existing_gateways DRY_RUN_GATEWAYSelif subnet_id if DRY_RUN_GATEWAYS[0]['subnet_id'] subnet_id existing_gateways DRY_RUN_GATEWAYSerr_msg '{0}Retrievinggateways' format DRY_RUN_MSGS except botocore exceptions ClientError as e err_msg str e return gateways_retrieved err_msg existing_gateways
def _entity_from_response_type feature_type results detected_objects []if feature_type _FACE_ANNOTATIONS detected_objects extend Face from_api_repr face for face in results elif feature_type _IMAGE_PROPERTIES_ANNOTATION return ImagePropertiesAnnotation from_api_repr results elif feature_type _SAFE_SEARCH_ANNOTATION return SafeSearchAnnotation from_api_repr results else for result in results detected_objects append EntityAnnotation from_api_repr result return detected_objects
def get_user bootinfo bootinfo user frappe get_user load_user
def quote_windows value value list2cmdline [value] return value
def get_combination_hash_from_variable_mapping parent variables mapping {}for variable_identifier value_identifier in variables items variable _ ProductVariationVariable objects get_or_create product parent identifier force_text variable_identifier value _ ProductVariationVariableValue objects get_or_create variable variable identifier force_text value_identifier mapping[variable] valuereturn hash_combination mapping
def setup_platform hass config add_devices_callback discovery_info None def run_setup event 'DelaythesetupuntilHomeAssistantisfullyinitialized \n\nThisallowsanyentitiestobecreatedalready\n'options config get CONF_OPTIONS if options get 'units' is None options['units'] hass config units nametravel_mode config get CONF_TRAVEL_MODE mode options get CONF_MODE if travel_mode is not None wstr 'GoogleTravelTime travel_modeisdeprecated pleaseaddmodetotheoptionsdictionaryinstead '_LOGGER warning wstr if mode is None options[CONF_MODE] travel_modetitled_mode options get CONF_MODE title formatted_name '{}-{}' format DEFAULT_NAME titled_mode name config get CONF_NAME formatted_name api_key config get CONF_API_KEY origin config get CONF_ORIGIN destination config get CONF_DESTINATION sensor GoogleTravelTimeSensor hass name api_key origin destination options if sensor valid_api_connection add_devices_callback [sensor] hass bus listen_once EVENT_HOMEASSISTANT_START run_setup
def stub_verify conn cert errno errdepth code return True
def stub_verify conn cert errno errdepth code return True
def stub_verify conn cert errno errdepth code return True
def stub_verify conn cert errno errdepth code return True
def tunnel_connection socket addr server keyfile None password None paramiko None timeout 60 new_url tunnel open_tunnel addr server keyfile keyfile password password paramiko paramiko timeout timeout socket connect new_url return tunnel
@bp route '/delete' methods ['GET' 'POST'] @require_logindef delete return 'notready'
@command ' mv sw \\s* \\d{1 4} \\s*[\\s ]\\s* \\d{1 4} ' def songlist_mv_sw action a b i j int a - 1 int b - 1 if action 'mv' g model songs insert j g model songs pop i g message util F 'songmove' % g model[j] title b elif action 'sw' g model[i] g model[j] g model[j] g model[i] g message util F 'songsw' % min a b max a b g content content generate_songlist_display
@command ' mv sw \\s* \\d{1 4} \\s*[\\s ]\\s* \\d{1 4} ' def songlist_mv_sw action a b i j int a - 1 int b - 1 if action 'mv' g model songs insert j g model songs pop i g message util F 'songmove' % g model[j] title b elif action 'sw' g model[i] g model[j] g model[j] g model[i] g message util F 'songsw' % min a b max a b g content content generate_songlist_display
def xblock_local_resource_url block uri xblock_class getattr block __class__ 'unmixed_class' block __class__ if settings PIPELINE_ENABLED or not settings REQUIRE_DEBUG return staticfiles_storage url 'xblock/resources/{package_name}/{path}' format package_name xblock_class __module__ path uri else return reverse 'xblock_resource_url' kwargs {'block_type' block scope_ids block_type 'uri' uri}
def cas key value old_value store load if key not in store return Falseif store[key] old_value return Falsestore[key] valuedump store return True
def status name sig None cmd 'svstat{0}' format _service_path name out __salt__['cmd run_stdout'] cmd python_shell False try pid re search '\\ pid \\d+ \\ ' out group 1 except AttributeError pid ''return pid
def batch_shuffle index_array batch_size batch_count int len index_array / batch_size last_batch index_array[ batch_count * batch_size ]index_array index_array[ batch_count * batch_size ]index_array index_array reshape batch_count batch_size np random shuffle index_array index_array index_array flatten return np append index_array last_batch
def setBlocking fd flags fcntl fcntl fd fcntl F_GETFL flags flags & ~ os O_NONBLOCK fcntl fcntl fd fcntl F_SETFL flags
def report_config_interaction modified modifiers if isinstance modified str modified modified if isinstance modifiers str modifiers modifiers for var in modified VAR_MODIFIERS setdefault var set update modifiers
def exists storage indexname None if indexname is None indexname _DEF_INDEX_NAMEtry ix storage open_index indexname gen ix latest_generation ix close return gen > -1 except EmptyIndexError passreturn False
def _register_make cls assert cls nxm_headers is not None assert cls nxm_headers is not [] for nxm_header in cls nxm_headers assert nxm_header not in _MF_FIELDS _MF_FIELDS[nxm_header] cls makereturn cls
def test_install_folder_using_relative_path script script scratch_path join 'initools' mkdir script scratch_path join 'initools' 'mock' mkdir pkg_path script scratch_path / 'initools' / 'mock' pkg_path join 'setup py' write mock100_setup_py result script pip 'install' Path 'initools' / 'mock' egg_folder script site_packages / 'mock-100 1-py%s egg-info' % pyversion assert egg_folder in result files_created str result
@app route '/v1/repositories/<path repository>/' methods ['DELETE'] @app route '/v1/repositories/<path repository>/tags' methods ['DELETE'] @toolkit parse_repository_name@toolkit requires_authdef delete_repository namespace repository logger debug '[delete_repository]namespace {0} repository {1}' format namespace repository try for tag_name tag_content in get_tags namespace namespace repository repository items delete_tag namespace namespace repository repository tag tag_name store remove store repository_path namespace namespace repository repository except exceptions FileNotFoundError return toolkit api_error 'Repositorynotfound' 404 else sender flask current_app _get_current_object signals repository_deleted send sender namespace namespace repository repository return toolkit response
def kaiserord ripple width A abs ripple if A < 8 raise ValueError 'Requestedmaximumrippleattentuation%fistoosmallfortheKaiserformula ' % A beta kaiser_beta A numtaps A - 7 95 / 2 285 / np pi * width + 1 return int ceil numtaps beta
def kaiserord ripple width A abs ripple if A < 8 raise ValueError 'Requestedmaximumrippleattentuation%fistoosmallfortheKaiserformula ' % A beta kaiser_beta A numtaps A - 7 95 / 2 285 / np pi * width + 1 return int ceil numtaps beta
def nt_to_cw cur_enc cur_nt return array map int '' join [INT_TO_BS[cur_enc[x]] for x in cur_nt]
def nt_to_cw cur_enc cur_nt return array map int '' join [INT_TO_BS[cur_enc[x]] for x in cur_nt]
def nt_to_cw cur_enc cur_nt return array map int '' join [INT_TO_BS[cur_enc[x]] for x in cur_nt]
def chain_names sg_id sg_name prefix sg_label sg_id sg_name in_chain_name prefix + SUFFIX_IN out_chain_name prefix + SUFFIX_OUT return {'in' in_chain_name 'out' out_chain_name}
def column_index_from_string column fast False column column upper clen len column if not fast and not all 'A' < char < 'Z' for char in column msg 'ColumnstringmustcontainonlycharactersA-Z got%s' % column raise ColumnStringIndexException msg if clen 1 return ord column[0] - 64 elif clen 2 return 1 + ord column[0] - 65 * 26 + ord column[1] - 64 elif clen 3 return 1 + ord column[0] - 65 * 676 + 1 + ord column[1] - 65 * 26 + ord column[2] - 64 elif clen > 3 raise ColumnStringIndexException 'Columnstringindexcannotbelongerthan3characters' else raise ColumnStringIndexException 'Columnstringindexcannotbeempty'
def column_index_from_string column fast False column column upper clen len column if not fast and not all 'A' < char < 'Z' for char in column msg 'ColumnstringmustcontainonlycharactersA-Z got%s' % column raise ColumnStringIndexException msg if clen 1 return ord column[0] - 64 elif clen 2 return 1 + ord column[0] - 65 * 26 + ord column[1] - 64 elif clen 3 return 1 + ord column[0] - 65 * 676 + 1 + ord column[1] - 65 * 26 + ord column[2] - 64 elif clen > 3 raise ColumnStringIndexException 'Columnstringindexcannotbelongerthan3characters' else raise ColumnStringIndexException 'Columnstringindexcannotbeempty'
def column_index_from_string column fast False column column upper clen len column if not fast and not all 'A' < char < 'Z' for char in column msg 'ColumnstringmustcontainonlycharactersA-Z got%s' % column raise ColumnStringIndexException msg if clen 1 return ord column[0] - 64 elif clen 2 return 1 + ord column[0] - 65 * 26 + ord column[1] - 64 elif clen 3 return 1 + ord column[0] - 65 * 676 + 1 + ord column[1] - 65 * 26 + ord column[2] - 64 elif clen > 3 raise ColumnStringIndexException 'Columnstringindexcannotbelongerthan3characters' else raise ColumnStringIndexException 'Columnstringindexcannotbeempty'
def test_continue_on_collection_errors_maxfail testdir testdir makepyfile **COLLECTION_ERROR_PY_FILES res testdir runpytest '--continue-on-collection-errors' '--maxfail 3' assert res ret 2 res stdout fnmatch_lines ['collected2items/2errors' '*Interrupted stoppingafter3failures*' '*1failed 2error*']
def add_enrollment student_id course_id is_active True mode 'honor' enrollment {'created' datetime datetime now 'mode' mode 'is_active' is_active 'course' _get_fake_course_info course_id 'student' student_id}_ENROLLMENTS append enrollment return enrollment
def get_principal name ret {}cmd __execute_kadmin 'get_principal{0}' format name if cmd['retcode'] 0 or cmd['stderr'] ret['comment'] cmd['stderr'] splitlines [ -1 ]ret['result'] Falsereturn retfor i in cmd['stdout'] splitlines [1 ] prop val i split ' ' 1 ret[prop] valreturn ret
def cost_tour graph tour steps zip tour[0 -1 ] tour[1 ] cost sum [graph[ step_from step_to ] for step_from step_to in steps] return cost
def cost_tour graph tour steps zip tour[0 -1 ] tour[1 ] cost sum [graph[ step_from step_to ] for step_from step_to in steps] return cost
def get_pending_domain_join vname ' Default 'base_key 'SYSTEM\\CurrentControlSet\\Services\\Netlogon'avoid_key '{0}\\AvoidSpnSet' format base_key join_key '{0}\\JoinDomain' format base_key avoid_reg_ret __salt__['reg read_value'] 'HKLM' avoid_key vname if avoid_reg_ret['success'] log debug 'Foundkey %s' avoid_key return Trueelse log debug 'Unabletoaccesskey %s' avoid_key join_reg_ret __salt__['reg read_value'] 'HKLM' join_key vname if join_reg_ret['success'] log debug 'Foundkey %s' join_key return Trueelse log debug 'Unabletoaccesskey %s' join_key return False
def cliques graph threshold 3 a []for n in graph nodes c clique graph n id if len c > threshold c sort if c not in a a append c return a
def skip_for_dialect dialect def dec fn def wrap self *args **kwargs if self db_engine dialect name dialect raise unittest SkipTest "Notsupportedondialect'%s'" % dialect return fn self *args **kwargs return wrapreturn dec
def test_local_repo_typo tmpdir template_path os path join 'tests' 'unknown-repo' with pytest raises exceptions RepositoryNotFound as err repository determine_repo_dir template_path abbreviations {} clone_to_dir str tmpdir checkout None no_input True assert str err value 'Avalidrepositoryfor"{}"couldnotbefoundinthefollowinglocations \n{}' format template_path '\n' join [template_path str tmpdir / 'tests/unknown-repo' ]
def test_local_repo_typo tmpdir template_path os path join 'tests' 'unknown-repo' with pytest raises exceptions RepositoryNotFound as err repository determine_repo_dir template_path abbreviations {} clone_to_dir str tmpdir checkout None no_input True assert str err value 'Avalidrepositoryfor"{}"couldnotbefoundinthefollowinglocations \n{}' format template_path '\n' join [template_path str tmpdir / 'tests/unknown-repo' ]
def prepare_data filename tmp_prefix dbargs osm2pgsql projection args osm2pgsql split + ['--create' '--merc' '--prefix' tmp_prefix] for flag key in [ '-d' 'database' '-U' 'user' '-W' 'password' '-H' 'host' ] if key in dbargs args + flag dbargs[key] args + [filename]create Popen args stderr PIPE stdout PIPE create wait assert create returncode 0 "It'simportantthatosm2pgsqlactuallyworked " + create stderr read
def _num_plugins_cached return len plugin PLUGIN_CACHE keys
def check_cli module cli name module params['pn_name']show cli + 'trunk-showformatswitch nameno-show-headers' show shlex split show out module run_command show [1]out out split global TRUNK_EXISTSif name in out TRUNK_EXISTS Trueelse TRUNK_EXISTS False
def get_service hass config discovery_info None from pushbullet import PushBulletfrom pushbullet import InvalidKeyErrortry pushbullet PushBullet config[CONF_API_KEY] except InvalidKeyError _LOGGER error 'WrongAPIkeysupplied Getitathttps //www pushbullet com/account' return Nonereturn PushBulletNotificationService pushbullet
@open_tabdef interact_page data socket event json dumps data get 'interaction' {} try socket tab evaljs 'window livePortiaPage sendEvent %s ' % event except JsError as e print ecookies socket
@open_tabdef interact_page data socket event json dumps data get 'interaction' {} try socket tab evaljs 'window livePortiaPage sendEvent %s ' % event except JsError as e print ecookies socket
@open_tabdef interact_page data socket event json dumps data get 'interaction' {} try socket tab evaljs 'window livePortiaPage sendEvent %s ' % event except JsError as e print ecookies socket
@cronjobs registerdef reindex_users_that_contributed_yesterday if settings STAGE returntoday datetime now yesterday today - timedelta days 1 user_ids list Answer objects filter created__gte yesterday created__lt today values_list 'creator_id' flat True user_ids + list Revision objects filter created__gte yesterday created__lt today values_list 'creator_id' flat True user_ids + list Revision objects filter reviewed__gte yesterday reviewed__lt today values_list 'reviewer_id' flat True index_task delay UserMappingType list set user_ids
@cronjobs registerdef reindex_users_that_contributed_yesterday if settings STAGE returntoday datetime now yesterday today - timedelta days 1 user_ids list Answer objects filter created__gte yesterday created__lt today values_list 'creator_id' flat True user_ids + list Revision objects filter created__gte yesterday created__lt today values_list 'creator_id' flat True user_ids + list Revision objects filter reviewed__gte yesterday reviewed__lt today values_list 'reviewer_id' flat True index_task delay UserMappingType list set user_ids
def HasClass clsid return clsid in mapCLSIDToClass
def css *args return '\n' join [stylesheet_link url_for '/static/style/%s css?v %s' % name server_starttime for name in args]
def css *args return '\n' join [stylesheet_link url_for '/static/style/%s css?v %s' % name server_starttime for name in args]
def create_user username password email code "username '{username}' password '{password}' email '{email}' fromdjango contrib auth modelsimportUser User objects filter username username delete User objects create_superuser username email password "code code format username username password password email email shell_cmd 'foremanrun-econf/production envpythonmanage pyshell'with prefix 'source~ubuntu/venv/bin/activate' with cd '~ubuntu/cabot' sudo 'printf"{0}" {1}' format code shell_cmd
def _weight_mean_color graph src dst n diff graph node[dst]['meancolor'] - graph node[n]['meancolor'] diff np linalg norm diff return {'weight' diff}
def log_traceback severity logging ERROR debug False cherrypy log '' 'HTTP' severity severity traceback True
def unique values values com _asarray_tuplesafe values f lambda htype caster _unique_object values htype caster return _hashtable_algo f values
def unique values values com _asarray_tuplesafe values f lambda htype caster _unique_object values htype caster return _hashtable_algo f values
def setup_platform hass config add_devices discovery_info None hostname config get CONF_HOST port config get CONF_PORT login config get CONF_USERNAME password config get CONF_PASSWORD monitored_variables config get CONF_MONITORED_VARIABLES name config get CONF_NAME try hp_ilo_data HpIloData hostname port login password except ValueError as error _LOGGER error error return Falsedevices []for ilo_type in monitored_variables new_device HpIloSensor hp_ilo_data hp_ilo_data sensor_type SENSOR_TYPES get ilo_type client_name name devices append new_device add_devices devices
def get_collections return _do_mb_query 'collection' ''
def _truncate words cutlength stems {}for word in words stem word[ cutlength]try stems[stem] update [word] except KeyError stems[stem] set [word] return stems
def get_prefix ctx args search True if getattr args u'name' None if u'/' in args name raise CondaValueError u"'/'notallowedinenvironmentname %s" % args name getattr args u'json' False if args name ROOT_ENV_NAME return ctx root_dirif search return locate_prefix_by_name ctx args name else return join ctx envs_dirs[0] args name elif getattr args u'prefix' None return abspath expanduser args prefix else return ctx default_prefix
def get_prefix ctx args search True if getattr args u'name' None if u'/' in args name raise CondaValueError u"'/'notallowedinenvironmentname %s" % args name getattr args u'json' False if args name ROOT_ENV_NAME return ctx root_dirif search return locate_prefix_by_name ctx args name else return join ctx envs_dirs[0] args name elif getattr args u'prefix' None return abspath expanduser args prefix else return ctx default_prefix
def get_prefix ctx args search True if getattr args u'name' None if u'/' in args name raise CondaValueError u"'/'notallowedinenvironmentname %s" % args name getattr args u'json' False if args name ROOT_ENV_NAME return ctx root_dirif search return locate_prefix_by_name ctx args name else return join ctx envs_dirs[0] args name elif getattr args u'prefix' None return abspath expanduser args prefix else return ctx default_prefix
def get_prefix ctx args search True if getattr args u'name' None if u'/' in args name raise CondaValueError u"'/'notallowedinenvironmentname %s" % args name getattr args u'json' False if args name ROOT_ENV_NAME return ctx root_dirif search return locate_prefix_by_name ctx args name else return join ctx envs_dirs[0] args name elif getattr args u'prefix' None return abspath expanduser args prefix else return ctx default_prefix
def refresh_db if 'eix sync' in __salt__ return __salt__['eix sync'] if 'makeconf features_contains' in __salt__ and __salt__['makeconf features_contains'] 'webrsync-gpg' cmd 'emerge-webrsync-q'if salt utils which 'emerge-delta-webrsync' cmd 'emerge-delta-webrsync-q'return __salt__['cmd retcode'] cmd python_shell False 0 else if __salt__['cmd retcode'] 'emerge--askn--quiet--sync' python_shell False 0 return Truecmd 'emerge-webrsync-q'if salt utils which 'emerge-delta-webrsync' cmd 'emerge-delta-webrsync-q'return __salt__['cmd retcode'] cmd python_shell False 0
@pipeline mutator_stagedef apply_choices session task if task skip returnif task apply task apply_metadata plugins send 'import_task_apply' session session task task task add session lib
def pressure_network flow_rates Qtot k P k * flow_rates ** 2 F np hstack P[1 ] - P[0] flow_rates sum - Qtot return F
def pressure_network flow_rates Qtot k P k * flow_rates ** 2 F np hstack P[1 ] - P[0] flow_rates sum - Qtot return F
def pressure_network flow_rates Qtot k P k * flow_rates ** 2 F np hstack P[1 ] - P[0] flow_rates sum - Qtot return F
def _payment_accepted order_id auth_amount currency decision try order Order objects get id order_id except Order DoesNotExist raise CCProcessorDataException _ 'Thepaymentprocessoracceptedanorderwhosenumberisnotinoursystem ' if decision 'ACCEPT' if auth_amount order total_cost and currency lower order currency lower return {'accepted' True 'amt_charged' auth_amount 'currency' currency 'order' order}else ex CCProcessorWrongAmountException _ u'Theamountchargedbytheprocessor{charged_amount}{charged_amount_currency}isdifferentthanthetotalcostoftheorder{total_cost}{total_cost_currency} ' format charged_amount auth_amount charged_amount_currency currency total_cost order total_cost total_cost_currency order currency ex order orderraise exelse return {'accepted' False 'amt_charged' 0 'currency' 'usd' 'order' order}
def load_host_keys filename from paramiko hostkeys import HostKeysreturn HostKeys filename
def test_not_browsing_error hist with pytest raises ValueError as error1 hist nextitem assert str error1 value 'Currentlynotbrowsinghistory' with pytest raises ValueError as error2 hist previtem assert str error2 value 'Currentlynotbrowsinghistory'
@pytest mark networkdef test_no_clean_option_blocks_cleaning_after_install script data build script base_path / 'pip-build' script pip 'install' '--no-clean' '--no-index' '--build' build '--find-links %s' % data find_links 'simple' assert exists build
def read_pack_header read header read 12 if not header return None None if header[ 4] 'PACK' raise AssertionError 'Invalidpackheader%r' % header version unpack_from '>L' header 4 if version not in 2 3 raise AssertionError 'Versionwas%d' % version num_objects unpack_from '>L' header 8 return version num_objects
def exception_handler type value tb msg u''if hasattr value u'filename' and value filename is not None msg value filename + u' ' if hasattr value u'strerror' and value strerror is not None msg + value strerrorelse msg + six text_type value if len msg error_msg_qt msg
def exception_handler type value tb msg u''if hasattr value u'filename' and value filename is not None msg value filename + u' ' if hasattr value u'strerror' and value strerror is not None msg + value strerrorelse msg + six text_type value if len msg error_msg_qt msg
def encode_region name for tag language region iso639 iso3166 in LANGUAGE_REGION items if region name capitalize return iso3166
def donate import webbrowserwebbrowser open u'https //donate wikimedia org/w/index php?title Special FundraiserLandingPage' new 2
@utils no_4byte_paramsdef metadef_object_create context namespace_name object_dict session None session session or get_session return metadef_object_api create context namespace_name object_dict session
def _sinch x x2 x * x if abs x < 0 0135 return 1 + x2 / 6 0 * 1 + x2 / 20 0 * 1 + x2 / 42 0 else return np sinh x / x
def getEachWordCapitalized name withSpaces name lower replace '_' '' words withSpaces split '' capitalizedStrings []for word in words capitalizedStrings append word capitalize return '' join capitalizedStrings
def fix_bad_data django_obj dirty if isinstance django_obj models Node if django_obj title strip u'' django_obj title u'BlankTitle'dirty Trueif isinstance django_obj models Embargo if django_obj state u'active' django_obj state u'completed'dirty Trueelif django_obj state u'cancelled' django_obj state u'rejected'dirty Trueif isinstance django_obj models Retraction if django_obj state u'cancelled' django_obj state u'rejected'dirty Trueif django_obj state u'retracted' django_obj state u'completed'dirty Trueif django_obj state u'pending' django_obj state u'unapproved'dirty Truereturn django_obj dirty
def fix_bad_data django_obj dirty if isinstance django_obj models Node if django_obj title strip u'' django_obj title u'BlankTitle'dirty Trueif isinstance django_obj models Embargo if django_obj state u'active' django_obj state u'completed'dirty Trueelif django_obj state u'cancelled' django_obj state u'rejected'dirty Trueif isinstance django_obj models Retraction if django_obj state u'cancelled' django_obj state u'rejected'dirty Trueif django_obj state u'retracted' django_obj state u'completed'dirty Trueif django_obj state u'pending' django_obj state u'unapproved'dirty Truereturn django_obj dirty
def is_conflict exception exception_string str exception return any s in exception_string for s in CONFLICT_INDICATORS
def is_conflict exception exception_string str exception return any s in exception_string for s in CONFLICT_INDICATORS
def object_id_validator key activity_dict errors context activity_type activity_dict[ 'activity_type' ]if object_id_validators has_key activity_type object_id activity_dict[ 'object_id' ]return object_id_validators[activity_type] object_id context else raise Invalid 'Thereisnoobject_idvalidatorforactivitytype"%s"' % activity_type
def get_state_rules_stats exploration_id state_name exploration exp_services get_exploration_by_id exploration_id state exploration states[state_name]rule_keys []for group in state interaction answer_groups for rule in group rule_specs rule_keys append _OLD_SUBMIT_HANDLER_NAME rule stringify_classified_rule if state interaction default_outcome rule_keys append _OLD_SUBMIT_HANDLER_NAME exp_domain DEFAULT_RULESPEC_STR answer_logs stats_domain StateRuleAnswerLog get_multi exploration_id [{'state_name' state_name 'rule_str' rule_key[1]} for rule_key in rule_keys] results {}for ind answer_log in enumerate answer_logs results[' ' join rule_keys[ind] ] {'answers' answer_log get_top_answers 5 'rule_hits' answer_log total_answer_count}return results
def cycle_logfile logfile logfile_old logfile + ' old' if os path exists logfile if os path exists logfile_old os remove logfile_old os rename logfile logfile_old
def cycle_logfile logfile logfile_old logfile + ' old' if os path exists logfile if os path exists logfile_old os remove logfile_old os rename logfile logfile_old
def synchronize *klasses if threadmodule is not None for klass in klasses for methodName in klass synchronized hook addPre klass methodName _synchPre hook addPost klass methodName _synchPost
def log stats_resource None stats_source None log_level DEFAULT_LOG_LEVEL **kwargs if stats_source is not None kwargs stats_source if stats_resource is None if RESOURCE_ID not in kwargs or RESOURCE_NAME not in kwargs raise ValueError 'Missingrequiredstatslabels ' else if not hasattr stats_resource ConfWithId ID and hasattr stats_resource ConfWithId NAME raise ValueError 'Givenstatssourceismissingidornameattributes ' kwargs[RESOURCE_ID] stats_resource idkwargs[RESOURCE_NAME] stats_resource nameif TIMESTAMP not in kwargs kwargs[TIMESTAMP] datetime datetime utcfromtimestamp time time strftime '%Y-%m-%dT%H %M %S %fZ' _STATS_LOGGER log log_level json dumps kwargs
def _attempt_to_acquire_lock s3_fs lock_uri sync_wait_time job_key mins_to_expiration None key _lock_acquire_step_1 s3_fs lock_uri job_key mins_to_expiration if key is None return Falsetime sleep sync_wait_time return _lock_acquire_step_2 key job_key
def _attempt_to_acquire_lock s3_fs lock_uri sync_wait_time job_key mins_to_expiration None key _lock_acquire_step_1 s3_fs lock_uri job_key mins_to_expiration if key is None return Falsetime sleep sync_wait_time return _lock_acquire_step_2 key job_key
def epoch2datetime epoch if epoch is not None return datetime datetime fromtimestamp epoch tz UTC
def get_properties obj if hasattr obj 'keys' return obj keys elif hasattr obj '__dict__' return obj __dict__ keys return []
def _disabled funs ret []_disabled __salt__['grains get'] 'state_runs_disabled' for state in funs for _state in _disabled if ' *' in _state target_state _state split ' ' [0]target_state target_state + ' ' if not target_state endswith ' ' else target_state if state startswith target_state err 'Thestatefile"{0}"iscurrentlydisabledby"{1}" tore-enable runstate enable{1} ' format state _state ret append err continueelif _state state err 'Thestatefile"{0}"iscurrentlydisabled tore-enable runstate enable{0} ' format _state ret append err continuereturn ret
def get_patched_ast source sorted_children False return patch_ast ast parse source source sorted_children
def find_module modname modpath os path join os path dirname os path dirname __file__ 'Lib' return imp load_module modname *imp find_module modname [modpath]
@pytest fixturedef record_xml_property request request node warn code 'C3' message 'record_xml_propertyisanexperimentalfeature' xml getattr request config '_xml' None if xml is not None node_reporter xml node_reporter request node nodeid return node_reporter add_propertyelse def add_property_noop name value passreturn add_property_noop
@pytest fixturedef record_xml_property request request node warn code 'C3' message 'record_xml_propertyisanexperimentalfeature' xml getattr request config '_xml' None if xml is not None node_reporter xml node_reporter request node nodeid return node_reporter add_propertyelse def add_property_noop name value passreturn add_property_noop
def trueValues T R discountFactor assert discountFactor < 1 distr T copy res dot T R for i in range 1 int 10 / 1 0 - discountFactor distr dot distr T res + discountFactor ** i * dot distr R return res
def main ip_addr raw_input 'EnterIPaddress ' username 'pyclass'port 22ssh_conn pexpect spawn 'ssh-l{}{}-p{}' format username ip_addr port ssh_conn timeout 3login ssh_conn prompt find_prompt ssh_conn ssh_conn sendline 'terminallength0' ssh_conn expect prompt ssh_conn sendline 'showipintbrief' ssh_conn expect prompt print '\n>>>>'print ssh_conn beforeprint '>>>>\n'
def installed name default False user None ret {'name' name 'result' None 'comment' '' 'changes' {}}if __opts__['test'] ret['comment'] 'Ruby{0}issettobeinstalled' format name return retret _check_rvm ret user if ret['result'] is False if not __salt__['rvm install'] runas user ret['comment'] 'RVMfailedtoinstall 'return retelse return _check_and_install_ruby ret name default user user else return _check_and_install_ruby ret name default user user
def has_leading_dir paths common_prefix Nonefor path in paths prefix rest split_leading_dir path if not prefix return Falseelif common_prefix is None common_prefix prefixelif prefix common_prefix return Falsereturn True
def has_leading_dir paths common_prefix Nonefor path in paths prefix rest split_leading_dir path if not prefix return Falseelif common_prefix is None common_prefix prefixelif prefix common_prefix return Falsereturn True
def seq_exceeds_homopolymers curr_seq max_len 6 for base in 'ATGC' curr base * max_len + 1 if curr in curr_seq return Truereturn False
def seq_exceeds_homopolymers curr_seq max_len 6 for base in 'ATGC' curr base * max_len + 1 if curr in curr_seq return Truereturn False
def main import logginglogging basicConfig level logging DEBUG tvdb_instance Tvdb interactive True cache False print tvdb_instance['Lost']['seriesname']print tvdb_instance['Lost'][1][4]['episodename']
def getName obj def sanitize name return name replace ' ' '/' if isinstance obj _BuildStepFactory klass obj factoryelse klass type obj name ''klasses klass + inspect getmro klass for klass in klasses if hasattr klass '__module__' and klass __module__ startswith 'buildbot ' return sanitize name + klass __module__ + ' ' + klass __name__ else name + '>'return sanitize type obj __name__
def RelayStateHelper manager delay return internet TimerService delay _checkState manager
def RelayStateHelper manager delay return internet TimerService delay _checkState manager
def vg_absent name ret {'changes' {} 'comment' '' 'name' name 'result' True}if not __salt__['lvm vgdisplay'] name ret['comment'] 'VolumeGroup{0}alreadyabsent' format name elif __opts__['test'] ret['comment'] 'VolumeGroup{0}issettoberemoved' format name ret['result'] Nonereturn retelse changes __salt__['lvm vgremove'] name if not __salt__['lvm vgdisplay'] name ret['comment'] 'RemovedVolumeGroup{0}' format name ret['changes']['removed'] changeselse ret['comment'] 'FailedtoremoveVolumeGroup{0}' format name ret['result'] Falsereturn ret
def slaac value query '' try vtype ipaddr value 'type' if vtype 'address' v ipaddr value 'cidr' elif vtype 'network' v ipaddr value 'subnet' if ipaddr value 'version' 6 return Falsevalue netaddr IPNetwork v except return Falseif not query return Falsetry mac hwaddr query alias 'slaac' eui netaddr EUI mac except return Falsereturn eui ipv6 value network
def for_int_dtypes name 'dtype' no_bool False if no_bool return for_dtypes _int_dtypes name name else return for_dtypes _int_bool_dtypes name name
def irc_logins full_load pkt user_search re match irc_user_re full_load pass_search re match irc_pw_re full_load pass_search2 re search irc_pw_re2 full_load lower if user_search msg 'IRCnick %s' % user_search group 1 return msgif pass_search msg 'IRCpass %s' % pass_search group 1 return msgif pass_search2 msg 'IRCpass %s' % pass_search2 group 1 return msg
def reconstruct_interp_matrix idx proj if _is_real proj return backend idd_reconint idx + 1 proj else return backend idz_reconint idx + 1 proj
@anonymous_user_requireddef forgot_password form_class _security forgot_password_formif request json form form_class MultiDict request json else form form_class if form validate_on_submit send_reset_password_instructions form user if request json is None do_flash *get_message 'PASSWORD_RESET_REQUEST' email form user email if request json return _render_json form include_user False return _security render_template config_value 'FORGOT_PASSWORD_TEMPLATE' forgot_password_form form **_ctx 'forgot_password'
def get_public_ip return file_io read constants PUBLIC_IP_LOC rstrip
def parse time_str return parse_iso time_str
def refresh_modules async True try if async ret __salt__['event fire'] {} 'module_refresh' else eventer salt utils event get_event 'minion' opts __opts__ listen True ret __salt__['event fire'] {'notify' True} 'module_refresh' log trace 'refresh_moduleswaitingformodulerefreshtocomplete' eventer get_event tag '/salt/minion/minion_mod_complete' wait 30 except KeyError log error 'Eventmodulenotavailable Modulerefreshfailed ' ret Falsereturn ret
@pytest mark django_dbdef test_data_tp_bad with pytest raises IntegrityError TPData objects create
@pytest mark django_dbdef test_data_tp_bad with pytest raises IntegrityError TPData objects create
def test_language_portuguese lang Language 'pt-br' assert_equals lang code u'pt-br' assert_equals lang name u'Portuguese' assert_equals lang native u'Portugu\xeas' assert_equals lang feature u'Funcionalidade' assert_equals lang scenario u'Cen\xe1rio Cenario' assert_equals lang examples u'Exemplos Cen\xe1rios' assert_equals lang scenario_outline u'EsquemadoCen\xe1rio EsquemadoCenario'
def get_task_runner local_task_job if _TASK_RUNNER 'BashTaskRunner' return BashTaskRunner local_task_job elif _TASK_RUNNER 'CgroupTaskRunner' from airflow contrib task_runner cgroup_task_runner import CgroupTaskRunnerreturn CgroupTaskRunner local_task_job else raise AirflowException 'Unknowntaskrunnertype{}' format _TASK_RUNNER
def get_task_runner local_task_job if _TASK_RUNNER 'BashTaskRunner' return BashTaskRunner local_task_job elif _TASK_RUNNER 'CgroupTaskRunner' from airflow contrib task_runner cgroup_task_runner import CgroupTaskRunnerreturn CgroupTaskRunner local_task_job else raise AirflowException 'Unknowntaskrunnertype{}' format _TASK_RUNNER
def get_task_runner local_task_job if _TASK_RUNNER 'BashTaskRunner' return BashTaskRunner local_task_job elif _TASK_RUNNER 'CgroupTaskRunner' from airflow contrib task_runner cgroup_task_runner import CgroupTaskRunnerreturn CgroupTaskRunner local_task_job else raise AirflowException 'Unknowntaskrunnertype{}' format _TASK_RUNNER
def get_swap_size vm_ return config get_cloud_config_value 'swap' vm_ __opts__ default 128
def query_tags return read_url get_tags_url
def setup_platform hass config add_devices discovery_info None value_template config get CONF_VALUE_TEMPLATE if value_template is not None value_template hass hassadd_devices [MqttLock hass config get CONF_NAME config get CONF_STATE_TOPIC config get CONF_COMMAND_TOPIC config get CONF_QOS config get CONF_RETAIN config get CONF_PAYLOAD_LOCK config get CONF_PAYLOAD_UNLOCK config get CONF_OPTIMISTIC value_template ]
def _centos7_install_commands version installable_version get_installable_version flocker_version return sequence [run command 'yumcleanall' run command 'yuminstall-y{}' format get_repository_url distribution 'centos-7' flocker_version installable_version run_from_args ['yum' 'install'] + get_repo_options installable_version + ['-y' 'clusterhq-flocker-node' + version ] ]
def vgextend vgname devices if not vgname or not devices return 'Error vgnameanddevice s arebothrequired'if isinstance devices six string_types devices devices split ' ' cmd ['vgextend' vgname]for device in devices cmd append device out __salt__['cmd run'] cmd python_shell False splitlines vgdata {'Outputfromvgextend' out[0] strip }return vgdata
def get_features return get_var 'FEATURES'
def CommaSeparatedTokenizer return RegexTokenizer '[^ ]+' StripFilter
def makelist inlist listchar '' stringify False escape False encoding None if stringify inlist list_stringify inlist listdict {'[' '[%s]' ' ' ' %s ' '' '%s'}outline []if len inlist < 2 listchar listchar or '[' for item in inlist if not isinstance item list tuple if escape item quote_escape item outline append elem_quote item encoding encoding else outline append makelist item listchar or '[' stringify escape encoding return listdict[listchar] % ' ' join outline
def p_enumerator_2 t pass
def generate_patch_base x y base 0 0 x x valuesy y valuesy0 np insert y 0 base y0 np append y0 base x0 np insert x 0 x[0] x0 np append x0 x0[ -1 ] return x0 y0
def generate_all_classes modules_list [] launcher [] redirect_x False mipav_hacks False all_code {}for module in modules_list print u' ' * 80 print u'GeneratingDefinitionformodule{0}' format module print u'^' * 80 package code module generate_class module launcher redirect_x redirect_x mipav_hacks mipav_hacks cur_package all_codemodule_name package strip split u'' [0] split u' ' [ -1 ]for package in package strip split u'' [0] split u' ' [ -1 ] if package not in cur_package cur_package[package] {}cur_package cur_package[package]if module_name not in cur_package cur_package[module_name] {}cur_package[module_name][module] codeif os path exists u'__init__ py' os unlink u'__init__ py' crawl_code_struct all_code os getcwd
def generate_all_classes modules_list [] launcher [] redirect_x False mipav_hacks False all_code {}for module in modules_list print u' ' * 80 print u'GeneratingDefinitionformodule{0}' format module print u'^' * 80 package code module generate_class module launcher redirect_x redirect_x mipav_hacks mipav_hacks cur_package all_codemodule_name package strip split u'' [0] split u' ' [ -1 ]for package in package strip split u'' [0] split u' ' [ -1 ] if package not in cur_package cur_package[package] {}cur_package cur_package[package]if module_name not in cur_package cur_package[module_name] {}cur_package[module_name][module] codeif os path exists u'__init__ py' os unlink u'__init__ py' crawl_code_struct all_code os getcwd
def generate_all_classes modules_list [] launcher [] redirect_x False mipav_hacks False all_code {}for module in modules_list print u' ' * 80 print u'GeneratingDefinitionformodule{0}' format module print u'^' * 80 package code module generate_class module launcher redirect_x redirect_x mipav_hacks mipav_hacks cur_package all_codemodule_name package strip split u'' [0] split u' ' [ -1 ]for package in package strip split u'' [0] split u' ' [ -1 ] if package not in cur_package cur_package[package] {}cur_package cur_package[package]if module_name not in cur_package cur_package[module_name] {}cur_package[module_name][module] codeif os path exists u'__init__ py' os unlink u'__init__ py' crawl_code_struct all_code os getcwd
def generate_all_classes modules_list [] launcher [] redirect_x False mipav_hacks False all_code {}for module in modules_list print u' ' * 80 print u'GeneratingDefinitionformodule{0}' format module print u'^' * 80 package code module generate_class module launcher redirect_x redirect_x mipav_hacks mipav_hacks cur_package all_codemodule_name package strip split u'' [0] split u' ' [ -1 ]for package in package strip split u'' [0] split u' ' [ -1 ] if package not in cur_package cur_package[package] {}cur_package cur_package[package]if module_name not in cur_package cur_package[module_name] {}cur_package[module_name][module] codeif os path exists u'__init__ py' os unlink u'__init__ py' crawl_code_struct all_code os getcwd
def verify_user user if not user email raise ValidationError "Youcannotverifyanaccountwithnoemailset Youcansetthisuser'semailwith'pootleupdate_user_email%sEMAIL'" % user username try validate_email_unique user email user except ValidationError raise ValidationError "Thisuser'semailisnotunique Youcanfindduplicateemailswith'pootlefind_duplicate_emails'" existing_primary EmailAddress objects filter user user primary True if existing_primary exists existing_primary existing_primary first if not existing_primary verified existing_primary verified Trueexisting_primary save returnelse raise ValueError "User'%s'isalreadyverified" % user username sync_user_email_addresses user email_address EmailAddress objects filter user user email__iexact user email order_by 'primary' first email_address verified Trueemail_address primary Trueemail_address save
def verify_user user if not user email raise ValidationError "Youcannotverifyanaccountwithnoemailset Youcansetthisuser'semailwith'pootleupdate_user_email%sEMAIL'" % user username try validate_email_unique user email user except ValidationError raise ValidationError "Thisuser'semailisnotunique Youcanfindduplicateemailswith'pootlefind_duplicate_emails'" existing_primary EmailAddress objects filter user user primary True if existing_primary exists existing_primary existing_primary first if not existing_primary verified existing_primary verified Trueexisting_primary save returnelse raise ValueError "User'%s'isalreadyverified" % user username sync_user_email_addresses user email_address EmailAddress objects filter user user email__iexact user email order_by 'primary' first email_address verified Trueemail_address primary Trueemail_address save
def get_datasource name orgname None profile 'grafana' data get_datasources orgname orgname profile profile for datasource in data if datasource['name'] name return datasourcereturn None
def scrub_headers headers if isinstance headers dict headers headers items headers [ parse_header_string key parse_header_string val for key val in headers]if not logger_settings get 'redact_sensitive_headers' True return dict headers if logger_settings get 'reveal_sensitive_prefix' 16 < 0 logger_settings['reveal_sensitive_prefix'] 16return {key safe_value key val for key val in headers}
def scrub_headers headers if isinstance headers dict headers headers items headers [ parse_header_string key parse_header_string val for key val in headers]if not logger_settings get 'redact_sensitive_headers' True return dict headers if logger_settings get 'reveal_sensitive_prefix' 16 < 0 logger_settings['reveal_sensitive_prefix'] 16return {key safe_value key val for key val in headers}
def getScrollbarCanvasPortion scrollbar scrollbarBeginEnd scrollbar get return scrollbarBeginEnd[1] - scrollbarBeginEnd[0]
def decipher_shift msg key symbols None return encipher_shift msg - key symbols
def decipher_shift msg key symbols None return encipher_shift msg - key symbols
def sum_signs exprs is_pos all [expr is_positive for expr in exprs] is_neg all [expr is_negative for expr in exprs] return is_pos is_neg
@task name 'geonode tasks update create_document_thumbnail' queue 'update' def create_document_thumbnail object_id try document Document objects get id object_id except Document DoesNotExist returnimage document _render_thumbnail filename 'document-%s-thumb png' % document uuid document save_thumbnail filename image
def lookup path parent None user None exists None url build_url RESOURCE route 'lookup' params make_params path path parent parent user user exists exists return request 'get' url params params
def _check_nfft n n_fft n_overlap n_fft n if n_fft > n else n_fft n_overlap n_fft - 1 if n_overlap > n_fft else n_overlap return n_fft n_overlap
def group_add_asset group asset_id None asset_ip None if asset_id asset get_object Asset id asset_id else asset get_object Asset ip asset_ip if asset group asset_set add asset
def group_add_asset group asset_id None asset_ip None if asset_id asset get_object Asset id asset_id else asset get_object Asset ip asset_ip if asset group asset_set add asset
def deflate_and_base64_encode string_val return base64 b64encode zlib compress string_val [2 -4 ]
def relative_days current_wday wday dir if current_wday wday return 7 * dir if dir 1 return wday + 7 - current_wday % 7 else return current_wday + 7 - wday % 7 * -1
@require_contextdef to_device obj stream 0 copy True to None if to is None to new devicearray auto_device obj stream stream copy copy return toif copy to copy_to_device obj stream stream return to
@require_contextdef to_device obj stream 0 copy True to None if to is None to new devicearray auto_device obj stream stream copy copy return toif copy to copy_to_device obj stream stream return to
def check_status try ret salt utils http query 'https //api random org/' status True return ret['status'] 200 except return False
def get_default_user superusers get_user_model objects filter is_superuser True order_by 'id' if superusers count > 0 return superusers[0]else raise GeoNodeException 'Youmusthaveanadminaccountconfiguredbeforeimportingdata Try django-admin pycreatesuperuser'
def _select_iterables elements return itertools chain *[c _select_iterable for c in elements]
def rand_url randbits str random randint 1 2147483647 return 'https //url-' + randbits + ' com'
def item_field_getters funcs {}for plugin in find_plugins if plugin template_fields funcs update plugin template_fields return funcs
def item_field_getters funcs {}for plugin in find_plugins if plugin template_fields funcs update plugin template_fields return funcs
def item_field_getters funcs {}for plugin in find_plugins if plugin template_fields funcs update plugin template_fields return funcs
def merge_insert ins_chunks doc unbalanced_start balanced unbalanced_end split_unbalanced ins_chunks doc extend unbalanced_start if doc and not doc[ -1 ] endswith '' doc[ -1 ] + ''doc append '<ins>' if balanced and balanced[ -1 ] endswith '' balanced[ -1 ] balanced[ -1 ][ -1 ]doc extend balanced doc append '</ins>' doc extend unbalanced_end
def set_version_db apps schema_editor db_alias schema_editor connection aliasVersion apps get_model u'reversion' u'Version' content_types Version objects using db_alias order_by values_list u'content_type_id' u'content_type__app_label' u'content_type__model' distinct model_dbs defaultdict list for content_type_id app_label model_name in content_types try model live_apps get_model app_label model_name except LookupError db u'default'else db router db_for_write model model_dbs[db] append content_type_id if DEFAULT_DB_ALIAS in model_dbs and len model_dbs 1 Version objects using db_alias update db DEFAULT_DB_ALIAS else for db content_type_ids in model_dbs items Version objects using db_alias filter content_type__in content_type_ids update db db
def set_version_db apps schema_editor db_alias schema_editor connection aliasVersion apps get_model u'reversion' u'Version' content_types Version objects using db_alias order_by values_list u'content_type_id' u'content_type__app_label' u'content_type__model' distinct model_dbs defaultdict list for content_type_id app_label model_name in content_types try model live_apps get_model app_label model_name except LookupError db u'default'else db router db_for_write model model_dbs[db] append content_type_id if DEFAULT_DB_ALIAS in model_dbs and len model_dbs 1 Version objects using db_alias update db DEFAULT_DB_ALIAS else for db content_type_ids in model_dbs items Version objects using db_alias filter content_type__in content_type_ids update db db
@region cache_on_arguments expiration_time REFINER_EXPIRATION_TIME def get_series_episode series_id season episode result tvdb_client query_series_episodes series_id aired_season season aired_episode episode if result return tvdb_client get_episode result['data'][0]['id']
def s3_rheader_resource r _vars r get_varsif 'viewing' in _vars try tablename record_id _vars viewing rsplit ' ' 1 db current dbrecord db[tablename][record_id]except tablename r tablenamerecord r recordelse tablename r tablenamerecord r recordreturn tablename record
def main try s Shell _ _ cmds s process_args sys argv[1 ] if len cmds 0 s cmdloop except v sys exc_info [1]if getattr v '_handle_exception_saw_this' False passelse import tracebacktraceback print_exc sys exit 1
def infer_unit value from apcaccess status import ALL_UNITSfor unit in ALL_UNITS if value endswith unit return value[ - len unit ] INFERRED_UNITS get unit unit strip return value None
def _base64_unicode value as_bytes base64 b64encode value return as_bytes decode 'ascii'
def subplots nrows 1 ncols 1 sharex False sharey False squeeze True subplot_kw None gridspec_kw None **fig_kw fig figure **fig_kw axs fig subplots nrows nrows ncols ncols sharex sharex sharey sharey squeeze squeeze subplot_kw subplot_kw gridspec_kw gridspec_kw return fig axs
def subplots nrows 1 ncols 1 sharex False sharey False squeeze True subplot_kw None gridspec_kw None **fig_kw fig figure **fig_kw axs fig subplots nrows nrows ncols ncols sharex sharex sharey sharey squeeze squeeze subplot_kw subplot_kw gridspec_kw gridspec_kw return fig axs
def subplots nrows 1 ncols 1 sharex False sharey False squeeze True subplot_kw None gridspec_kw None **fig_kw fig figure **fig_kw axs fig subplots nrows nrows ncols ncols sharex sharex sharey sharey squeeze squeeze subplot_kw subplot_kw gridspec_kw gridspec_kw return fig axs
def _external_auth_intercept request mode if mode 'login' return external_auth_login request elif mode 'register' return external_auth_register request
def read_directory_changes handle recursive event_buffer ctypes create_string_buffer BUFFER_SIZE nbytes ctypes wintypes DWORD try ReadDirectoryChangesW handle ctypes byref event_buffer len event_buffer recursive WATCHDOG_FILE_NOTIFY_FLAGS ctypes byref nbytes None None except WindowsError as e if e winerror ERROR_OPERATION_ABORTED return [] 0 raise etry int_class longexcept NameError int_class intreturn event_buffer raw int_class nbytes value
def get_help return stem interpreter msg 'msg help' address DEFAULT_ARGS['control_address'] port DEFAULT_ARGS['control_port'] socket DEFAULT_ARGS['control_socket']
def with_backing_lock method def wrapped_method self *args **dargs already_have_lock self _backing_file_lock is not None if not already_have_lock self _lock_backing_file try return method self *args **dargs finally if not already_have_lock self _unlock_backing_file wrapped_method __name__ method __name__wrapped_method __doc__ method __doc__return wrapped_method
def listen description factory default None from twisted internet import reactor name args kw parse description factory default return getattr reactor 'listen' + name *args **kw
def recv_arrays socket headers socket recv_json if 'stop' in headers raise StopIterationarrays []for header in headers data socket recv buf buffer_ data array numpy frombuffer buf dtype numpy dtype header['descr'] array shape header['shape']if header['fortran_order'] array shape header['shape'][ -1 ]array array transpose arrays append array return arrays
def js_escaped_string string_for_js if string_for_js is None string_for_js ''string_for_js decode utf8 string_for_js string_for_js escapejs string_for_js return string_for_js
def joined name host user 'rabbit' ram_node None runas 'root' ret {'name' name 'result' True 'comment' '' 'changes' {}}status __salt__['rabbitmq cluster_status'] if '{0}@{1}' format user host in status ret['comment'] 'Alreadyincluster'return retif not __opts__['test'] result __salt__['rabbitmq join_cluster'] host user ram_node runas runas if 'Error' in result ret['result'] Falseret['comment'] result['Error']return retelif 'Join' in result ret['comment'] result['Join']ret['changes'] {'old' '' 'new' '{0}@{1}' format user host }if __opts__['test'] ret['result'] Noneret['comment'] 'Nodeissettojoincluster{0}@{1}' format user host return ret
def joined name host user 'rabbit' ram_node None runas 'root' ret {'name' name 'result' True 'comment' '' 'changes' {}}status __salt__['rabbitmq cluster_status'] if '{0}@{1}' format user host in status ret['comment'] 'Alreadyincluster'return retif not __opts__['test'] result __salt__['rabbitmq join_cluster'] host user ram_node runas runas if 'Error' in result ret['result'] Falseret['comment'] result['Error']return retelif 'Join' in result ret['comment'] result['Join']ret['changes'] {'old' '' 'new' '{0}@{1}' format user host }if __opts__['test'] ret['result'] Noneret['comment'] 'Nodeissettojoincluster{0}@{1}' format user host return ret
def in_random_order data indexes [i for i _ in enumerate data ]random shuffle indexes for i in indexes yield data[i]
def in_random_order data indexes [i for i _ in enumerate data ]random shuffle indexes for i in indexes yield data[i]
def setup_logging handler excluded_loggers EXCLUDED_LOGGER_DEFAULTS log_level logging INFO all_excluded_loggers set excluded_loggers + EXCLUDED_LOGGER_DEFAULTS logger logging getLogger logger setLevel log_level logger addHandler handler logger addHandler logging StreamHandler for logger_name in all_excluded_loggers logger logging getLogger logger_name logger propagate Falselogger addHandler logging StreamHandler
def setup_logging handler excluded_loggers EXCLUDED_LOGGER_DEFAULTS log_level logging INFO all_excluded_loggers set excluded_loggers + EXCLUDED_LOGGER_DEFAULTS logger logging getLogger logger setLevel log_level logger addHandler handler logger addHandler logging StreamHandler for logger_name in all_excluded_loggers logger logging getLogger logger_name logger propagate Falselogger addHandler logging StreamHandler
def setup_logging handler excluded_loggers EXCLUDED_LOGGER_DEFAULTS log_level logging INFO all_excluded_loggers set excluded_loggers + EXCLUDED_LOGGER_DEFAULTS logger logging getLogger logger setLevel log_level logger addHandler handler logger addHandler logging StreamHandler for logger_name in all_excluded_loggers logger logging getLogger logger_name logger propagate Falselogger addHandler logging StreamHandler
def setup_logging handler excluded_loggers EXCLUDED_LOGGER_DEFAULTS log_level logging INFO all_excluded_loggers set excluded_loggers + EXCLUDED_LOGGER_DEFAULTS logger logging getLogger logger setLevel log_level logger addHandler handler logger addHandler logging StreamHandler for logger_name in all_excluded_loggers logger logging getLogger logger_name logger propagate Falselogger addHandler logging StreamHandler
def main description project_id day month year hours minutes source_bucket access_key secret_access_key sink_bucket credentials GoogleCredentials get_application_default storagetransfer discovery build 'storagetransfer' 'v1' credentials credentials transfer_job {'description' description 'status' 'ENABLED' 'projectId' project_id 'schedule' {'scheduleStartDate' {'day' day 'month' month 'year' year} 'scheduleEndDate' {'day' day 'month' month 'year' year} 'startTimeOfDay' {'hours' hours 'minutes' minutes}} 'transferSpec' {'awsS3DataSource' {'bucketName' source_bucket 'awsAccessKey' {'accessKeyId' access_key 'secretAccessKey' secret_access_key}} 'gcsDataSink' {'bucketName' sink_bucket}}}result storagetransfer transferJobs create body transfer_job execute print 'ReturnedtransferJob {}' format json dumps result indent 4
def delay_exponential base growth_factor attempts if base 'rand' base random random elif base < 0 raise ValueError "The'base'parammustbegreaterthan0 got %s" % base time_to_sleep base * growth_factor ** attempts - 1 return time_to_sleep
def add_extension name ext languages []for language in registered_languages for generator in language html_generators if name generator name language extension_patterns append ext
def _difftrap function interval numtraps if numtraps < 0 raise ValueError 'numtrapsmustbe>0indifftrap ' elif numtraps 1 return 0 5 * function interval[0] + function interval[1] else numtosum numtraps / 2 h float interval[1] - interval[0] / numtosum lox interval[0] + 0 5 * h points lox + h * np arange numtosum s np sum function points axis 0 return s
def get_minibatches_idx n minibatch_size shuffle False idx_list numpy arange n dtype 'int32' if shuffle numpy random shuffle idx_list minibatches []minibatch_start 0for i in range n // minibatch_size minibatches append idx_list[minibatch_start minibatch_start + minibatch_size ] minibatch_start + minibatch_sizeif minibatch_start n minibatches append idx_list[minibatch_start ] return zip range len minibatches minibatches
def get_minibatches_idx n minibatch_size shuffle False idx_list numpy arange n dtype 'int32' if shuffle numpy random shuffle idx_list minibatches []minibatch_start 0for i in range n // minibatch_size minibatches append idx_list[minibatch_start minibatch_start + minibatch_size ] minibatch_start + minibatch_sizeif minibatch_start n minibatches append idx_list[minibatch_start ] return zip range len minibatches minibatches
def _next_datetime_with_utc_hour table_name utc_hour today datetime date today start_date_time datetime datetime year today year month today month day today day hour utc_hour minute _get_deterministic_value_for_table_name table_name 60 second _get_deterministic_value_for_table_name table_name 60 if start_date_time < datetime datetime utcnow one_day datetime timedelta days 1 start_date_time + one_dayreturn start_date_time
def relative_time d other None ndigits 0 drt unit decimal_relative_time d other ndigits cardinalize True phrase 'ago'if drt < 0 phrase 'fromnow'return '%g%s%s' % abs drt unit phrase
def test_read_user_dict_default_value mocker mock_prompt mocker patch u'cookiecutter prompt click prompt' autospec True return_value u'default' val read_user_dict u'name' {u'project_slug' u'pytest-plugin'} assert mock_prompt call_args mocker call u'name' type click STRING default u'default' value_proc process_json assert val {u'project_slug' u'pytest-plugin'}
def test_read_user_dict_default_value mocker mock_prompt mocker patch u'cookiecutter prompt click prompt' autospec True return_value u'default' val read_user_dict u'name' {u'project_slug' u'pytest-plugin'} assert mock_prompt call_args mocker call u'name' type click STRING default u'default' value_proc process_json assert val {u'project_slug' u'pytest-plugin'}
def get_stream_type env args if not env stdout_isatty and not args prettify Stream partial RawStream chunk_size RawStream CHUNK_SIZE_BY_LINE if args stream else RawStream CHUNK_SIZE elif args prettify Stream partial PrettyStream if args stream else BufferedPrettyStream env env conversion Conversion formatting Formatting env env groups args prettify color_scheme args style explicit_json args json else Stream partial EncodedStream env env return Stream
@require_GETdef wiki_rows request readout_slug product _get_product request readout _kb_readout request readout_slug READOUTS locale request GET get 'locale' mode smart_int request GET get 'mode' None product product max_rows smart_int request GET get 'max' fallback None return HttpResponse readout render max_rows max_rows
def _csr_obtain_cert config le_client csr typ config actual_csr certr chain le_client obtain_certificate_from_csr config domains csr typ if config dry_run logger debug 'Dryrun skippingsavingcertificateto%s' config cert_path else cert_path _ cert_fullchain le_client save_certificate certr chain config cert_path config chain_path config fullchain_path _report_new_cert config cert_path cert_fullchain
def _csr_obtain_cert config le_client csr typ config actual_csr certr chain le_client obtain_certificate_from_csr config domains csr typ if config dry_run logger debug 'Dryrun skippingsavingcertificateto%s' config cert_path else cert_path _ cert_fullchain le_client save_certificate certr chain config cert_path config chain_path config fullchain_path _report_new_cert config cert_path cert_fullchain
def _csr_obtain_cert config le_client csr typ config actual_csr certr chain le_client obtain_certificate_from_csr config domains csr typ if config dry_run logger debug 'Dryrun skippingsavingcertificateto%s' config cert_path else cert_path _ cert_fullchain le_client save_certificate certr chain config cert_path config chain_path config fullchain_path _report_new_cert config cert_path cert_fullchain
def _csr_obtain_cert config le_client csr typ config actual_csr certr chain le_client obtain_certificate_from_csr config domains csr typ if config dry_run logger debug 'Dryrun skippingsavingcertificateto%s' config cert_path else cert_path _ cert_fullchain le_client save_certificate certr chain config cert_path config chain_path config fullchain_path _report_new_cert config cert_path cert_fullchain
def _csr_obtain_cert config le_client csr typ config actual_csr certr chain le_client obtain_certificate_from_csr config domains csr typ if config dry_run logger debug 'Dryrun skippingsavingcertificateto%s' config cert_path else cert_path _ cert_fullchain le_client save_certificate certr chain config cert_path config chain_path config fullchain_path _report_new_cert config cert_path cert_fullchain
def _csr_obtain_cert config le_client csr typ config actual_csr certr chain le_client obtain_certificate_from_csr config domains csr typ if config dry_run logger debug 'Dryrun skippingsavingcertificateto%s' config cert_path else cert_path _ cert_fullchain le_client save_certificate certr chain config cert_path config chain_path config fullchain_path _report_new_cert config cert_path cert_fullchain
def _csr_obtain_cert config le_client csr typ config actual_csr certr chain le_client obtain_certificate_from_csr config domains csr typ if config dry_run logger debug 'Dryrun skippingsavingcertificateto%s' config cert_path else cert_path _ cert_fullchain le_client save_certificate certr chain config cert_path config chain_path config fullchain_path _report_new_cert config cert_path cert_fullchain
def _csr_obtain_cert config le_client csr typ config actual_csr certr chain le_client obtain_certificate_from_csr config domains csr typ if config dry_run logger debug 'Dryrun skippingsavingcertificateto%s' config cert_path else cert_path _ cert_fullchain le_client save_certificate certr chain config cert_path config chain_path config fullchain_path _report_new_cert config cert_path cert_fullchain
@FileSystem in_directory current_directory 'django' 'couves' def test_django_agains_couves status out run_scenario expect 'Couvesbeforeall' to be within out expect 'Couvesafterall' to be within out
def _require_valid_version version_from_payload exploration_version if version_from_payload is None raise base BaseHandler InvalidInputException 'InvalidPOSTrequest aversionmustbespecified ' if version_from_payload exploration_version raise base BaseHandler InvalidInputException 'Tryingtoupdateversion%sofexplorationfromversion%s whichistooold Pleasereloadthepageandtryagain ' % exploration_version version_from_payload
def _clean_flags args caller flags ''if args is None return flagsallowed 'a' 'B' 'h' 'H' 'i' 'k' 'l' 'P' 't' 'T' 'x' 'v' for flag in args if flag in allowed flags + flagelse raise CommandExecutionError 'Invalidflagpassedto{0}' format caller return flags
def assert_rpm_content test_case expected_paths package_path output check_output ['rpm' '--query' '--list' '--package' package_path path] actual_paths set map FilePath output splitlines test_case assertEqual expected_paths actual_paths
def test_list_with_newlines t ascii read ['abc' '123\n' '456\n' '\n' '\n'] assert t colnames ['abc'] assert len t 2 assert t[0][0] 123 assert t[1][0] 456
def get_free_range parent_range excluded_ranges size PRIMARY_VIP_RANGE_SIZE free_cidrs netaddr IPSet [parent_range] - netaddr IPSet excluded_ranges for cidr in free_cidrs iter_cidrs if cidr prefixlen < size return '%s/%s' % cidr network size raise ValueError _ 'Networkofsize% size s fromIPrange% parent_range sexcludingIPranges% excluded_ranges swasnotfound ' % {'size' size 'parent_range' parent_range 'excluded_ranges' excluded_ranges}
def _bound_state_log_lik X initial_bound precs means covariance_type n_components n_features means shapen_samples X shape[0]bound np empty n_samples n_components bound[ ] initial_boundif covariance_type in ['diag' 'spherical'] for k in range n_components d X - means[k] bound[ k] - 0 5 * np sum d * d * precs[k] axis 1 elif covariance_type 'tied' for k in range n_components bound[ k] - 0 5 * _sym_quad_form X means[k] precs elif covariance_type 'full' for k in range n_components bound[ k] - 0 5 * _sym_quad_form X means[k] precs[k] return bound
def build_dataset n_samples 50 n_features 200 n_informative_features 10 n_targets 1 random_state np random RandomState 0 if n_targets > 1 w random_state randn n_features n_targets else w random_state randn n_features w[n_informative_features ] 0 0X random_state randn n_samples n_features y np dot X w X_test random_state randn n_samples n_features y_test np dot X_test w return X y X_test y_test
def build_dataset n_samples 50 n_features 200 n_informative_features 10 n_targets 1 random_state np random RandomState 0 if n_targets > 1 w random_state randn n_features n_targets else w random_state randn n_features w[n_informative_features ] 0 0X random_state randn n_samples n_features y np dot X w X_test random_state randn n_samples n_features y_test np dot X_test w return X y X_test y_test
def sokalmichener u v u _validate_vector u v _validate_vector v if u dtype bool ntt u & v sum nff ~ u & ~ v sum else ntt u * v sum nff 1 0 - u * 1 0 - v sum nft ntf _nbool_correspond_ft_tf u v return float 2 0 * ntf + nft / float ntt + nff + 2 0 * ntf + nft
def _date_lookup_for_field field date if isinstance field models DateTimeField date_range datetime datetime combine date datetime time min datetime datetime combine date datetime time max return { '%s__range' % field name date_range}else return {field name date}
def _date_lookup_for_field field date if isinstance field models DateTimeField date_range datetime datetime combine date datetime time min datetime datetime combine date datetime time max return { '%s__range' % field name date_range}else return {field name date}
def _labels_inertia_precompute_dense X x_squared_norms centers distances n_samples X shape[0] labels mindist pairwise_distances_argmin_min X X Y centers metric 'euclidean' metric_kwargs {'squared' True} labels labels astype np int32 if n_samples distances shape[0] distances[ ] mindistinertia mindist sum return labels inertia
def oauth_dance app_name consumer_key consumer_secret token_filename None open_browser True print "Hithere We'regonnagetyouallsetuptouse%s " % app_name twitter Twitter auth OAuth '' '' consumer_key consumer_secret format '' api_version None oauth_token oauth_token_secret parse_oauth_tokens twitter oauth request_token oauth_callback 'oob' oauth_url 'https //api twitter com/oauth/authorize?oauth_token ' + oauth_token oauth_verifier get_oauth_pin oauth_url open_browser twitter Twitter auth OAuth oauth_token oauth_token_secret consumer_key consumer_secret format '' api_version None oauth_token oauth_token_secret parse_oauth_tokens twitter oauth access_token oauth_verifier oauth_verifier if token_filename write_token_file token_filename oauth_token oauth_token_secret print print "That'sit Yourauthorizationkeyshavebeenwrittento%s " % token_filename return oauth_token oauth_token_secret
def oauth_dance app_name consumer_key consumer_secret token_filename None open_browser True print "Hithere We'regonnagetyouallsetuptouse%s " % app_name twitter Twitter auth OAuth '' '' consumer_key consumer_secret format '' api_version None oauth_token oauth_token_secret parse_oauth_tokens twitter oauth request_token oauth_callback 'oob' oauth_url 'https //api twitter com/oauth/authorize?oauth_token ' + oauth_token oauth_verifier get_oauth_pin oauth_url open_browser twitter Twitter auth OAuth oauth_token oauth_token_secret consumer_key consumer_secret format '' api_version None oauth_token oauth_token_secret parse_oauth_tokens twitter oauth access_token oauth_verifier oauth_verifier if token_filename write_token_file token_filename oauth_token oauth_token_secret print print "That'sit Yourauthorizationkeyshavebeenwrittento%s " % token_filename return oauth_token oauth_token_secret
def parse_headers content_disposition content_disposition content_disposition decode 'iso-8859-1' content_disposition normalize_ws content_disposition parsed peg parse content_disposition ContentDispositionValue return _ContentDisposition disposition parsed dtype assocs parsed params
def send_message sock message s message SerializeToString packed_len struct pack '>L' len s packed_message packed_len + s sock send packed_message
def get_default_parser usage '%prog[options]<start stop status>' parser OptionParser usage usage parser add_option '--debug' action 'store_true' help 'Runintheforeground logtostdout' parser add_option '--syslog' action 'store_true' help 'Writelogstosyslog' parser add_option '--nodaemon' action 'store_true' help 'Runintheforeground' parser add_option '--profile' help 'Recordperformanceprofiledatatothegivenfile' parser add_option '--profiler' help 'Specifytheprofilertouse' parser add_option '--pidfile' default None help 'Writepidtothegivenfile' parser add_option '--umask' default None help 'Usethegivenumaskwhencreatingfiles' parser add_option '--config' default None help 'Usethegivenconfigfile' parser add_option '--whitelist' default None help 'Usethegivenwhitelistfile' parser add_option '--blacklist' default None help 'Usethegivenblacklistfile' parser add_option '--logdir' default None help 'Writelogsinthegivendirectory' parser add_option '--instance' default 'a' help 'Manageaspecificcarboninstance' return parser
def test_sample_wrong_X_dft_ratio cc ClusterCentroids random_state RND_SEED cc fit X Y assert_raises RuntimeError cc sample np random random 100 40 np array [0] * 50 + [1] * 50
def test_sample_wrong_X_dft_ratio cc ClusterCentroids random_state RND_SEED cc fit X Y assert_raises RuntimeError cc sample np random random 100 40 np array [0] * 50 + [1] * 50
def cluster_remove version name 'main' stop False cmd [salt utils which 'pg_dropcluster' ]if stop cmd + ['--stop']cmd + [version name]cmdstr '' join [pipes quote c for c in cmd] ret __salt__['cmd run_all'] cmdstr python_shell False if ret get 'retcode' 0 0 log error 'ErrorremovingaPostgresqlcluster{0}/{1}' format version name else ret['changes'] 'Successfullyremovedcluster{0}/{1}' format version name return ret
def local_extra_dirs func def wraps self *args **kwargs if kwargs get 'base_dir' None is None return func self *args **kwargs else for c in self __class__ __mro__ if c __name__ 'DiskObjectStore' return getattr c func __name__ self *args **kwargs raise Exception "CouldnotcallDiskObjectStore's%smethod doesyourObjectStoreplugininheritfromDiskObjectStore?" % func __name__ return wraps
def local_extra_dirs func def wraps self *args **kwargs if kwargs get 'base_dir' None is None return func self *args **kwargs else for c in self __class__ __mro__ if c __name__ 'DiskObjectStore' return getattr c func __name__ self *args **kwargs raise Exception "CouldnotcallDiskObjectStore's%smethod doesyourObjectStoreplugininheritfromDiskObjectStore?" % func __name__ return wraps
def get_resources_dests resources_root rules def get_rel_path base path base base replace os path sep '/' path path replace os path sep '/' assert path startswith base return path[len base ] lstrip '/' destinations {}for base suffix dest in rules prefix os path join resources_root base for abs_base in iglob prefix abs_glob os path join abs_base suffix for abs_path in iglob abs_glob resource_file get_rel_path resources_root abs_path if dest is None destinations pop resource_file None else rel_path get_rel_path abs_base abs_path rel_dest dest replace os path sep '/' rstrip '/' destinations[resource_file] rel_dest + '/' + rel_path return destinations
def _search_software target search_results {}software dict _get_reg_software items for key value in six iteritems software if key is not None if target lower in key lower search_results[key] valuereturn search_results
def _search_software target search_results {}software dict _get_reg_software items for key value in six iteritems software if key is not None if target lower in key lower search_results[key] valuereturn search_results
def _salt_cloud_force_ascii exc if not isinstance exc UnicodeEncodeError UnicodeTranslateError raise TypeError "Can'thandle{0}" format exc unicode_trans {u'\xa0' u'' u'\u2013' u'-'}if exc object[exc start exc end] in unicode_trans return unicode_trans[exc object[exc start exc end]] exc end raise exc
def _salt_cloud_force_ascii exc if not isinstance exc UnicodeEncodeError UnicodeTranslateError raise TypeError "Can'thandle{0}" format exc unicode_trans {u'\xa0' u'' u'\u2013' u'-'}if exc object[exc start exc end] in unicode_trans return unicode_trans[exc object[exc start exc end]] exc end raise exc
def _salt_cloud_force_ascii exc if not isinstance exc UnicodeEncodeError UnicodeTranslateError raise TypeError "Can'thandle{0}" format exc unicode_trans {u'\xa0' u'' u'\u2013' u'-'}if exc object[exc start exc end] in unicode_trans return unicode_trans[exc object[exc start exc end]] exc end raise exc
def _salt_cloud_force_ascii exc if not isinstance exc UnicodeEncodeError UnicodeTranslateError raise TypeError "Can'thandle{0}" format exc unicode_trans {u'\xa0' u'' u'\u2013' u'-'}if exc object[exc start exc end] in unicode_trans return unicode_trans[exc object[exc start exc end]] exc end raise exc
def _salt_cloud_force_ascii exc if not isinstance exc UnicodeEncodeError UnicodeTranslateError raise TypeError "Can'thandle{0}" format exc unicode_trans {u'\xa0' u'' u'\u2013' u'-'}if exc object[exc start exc end] in unicode_trans return unicode_trans[exc object[exc start exc end]] exc end raise exc
def _salt_cloud_force_ascii exc if not isinstance exc UnicodeEncodeError UnicodeTranslateError raise TypeError "Can'thandle{0}" format exc unicode_trans {u'\xa0' u'' u'\u2013' u'-'}if exc object[exc start exc end] in unicode_trans return unicode_trans[exc object[exc start exc end]] exc end raise exc
def _salt_cloud_force_ascii exc if not isinstance exc UnicodeEncodeError UnicodeTranslateError raise TypeError "Can'thandle{0}" format exc unicode_trans {u'\xa0' u'' u'\u2013' u'-'}if exc object[exc start exc end] in unicode_trans return unicode_trans[exc object[exc start exc end]] exc end raise exc
def _salt_cloud_force_ascii exc if not isinstance exc UnicodeEncodeError UnicodeTranslateError raise TypeError "Can'thandle{0}" format exc unicode_trans {u'\xa0' u'' u'\u2013' u'-'}if exc object[exc start exc end] in unicode_trans return unicode_trans[exc object[exc start exc end]] exc end raise exc
def _salt_cloud_force_ascii exc if not isinstance exc UnicodeEncodeError UnicodeTranslateError raise TypeError "Can'thandle{0}" format exc unicode_trans {u'\xa0' u'' u'\u2013' u'-'}if exc object[exc start exc end] in unicode_trans return unicode_trans[exc object[exc start exc end]] exc end raise exc
def _salt_cloud_force_ascii exc if not isinstance exc UnicodeEncodeError UnicodeTranslateError raise TypeError "Can'thandle{0}" format exc unicode_trans {u'\xa0' u'' u'\u2013' u'-'}if exc object[exc start exc end] in unicode_trans return unicode_trans[exc object[exc start exc end]] exc end raise exc
def _active_mounts_freebsd ret for line in __salt__['cmd run_stdout'] 'mount-p' split '\n' comps re sub '\\s+' '' line split ret[comps[1]] {'device' comps[0] 'fstype' comps[2] 'opts' _resolve_user_group_names comps[3] split ' ' }return ret
def _parse_iso_timestamp entry if not isinstance entry str str_type raise ValueError 'parse_iso_timestamp inputmustbeastr gota%s' % type entry if ' ' in entry timestamp_str microseconds entry split ' ' else timestamp_str microseconds entry '000000' if len microseconds 6 or not microseconds isdigit raise ValueError "timestamp'smicrosecondsshouldbesixdigits" if timestamp_str[10] 'T' timestamp_str timestamp_str[ 10] + '' + timestamp_str[11 ] else raise ValueError "timestampdidn'tcontaindelimeter'T'betweendateandtime" timestamp _parse_timestamp timestamp_str return timestamp + datetime timedelta microseconds int microseconds
def get_non_private_exploration_summaries return _get_exploration_summaries_from_models exp_models ExpSummaryModel get_non_private
def test_ast_unicode_strings def _compile_string s hy_s HyString s hy_s start_line hy_s end_line 0hy_s start_column hy_s end_column 0code hy_compile [hy_s] u'__main__' return code body[0] value sassert _compile_string u'test' u'test' assert _compile_string u'\u03b1\u03b2' u'\u03b1\u03b2' assert _compile_string u'\xc3\xa9' u'\xc3\xa9'
def op_abs_tmul lin_op value if lin_op type is lo NEG result valueelif lin_op type is lo MUL coeff mul lin_op data {} True if np isscalar coeff result coeff * value else result coeff T * value elif lin_op type is lo DIV divisor mul lin_op data {} True result value / divisor elif lin_op type is lo CONV result conv_mul lin_op value True True else result op_tmul lin_op value return result
def make_edge_table bt data asarray [d for d in bt iter_data axis 'observation' dense True ] oids asarray bt ids axis 'observation' header '#Sample DCTB OTU DCTB Abundance'lines [header]for sample in bt ids sample_ind bt index sample 'sample' otu_ids oids[data[ sample_ind] nonzero [0]]otu_abs data[ sample_ind][data[ sample_ind] nonzero [0]]connections [ '%s DCTB %s DCTB %s' % sample otu ab for otu ab in zip otu_ids otu_abs ]lines extend connections return lines
def make_edge_table bt data asarray [d for d in bt iter_data axis 'observation' dense True ] oids asarray bt ids axis 'observation' header '#Sample DCTB OTU DCTB Abundance'lines [header]for sample in bt ids sample_ind bt index sample 'sample' otu_ids oids[data[ sample_ind] nonzero [0]]otu_abs data[ sample_ind][data[ sample_ind] nonzero [0]]connections [ '%s DCTB %s DCTB %s' % sample otu ab for otu ab in zip otu_ids otu_abs ]lines extend connections return lines
def make_edge_table bt data asarray [d for d in bt iter_data axis 'observation' dense True ] oids asarray bt ids axis 'observation' header '#Sample DCTB OTU DCTB Abundance'lines [header]for sample in bt ids sample_ind bt index sample 'sample' otu_ids oids[data[ sample_ind] nonzero [0]]otu_abs data[ sample_ind][data[ sample_ind] nonzero [0]]connections [ '%s DCTB %s DCTB %s' % sample otu ab for otu ab in zip otu_ids otu_abs ]lines extend connections return lines
def read_data filename with zipfile ZipFile filename as f data tf compat as_str f read f namelist [0] split return data
def get_namespace name os environ get _ENV_CURRENT_NAMESPACE None if name is None name _config default_namespace_for_request if name is not None set_namespace name if name is None name ''return name
def is_dir info if hasattr info u'isdir' return info isdir else base os path basename info filename return not base
def region_codes_for_country_code country_code regions COUNTRY_CODE_TO_REGION_CODE get country_code None if regions is None return else return regions
def region_codes_for_country_code country_code regions COUNTRY_CODE_TO_REGION_CODE get country_code None if regions is None return else return regions
def get_template_from_request request obj None no_current_page False template Noneif len get_cms_setting 'TEMPLATES' 1 return get_cms_setting 'TEMPLATES' [0][0]if hasattr request 'POST' and 'template' in request POST template request POST['template']elif hasattr request 'GET' and 'template' in request GET template request GET['template']if not template and obj is not None template obj get_template if not template and not no_current_page and hasattr request 'current_page' current_page request current_pageif hasattr current_page 'get_template' template current_page get_template if template is not None and template in dict get_cms_setting 'TEMPLATES' keys if template constants TEMPLATE_INHERITANCE_MAGIC and obj return obj get_template return templatereturn get_cms_setting 'TEMPLATES' [0][0]
def create body url build_url RESOURCE return request 'post' url json body
def qcut x q labels None retbins False precision 3 duplicates 'raise' x_is_series series_index name x _preprocess_for_cut x x dtype _coerce_to_type x if is_integer q quantiles np linspace 0 1 q + 1 else quantiles qbins algos quantile x quantiles fac bins _bins_to_cuts x bins labels labels precision precision include_lowest True dtype dtype duplicates duplicates return _postprocess_for_cut fac bins retbins x_is_series series_index name
@_ensure_existsdef reboot name kill False if _sd_version > 219 if state name 'running' ret _machinectl 'reboot{0}' format name else return start name else cmd 'systemctlstopsystemd-nspawn@{0}' format name ret __salt__['cmd run_all'] cmd python_shell False if ret['retcode'] 0 __context__['retcode'] salt defaults exitcodes EX_UNAVAILABLEreturn Falsecmd 'systemctlstartsystemd-nspawn@{0}' format name ret __salt__['cmd run_all'] cmd python_shell False if ret['retcode'] 0 __context__['retcode'] salt defaults exitcodes EX_UNAVAILABLEreturn Falsereturn True
def _connect_client trig_queue t0 time time while time time - t0 < _max_wait and _server is None or not _server _running time sleep 0 01 assert_true _server is not None and _server _running stim_client StimClient 'localhost' port 4218 t0 time time while time time - t0 < _max_wait and not _have_put_in_trigger time sleep 0 01 assert_true _have_put_in_trigger trig_queue put stim_client get_trigger stim_client close
def list_monitor_data kwargs None call None if call 'function' raise SaltCloudSystemExit 'Thelist_monitor_datamustbecalledwith-for--function ' if not isinstance kwargs dict kwargs {}ret {}params {'Action' 'GetMonitorData' 'RegionId' get_location }if 'name' in kwargs params['InstanceId'] kwargs['name']items query params params monitorData items['MonitorData']for data in monitorData['InstanceMonitorData'] ret[data['InstanceId']] {}for item in data ret[data['InstanceId']][item] str data[item] return ret
def initialize db configure_db
def _init_once path os getenv 'LIBCLOUD_DEBUG' if path mode 'a'from libcloud utils py3 import PY3if path in ['/dev/stderr' '/dev/stdout'] and PY3 mode 'w'fo codecs open path mode encoding 'utf8' enable_debug fo if have_paramiko paramiko common logging basicConfig level paramiko common DEBUG
def suite suite unittest TestSuite suite addTest unittest makeSuite TestMarkdown suite addTest unittest makeSuite TestBlockParser suite addTest unittest makeSuite TestBlockParserState suite addTest unittest makeSuite TestHtmlStash suite addTest unittest makeSuite TestOrderedDict for filename in os listdir 'markdown/extensions' if filename endswith ' py' module 'markdown extensions %s' % filename[ -3 ] try suite addTest DocTestSuite module except ValueErrorreturn suite
def _concatenate2 arrays axes [] if isinstance arrays Iterator arrays list arrays if not isinstance arrays list tuple return arraysif len axes > 1 arrays [_concatenate2 a axes axes[1 ] for a in arrays]return np concatenate arrays axis axes[0]
def test_http_header_encoding mocked_socket mock MagicMock mocked_socket sendall mock MagicMock mocked_request mock MagicMock response Response mocked_request mocked_socket None response headers append 'foo' u'h\xe4der' with pytest raises UnicodeEncodeError response send_headers tosend response default_headers tosend extend [ '%s %s\r\n' % k v for k v in response headers] header_str '%s\r\n' % '' join tosend with pytest raises UnicodeEncodeError mocked_socket sendall util to_bytestring header_str 'ascii'
def set_restart_freeze enabled state salt utils mac_utils validate_enabled enabled cmd 'systemsetup-setrestartfreeze{0}' format state salt utils mac_utils execute_return_success cmd return salt utils mac_utils confirm_updated state get_restart_freeze True
def set_subnet_name name cmd 'systemsetup-setlocalsubnetname"{0}"' format name salt utils mac_utils execute_return_success cmd return salt utils mac_utils confirm_updated name get_subnet_name
def __virtual__ if not salt utils is_windows return False 'ModuleonlyavailableonWindows' return __virtualname__
def unsafe_inline_enabled response non_report_only_policies retrieve_csp_policies response report_only_policies retrieve_csp_policies response True policies_all merge_policies_dict non_report_only_policies report_only_policies if len policies_all > 0 for directive_name in policies_all if directive_name lower CSP_DIRECTIVE_SCRIPT and directive_name lower CSP_DIRECTIVE_STYLE continuefor directive_value in policies_all[directive_name] if directive_value strip lower CSP_DIRECTIVE_VALUE_UNSAFE_INLINE return Truereturn False
@transaction non_atomic_requests@require_POST@ensure_csrf_cookie@cache_control no_cache True no_store True must_revalidate True @require_level 'staff' def export_ora2_data request course_id course_key SlashSeparatedCourseKey from_deprecated_string course_id try lms djangoapps instructor_task api submit_export_ora2_data request course_key success_status _ 'TheORAdatareportisbeinggenerated ' return JsonResponse {'status' success_status} except AlreadyRunningError already_running_status _ "AnORAdatareportgenerationtaskisalreadyinprogress Checkthe'PendingTasks'tableforthestatusofthetask Whencompleted thereportwillbeavailablefordownloadinthetablebelow " return JsonResponse {'status' already_running_status}
@transaction non_atomic_requests@require_POST@ensure_csrf_cookie@cache_control no_cache True no_store True must_revalidate True @require_level 'staff' def export_ora2_data request course_id course_key SlashSeparatedCourseKey from_deprecated_string course_id try lms djangoapps instructor_task api submit_export_ora2_data request course_key success_status _ 'TheORAdatareportisbeinggenerated ' return JsonResponse {'status' success_status} except AlreadyRunningError already_running_status _ "AnORAdatareportgenerationtaskisalreadyinprogress Checkthe'PendingTasks'tableforthestatusofthetask Whencompleted thereportwillbeavailablefordownloadinthetablebelow " return JsonResponse {'status' already_running_status}
def get_random_id id ''for n in range 6 letter random choice letter_choices id + letterreturn id
@transaction atomicdef mass_get_or_create model_class base_queryset id_field default_dict global_defaults current_instances list base_queryset current_ids set [unicode getattr c id_field for c in current_instances] given_ids map unicode default_dict keys new_ids [g for g in given_ids if g not in current_ids ]prepared_models []for new_id in new_ids defaults default_dict[new_id]defaults[id_field] new_iddefaults update global_defaults model_instance model_class **defaults prepared_models append model_instance if hasattr model_class objects 'bulk_create' model_class objects bulk_create prepared_models else [m save for m in prepared_models]inserted_model_instances prepared_modelsreturn current_instances inserted_model_instances
@transaction atomicdef mass_get_or_create model_class base_queryset id_field default_dict global_defaults current_instances list base_queryset current_ids set [unicode getattr c id_field for c in current_instances] given_ids map unicode default_dict keys new_ids [g for g in given_ids if g not in current_ids ]prepared_models []for new_id in new_ids defaults default_dict[new_id]defaults[id_field] new_iddefaults update global_defaults model_instance model_class **defaults prepared_models append model_instance if hasattr model_class objects 'bulk_create' model_class objects bulk_create prepared_models else [m save for m in prepared_models]inserted_model_instances prepared_modelsreturn current_instances inserted_model_instances
@transaction atomicdef mass_get_or_create model_class base_queryset id_field default_dict global_defaults current_instances list base_queryset current_ids set [unicode getattr c id_field for c in current_instances] given_ids map unicode default_dict keys new_ids [g for g in given_ids if g not in current_ids ]prepared_models []for new_id in new_ids defaults default_dict[new_id]defaults[id_field] new_iddefaults update global_defaults model_instance model_class **defaults prepared_models append model_instance if hasattr model_class objects 'bulk_create' model_class objects bulk_create prepared_models else [m save for m in prepared_models]inserted_model_instances prepared_modelsreturn current_instances inserted_model_instances
@transaction atomicdef mass_get_or_create model_class base_queryset id_field default_dict global_defaults current_instances list base_queryset current_ids set [unicode getattr c id_field for c in current_instances] given_ids map unicode default_dict keys new_ids [g for g in given_ids if g not in current_ids ]prepared_models []for new_id in new_ids defaults default_dict[new_id]defaults[id_field] new_iddefaults update global_defaults model_instance model_class **defaults prepared_models append model_instance if hasattr model_class objects 'bulk_create' model_class objects bulk_create prepared_models else [m save for m in prepared_models]inserted_model_instances prepared_modelsreturn current_instances inserted_model_instances
def fasthash string md4 hashlib new 'md4' md4 update string return md4 hexdigest
def group_snapshot_get_all context filters None return IMPL group_snapshot_get_all context filters
def _context_factory_and_credential path host port ca Certificate loadPEM path child 'cluster crt' getContent node_credential NodeCredential from_path path 'node' policy ControlServicePolicy ca_certificate ca client_credential node_credential credential return _TLSContext context_factory policy creatorForNetloc host port node_credential node_credential
def _context_factory_and_credential path host port ca Certificate loadPEM path child 'cluster crt' getContent node_credential NodeCredential from_path path 'node' policy ControlServicePolicy ca_certificate ca client_credential node_credential credential return _TLSContext context_factory policy creatorForNetloc host port node_credential node_credential
def _credit_course_requirements course_key student if not settings FEATURES get 'ENABLE_CREDIT_ELIGIBILITY' False and is_credit_course course_key return Noneenrollment CourseEnrollment get_enrollment student course_key if enrollment and enrollment mode not in REQUIREMENTS_DISPLAY_MODES return Nonenon_eligible_statuses ['failed' 'declined']requirement_statuses get_credit_requirement_status course_key student username if is_user_eligible_for_credit student username course_key eligibility_status 'eligible'elif any requirement['status'] in non_eligible_statuses for requirement in requirement_statuses eligibility_status 'not_eligible'else eligibility_status 'partial_eligible'return {'eligibility_status' eligibility_status 'requirements' requirement_statuses}
def rs_swap a b d {}for rsa in a d[rsa] [rsb for rsb in b if rsa symbol rsb symbol ][0]return d
def _check_for_int x try y int x except OverflowError ValueError passelse if x x and y x return yreturn x
def hilbert x N None axis -1 x asarray x if iscomplexobj x raise ValueError 'xmustbereal ' if N is None N x shape[axis]if N < 0 raise ValueError 'Nmustbepositive ' Xf fftpack fft x N axis axis h zeros N if N % 2 0 h[0] h[ N // 2 ] 1h[1 N // 2 ] 2else h[0] 1h[1 N + 1 // 2 ] 2if x ndim > 1 ind [newaxis] * x ndim ind[axis] slice None h h[ind]x fftpack ifft Xf * h axis axis return x
def _get_folder_info folder folder_info ''if folder and folder parent folder_info _get_folder_info folder parent folder_info + '%s%s' % folder name replace 'Unnamedfolder' '' folder description or '' return folder_info
def run_discovery entry_points_iter cached False reg_cache {}if cached reg_cache cache registry_cache discovery QtWidgetDiscovery cached_descriptions reg_cache registry QtWidgetRegistry discovery found_category connect registry register_category discovery found_widget connect registry register_widget discovery run if cached cache save_registry_cache reg_cache return registry
def expand_substates states def nbits n 'numberofbitssetinnbase2'nb 0while n n rem divmod n 2 nb + remreturn nbstatelist []for state in states substates list set state & x for x in states substates sort key nbits reverse True statelist append substates return statelist
def get_collection_snapshots_metadata collection_id collection get_collection_by_id collection_id current_version collection versionversion_nums range 1 current_version + 1 return collection_models CollectionModel get_snapshots_metadata collection_id version_nums
def _MakeCronListIntoYaml cron_list statements ['cron ']for cron in cron_list statements + cron ToYaml return '\n' join statements + '\n'
def _MakeCronListIntoYaml cron_list statements ['cron ']for cron in cron_list statements + cron ToYaml return '\n' join statements + '\n'
def _build_bounding_box_lookup bounding_box_file lines tf gfile FastGFile bounding_box_file 'r' readlines images_to_bboxes {}num_bbox 0num_image 0for l in lines if l parts l split ' ' assert len parts 5 'Failedtoparse %s' % l filename parts[0]xmin float parts[1] ymin float parts[2] xmax float parts[3] ymax float parts[4] box [xmin ymin xmax ymax]if filename not in images_to_bboxes images_to_bboxes[filename] []num_image + 1images_to_bboxes[filename] append box num_bbox + 1print 'Successfullyread%dboundingboxesacross%dimages ' % num_bbox num_image return images_to_bboxes
def _build_bounding_box_lookup bounding_box_file lines tf gfile FastGFile bounding_box_file 'r' readlines images_to_bboxes {}num_bbox 0num_image 0for l in lines if l parts l split ' ' assert len parts 5 'Failedtoparse %s' % l filename parts[0]xmin float parts[1] ymin float parts[2] xmax float parts[3] ymax float parts[4] box [xmin ymin xmax ymax]if filename not in images_to_bboxes images_to_bboxes[filename] []num_image + 1images_to_bboxes[filename] append box num_bbox + 1print 'Successfullyread%dboundingboxesacross%dimages ' % num_bbox num_image return images_to_bboxes
def simple_paginate request queryset per_page 20 p paginator SimplePaginator queryset per_page page p page request GET get 'page' 1 page url build_paged_url request return page
def adjust_string original length if not length > 0 raise AssertionErrorresulting_string originalif len resulting_string > length unit_db resulting_string endswith 'dB' and resulting_string find ' ' -1 if len resulting_string strip > length and unit_db resulting_string resulting_string[ -2 ]if len resulting_string > length for char in '' '_' 'i' 'o' 'u' 'e' 'a' offset 0 if char '' else 1 while len resulting_string > length and resulting_string rfind char offset > 0 char_pos resulting_string rfind char offset resulting_string resulting_string[ char_pos] + resulting_string[ char_pos + 1 ] resulting_string resulting_string[ length]resulting_string len resulting_string < length and resulting_string ljust length return resulting_string
def test_str mlp MLP nvis 2 layers [Linear 2 'h0' irange 0 Linear 2 'h1' irange 0 ] s str mlp assert isinstance s six string_types
def _get_branch_head branch return '%s@head' % branch
def setup_platform hass config add_devices discovery_info None import xmltodictname config get CONF_NAME station config get CONF_STATION try response requests get _RESOURCE timeout 5 if any str station location get '@StrNr' for location in xmltodict parse response text ['AKT_Data']['MesPar'] is False _LOGGER error 'Thegivenstationdoesnotexist %s' station return Falseexcept requests exceptions ConnectionError _LOGGER error 'TheURLisnotaccessible' return Falsedata HydrologicalData station add_devices [SwissHydrologicalDataSensor name data ]
@hook command 'wordrandom' 'randomword' autohelp False def random_word if not api_key return 'ThiscommandrequiresanAPIkeyfromwordnik com 'url API_URL + 'words json/randomWord' params {'api_key' api_key 'hasDictionarydef' 'true' 'vulgar' 'true'}json requests get url params params json if json word json['word']return 'Yourrandomwordis\x02{}\x02 ' format word else return 'TherewasaproblemcontactingtheWordnikAPI '
@FileSystem in_directory current_directory 'django' 'alfaces' def test_django_agains_alfaces status out commands getstatusoutput 'pythonmanage pyharvest--verbosity 3--no-color' assert_equals status 0 out assert 'TestthedjangoappDONOTHING' in out assert 'TestthedjangoappFOOBAR' in out
@FileSystem in_directory current_directory 'django' 'alfaces' def test_django_agains_alfaces status out commands getstatusoutput 'pythonmanage pyharvest--verbosity 3--no-color' assert_equals status 0 out assert 'TestthedjangoappDONOTHING' in out assert 'TestthedjangoappFOOBAR' in out
def erase_menu stdscr menu_y stdscr move menu_y 0 stdscr clrtoeol stdscr move menu_y + 1 0 stdscr clrtoeol
def erase_menu stdscr menu_y stdscr move menu_y 0 stdscr clrtoeol stdscr move menu_y + 1 0 stdscr clrtoeol
def ALL_REGIONS_WITH_CONTENT_RATINGS return [x for x in ALL_REGIONS if x ratingsbody]
def ALL_REGIONS_WITH_CONTENT_RATINGS return [x for x in ALL_REGIONS if x ratingsbody]
def create_user username password permissions users None _uids set if users is None users list_users if username in users log warning "'{0}'alreadyexists" format username return Falsefor idx in six iterkeys users _uids add users[idx]['index'] uid sorted list set range 2 12 - _uids reverse True pop if not __execute_cmd 'config-gcfgUserAdmin-ocfgUserAdminUserName-i{0}{1}' format uid username delete_user username uid return Falseif not set_permissions username permissions uid log warning 'unabletosetuserpermissions' delete_user username uid return Falseif not change_password username password uid log warning 'unabletosetuserpassword' delete_user username uid return Falseif not __execute_cmd 'config-gcfgUserAdmin-ocfgUserAdminEnable-i{0}1' format uid delete_user username uid return Falsereturn True
def title_to_url title max_length 50 title _force_unicode title title rx_whitespace sub '_' title title rx_notsafe sub '' title title rx_underscore sub '_' title title title strip '_' title title lower if len title > max_length title title[ max_length]last_word title rfind '_' if last_word > 0 title title[ last_word]return title or '_'
def country_timezones iso3166_code iso3166_code iso3166_code upper if not _country_timezones_cache zone_tab open_resource 'zone tab' for line in zone_tab if line startswith '#' continue code coordinates zone line split None 4 [ 3]try _country_timezones_cache[code] append zone except KeyError _country_timezones_cache[code] [zone]return _country_timezones_cache[iso3166_code]
def country_timezones iso3166_code iso3166_code iso3166_code upper if not _country_timezones_cache zone_tab open_resource 'zone tab' for line in zone_tab if line startswith '#' continue code coordinates zone line split None 4 [ 3]try _country_timezones_cache[code] append zone except KeyError _country_timezones_cache[code] [zone]return _country_timezones_cache[iso3166_code]
def vm_disk_save name kwargs None call None if call 'action' raise SaltCloudSystemExit 'Thevm_disk_saveactionmustbecalledwith-aor--action ' if kwargs is None kwargs {}disk_id kwargs get 'disk_id' None image_name kwargs get 'image_name' None image_type kwargs get 'image_type' '' snapshot_id int kwargs get 'snapshot_id' '-1' if disk_id is None or image_name is None raise SaltCloudSystemExit "Thevm_disk_savefunctionrequiresa'disk_id'andan'image_name'tobeprovided " server user password _get_xml_rpc auth ' ' join [user password] vm_id int get_vm_id kwargs {'name' name} response server one vm disksave auth vm_id int disk_id image_name image_type snapshot_id data {'action' 'vm disksave' 'saved' response[0] 'image_id' response[1] 'error_code' response[2]}return data
def vm_disk_save name kwargs None call None if call 'action' raise SaltCloudSystemExit 'Thevm_disk_saveactionmustbecalledwith-aor--action ' if kwargs is None kwargs {}disk_id kwargs get 'disk_id' None image_name kwargs get 'image_name' None image_type kwargs get 'image_type' '' snapshot_id int kwargs get 'snapshot_id' '-1' if disk_id is None or image_name is None raise SaltCloudSystemExit "Thevm_disk_savefunctionrequiresa'disk_id'andan'image_name'tobeprovided " server user password _get_xml_rpc auth ' ' join [user password] vm_id int get_vm_id kwargs {'name' name} response server one vm disksave auth vm_id int disk_id image_name image_type snapshot_id data {'action' 'vm disksave' 'saved' response[0] 'image_id' response[1] 'error_code' response[2]}return data
def vm_disk_save name kwargs None call None if call 'action' raise SaltCloudSystemExit 'Thevm_disk_saveactionmustbecalledwith-aor--action ' if kwargs is None kwargs {}disk_id kwargs get 'disk_id' None image_name kwargs get 'image_name' None image_type kwargs get 'image_type' '' snapshot_id int kwargs get 'snapshot_id' '-1' if disk_id is None or image_name is None raise SaltCloudSystemExit "Thevm_disk_savefunctionrequiresa'disk_id'andan'image_name'tobeprovided " server user password _get_xml_rpc auth ' ' join [user password] vm_id int get_vm_id kwargs {'name' name} response server one vm disksave auth vm_id int disk_id image_name image_type snapshot_id data {'action' 'vm disksave' 'saved' response[0] 'image_id' response[1] 'error_code' response[2]}return data
def find_best_app module from import Flaskfor attr_name in 'app' 'application' app getattr module attr_name None if app is not None and isinstance app Flask return appmatches [v for k v in iteritems module __dict__ if isinstance v Flask ]if len matches 1 return matches[0]raise NoAppException 'Failedtofindapplicationinmodule"%s" AreyousureitcontainsaFlaskapplication?MaybeyouwrappeditinaWSGImiddlewareoryouareusingafactoryfunction ' % module __name__
def find_best_app module from import Flaskfor attr_name in 'app' 'application' app getattr module attr_name None if app is not None and isinstance app Flask return appmatches [v for k v in iteritems module __dict__ if isinstance v Flask ]if len matches 1 return matches[0]raise NoAppException 'Failedtofindapplicationinmodule"%s" AreyousureitcontainsaFlaskapplication?MaybeyouwrappeditinaWSGImiddlewareoryouareusingafactoryfunction ' % module __name__
def find_guest_agent base_dir if CONF xenserver disable_agent return Falseagent_rel_path CONF xenserver agent_pathagent_path os path join base_dir agent_rel_path if os path isfile agent_path LOG info _LI 'XenServertoolsinstalledinthisimagearecapableofnetworkinjection Networkingfileswillnotbemanipulated' return Truexe_daemon_filename os path join base_dir 'usr' 'sbin' 'xe-daemon' if os path isfile xe_daemon_filename LOG info _LI 'XenServertoolsarepresentinthisimagebutarenotcapableofnetworkinjection' else LOG info _LI 'XenServertoolsarenotinstalledinthisimage' return False
@pytest mark parametrize 'parallel' [True False] def test_lstrip_whitespace parallel read_basic text '\n1 2 DCTB 3\nA DCTB DCTB B C\na b c\n' + '\n' table read_basic text delimiter ' ' parallel parallel expected Table [['A' 'a'] ['B' 'b'] ['C' 'c']] names '1' '2' '3' assert_table_equal table expected
@pytest mark django_dbdef test_directory_create_bad root with pytest raises ValidationError Directory objects create name 'name' parent None with pytest raises ValidationError Directory objects create name '' parent root
def _find_monitor_type cs vtype return utils find_resource cs monitor_types vtype
def make_app app_name test_path_name os chdir '%s/applications/%s' % os environ['WEB2PY_PATH'] app_name sys path append '%s/applications/%s' % os environ['WEB2PY_PATH'] app_name os mkdir 'private' os mkdir 'databases' os mkdir 'models' os mkdir 'controllers' os mkdir 'cron' os mkdir 'languages' os mkdir 'cache' os mkdir 'modules' os mkdir 'static' os mkdir 'views' models_path os path join os getcwd 'models' src os listdir test_path_name for files in src file_path os path join test_path_name files if os path isfile file_path shutil copy file_path models_path
def _path_to_string path return ' ' join path
def excview_tween_factory handler registry def excview_tween request attrs request __dict__try response handler request except Exception as exc attrs['exc_info'] sys exc_info attrs['exception'] excif 'response' in attrs del attrs['response']request_iface attrs get 'request_iface' IRequest provides providedBy exc try response _call_view registry request exc provides '' view_classifier IExceptionViewClassifier request_iface request_iface combined except PredicateMismatch response Noneif response is None reraise *attrs['exc_info'] return responsereturn excview_tween
def excview_tween_factory handler registry def excview_tween request attrs request __dict__try response handler request except Exception as exc attrs['exc_info'] sys exc_info attrs['exception'] excif 'response' in attrs del attrs['response']request_iface attrs get 'request_iface' IRequest provides providedBy exc try response _call_view registry request exc provides '' view_classifier IExceptionViewClassifier request_iface request_iface combined except PredicateMismatch response Noneif response is None reraise *attrs['exc_info'] return responsereturn excview_tween
def excview_tween_factory handler registry def excview_tween request attrs request __dict__try response handler request except Exception as exc attrs['exc_info'] sys exc_info attrs['exception'] excif 'response' in attrs del attrs['response']request_iface attrs get 'request_iface' IRequest provides providedBy exc try response _call_view registry request exc provides '' view_classifier IExceptionViewClassifier request_iface request_iface combined except PredicateMismatch response Noneif response is None reraise *attrs['exc_info'] return responsereturn excview_tween
def excview_tween_factory handler registry def excview_tween request attrs request __dict__try response handler request except Exception as exc attrs['exc_info'] sys exc_info attrs['exception'] excif 'response' in attrs del attrs['response']request_iface attrs get 'request_iface' IRequest provides providedBy exc try response _call_view registry request exc provides '' view_classifier IExceptionViewClassifier request_iface request_iface combined except PredicateMismatch response Noneif response is None reraise *attrs['exc_info'] return responsereturn excview_tween
def argrelmax data axis 0 order 1 mode 'clip' return argrelextrema data np greater axis order mode
def set_block_scalar_indent TokenClass def callback lexer match context text match group context block_scalar_indent Noneif not text returnincrement match group 1 if increment current_indent max context indent 0 increment int increment context block_scalar_indent current_indent + increment if text yield match start TokenClass text context pos match end return callback
def _upload_folder cf folder container ttl None headers None total_bytes 0for root dirs files in os walk folder for fname in files full_path os path join root fname obj_name os path relpath full_path folder obj_size os path getsize full_path cf upload_file container full_path obj_name obj_name return_none True ttl ttl headers headers total_bytes + obj_sizereturn total_bytes
def files_contains file1 file2 attributes None local_file open file1 'U' readlines history_data open file2 'U' read lines_diff int attributes get 'lines_diff' 0 line_diff_count 0while local_file contains local_file pop 0 rstrip '\n\r' if contains not in history_data line_diff_count + 1if line_diff_count > lines_diff raise AssertionError "Failedtofind'%s'inhistorydata lines_diff %i \n" % contains lines_diff
def files_contains file1 file2 attributes None local_file open file1 'U' readlines history_data open file2 'U' read lines_diff int attributes get 'lines_diff' 0 line_diff_count 0while local_file contains local_file pop 0 rstrip '\n\r' if contains not in history_data line_diff_count + 1if line_diff_count > lines_diff raise AssertionError "Failedtofind'%s'inhistorydata lines_diff %i \n" % contains lines_diff
def files_contains file1 file2 attributes None local_file open file1 'U' readlines history_data open file2 'U' read lines_diff int attributes get 'lines_diff' 0 line_diff_count 0while local_file contains local_file pop 0 rstrip '\n\r' if contains not in history_data line_diff_count + 1if line_diff_count > lines_diff raise AssertionError "Failedtofind'%s'inhistorydata lines_diff %i \n" % contains lines_diff
def new_post_mails reply users_and_watches post_url add_utm reply get_absolute_url 'kbforums-post' c {'post' reply content 'post_html' reply content_parsed 'author' reply creator 'host' Site objects get_current domain 'thread' reply thread title 'forum' reply thread document title 'post_url' post_url}return emails_with_users_and_watches subject _lazy u'Re {forum}-{thread}' text_template 'kbforums/email/new_post ltxt' html_template 'kbforums/email/new_post html' context_vars c users_and_watches users_and_watches
def setup_module try ret call ['sphinx-build' '--help'] stdout PIPE stderr PIPE except OSError pytest skip 'Needsphinx-buildonpathforthesetests' if ret 0 raise RuntimeError 'sphinx-builddoesnotreturn0'
def _get_options return {'host' __opts__ get 'couchbase host' 'salt' 'port' __opts__ get 'couchbase port' 8091 'bucket' __opts__ get 'couchbase bucket' 'salt' 'password' __opts__ get 'couchbase password' '' }
def get_user path follow_symlinks True return uid_to_user get_uid path follow_symlinks
def get_user path follow_symlinks True return uid_to_user get_uid path follow_symlinks
def testAgent path agent port DEFAULT_PORT agent adaptAgentObject BenchmarkingAgent agent experiment RLCExperiment path str port experiment start clientAgent ClientAgent agent clientAgent connect DEFAULT_HOST port CLIENT_TIMEOUT logging info 'Agentconnected' clientAgent runAgentEventLoop clientAgent close logging info 'Agentfinished' experiment stop return agent agent benchmark
def testAgent path agent port DEFAULT_PORT agent adaptAgentObject BenchmarkingAgent agent experiment RLCExperiment path str port experiment start clientAgent ClientAgent agent clientAgent connect DEFAULT_HOST port CLIENT_TIMEOUT logging info 'Agentconnected' clientAgent runAgentEventLoop clientAgent close logging info 'Agentfinished' experiment stop return agent agent benchmark
def track_closed cls class TrackingClosed cls def __init__ self *a **kw super TrackingClosed self __init__ *a **kw self closed Falsedef close self super TrackingClosed self close self closed Truereturn TrackingClosed
@np deprecate new_name 'expm' def expm3 A q 20 A _asarray_square A n A shape[0]t A dtype charif t not in ['f' 'F' 'd' 'D'] A A astype 'd' t 'd'eA np identity n dtype t trm np identity n dtype t castfunc cast[t]for k in range 1 q trm[ ] trm dot A / castfunc k eA + trmreturn eA
def node_state id_ states_int {0 'RUNNING' 1 'REBOOTING' 2 'TERMINATED' 3 'PENDING' 4 'UNKNOWN' 5 'STOPPED' 6 'SUSPENDED' 7 'ERROR' 8 'PAUSED'}states_str {'running' 'RUNNING' 'rebooting' 'REBOOTING' 'starting' 'STARTING' 'terminated' 'TERMINATED' 'pending' 'PENDING' 'unknown' 'UNKNOWN' 'stopping' 'STOPPING' 'stopped' 'STOPPED' 'suspended' 'SUSPENDED' 'error' 'ERROR' 'paused' 'PAUSED' 'reconfiguring' 'RECONFIGURING'}return states_str[id_] if isinstance id_ string_types else states_int[id_]
def _portal_maintenance now time time reason 'Idletimeoutexceeded disconnecting 'for session in [sess for sess in PORTAL_SESSIONS values if now - sess cmd_last > _IDLE_TIMEOUT ] session disconnect reason reason PORTAL_SESSIONS disconnect session
def _set_bundle_properties bundle root namespace bundle name root get 'name'
@verbosedef fit_sphere_to_headshape info dig_kinds 'auto' units 'm' verbose None if not isinstance units string_types or units not in 'm' 'mm' raise ValueError 'unitsmustbea"m"or"mm"' radius origin_head origin_device _fit_sphere_to_headshape info dig_kinds if units 'mm' radius * 1000 0origin_head * 1000 0origin_device * 1000 0return radius origin_head origin_device
@verbosedef fit_sphere_to_headshape info dig_kinds 'auto' units 'm' verbose None if not isinstance units string_types or units not in 'm' 'mm' raise ValueError 'unitsmustbea"m"or"mm"' radius origin_head origin_device _fit_sphere_to_headshape info dig_kinds if units 'mm' radius * 1000 0origin_head * 1000 0origin_device * 1000 0return radius origin_head origin_device
@core_helperdef get_request_param parameter_name default None return request params get parameter_name default
@core_helperdef get_request_param parameter_name default None return request params get parameter_name default
def sanitize_timeout timeout if timeout > 30 * 24 * 60 * 60 timeout + time time return timeout
def sanitize_timeout timeout if timeout > 30 * 24 * 60 * 60 timeout + time time return timeout
def _get_n_jobs n_jobs if n_jobs < 0 return max cpu_count + 1 + n_jobs 1 elif n_jobs 0 raise ValueError 'Parametern_jobs 0hasnomeaning ' else return n_jobs
def elu x return theano tensor switch x > 0 x theano tensor exp x - 1
def arity rel if len rel 0 return 0return len list rel [0]
def simulate_head app path **kwargs return simulate_request app 'HEAD' path **kwargs
@contextmanagerdef isolate_lru_cache lru_cache_object lru_cache_object cache_clear try yield finally lru_cache_object cache_clear
def compute_cluster_state node_state additional_node_states nonmanifest_datasets return DeploymentState nodes {node_state} additional_node_states nonmanifest_datasets {dataset dataset_id dataset for dataset in nonmanifest_datasets}
def get_repository_in_tool_shed app id return get_repository_query app get app security decode_id id
def generate_secret_key return _get_random_string
def get_credential_storage filename client_id user_agent scope warn_on_readonly True filename os path realpath os path expanduser filename _multistores_lock acquire try multistore _multistores setdefault filename _MultiStore filename warn_on_readonly finally _multistores_lock release if type scope is list scope '' join scope return multistore _get_storage client_id user_agent scope
def team_absent name profile 'github' **kwargs ret {'name' name 'changes' {} 'result' None 'comment' ''}target __salt__['github get_team'] name profile profile **kwargs if not target ret['comment'] 'Team{0}doesnotexist' format name ret['result'] Truereturn retelse if __opts__['test'] ret['comment'] 'Team{0}willbedeleted' format name ret['result'] Nonereturn retresult __salt__['github remove_team'] name profile profile **kwargs if result ret['comment'] 'Deletedteam{0}' format name ret['changes'] setdefault 'old' 'Team{0}exists' format name ret['changes'] setdefault 'new' 'Team{0}deleted' format name ret['result'] Trueelse ret['comment'] 'Failedtodelete{0}' format name ret['result'] Falsereturn ret
def merge_pr num pr gh_api get_pull_request gh_project num repo pr['head']['repo']['clone_url']branch pr['head']['ref']mergeable merge_branch repo repo branch branch if not mergeable cmd 'gitpull' + repo + '' + branch not_merged[str num ] cmdprint ' 'print 'Somethingwentwrongmergingthisbranch youcantryitmanuallybyrunngin 'print cmdprint ' '
def merge_pr num pr gh_api get_pull_request gh_project num repo pr['head']['repo']['clone_url']branch pr['head']['ref']mergeable merge_branch repo repo branch branch if not mergeable cmd 'gitpull' + repo + '' + branch not_merged[str num ] cmdprint ' 'print 'Somethingwentwrongmergingthisbranch youcantryitmanuallybyrunngin 'print cmdprint ' '
def _find_option_with_arg argv short_opts None long_opts None for i arg in enumerate argv if arg startswith u'-' if long_opts and arg startswith u'--' name sep val arg partition u' ' if name in long_opts return val if sep else argv[ i + 1 ] if short_opts and arg in short_opts return argv[ i + 1 ]raise KeyError u' ' join short_opts or [] + long_opts or []
def poll_until predicate steps sleep None if sleep is None sleep time sleepfor step in steps result predicate if result return resultsleep step result predicate if result return resultraise LoopExceeded predicate result
def ansible_group_init ansible_inv group_name ansible_inv[group_name] {}ansible_inv[group_name]['hosts'] []ansible_inv[group_name]['vars'] {}
@declareddef perf obj_ref metric_name obj get_object obj_ref p PerfDatas obj perf_data if metric_name in p logger debug '[trigger]Ifoundtheperfdata' return p[metric_name] valuelogger debug '[trigger]Iaminperfcommand' return None
def show_snapshot kwargs None call None if call 'function' raise SaltCloudSystemExit 'Theshow_snapshotfunctionmustbecalledwith-for--function ' if not kwargs or 'name' not in kwargs log error 'Mustspecifyname ' return Falseconn get_conn return _expand_item conn ex_get_snapshot kwargs['name']
def x_server_test f cond msg if isinstance f type types ClassType if not cond f __unittest_skip__ Truef __unittest_skip_why__ msgreturn felse @unittest skipIf not cond msg def wrapped_fn *args **kwargs return f *args **kwargs return wrapped_fn
def greedy_partition counts n buckets [[] for i in range n ]fill_levels [0 for i in range n ]for key in sorted counts reverse True key lambda c counts[c] smallest fill_levels index min fill_levels buckets[smallest] append key fill_levels[smallest] + counts[key]return buckets fill_levels
def greedy_partition counts n buckets [[] for i in range n ]fill_levels [0 for i in range n ]for key in sorted counts reverse True key lambda c counts[c] smallest fill_levels index min fill_levels buckets[smallest] append key fill_levels[smallest] + counts[key]return buckets fill_levels
def find_subdirectories package try subdirectories next os walk package_to_path package [1]except StopIteration subdirectories []return subdirectories
def thrift2json tft if isinstance tft type None return Noneif isinstance tft float int long complex basestring return tftif isinstance tft dict d {}for key val in tft iteritems d[key] thrift2json val return dif isinstance tft list return [thrift2json x for x in tft]if isinstance tft set return dict x True for x in tft json {}d {}if hasattr tft '__dict__' d tft __dict__elif hasattr tft '__slots__' d tft __slots__else return {}for k in d v getattr tft k json[k] thrift2json v return json
def thrift2json tft if isinstance tft type None return Noneif isinstance tft float int long complex basestring return tftif isinstance tft dict d {}for key val in tft iteritems d[key] thrift2json val return dif isinstance tft list return [thrift2json x for x in tft]if isinstance tft set return dict x True for x in tft json {}d {}if hasattr tft '__dict__' d tft __dict__elif hasattr tft '__slots__' d tft __slots__else return {}for k in d v getattr tft k json[k] thrift2json v return json
def datetime_from_timestamp timestamp dt DATETIME_EPOC + datetime timedelta seconds timestamp return dt
def _init_tab_completion log completion debug 'Initializingtabcompletion ' with debug log_time log completion 'tabcompletioninit' model miscmodels TabCompletionModel _instances[usertypes Completion tab] model
def s3_include_debug_css request current requestfolder request folderappname request applicationsettings current deployment_settingstheme settings get_theme location current response s3 theme_locationcss_cfg '%s/modules/templates/%s%s/css cfg' % folder location theme try f open css_cfg 'r' except raise HTTP 500 'Themeconfigurationfilemissing modules/templates/%s%s/css cfg' % location theme files f readlines files files[ -1 ]include ''for file in files if file[0] '#' include '%s\n<linkhref "/%s/static/styles/%s"rel "stylesheet"type "text/css"/>' % include appname file[ -1 ] f close return XML include
def task_track request_info task_info event_type event page None full_event dict event **task_info with eventtracker get_tracker context 'edx course task' contexts course_context_from_url page event {'username' request_info get 'username' 'unknown' 'ip' request_info get 'ip' 'unknown' 'event_source' 'task' 'event_type' event_type 'event' full_event 'agent' request_info get 'agent' 'unknown' 'page' page 'time' datetime datetime utcnow 'host' request_info get 'host' 'unknown' 'context' eventtracker get_tracker resolve_context }log_event event
def task_track request_info task_info event_type event page None full_event dict event **task_info with eventtracker get_tracker context 'edx course task' contexts course_context_from_url page event {'username' request_info get 'username' 'unknown' 'ip' request_info get 'ip' 'unknown' 'event_source' 'task' 'event_type' event_type 'event' full_event 'agent' request_info get 'agent' 'unknown' 'page' page 'time' datetime datetime utcnow 'host' request_info get 'host' 'unknown' 'context' eventtracker get_tracker resolve_context }log_event event
def tridi_inverse_iteration d e w x0 None rtol 1e-08 eig_diag d - w if x0 is None x0 np random randn len d x_prev np zeros_like x0 norm_x np linalg norm x0 x0 / norm_xwhile np linalg norm np abs x0 - np abs x_prev > rtol x_prev x0 copy tridisolve eig_diag e x0 norm_x np linalg norm x0 x0 / norm_xreturn x0
def links dev return info dev get 'S' None
def set_serv_parms service args import _winreguargs []for arg in args uargs append unicoder arg try key _winreg CreateKey _winreg HKEY_LOCAL_MACHINE _SERVICE_KEY + service _winreg SetValueEx key _SERVICE_PARM None _winreg REG_MULTI_SZ uargs _winreg CloseKey key except WindowsError return Falsereturn True
def write trees file format **kwargs if isinstance trees BaseTree Tree BaseTree Clade trees [trees]with File as_handle file 'w+' as fp n getattr supported_formats[format] 'write' trees fp **kwargs return n
@taskdef server ctx host None port 5000 debug True gitlogs False if os environ get 'WERKZEUG_RUN_MAIN' 'true' or not debug if os environ get 'WEB_REMOTE_DEBUG' None import pydevdremote_parts os environ get 'WEB_REMOTE_DEBUG' split ' ' pydevd settrace remote_parts[0] port int remote_parts[1] suspend False stdoutToServer True stderrToServer True if gitlogs git_logs ctx from website app import init_appos environ['DJANGO_SETTINGS_MODULE'] 'api base settings'app init_app set_backends True routes True settings API_SERVER_PORT portelse from framework flask import appcontext Noneif settings SECURE_MODE context settings OSF_SERVER_CERT settings OSF_SERVER_KEY app run host host port port debug debug threaded debug extra_files [settings ASSET_HASH_PATH] ssl_context context
def _state_description vm_state _shutdown_terminate name _STATE_DESCRIPTION_MAP get vm_state vm_state return {'code' inst_state name_to_code name 'name' name}
def libvlc_media_player_get_state p_mi f _Cfunctions get 'libvlc_media_player_get_state' None or _Cfunction 'libvlc_media_player_get_state' 1 None State MediaPlayer return f p_mi
def setup_platform hass config add_devices discovery_info None import eiscpfrom eiscp import eISCPhost config get CONF_HOST hosts []if CONF_HOST in config and host not in KNOWN_HOSTS try hosts append OnkyoDevice eiscp eISCP host config get CONF_SOURCES name config get CONF_NAME KNOWN_HOSTS append host except OSError _LOGGER error 'Unabletoconnecttoreceiverat%s ' host else for receiver in eISCP discover if receiver host not in KNOWN_HOSTS hosts append OnkyoDevice receiver config get CONF_SOURCES KNOWN_HOSTS append receiver host add_devices hosts
def set_file name source template None context None defaults None **kwargs ret {'name' name 'changes' {} 'result' True 'comment' ''}if context is None context {}elif not isinstance context dict ret['result'] Falseret['comment'] 'Contextmustbeformedasadict'return retif defaults is None defaults {}elif not isinstance defaults dict ret['result'] Falseret['comment'] 'Defaultsmustbeformedasadict'return retif __opts__['test'] ret['result'] Noneret['comment'] 'Debconfselectionswouldhavebeenset 'return retif template result __salt__['debconf set_template'] source template context defaults **kwargs else result __salt__['debconf set_file'] source **kwargs if result ret['comment'] 'Debconfselectionswereset 'else ret['result'] Falseret['comment'] 'Unabletosetdebconfselectionsfromfile 'return ret
def _init_signals import signalimport threadimport sysprev_handler signal getsignal signal SIGINT def thread_interrupt_handler signum frame thread do_terminate_threads if callable prev_handler prev_handler signum frame raise KeyboardInterrupt try signal signal signal SIGINT thread_interrupt_handler except ValueError print >>sys stderr 'Failedtosetupthread-interrupthandler Thisisusuallynotcritical'
def _init_signals import signalimport threadimport sysprev_handler signal getsignal signal SIGINT def thread_interrupt_handler signum frame thread do_terminate_threads if callable prev_handler prev_handler signum frame raise KeyboardInterrupt try signal signal signal SIGINT thread_interrupt_handler except ValueError print >>sys stderr 'Failedtosetupthread-interrupthandler Thisisusuallynotcritical'
def add_wildcards arg if not arg startswith u'*' arg u'*' + arg if not arg endswith u'*' arg arg + u'*' return arg
def add_wildcards arg if not arg startswith u'*' arg u'*' + arg if not arg endswith u'*' arg arg + u'*' return arg
def add_wildcards arg if not arg startswith u'*' arg u'*' + arg if not arg endswith u'*' arg arg + u'*' return arg
def _get_service service profile if isinstance profile dict and 'service' in profile return profile['service']return service
def HBox *args **kwargs return Row *args **kwargs
@require_POSTdef send_recovery_email request form UserRecoveryEmailForm data request POST if form is_valid form save request request return redirect 'users recovery_email_sent' else return HttpResponseBadRequest 'Invalidrequest '
@command 'url_file\\s \\S+ ' def yt_url_file file_name try with open file_name 'r' as fo output '' join [line strip for line in fo if line strip ] except IOError g message c r + 'Errorwhileopeningthefile checkthevalidityofthepath' + c w g content g content or content generate_songlist_display zeromsg g message returnyt_url output
@command 'url_file\\s \\S+ ' def yt_url_file file_name try with open file_name 'r' as fo output '' join [line strip for line in fo if line strip ] except IOError g message c r + 'Errorwhileopeningthefile checkthevalidityofthepath' + c w g content g content or content generate_songlist_display zeromsg g message returnyt_url output
@copydoc StrCat def cat lhs rhs sep None if not isstring rhs dshape raise TypeError 'canonlyconcatstringcolumns' _validate_optional sep 'sep' basestring 'string' return StrCat lhs rhs sep sep
def DefaultController name controllers DefaultControllers **kwargs controller findController controllers if not controller raise Exception 'CouldnotfindadefaultOpenFlowcontroller' return controller name **kwargs
def DefaultController name controllers DefaultControllers **kwargs controller findController controllers if not controller raise Exception 'CouldnotfindadefaultOpenFlowcontroller' return controller name **kwargs
@cleanupdef test__EventCollection__get_color _ coll props generate_EventCollection_plot np testing assert_array_equal props[u'color'] coll get_color check_allprop_array coll get_colors props[u'color']
@cleanupdef test__EventCollection__get_color _ coll props generate_EventCollection_plot np testing assert_array_equal props[u'color'] coll get_color check_allprop_array coll get_colors props[u'color']
@memoizedef selection_model return SelectionModel
def autoload global _APPLICATIONS_LOADEDif _APPLICATIONS_LOADED return Falsefrom django_assets conf import settingsfor app in settings INSTALLED_APPS try app_path import_module app __path__except AttributeError continuetry imp find_module 'assets' app_path except ImportError continueimport_module '%s assets' % app _APPLICATIONS_LOADED True
def normalizeUnicode value return unicodedata normalize 'NFKD' value encode 'ascii' 'ignore' if isinstance value unicode else value
def _build_image data cmap 'gray' import matplotlib pyplot as pltfrom matplotlib figure import Figurefrom matplotlib backends backend_agg import FigureCanvasAgg as FigureCanvasfigsize data shape[ -1 ]if figsize[0] 1 figsize tuple figsize[1 ] data data[ 0]fig Figure figsize figsize dpi 1 0 frameon False FigureCanvas fig cmap getattr plt cm cmap plt cm gray fig figimage data cmap cmap output BytesIO fig savefig output dpi 1 0 format 'png' return base64 b64encode output getvalue decode 'ascii'
def _build_image data cmap 'gray' import matplotlib pyplot as pltfrom matplotlib figure import Figurefrom matplotlib backends backend_agg import FigureCanvasAgg as FigureCanvasfigsize data shape[ -1 ]if figsize[0] 1 figsize tuple figsize[1 ] data data[ 0]fig Figure figsize figsize dpi 1 0 frameon False FigureCanvas fig cmap getattr plt cm cmap plt cm gray fig figimage data cmap cmap output BytesIO fig savefig output dpi 1 0 format 'png' return base64 b64encode output getvalue decode 'ascii'
def compareAreaAscending loopArea otherLoopArea if loopArea area < otherLoopArea area return -1 return int loopArea area > otherLoopArea area
def compareAreaAscending loopArea otherLoopArea if loopArea area < otherLoopArea area return -1 return int loopArea area > otherLoopArea area
def _get_default_entrance_exam_minimum_pct entrance_exam_minimum_score_pct float settings ENTRANCE_EXAM_MIN_SCORE_PCT if entrance_exam_minimum_score_pct is_integer entrance_exam_minimum_score_pct entrance_exam_minimum_score_pct / 100 return entrance_exam_minimum_score_pct
def _get_default_entrance_exam_minimum_pct entrance_exam_minimum_score_pct float settings ENTRANCE_EXAM_MIN_SCORE_PCT if entrance_exam_minimum_score_pct is_integer entrance_exam_minimum_score_pct entrance_exam_minimum_score_pct / 100 return entrance_exam_minimum_score_pct
@cache_permissiondef can_accept_suggestion user translation return can_edit user translation 'trans accept_suggestion'
def product iter_a iter_b for a in iter_a for b in iter_b yield a b
def product iter_a iter_b for a in iter_a for b in iter_b yield a b
def product iter_a iter_b for a in iter_a for b in iter_b yield a b
def create_executable dist Distribution name 'spyder'ver spyder __version__try imp find_module 'PyQt4' python_qt 'pyqt'except ImportError python_qt 'pyside'dist setup name 'Spyder' version ver script 'spyder/spyder py' description 'ScientificPYthonDevelopmentEnviRonment' target_name '%s exe' % name icon '%s ico' % name target_dir '%s-win32-%s-sa-%s' % name python_qt ver spyder add_to_distribution dist dist add_modules 'matplotlib' 'h5py' 'scipy io' 'guidata' 'pygments' try import guiqwtdist add_modules 'guiqwt' except ImportError passdist includes + ['spyder scientific_startup' 'spyder widgets externalshell sitecustomize']dist excludes + ['sphinx' 'zmq' 'IPython']if osp isfile 'Spyderdoc chm' dist add_data_file 'Spyderdoc chm' dist add_data_file osp join 'rope' 'base' 'default_config py' dist build 'cx_Freeze'
@pytest mark django_dbdef test_data_store tp0 store StoreDBFactory name 'foo po' parent tp0 directory translation_project tp0 assert repr store data '<StoreData %s>' % store pootle_path
@pytest mark django_dbdef test_data_store tp0 store StoreDBFactory name 'foo po' parent tp0 directory translation_project tp0 assert repr store data '<StoreData %s>' % store pootle_path
def update_resource zone resource_type resource_selector **kwargs return _resource 'update' zone resource_type resource_selector **kwargs
def rm_stored_dir dir_path storage default_storage empty_dirs []for root dirs files in walk_storage dir_path for fn in files storage delete '%s/%s' % root fn empty_dirs insert 0 root empty_dirs append dir_path for dn in empty_dirs storage delete dn
def rm_stored_dir dir_path storage default_storage empty_dirs []for root dirs files in walk_storage dir_path for fn in files storage delete '%s/%s' % root fn empty_dirs insert 0 root empty_dirs append dir_path for dn in empty_dirs storage delete dn
def get_page self suffix try return int self REQUEST[ 'page%s' % suffix ] except KeyError ValueError TypeError return 1
def get_page self suffix try return int self REQUEST[ 'page%s' % suffix ] except KeyError ValueError TypeError return 1
def error_summary error_dict def prettify field_name field_name re sub ' ?< \\w [Uu]rl ? \\w ' 'URL' field_name replace '_' '' capitalize return _ field_name replace '_' '' summary {}for key error in error_dict iteritems if key 'resources' summary[_ 'Resources' ] _ 'Packageresource s invalid' elif key 'extras' summary[_ 'Extras' ] _ 'MissingValue' elif key 'extras_validation' summary[_ 'Extras' ] error[0]else summary[_ prettify key ] error[0]return summary
def error_summary error_dict def prettify field_name field_name re sub ' ?< \\w [Uu]rl ? \\w ' 'URL' field_name replace '_' '' capitalize return _ field_name replace '_' '' summary {}for key error in error_dict iteritems if key 'resources' summary[_ 'Resources' ] _ 'Packageresource s invalid' elif key 'extras' summary[_ 'Extras' ] _ 'MissingValue' elif key 'extras_validation' summary[_ 'Extras' ] error[0]else summary[_ prettify key ] error[0]return summary
def customize_compiler _config_vars _find_appropriate_compiler _config_vars _remove_unsupported_archs _config_vars _override_all_archs _config_vars return _config_vars
def makedirs path try os makedirs path except OSError logging error "OSErrorwhilecreatingdir'{0}'" format path return Falsereturn True
def xframe_options_exempt view_func def wrapped_view *args **kwargs resp view_func *args **kwargs resp xframe_options_exempt Truereturn respreturn wraps view_func wrapped_view
def xframe_options_exempt view_func def wrapped_view *args **kwargs resp view_func *args **kwargs resp xframe_options_exempt Truereturn respreturn wraps view_func wrapped_view
def xframe_options_exempt view_func def wrapped_view *args **kwargs resp view_func *args **kwargs resp xframe_options_exempt Truereturn respreturn wraps view_func wrapped_view
def xframe_options_exempt view_func def wrapped_view *args **kwargs resp view_func *args **kwargs resp xframe_options_exempt Truereturn respreturn wraps view_func wrapped_view
def xframe_options_exempt view_func def wrapped_view *args **kwargs resp view_func *args **kwargs resp xframe_options_exempt Truereturn respreturn wraps view_func wrapped_view
def xframe_options_exempt view_func def wrapped_view *args **kwargs resp view_func *args **kwargs resp xframe_options_exempt Truereturn respreturn wraps view_func wrapped_view
@register filterdef thumbnailer obj relative_name None return get_thumbnailer obj relative_name relative_name
def test_iterator x csr_matrix [[1 2 0] [0 0 3] [4 0 5]] ds SparseDataset from_scipy_sparse_dataset x it ds iterator mode 'sequential' batch_size 1 it next
def apply_special_queries query specials for i in specials query FILTERS_LIST[i] specials[i] query return query
def patch_environ case key value old_environ os environ copy def cleanup os environ clear os environ update old_environ os environ[key] valuecase addCleanup cleanup
def get_course_info_section request user course section_key info_module get_course_info_section_module request user course section_key html ''if info_module is not None try html info_module render STUDENT_VIEW contentexcept Exception html render_to_string 'courseware/error-message html' None log exception u'Errorrenderingcourse_id %s section_key %s' unicode course id section_key return html
def get_course_info_section request user course section_key info_module get_course_info_section_module request user course section_key html ''if info_module is not None try html info_module render STUDENT_VIEW contentexcept Exception html render_to_string 'courseware/error-message html' None log exception u'Errorrenderingcourse_id %s section_key %s' unicode course id section_key return html
def get_unknown_opttrans_attr path path_attrs path pathattr_mapunknown_opt_tran_attrs {}for _ attr in path_attrs items if isinstance attr BGPPathAttributeUnknown and attr flags & BGP_ATTR_FLAG_OPTIONAL BGP_ATTR_FLAG_TRANSITIVE or isinstance attr BGPPathAttributeAs4Path or isinstance attr BGPPathAttributeAs4Aggregator unknown_opt_tran_attrs[attr type] attrreturn unknown_opt_tran_attrs
def test_correct_pip_version script result script pip '--version' dir re match 'pip\\d \\ [\\d] + \\ ? rc dev pre post \\d+ ?from * \\ python\\d [\\d] +\\ $' result stdout group 4 pip_folder join SRC_DIR 'pip' pip_folder_outputed join dir 'pip' diffs filecmp dircmp pip_folder pip_folder_outputed mismatch_py [x for x in diffs left_only + diffs right_only + diffs diff_files if x endswith ' py' ]assert not mismatch_py 'mismatchedsourcefilesin%rand%r %r' % pip_folder pip_folder_outputed mismatch_py
def test_correct_pip_version script result script pip '--version' dir re match 'pip\\d \\ [\\d] + \\ ? rc dev pre post \\d+ ?from * \\ python\\d [\\d] +\\ $' result stdout group 4 pip_folder join SRC_DIR 'pip' pip_folder_outputed join dir 'pip' diffs filecmp dircmp pip_folder pip_folder_outputed mismatch_py [x for x in diffs left_only + diffs right_only + diffs diff_files if x endswith ' py' ]assert not mismatch_py 'mismatchedsourcefilesin%rand%r %r' % pip_folder pip_folder_outputed mismatch_py
def set_selinux_context path user None role None type None range None if not any user role type range return Falsecmd ['chcon']if user cmd extend ['-u' user] if role cmd extend ['-r' role] if type cmd extend ['-t' type] if range cmd extend ['-l' range] cmd append path ret not __salt__['cmd retcode'] cmd python_shell False if ret return get_selinux_context path else return ret
def get_headers data extra_headers None response_headers {'Content-Length' str len data }if extra_headers response_headers update extra_headers return list response_headers items
def get_headers data extra_headers None response_headers {'Content-Length' str len data }if extra_headers response_headers update extra_headers return list response_headers items
def _get_elem_at_rank rank data n_negative n_zeros if rank < n_negative return data[rank]if rank - n_negative < n_zeros return 0return data[ rank - n_zeros ]
def getReadCraftSequence return skeinforge_profile getCraftTypePluginModule getCraftSequence
def setVideoMode videoMode if haveBitsDLL return _bits bitsSetVideoMode videoMode else return 1
def _domain_config_finder conf_dir LOG info _LI 'Scanning%rfordomainconfigfiles' conf_dir for r d f in os walk conf_dir for fname in f if fname startswith DOMAIN_CONF_FHEAD and fname endswith DOMAIN_CONF_FTAIL if fname count ' ' > 2 domain_name fname[len DOMAIN_CONF_FHEAD - len DOMAIN_CONF_FTAIL ] yield os path join r fname domain_name continueLOG warning _LW 'Ignoringfile %s whilescanningdomainconfigdirectory' fname
def color_style if not supports_color style no_style else SPIDER_COLORS os environ get 'SPIDER_COLORS' '' color_settings termcolors parse_color_setting SPIDER_COLORS if color_settings class dummy passstyle dummy for role in termcolors PALETTES[termcolors NOCOLOR_PALETTE] format color_settings get role {} setattr style role termcolors make_style **format style ERROR_OUTPUT style ERRORelse style no_style return style
def mapk actual predicted k 10 return np mean [apk a p k for a p in zip actual predicted ]
def checkT1s T1_files cw256 False import sysimport nibabel as nibfrom nipype utils filemanip import filename_to_listT1_files filename_to_list T1_files if len T1_files 0 print "ERROR NoT1'sGiven"sys exit -1 shape nib load T1_files[0] shapefor t1 in T1_files[1 ] if nib load t1 shape shape print 'ERROR T1snotthesamesize Cannotprocess{0}and{1}together' format T1_files[0] t1 sys exit -1 origvol_names ['{0 03d} mgz' format i + 1 for i in range len T1_files ]if not cw256 and any dim > 256 for dim in shape print 'SettingMRIConverttocropimagesto256FOV'cw256 Trueresample_type 'cubic' if len T1_files > 1 else 'interpolate' return T1_files cw256 resample_type origvol_names
def _override_setuptools req if req project_name 'setuptools' if not len req specs return Truefor comparator version in req specs if comparator in [' ' '> ' '>'] if '0 7' in version return Falsereturn Truereturn False
def _override_setuptools req if req project_name 'setuptools' if not len req specs return Truefor comparator version in req specs if comparator in [' ' '> ' '>'] if '0 7' in version return Falsereturn Truereturn False
def _override_setuptools req if req project_name 'setuptools' if not len req specs return Truefor comparator version in req specs if comparator in [' ' '> ' '>'] if '0 7' in version return Falsereturn Truereturn False
def cr method method _api 'cr'return method
def create_datacenter dcname None service_instance None folder None if len dcname > 79 raise ValueError 'Thenameofthedatacentermustbeunder80characters ' if folder is None folder service_instance content rootFolderif folder is not None and isinstance folder vim Folder dc_moref folder CreateDatacenter name dcname return dc_moref
def recall_score y_true y_pred labels None pos_label 1 average 'binary' sample_weight None _ r _ _ precision_recall_fscore_support y_true y_pred labels labels pos_label pos_label average average warn_for 'recall' sample_weight sample_weight return r
def role_get name user None host None port None maintenance_db None password None runas None return_password False all_users user_list user user host host port port maintenance_db maintenance_db password password runas runas return_password return_password try return all_users get name None except AttributeError log error 'CouldnotretrievePostgresrole IsPostgresrunning?' return None
def sparse v for f w in list v items if w 0 del v[f]return v
def sparse v for f w in list v items if w 0 del v[f]return v
def main run_it tools Control setup ORIGINAL_CAPTION state_dict {c MAIN_MENU main_menu Menu c LOAD_SCREEN load_screen LoadScreen c TIME_OUT load_screen TimeOut c GAME_OVER load_screen GameOver c LEVEL1 level1 Level1 }run_it setup_states state_dict c MAIN_MENU run_it main
def make_pass_decorator object_type ensure False def decorator f def new_func *args **kwargs ctx get_current_context if ensure obj ctx ensure_object object_type else obj ctx find_object object_type if obj is None raise RuntimeError 'Managedtoinvokecallbackwithoutacontextobjectoftype%rexisting' % object_type __name__ return ctx invoke f obj *args[1 ] **kwargs return update_wrapper new_func f return decorator
def make_pass_decorator object_type ensure False def decorator f def new_func *args **kwargs ctx get_current_context if ensure obj ctx ensure_object object_type else obj ctx find_object object_type if obj is None raise RuntimeError 'Managedtoinvokecallbackwithoutacontextobjectoftype%rexisting' % object_type __name__ return ctx invoke f obj *args[1 ] **kwargs return update_wrapper new_func f return decorator
def my_update_subtask_status entry_id current_task_id new_subtask_status entry InstructorTask objects get pk entry_id subtask_dict json loads entry subtasks subtask_status_info subtask_dict['status']current_subtask_status SubtaskStatus from_dict subtask_status_info[current_task_id] current_retry_count current_subtask_status get_retry_count new_retry_count new_subtask_status get_retry_count if current_retry_count < new_retry_count update_subtask_status entry_id current_task_id new_subtask_status
def my_update_subtask_status entry_id current_task_id new_subtask_status entry InstructorTask objects get pk entry_id subtask_dict json loads entry subtasks subtask_status_info subtask_dict['status']current_subtask_status SubtaskStatus from_dict subtask_status_info[current_task_id] current_retry_count current_subtask_status get_retry_count new_retry_count new_subtask_status get_retry_count if current_retry_count < new_retry_count update_subtask_status entry_id current_task_id new_subtask_status
def getLocalAndroidPath client args localPath os path join args localOutputFolder '{0}-{1}' format client conn modules['pupydroid utils'] getAndroidID client desc['user'] if not os path exists localPath logging info 'Creating{0}folderlocally' format localPath os makedirs localPath return localPath
def getLocalAndroidPath client args localPath os path join args localOutputFolder '{0}-{1}' format client conn modules['pupydroid utils'] getAndroidID client desc['user'] if not os path exists localPath logging info 'Creating{0}folderlocally' format localPath os makedirs localPath return localPath
def sha256_digest instr if six PY3 b salt utils to_bytes instr return hashlib sha256 b hexdigest return hashlib sha256 instr hexdigest
def is_mutating status if not status return Falsemutating set [u'insert' u'update' u'delete'] return status split None 1 [0] lower in mutating
@depends HAS_PYVMOMI def list_non_ssds host username password protocol None port None host_names None service_instance salt utils vmware get_service_instance host host username username password password protocol protocol port port host_names _check_hosts service_instance host host_names ret {}names []for host_name in host_names host_ref _get_host_ref service_instance host host_name host_name disks _get_host_non_ssds host_ref for disk in disks names append disk canonicalName ret update {host_name names} return ret
def get_request_api_cpu_usage if apiproxy return apiproxy GetRequestApiCpuUsage return 0
def create_simple_binding jboss_config binding_name value profile None log debug ' MODULEFUNCTION jboss7 create_simple_binding binding_name %s value %s profile %s' binding_name value profile operation '/subsystem naming/binding "{binding_name}" add binding-type simple value "{value}" ' format binding_name binding_name value __escape_binding_value value if profile is not None operation '/profile "{profile}"' format profile profile + operation return __salt__['jboss7_cli run_operation'] jboss_config operation
def create_simple_binding jboss_config binding_name value profile None log debug ' MODULEFUNCTION jboss7 create_simple_binding binding_name %s value %s profile %s' binding_name value profile operation '/subsystem naming/binding "{binding_name}" add binding-type simple value "{value}" ' format binding_name binding_name value __escape_binding_value value if profile is not None operation '/profile "{profile}"' format profile profile + operation return __salt__['jboss7_cli run_operation'] jboss_config operation
def create_simple_binding jboss_config binding_name value profile None log debug ' MODULEFUNCTION jboss7 create_simple_binding binding_name %s value %s profile %s' binding_name value profile operation '/subsystem naming/binding "{binding_name}" add binding-type simple value "{value}" ' format binding_name binding_name value __escape_binding_value value if profile is not None operation '/profile "{profile}"' format profile profile + operation return __salt__['jboss7_cli run_operation'] jboss_config operation
def copy_constr constr func expr func constr expr return type constr expr constr constr_id constr size
def image_volume_cache_create context host cluster_name image_id image_updated_at volume_id size return IMPL image_volume_cache_create context host cluster_name image_id image_updated_at volume_id size
def apply_android_specific_fixes font font_data delete_from_cmap font [8419 8593 8595] for table in ['LTSH' 'hdmx' 'VDMX' 'gasp'] if table in font del font[table]glyph_set font getGlyphSet ellipsis glyph_set['ellipsis'] _glyphfor component in ellipsis components component flags & ~ 1 << 2
def presence table s3db pr_presencetable pe_id readable Truetable pe_id label 'Name'table pe_id represent s3db pr_person_id representtable observer readable Falsetable presence_condition readable Falsereturn s3_rest_controller
def presence table s3db pr_presencetable pe_id readable Truetable pe_id label 'Name'table pe_id represent s3db pr_person_id representtable observer readable Falsetable presence_condition readable Falsereturn s3_rest_controller
def add_lowess ax lines_idx 0 frac 0 2 **lowess_kwargs y0 ax get_lines [lines_idx] _yx0 ax get_lines [lines_idx] _xlres lowess y0 x0 frac frac **lowess_kwargs ax plot lres[ 0] lres[ 1] 'r' lw 1 5 return ax figure
def get_connection_string raw_ips file_io read RABBITMQ_LOCATION_FILE ips raw_ips split '\n' rabbitmq_ip ips[0]return 'amqp //guest guest@' + rabbitmq_ip + ' ' + str RABBITMQ_PORT + '//'
def test_export tpot_obj TPOTClassifier try tpot_obj export 'test_export py' assert Falseexcept ValueError pass
def localpath *args plist [ROOT] + list args return os path abspath pjoin *plist
def valid_max_age number if isinstance number basestring try number long number except ValueError TypeError return Falseif number > 0 and number % 1 0 return Truereturn False
def get_icon icon_name if icon_name pixmap get_pixmap icon_name if pixmap is None return QIcon I icon_name else return QIcon pixmap return QIcon
def extract dicts def _extract_value data if 'k' in data and 'v' in data return data['k'] data['v'] return k extract v for k v in data items if hasattr dicts 'items' return dict _extract_value dicts extracted {}for d in dicts extracted update extract d return extracted
def _greenthread_yield dbapi_con con_record greenthread sleep 0
def _greenthread_yield dbapi_con con_record greenthread sleep 0
def _len_guards M if int M M or M < 0 raise ValueError 'WindowlengthMmustbeanon-negativeinteger' return M < 1
@contextmanagerdef dummy yield
@with_sessiondef log_once message logger logging getLogger u'log_once' once_level logging INFO suppressed_level f_logger VERBOSE session None from flexget manager import managerif not manager log warning u'DBnotinitialized log_oncewillnotworkproperly ' logger log once_level message returndigest hashlib md5 digest update message encode u'latin1' u'replace' md5sum digest hexdigest if session query LogMessage filter_by md5sum md5sum first logger log suppressed_level message return Falserow LogMessage md5sum session add row logger log once_level message return True
def _tolerateErrors wrapped def infoCallback connection where ret try return wrapped connection where ret except f Failure log err f 'Errorduringinfo_callback' connection get_app_data failVerification f return infoCallback
def get_language_from_request request check_path False if check_path lang_code get_language_from_path request path_info if lang_code is not None return lang_codesupported_lang_codes get_languages if hasattr request 'session' lang_code request session get LANGUAGE_SESSION_KEY if lang_code in supported_lang_codes and lang_code is not None and check_for_language lang_code return lang_codelang_code request COOKIES get settings LANGUAGE_COOKIE_NAME try return get_supported_language_variant lang_code except LookupError passaccept request META get 'HTTP_ACCEPT_LANGUAGE' '' for accept_lang unused in parse_accept_lang_header accept if accept_lang '*' breakif not language_code_re search accept_lang continuetry return get_supported_language_variant accept_lang except LookupError continuetry return get_supported_language_variant settings LANGUAGE_CODE except LookupError return settings LANGUAGE_CODE
def get_language_from_request request check_path False if check_path lang_code get_language_from_path request path_info if lang_code is not None return lang_codesupported_lang_codes get_languages if hasattr request 'session' lang_code request session get LANGUAGE_SESSION_KEY if lang_code in supported_lang_codes and lang_code is not None and check_for_language lang_code return lang_codelang_code request COOKIES get settings LANGUAGE_COOKIE_NAME try return get_supported_language_variant lang_code except LookupError passaccept request META get 'HTTP_ACCEPT_LANGUAGE' '' for accept_lang unused in parse_accept_lang_header accept if accept_lang '*' breakif not language_code_re search accept_lang continuetry return get_supported_language_variant accept_lang except LookupError continuetry return get_supported_language_variant settings LANGUAGE_CODE except LookupError return settings LANGUAGE_CODE
def approx_jacobian x func epsilon *args x0 asfarray x f0 atleast_1d func * x0 + args jac zeros [len x0 len f0 ] dx zeros len x0 for i in range len x0 dx[i] epsilonjac[i] func * x0 + dx + args - f0 / epsilon dx[i] 0 0return jac transpose
def run_parallel_map_providers_query data queue None reinit_crypto cloud Cloud data['opts'] try with context func_globals_inject cloud clouds[data['fun']] __active_provider_name__ ' ' join [data['alias'] data['driver']] return data['alias'] data['driver'] salt utils simple_types_filter cloud clouds[data['fun']] except Exception as err log debug "Failedtoexecute'{0} 'whilequeryingforrunningnodes {1}" format data['fun'] err exc_info_on_loglevel logging DEBUG return data['alias'] data['driver']
def EncodeRspFileList args if not args return ''if args[0] startswith 'call' call program args[0] split '' 1 program call + '' + os path normpath program else program os path normpath args[0] return program + '' + '' join QuoteForRspFile arg for arg in args[1 ]
def _pwdGetByName username if pwd is None return Nonereturn pwd getpwnam username
def _pwdGetByName username if pwd is None return Nonereturn pwd getpwnam username
def _pwdGetByName username if pwd is None return Nonereturn pwd getpwnam username
def _settings_closed events params params['fig_options'] None
def orderedSet iterable res []for el in iterable if el not in res res append el return res
def orderedSet iterable res []for el in iterable if el not in res res append el return res
def csvheader parent nodelist header ''for subnode in nodelist if subnode nodeType subnode ELEMENT_NODE header header + ' ' + parent + ' ' + subnode tagName return header[1 ] + '\n'
def spawn_collector col LOG info '%s interval %d needstobespawned' col name col interval try col proc subprocess Popen col filename stdout subprocess PIPE stderr subprocess PIPE close_fds True preexec_fn os setsid except OSError as e LOG error 'Failedtospawncollector%s %s' % col filename e returncol lastspawn int time time col last_datapoint col lastspawnset_nonblocking col proc stdout fileno set_nonblocking col proc stderr fileno if col proc pid > 0 col dead FalseLOG info 'spawned%s pid %d ' col name col proc pid returnLOG error 'failedtospawncollector %s' col filename
def spawn_collector col LOG info '%s interval %d needstobespawned' col name col interval try col proc subprocess Popen col filename stdout subprocess PIPE stderr subprocess PIPE close_fds True preexec_fn os setsid except OSError as e LOG error 'Failedtospawncollector%s %s' % col filename e returncol lastspawn int time time col last_datapoint col lastspawnset_nonblocking col proc stdout fileno set_nonblocking col proc stderr fileno if col proc pid > 0 col dead FalseLOG info 'spawned%s pid %d ' col name col proc pid returnLOG error 'failedtospawncollector %s' col filename
def supported_uri_schemes uri_schemes supported_schemes set registry Gst Registry get for factory in registry get_feature_list Gst ElementFactory for uri in factory get_uri_protocols if uri in uri_schemes supported_schemes add uri return supported_schemes
def supported_uri_schemes uri_schemes supported_schemes set registry Gst Registry get for factory in registry get_feature_list Gst ElementFactory for uri in factory get_uri_protocols if uri in uri_schemes supported_schemes add uri return supported_schemes
def create_datasource orgname None profile 'grafana' **kwargs if isinstance profile string_types profile __salt__['config option'] profile if orgname switch_org orgname profile response requests post '{0}/api/datasources' format profile['grafana_url'] json kwargs auth _get_auth profile headers _get_headers profile timeout profile get 'grafana_timeout' 3 if response status_code > 400 response raise_for_status return response json
def _get_thread_and_context request thread_id retrieve_kwargs None retrieve_kwargs retrieve_kwargs or {} try if 'with_responses' not in retrieve_kwargs retrieve_kwargs['with_responses'] Falseif 'mark_as_read' not in retrieve_kwargs retrieve_kwargs['mark_as_read'] Falsecc_thread Thread id thread_id retrieve **retrieve_kwargs course_key CourseKey from_string cc_thread['course_id'] course _get_course course_key request user context get_context course request cc_thread if not context['is_requester_privileged'] and cc_thread['group_id'] and is_commentable_cohorted course id cc_thread['commentable_id'] requester_cohort get_cohort_id request user course id if requester_cohort is not None and cc_thread['group_id'] requester_cohort raise ThreadNotFoundError 'Threadnotfound ' return cc_thread context except CommentClientRequestError raise ThreadNotFoundError 'Threadnotfound '
def _find_shallow store heads depth parents {}def get_parents sha result parents get sha None if not result result store[sha] parentsparents[sha] resultreturn resulttodo []for head_sha in heads obj store peel_sha head_sha if isinstance obj Commit todo append obj id 1 not_shallow set shallow set while todo sha cur_depth todo pop if cur_depth < depth not_shallow add sha new_depth cur_depth + 1 todo extend p new_depth for p in get_parents sha else shallow add sha return shallow not_shallow
def configure_cache client test_name client http_client cache_test_name test_namecache_name client http_client get_cache_file_name if options get_value 'clearcache' 'true' client http_client delete_session cache_name client http_client use_cached_session cache_name
def adjust_recursive_directory_permissions pre_existing_dir new_directory_list module directory_args changed if len new_directory_list > 0 working_dir os path join pre_existing_dir new_directory_list pop 0 directory_args['path'] working_dirchanged module set_fs_attributes_if_different directory_args changed changed adjust_recursive_directory_permissions working_dir new_directory_list module directory_args changed return changed
def set_sff_trimpoints sff_dir technical_lengths for lib_id sff_fp in get_per_lib_sff_fps sff_dir try readlength technical_lengths[lib_id]except KeyError continuesff_data parse_binary_sff open sff_fp True clipped_header clipped_reads set_clip_qual_left sff_data readlength fd temp_fp mkstemp dir sff_dir close fd with open temp_fp 'w' as f write_binary_sff f clipped_header clipped_reads move temp_fp sff_fp
def set_sff_trimpoints sff_dir technical_lengths for lib_id sff_fp in get_per_lib_sff_fps sff_dir try readlength technical_lengths[lib_id]except KeyError continuesff_data parse_binary_sff open sff_fp True clipped_header clipped_reads set_clip_qual_left sff_data readlength fd temp_fp mkstemp dir sff_dir close fd with open temp_fp 'w' as f write_binary_sff f clipped_header clipped_reads move temp_fp sff_fp
def common_params task_instance task_cls if not isinstance task_cls task Register raise TypeError 'task_clsmustbeanuninstantiatedTask' task_instance_param_names dict task_instance get_params keys task_cls_params_dict dict task_cls get_params task_cls_param_names task_cls_params_dict keys common_param_names set task_instance_param_names intersection set task_cls_param_names common_param_vals [ key task_cls_params_dict[key] for key in common_param_names]common_kwargs dict key task_instance param_kwargs[key] for key in common_param_names vals dict task_instance get_param_values common_param_vals [] common_kwargs return vals
def common_params task_instance task_cls if not isinstance task_cls task Register raise TypeError 'task_clsmustbeanuninstantiatedTask' task_instance_param_names dict task_instance get_params keys task_cls_params_dict dict task_cls get_params task_cls_param_names task_cls_params_dict keys common_param_names set task_instance_param_names intersection set task_cls_param_names common_param_vals [ key task_cls_params_dict[key] for key in common_param_names]common_kwargs dict key task_instance param_kwargs[key] for key in common_param_names vals dict task_instance get_param_values common_param_vals [] common_kwargs return vals
def getPythonContainers meth containers []containers append meth im_class moduleName meth im_class __module__while moduleName is not None module sys modules get moduleName None if module is None module __import__ moduleName containers append module moduleName getattr module '__module__' None return containers
def kendall v1 v2 v1 v2 array v1 array v2 if not v1 size v2 size > 1 raise ValueError "Oneormorevectorsisn'tlongenoughtocorrelateortheyhaveunequallengths " return kendalltau v1 v2 [0]
def kendall v1 v2 v1 v2 array v1 array v2 if not v1 size v2 size > 1 raise ValueError "Oneormorevectorsisn'tlongenoughtocorrelateortheyhaveunequallengths " return kendalltau v1 v2 [0]
def getDictionaryString dictionary output cStringIO StringIO keys dictionary keys keys sort for key in keys addValueToOutput 0 key output dictionary[key] return output getvalue
def _impute df observations censorship transform_in transform_out uncensored_mask df[censorship] False censored_mask df[censorship] True fit_params stats linregress df['Zprelim'][uncensored_mask] transform_in df[observations][uncensored_mask] slope intercept fit_params[ 2]df loc[ 'estimated'] transform_out slope * df['Zprelim'][censored_mask] + intercept df loc[ 'final'] numpy where df[censorship] df['estimated'] df[observations] return df
def n_deep obj names for name in names try obj getattr obj name except KeyError raise APIError 'Thisobjectismissingthe%sattribute ' % name obj return obj
def ensure_dir_exists dirname try os makedirs dirname except OSError as e if e errno errno EEXIST raise
def recognize_derivative a d DE z None flag True a d a cancel d include True q r a div d Np Sp splitfactor_sqf d DE coefficientD True z z j 1for s i in Sp delta_a delta_d H laurent_series r d s j DE g gcd d H[ -1 ] as_poly if g is not d flag Falsebreakj j + 1 return flag
def _createStructFormat format {BIG_ENDIAN {} LITTLE_ENDIAN {}}for struct_format in 'BHILQ' try size calcsize struct_format format[BIG_ENDIAN][size] '>%s' % struct_format format[LITTLE_ENDIAN][size] '<%s' % struct_format except struct_error passreturn format
def _split_line s parts out {}start 0for name length in parts out[name] s[start start + length ] strip start + lengthdel out['_']return out
def _remove_unsupported_archs _config_vars if 'CC' in os environ return _config_varsif re search '-arch\\s+ppc' _config_vars['CFLAGS'] is not None status os system "echo'intmain{} ' '%s'-c-archppc-xc-o/dev/null/dev/null2>/dev/null" % _config_vars['CC'] replace "'" '\'"\'"\'' if status for cv in _UNIVERSAL_CONFIG_VARS if cv in _config_vars and cv not in os environ flags _config_vars[cv]flags re sub '-arch\\s+ppc\\w*\\s' '' flags _save_modified_value _config_vars cv flags return _config_vars
def get_delete_id_list user site check_global True use_cache True page_ids _get_page_ids_for_action user user site site action 'delete_page' check_global check_global use_cache use_cache return page_ids
def get_delete_id_list user site check_global True use_cache True page_ids _get_page_ids_for_action user user site site action 'delete_page' check_global check_global use_cache use_cache return page_ids
def register_mode name mode if name in predefined_modes raise ValueError 'Modenamealreadytaken %s' % name predefined_modes[name] mode
def update_collection_status_in_search collection_id rights rights_manager get_collection_rights collection_id if rights status rights_manager ACTIVITY_STATUS_PRIVATE delete_documents_from_search_index [collection_id] else patch_collection_search_document rights id _collection_rights_to_search_dict rights
@register simple_tag takes_context True def escape_explicit context return escape 'Hello{0} ' format context['name']
def rss_mail feed jobs parm {'amount' len jobs 'feed' feed 'jobs' jobs}return send_with_template 'rss' parm
def signal_to_exception signum frame if signum signal SIGALRM raise SIGALRMException if signum signal SIGHUP raise SIGHUPException if signum signal SIGUSR1 raise SIGUSR1Exception if signum signal SIGUSR2 raise SIGUSR2Exception raise SignalException signum
def _module_versions lines []modules collections OrderedDict [ 'sip' ['SIP_VERSION_STR'] 'colorama' ['VERSION' '__version__'] 'pypeg2' ['__version__'] 'jinja2' ['__version__'] 'pygments' ['__version__'] 'yaml' ['__version__'] 'cssutils' ['__version__'] 'typing' [] 'PyQt5 QtWebEngineWidgets' [] ] for name attributes in modules items try module importlib import_module name except ImportError text '{} no' format name else for attr in attributes try text '{} {}' format name getattr module attr except AttributeError passelse breakelse text '{} yes' format name lines append text return lines
def permission_blacked_out course role_names permission_name return not course forum_posts_allowed and role_names {FORUM_ROLE_STUDENT} and any [permission_name startswith prefix for prefix in ['edit' 'update' 'create']]
def permission_blacked_out course role_names permission_name return not course forum_posts_allowed and role_names {FORUM_ROLE_STUDENT} and any [permission_name startswith prefix for prefix in ['edit' 'update' 'create']]
def _dscl cmd ctype 'create' if __grains__['osrelease_info'] < 10 8 source noderoot ' ' '' else source noderoot 'localhost' '/Local/Default' if noderoot cmd[0] noderoot + cmd[0] return __salt__['cmd run_all'] ['dscl' source '-' + ctype ] + cmd output_loglevel 'quiet' if ctype 'passwd' else 'debug' python_shell False
def gen_batches n batch_size start 0for _ in range int n // batch_size end start + batch_size yield slice start end start endif start < n yield slice start n
def gen_batches n batch_size start 0for _ in range int n // batch_size end start + batch_size yield slice start end start endif start < n yield slice start n
@evalcontextfilterdef do_tojson eval_ctx value indent None policies eval_ctx environment policiesdumper policies['json dumps_function']options policies['json dumps_kwargs']if indent is not None options dict options options['indent'] indentreturn htmlsafe_json_dumps value dumper dumper **options
def _delete context namespace_name resource_type_name namespace_id resource_type_id session db_rec _get context namespace_name resource_type_name namespace_id resource_type_id session session delete db_rec session flush return db_rec to_dict
def angle_axis2quat theta vector is_normalized False vector np array vector if not is_normalized vector vector / math sqrt np dot vector vector t2 theta / 2 0 st2 math sin t2 return np concatenate [math cos t2 ] vector * st2
def clear_old_snapshots logging info 'RemovingoldCassandrasnapshots ' try subprocess check_call [NODE_TOOL 'clearsnapshot'] except CalledProcessError as error logging error 'ErrorwhiledeletingoldCassandrasnapshots Error {0}' format str error
def clear_old_snapshots logging info 'RemovingoldCassandrasnapshots ' try subprocess check_call [NODE_TOOL 'clearsnapshot'] except CalledProcessError as error logging error 'ErrorwhiledeletingoldCassandrasnapshots Error {0}' format str error
def clear_old_snapshots logging info 'RemovingoldCassandrasnapshots ' try subprocess check_call [NODE_TOOL 'clearsnapshot'] except CalledProcessError as error logging error 'ErrorwhiledeletingoldCassandrasnapshots Error {0}' format str error
def getPath edges pathIndexes loop z path []for pathIndexIndex in xrange len pathIndexes pathIndex pathIndexes[pathIndexIndex]edge edges[pathIndex]carveIntersection getCarveIntersectionFromEdge edge loop z path append carveIntersection return path
def _prompt_user_variable var_name default_value return click prompt var_name default default_value
@blueprint route '/<job_id> json' methods ['GET'] @blueprint route '/<job_id>' methods ['GET'] def show job_id job scheduler get_job job_id if job is None raise werkzeug exceptions NotFound 'Jobnotfound' related_jobs scheduler get_related_jobs job if request_wants_json return flask jsonify job json_dict True elif isinstance job model_images ImageClassificationModelJob return model_images classification views show job related_jobs related_jobs elif isinstance job model_images GenericImageModelJob return model_images generic views show job related_jobs related_jobs else raise werkzeug exceptions BadRequest 'Invalidjobtype'
def _add_file file_path _db_content['files'] append file_path
def attach_handlers app settings if settings USE_POSTGRES add_handlers app django_handlers handlers else add_handlers app mongo_handlers handlers add_handlers app celery_task_handlers handlers add_handlers app transaction_handlers handlers add_handlers app postcommit_handlers handlers add_handlers app {'before_request' framework sessions prepare_private_key} add_handlers app {'before_request' framework sessions before_request 'after_request' framework sessions after_request} return app
def add_reload_hook fn _reload_hooks append fn
def add_reload_hook fn _reload_hooks append fn
def libvlc_media_player_get_rate p_mi f _Cfunctions get 'libvlc_media_player_get_rate' None or _Cfunction 'libvlc_media_player_get_rate' 1 None ctypes c_float MediaPlayer return f p_mi
def make_get_public_certificates_call rpc request app_identity_service_pb GetPublicCertificateForAppRequest response app_identity_service_pb GetPublicCertificateForAppResponse def get_certs_result rpc 'Checksuccess handleexceptions andreturnconvertedRPCresult \n\nThismethodwaitsfortheRPCifithasnotyetfinished andcallsthe\npost-callhooksonthefirstinvocation \n\nArgs \nrpc AUserRPCobject \n\nReturns \nAlistofPublicCertificateobject \n'assert rpc service _APP_IDENTITY_SERVICE_NAME repr rpc service assert rpc method _GET_CERTS_METHOD_NAME repr rpc method try rpc check_success except apiproxy_errors ApplicationError as err raise _to_app_identity_error err result []for cert in response public_certificate_list_list result append PublicCertificate cert key_name cert x509_certificate_pem return resultrpc make_call _GET_CERTS_METHOD_NAME request response get_certs_result
def _session config __salt__['config option'] 'zenoss' session requests session session auth config get 'username' config get 'password' session verify Falsesession headers update {'Content-type' 'application/json charset utf-8'} return session
def _session config __salt__['config option'] 'zenoss' session requests session session auth config get 'username' config get 'password' session verify Falsesession headers update {'Content-type' 'application/json charset utf-8'} return session
def partial_project endog exog x1 x2 endog exog params np linalg pinv x2 dot x1 predicted x2 dot params residual x1 - predicted res Bunch params params fittedvalues predicted resid residual return res
def partial_project endog exog x1 x2 endog exog params np linalg pinv x2 dot x1 predicted x2 dot params residual x1 - predicted res Bunch params params fittedvalues predicted resid residual return res
def partial_project endog exog x1 x2 endog exog params np linalg pinv x2 dot x1 predicted x2 dot params residual x1 - predicted res Bunch params params fittedvalues predicted resid residual return res
def authenticate func c expose_request False if not python callable func raise TypeError 'funcmustbecallable' if not python callable c raise TypeError 'Authenticatormustbecallable' attr funcif isinstance func types UnboundMethodType attr func im_funcif expose_request is True c globals ['expose_request'] c setattr attr '_pyamf_authenticator' c return func
def install_templates_translations generator if 'JINJA_ENVIRONMENT' in generator settings jinja_extensions generator settings['JINJA_ENVIRONMENT'] get 'extensions' [] else jinja_extensions generator settings['JINJA_EXTENSIONS']if 'jinja2 ext i18n' in jinja_extensions domain generator settings get 'I18N_GETTEXT_DOMAIN' 'messages' localedir generator settings get 'I18N_GETTEXT_LOCALEDIR' if localedir is None localedir os path join generator theme 'translations' current_lang generator settings['DEFAULT_LANG']if current_lang generator settings get 'I18N_TEMPLATES_LANG' _MAIN_LANG translations gettext NullTranslations else langs [current_lang]try translations gettext translation domain localedir langs except IOError OSError _LOGGER error "Cannotfindtranslationsforlanguage'{}'in'{}'withdomain'{}' InstallingNullTranslations " format langs[0] localedir domain translations gettext NullTranslations newstyle generator settings get 'I18N_GETTEXT_NEWSTYLE' True generator env install_gettext_translations translations newstyle
def install_templates_translations generator if 'JINJA_ENVIRONMENT' in generator settings jinja_extensions generator settings['JINJA_ENVIRONMENT'] get 'extensions' [] else jinja_extensions generator settings['JINJA_EXTENSIONS']if 'jinja2 ext i18n' in jinja_extensions domain generator settings get 'I18N_GETTEXT_DOMAIN' 'messages' localedir generator settings get 'I18N_GETTEXT_LOCALEDIR' if localedir is None localedir os path join generator theme 'translations' current_lang generator settings['DEFAULT_LANG']if current_lang generator settings get 'I18N_TEMPLATES_LANG' _MAIN_LANG translations gettext NullTranslations else langs [current_lang]try translations gettext translation domain localedir langs except IOError OSError _LOGGER error "Cannotfindtranslationsforlanguage'{}'in'{}'withdomain'{}' InstallingNullTranslations " format langs[0] localedir domain translations gettext NullTranslations newstyle generator settings get 'I18N_GETTEXT_NEWSTYLE' True generator env install_gettext_translations translations newstyle
def make_sparse_spd_matrix dim 1 alpha 0 95 norm_diag False smallest_coef 0 1 largest_coef 0 9 random_state None random_state check_random_state random_state chol - np eye dim aux random_state rand dim dim aux[ aux < alpha ] 0aux[ aux > alpha ] smallest_coef + largest_coef - smallest_coef * random_state rand np sum aux > alpha aux np tril aux k -1 permutation random_state permutation dim aux aux[permutation] T[permutation]chol + auxprec np dot chol T chol if norm_diag d np diag prec reshape 1 prec shape[0] d 1 0 / np sqrt d prec * dprec * d Treturn prec
def testOnSequenceData module dataset target dataset getField 'target' output ModuleValidator calculateModuleOutput module dataset ends SequenceHelper getSequenceEnds dataset summed_output zeros dataset outdim class_output []class_target []for j in range len output summed_output + output[j]if j in ends class_output append argmax summed_output class_target append argmax target[j] summed_output zeros dataset outdim class_output array class_output class_target array class_target return Validator classificationPerformance class_output class_target
def testOnSequenceData module dataset target dataset getField 'target' output ModuleValidator calculateModuleOutput module dataset ends SequenceHelper getSequenceEnds dataset summed_output zeros dataset outdim class_output []class_target []for j in range len output summed_output + output[j]if j in ends class_output append argmax summed_output class_target append argmax target[j] summed_output zeros dataset outdim class_output array class_output class_target array class_target return Validator classificationPerformance class_output class_target
def auto_configure_disk session vdi_ref new_gb with vdi_attached_here session vdi_ref read_only False as dev partitions _get_partitions dev if len partitions 1 return _num start old_sectors ptype partitions[0]if ptype in 'ext3' 'ext4' new_sectors new_gb * 1024 * 1024 * 1024 / SECTOR_SIZE _resize_part_and_fs dev start old_sectors new_sectors
def hookspath return [curdir]
def getLoopsInOrderOfArea compareAreaFunction loops loopAreas []for loop in loops loopArea LoopArea loop loopAreas append loopArea loopAreas sort compareAreaFunction loopsInDescendingOrderOfArea []for loopArea in loopAreas loopsInDescendingOrderOfArea append loopArea loop return loopsInDescendingOrderOfArea
def neighbors neighbor None return __proxy__['napalm call'] 'get_bgp_neighbors_detail' **{'neighbor_address' neighbor}
def neighbors neighbor None return __proxy__['napalm call'] 'get_bgp_neighbors_detail' **{'neighbor_address' neighbor}
def neighbors neighbor None return __proxy__['napalm call'] 'get_bgp_neighbors_detail' **{'neighbor_address' neighbor}
def neighbors neighbor None return __proxy__['napalm call'] 'get_bgp_neighbors_detail' **{'neighbor_address' neighbor}
def nl2br string return unicodifier string replace u'\n' u'<br>\n'
def nl2br string return unicodifier string replace u'\n' u'<br>\n'
def get_stock_information_html supplier product stock StockCount objects filter product product supplier supplier first context {'div_id' get_stock_information_div_id supplier product 'sales_decimals' product sales_unit decimals if product sales_unit else 0 'sales_unit' product sales_unit short_name if product sales_unit else '' 'stock' stock}if 'shuup notify' in settings INSTALLED_APPS context['alert_limit'] Truereturn render_to_string 'shuup/simple_supplier/admin/stock_information jinja' context
def get_stock_information_html supplier product stock StockCount objects filter product product supplier supplier first context {'div_id' get_stock_information_div_id supplier product 'sales_decimals' product sales_unit decimals if product sales_unit else 0 'sales_unit' product sales_unit short_name if product sales_unit else '' 'stock' stock}if 'shuup notify' in settings INSTALLED_APPS context['alert_limit'] Truereturn render_to_string 'shuup/simple_supplier/admin/stock_information jinja' context
def get_stock_information_html supplier product stock StockCount objects filter product product supplier supplier first context {'div_id' get_stock_information_div_id supplier product 'sales_decimals' product sales_unit decimals if product sales_unit else 0 'sales_unit' product sales_unit short_name if product sales_unit else '' 'stock' stock}if 'shuup notify' in settings INSTALLED_APPS context['alert_limit'] Truereturn render_to_string 'shuup/simple_supplier/admin/stock_information jinja' context
def uses_shib course return course enrollment_domain and course enrollment_domain startswith settings SHIBBOLETH_DOMAIN_PREFIX
def uses_shib course return course enrollment_domain and course enrollment_domain startswith settings SHIBBOLETH_DOMAIN_PREFIX
def uses_shib course return course enrollment_domain and course enrollment_domain startswith settings SHIBBOLETH_DOMAIN_PREFIX
def uses_shib course return course enrollment_domain and course enrollment_domain startswith settings SHIBBOLETH_DOMAIN_PREFIX
def push img if not isinstance img np ndarray raise ValueError 'Canonlypushndarraystotheimagestack ' image_stack append img
def symlink src link if sys getwindowsversion major < 6 raise SaltInvocationError 'SymlinksareonlysupportedonWindowsVistaorlater ' if not os path exists src raise SaltInvocationError 'Thegivensourcepathdoesnotexist ' if not os path isabs src raise SaltInvocationError 'Filepathmustbeabsolute ' src os path normpath src link os path normpath link is_dir os path isdir src try win32file CreateSymbolicLink link src int is_dir return Trueexcept pywinerror as exc raise CommandExecutionError "Couldnotcreate'{0}'-[{1}]{2}" format link exc winerror exc strerror
def symlink src link if sys getwindowsversion major < 6 raise SaltInvocationError 'SymlinksareonlysupportedonWindowsVistaorlater ' if not os path exists src raise SaltInvocationError 'Thegivensourcepathdoesnotexist ' if not os path isabs src raise SaltInvocationError 'Filepathmustbeabsolute ' src os path normpath src link os path normpath link is_dir os path isdir src try win32file CreateSymbolicLink link src int is_dir return Trueexcept pywinerror as exc raise CommandExecutionError "Couldnotcreate'{0}'-[{1}]{2}" format link exc winerror exc strerror
def _has_beta version dev_releases return version in [re search ' \\d+\\ +\\d+' s group 0 for s in dev_releases keys ]
def test_no_translation old_lang translation get_language try translation activate 'pt-br' with no_translation assert translation get_language lower settings LANGUAGE_CODE lower assert translation get_language 'pt-br' with no_translation 'es' assert translation get_language 'es' assert translation get_language 'pt-br' finally translation activate old_lang
def test_no_translation old_lang translation get_language try translation activate 'pt-br' with no_translation assert translation get_language lower settings LANGUAGE_CODE lower assert translation get_language 'pt-br' with no_translation 'es' assert translation get_language 'es' assert translation get_language 'pt-br' finally translation activate old_lang
def test_no_translation old_lang translation get_language try translation activate 'pt-br' with no_translation assert translation get_language lower settings LANGUAGE_CODE lower assert translation get_language 'pt-br' with no_translation 'es' assert translation get_language 'es' assert translation get_language 'pt-br' finally translation activate old_lang
def write_record_pair read1 read2 fileobj _rec_pair u'@%s\n%s\n+\n%s\n' * 2 _rec_pair_no_qual u'>%s\n%s\n' * 2 if hasattr read1 u'quality' assert hasattr read2 u'quality' recstr _rec_pair % read1 name read1 sequence read1 quality read2 name read2 sequence read2 quality else recstr _rec_pair_no_qual % read1 name read1 sequence read2 name read2 sequence try fileobj write bytes recstr u'ascii' except TypeError fileobj write recstr
def test_cli_roles_override_decorator_roles @roles 'r1' def command passeq_effective_roles command ['r2'] cli_roles ['r2'] env {'roledefs' fake_roles}
def _TestAuthFacebookUser action tester user_dict device_dict None user_cookie None ident_dict {'key' 'FacebookGraph %s' % user_dict['id'] 'authority' 'Facebook' 'access_token' 'access_token'}if device_dict device_dict pop 'device_uuid' None device_dict pop 'test_udid' None with mock patch 'tornado httpclient AsyncHTTPClient' MockAsyncHTTPClient as mock_client mock_client map 'https //graph facebook com/oauth/access_token' 'access_token %s&expires 3600' % ident_dict['access_token'] auth_test _AddMockJSONResponse mock_client 'https //graph facebook com/me\\?' user_dict auth_test _AddMockJSONResponse mock_client 'https //graph facebook com/me/photos\\?' {'data' []} auth_test _AddMockJSONResponse mock_client 'https //graph facebook com/me/friends\\?' {'data' []} response auth_test _AuthFacebookOrGoogleUser tester action user_dict ident_dict device_dict user_cookie return auth_test _ValidateAuthUser tester action user_dict ident_dict device_dict user_cookie response
def make_traceback exc_info source_hint None exc_type exc_value tb exc_infoif isinstance exc_value TemplateSyntaxError exc_info translate_syntax_error exc_value source_hint initial_skip 0else initial_skip 1return translate_exception exc_info initial_skip
def imdisplay imarray screen None a pg surfarray make_surface imarray swapaxes 0 1 if screen is None screen pg display set_mode imarray shape[ 2][ -1 ] screen blit a 0 0 pg display flip
def imdisplay imarray screen None a pg surfarray make_surface imarray swapaxes 0 1 if screen is None screen pg display set_mode imarray shape[ 2][ -1 ] screen blit a 0 0 pg display flip
def set_debug_function func_cb debug print_to_stdout warnings True notices True speed True debug debug_function func_cbdebug enable_warning warningsdebug enable_notice noticesdebug enable_speed speed
def set_debug_function func_cb debug print_to_stdout warnings True notices True speed True debug debug_function func_cbdebug enable_warning warningsdebug enable_notice noticesdebug enable_speed speed
def test_feature_max_length_on_step_sentence feature Feature from_string FEATURE4 assert_equals feature max_length 55
def validate_files pelican for dirpath _ filenames in os walk pelican settings['OUTPUT_PATH'] for name in filenames if should_validate name filepath os path join dirpath name validate filepath
def get_permission_types return _PERMISSION_TYPES keys
@lru_cache def system_start_time start_time parameter time time 'systemstarttime' btime_line _get_line '/proc/stat' 'btime' parameter try result float btime_line strip split [1] _log_runtime parameter '/proc/stat[btime]' start_time return resultexcept exc IOError 'unabletoparsethe/proc/statbtimeentry %s' % btime_line _log_failure parameter exc raise exc
def l2_regularizer weight 1 0 scope None def regularizer tensor with tf name_scope scope 'L2Regularizer' [tensor] l2_weight tf convert_to_tensor weight dtype tensor dtype base_dtype name 'weight' return tf multiply l2_weight tf nn l2_loss tensor name 'value' return regularizer
def _extended_lookup connection project key_pbs missing None deferred None eventual False transaction_id None if missing is not None and missing [] raise ValueError 'missingmustbeNoneoranemptylist' if deferred is not None and deferred [] raise ValueError 'deferredmustbeNoneoranemptylist' results []loop_num 0while loop_num < _MAX_LOOPS loop_num + 1 results_found missing_found deferred_found connection lookup project project key_pbs key_pbs eventual eventual transaction_id transaction_id results extend results_found if missing is not None missing extend missing_found if deferred is not None deferred extend deferred_found breakif len deferred_found 0 breakkey_pbs deferred_foundreturn results
def html_unescape t return encode_decode htmldecode t
def _clean_salt_variables params variable_prefix '__' list list map params pop [k for k in params if k startswith variable_prefix ] return params
def sort_torrent_fulltext data_set norm_num_seeders normalize_data_dict data_set 'num_seeders' 'infohash' norm_neg_votes normalize_data_dict data_set 'neg_votes' 'infohash' norm_subscriptions normalize_data_dict data_set 'subscriptions' 'infohash' for data in data_set score 0 8 * norm_num_seeders[data get 'infohash' ] - 0 1 * norm_neg_votes[data get 'infohash' ] + 0 1 * norm_subscriptions[data get 'infohash' ] data get 'relevance_score' [ -1 ] scoredata_set sort key lambda d d get 'relevance_score' reverse True
@contextlib contextmanagerdef create_file_backed_module code with create_tempfile as temp module test_utils build_module code module file temp yield module
def check_status try ret salt utils http query 'https //rubygems org' status True except Exception return Falsereturn ret['status'] 200
def premetadata_create_account_stat_table self conn put_timestamp conn executescript "\nCREATETABLEaccount_stat \naccountTEXT \ncreated_atTEXT \nput_timestampTEXTDEFAULT'0' \ndelete_timestampTEXTDEFAULT'0' \ncontainer_countINTEGER \nobject_countINTEGERDEFAULT0 \nbytes_usedINTEGERDEFAULT0 \nhashTEXTdefault'00000000000000000000000000000000' \nidTEXT \nstatusTEXTDEFAULT'' \nstatus_changed_atTEXTDEFAULT'0'\n \n\nINSERTINTOaccount_stat container_count VALUES 0 \n" conn execute '\nUPDATEaccount_statSETaccount ? created_at ? id ? \nput_timestamp ?\n' self account Timestamp time internal str uuid4 put_timestamp
def interpret_conf_limits conf name_prefix info None conf_limits []for conf_key in conf if conf_key startswith name_prefix cont_size int conf_key[len name_prefix ] rate float conf[conf_key] conf_limits append cont_size rate conf_limits sort ratelimits []conf_limits_info list conf_limits while conf_limits cur_size cur_rate conf_limits pop 0 if conf_limits next_size next_rate conf_limits[0]slope float next_rate - float cur_rate / next_size - cur_size def new_scope cur_size slope cur_rate return lambda x x - cur_size * slope + cur_rate line_func new_scope cur_size slope cur_rate else line_func lambda x cur_rate ratelimits append cur_size cur_rate line_func if info is None return ratelimitselse return ratelimits conf_limits_info
def get_exc_from_name name exc Nonetry return rc_exc_cache[name]except KeyError m rc_exc_regex match name if m base m group 1 rc_or_sig_name m group 2 if base 'SignalException' try rc - int rc_or_sig_name except ValueError rc - getattr signal rc_or_sig_name else rc int rc_or_sig_name exc get_rc_exc rc return exc
def sync_table model keyspaces None connections None context _get_context keyspaces connections for connection keyspace in context with query ContextQuery model keyspace keyspace as m _sync_table m connection connection
def localSslFixup host sslContext if not sslContext and host in ['localhost' '127 0 0 1' ' 1'] import sslif hasattr ssl '_create_unverified_context' sslContext ssl _create_unverified_context return sslContext
def localSslFixup host sslContext if not sslContext and host in ['localhost' '127 0 0 1' ' 1'] import sslif hasattr ssl '_create_unverified_context' sslContext ssl _create_unverified_context return sslContext
def localSslFixup host sslContext if not sslContext and host in ['localhost' '127 0 0 1' ' 1'] import sslif hasattr ssl '_create_unverified_context' sslContext ssl _create_unverified_context return sslContext
def localSslFixup host sslContext if not sslContext and host in ['localhost' '127 0 0 1' ' 1'] import sslif hasattr ssl '_create_unverified_context' sslContext ssl _create_unverified_context return sslContext
def _combine_similar_molecules molecules_list new_guys_start_idx 0while new_guys_start_idx < len molecules_list combined [False] * len molecules_list new_guys []for j in xrange new_guys_start_idx len molecules_list for i in xrange 0 j if combined[i] continue g1 m1 g2 m2 molecules_list[i] molecules_list[j] js _jaccard_similarity g1 g2 if js > JACCARD_THRESHOLD new_guys append g1 union g2 m1 union m2 combined[i] combined[j] True True breakmolecules_list [molecule for molecule was_combined in zip molecules_list combined if not was_combined ]new_guys_start_idx len molecules_list molecules_list extend new_guys return molecules_list
def _find_compound_unit numerator_unit denominator_unit locale LC_NUMERIC locale Locale parse locale numerator_unit _find_unit_pattern numerator_unit locale locale denominator_unit _find_unit_pattern denominator_unit locale locale if not numerator_unit and denominator_unit return Nonebare_numerator_unit numerator_unit split '-' 1 [ -1 ]bare_denominator_unit denominator_unit split '-' 1 [ -1 ]return _find_unit_pattern '%s-per-%s' % bare_numerator_unit bare_denominator_unit locale locale
@comm_guard FreeVariable ANY_TYPE def unify_walk fv o U v BoundVariable '?' o return U merge v fv
@comm_guard FreeVariable ANY_TYPE def unify_walk fv o U v BoundVariable '?' o return U merge v fv
def translate formula out []for part in re split ' \\w+ ? \\w+ ? ' formula m re match '^ [A-Z]+ [1-9][0-9]* ? [A-Z]+ [1-9][0-9]* ?$' part if m is None out append part else x1 y1 x2 y2 m groups x1 colname2num x1 if x2 is None s 'cell %s %s ' % x1 y1 else x2 colname2num x2 s 'cells %s %s %s %s ' % x1 y1 x2 y2 out append s return '' join out
def violin_stats X method points 100 vpstats []X _reshape_2D X for x in X stats {}min_val np min x max_val np max x coords np linspace min_val max_val points stats[u'vals'] method x coords stats[u'coords'] coordsstats[u'mean'] np mean x stats[u'median'] np median x stats[u'min'] min_valstats[u'max'] max_valvpstats append stats return vpstats
def violin_stats X method points 100 vpstats []X _reshape_2D X for x in X stats {}min_val np min x max_val np max x coords np linspace min_val max_val points stats[u'vals'] method x coords stats[u'coords'] coordsstats[u'mean'] np mean x stats[u'median'] np median x stats[u'min'] min_valstats[u'max'] max_valvpstats append stats return vpstats
def firebase_patch path value None response content _get_http request path method 'PATCH' body value return json loads content
def firebase_patch path value None response content _get_http request path method 'PATCH' body value return json loads content
def firebase_patch path value None response content _get_http request path method 'PATCH' body value return json loads content
def _StripSeparators value return re sub '[]*' '' re sub _WORD_SEPARATOR_RE '' value
def rmsdiff_2011 im1 im2 diff ImageChops difference im1 im2 h diff histogram sq value * idx ** 2 for idx value in enumerate h sum_of_squares sum sq rms math sqrt sum_of_squares / float im1 size[0] * im1 size[1] return rms
def callMultipleInThread tupleList from twisted internet import reactorreactor callInThread _runMultiple tupleList
def plugins_update for env in PluginGlobals env_registry values for service in env services copy if service __class__ not in _PLUGINS_CLASS service deactivate import ckan config environment as environmentenvironment update_config
def plugins_update for env in PluginGlobals env_registry values for service in env services copy if service __class__ not in _PLUGINS_CLASS service deactivate import ckan config environment as environmentenvironment update_config
def plugins_update for env in PluginGlobals env_registry values for service in env services copy if service __class__ not in _PLUGINS_CLASS service deactivate import ckan config environment as environmentenvironment update_config
def plugins_update for env in PluginGlobals env_registry values for service in env services copy if service __class__ not in _PLUGINS_CLASS service deactivate import ckan config environment as environmentenvironment update_config
def plugins_update for env in PluginGlobals env_registry values for service in env services copy if service __class__ not in _PLUGINS_CLASS service deactivate import ckan config environment as environmentenvironment update_config
def count_discussions_handler sender **kwargs if kwargs get 'instance' and kwargs get 'created' returncomment 'comment' in kwargs and kwargs['comment'] or kwargs['instance'] entry comment content_objectif isinstance entry Entry entry comment_count entry comments count entry pingback_count entry pingbacks count entry trackback_count entry trackbacks count entry save update_fields ['comment_count' 'pingback_count' 'trackback_count']
def map_wms request mapid map_obj _resolve_map request mapid 'base view_resourcebase' _PERMISSION_MSG_VIEW if request method 'PUT' try layerGroupName map_obj publish_layer_group response dict layerGroupName layerGroupName ows getattr ogc_server_settings 'ows' '' return HttpResponse json dumps response content_type 'application/json' except return HttpResponseServerError if request method 'GET' response dict layerGroupName getattr map_obj layer_group 'name' '' ows getattr ogc_server_settings 'ows' '' return HttpResponse json dumps response content_type 'application/json' return HttpResponseNotAllowed ['PUT' 'GET']
def map_wms request mapid map_obj _resolve_map request mapid 'base view_resourcebase' _PERMISSION_MSG_VIEW if request method 'PUT' try layerGroupName map_obj publish_layer_group response dict layerGroupName layerGroupName ows getattr ogc_server_settings 'ows' '' return HttpResponse json dumps response content_type 'application/json' except return HttpResponseServerError if request method 'GET' response dict layerGroupName getattr map_obj layer_group 'name' '' ows getattr ogc_server_settings 'ows' '' return HttpResponse json dumps response content_type 'application/json' return HttpResponseNotAllowed ['PUT' 'GET']
def map_wms request mapid map_obj _resolve_map request mapid 'base view_resourcebase' _PERMISSION_MSG_VIEW if request method 'PUT' try layerGroupName map_obj publish_layer_group response dict layerGroupName layerGroupName ows getattr ogc_server_settings 'ows' '' return HttpResponse json dumps response content_type 'application/json' except return HttpResponseServerError if request method 'GET' response dict layerGroupName getattr map_obj layer_group 'name' '' ows getattr ogc_server_settings 'ows' '' return HttpResponse json dumps response content_type 'application/json' return HttpResponseNotAllowed ['PUT' 'GET']
def permitted func @functools wraps func def wrapper request *args **kwargs '\nWrapperfortheviewthatonlycallstheviewiftheuserisauthorized \n'def fetch_content '\nExtracttheforumobjectfromthekeywordargumentstotheview \n'if 'thread_id' in kwargs content cc Thread find kwargs['thread_id'] to_dict elif 'comment_id' in kwargs content cc Comment find kwargs['comment_id'] to_dict elif 'commentable_id' in kwargs content cc Commentable find kwargs['commentable_id'] to_dict else content Nonereturn contentcourse_key CourseKey from_string kwargs['course_id'] if check_permissions_by_view request user course_key fetch_content request view_name return func request *args **kwargs else return JsonError 'unauthorized' status 401 return wrapper
def auto_through field return not field rel through or getattr getattr field rel through '_meta' None 'auto_created' False
def auto_through field return not field rel through or getattr getattr field rel through '_meta' None 'auto_created' False
def st_mode_to_octal mode try return oct mode [ -4 ]except TypeError IndexError return ''
def namignizer_iterator names counts batch_size num_steps epoch_size name_distribution counts / counts sum for i in range epoch_size data np zeros batch_size * num_steps + 1 samples np random choice names size batch_size * num_steps // 2 replace True p name_distribution data_index 0for sample in samples if data_index > batch_size * num_steps breakfor letter in map _letter_to_number sample + [_EON] if data_index > batch_size * num_steps breakdata[data_index] letterdata_index + 1x data[ batch_size * num_steps ] reshape batch_size num_steps y data[1 batch_size * num_steps + 1 ] reshape batch_size num_steps yield x y
def get_blocks_with_unallocated module cp_driver lb_driver network_domain total_unallocated_ips 0all_blocks list_public_ip_blocks module cp_driver network_domain unalloc_blocks []unalloc_addresses []for block in all_blocks d_blocks get_block_allocation module cp_driver lb_driver network_domain block i 0for addr in d_blocks['addresses'] if addr['allocated'] is False if i 0 unalloc_blocks append d_blocks unalloc_addresses append addr['address'] total_unallocated_ips + 1i + 1return {'unallocated_count' total_unallocated_ips 'ip_blocks' unalloc_blocks 'unallocated_addresses' unalloc_addresses}
def convert_kvp_str_to_list data kvp [x strip for x in data split ' ' 1 ]if len kvp 2 and kvp[0] return kvpmsg _ "'%s'isnotoftheform<key> [value]" % data raise q_exc InvalidInput error_message msg
def valid_value value quote default_cookie_quote unquote default_unquote if value is None return Falseencoded encode_cookie_value value quote quote decoded parse_string encoded unquote unquote decoded_normalized normalize 'NFKD' decoded if not isinstance decoded bytes else decoded value_normalized normalize 'NFKD' value if not isinstance value bytes else value if decoded_normalized value_normalized return Truereturn False
def reset name runas None return prlctl 'reset' _sdecode name runas runas
def reset name runas None return prlctl 'reset' _sdecode name runas runas
def _get_drivers global driversif drivers is None drivers []for notification_driver in CONF list_notifier_drivers try drivers append importutils import_module notification_driver except ImportError as e drivers append ImportFailureNotifier e return drivers
def rsentence length 4 return '' join rword random randint 4 9 for i in range length
def glob_list obj *args **kwargs return obj __glob_list__ *args **kwargs
def glob_list obj *args **kwargs return obj __glob_list__ *args **kwargs
def selectSome strings requiredsubstrings [] requireAll True if len requiredsubstrings 0 return stringsres []for s in strings if requireAll bad Falsefor rs in requiredsubstrings if s find rs < 0 bad Truebreakif not bad res append s else for rs in requiredsubstrings if s find rs > 0 res append s breakreturn res
def get_theme_paths themes theme_dirs theme_paths []for theme in themes theme_base_dirs get_theme_base_dirs theme theme_dirs if not theme_base_dirs print "\x1b[91m\nSkipping'{theme}' \nTheme {theme} notfoundinanyofthethemedirs {theme_dirs} \x1b[00m" format theme theme theme_dirs ' ' join theme_dirs theme_paths extend theme_base_dirs return theme_paths
def user_absent name ret {'name' name 'result' False 'changes' {} 'comment' ''}old_user __salt__['nxos cmd'] 'get_user' username name if not old_user ret['result'] Trueret['comment'] 'Userdoesnotexist'return retif __opts__['test'] is True and old_user ret['result'] Noneret['comment'] 'Userwillberemoved'ret['changes']['old'] old_userret['changes']['new'] ''return ret__salt__['nxos cmd'] 'remove_user' username name if __salt__['nxos cmd'] 'get_user' username name ret['comment'] 'Failedtoremoveuser'else ret['result'] Trueret['comment'] 'Userremoved'ret['changes']['old'] old_userret['changes']['new'] ''return ret
def _RuleInputsAndOutputs rule trigger_file raw_inputs _FixPaths rule get 'inputs' [] raw_outputs _FixPaths rule get 'outputs' [] inputs OrderedSet outputs OrderedSet inputs add trigger_file for i in raw_inputs inputs add _RuleExpandPath i trigger_file for o in raw_outputs outputs add _RuleExpandPath o trigger_file return inputs outputs
def survey_getAllQuestionsForTemplate template_id s3db current s3dbsectable s3db survey_sectionq_ltable s3db survey_question_listqsntable s3db survey_questionquery q_ltable template_id template_id & q_ltable section_id sectable id & q_ltable question_id qsntable id rows current db query select qsntable id qsntable code qsntable name qsntable type sectable name q_ltable posn orderby q_ltable posn questions []for row in rows question {}question_row row survey_questionquestion['qstn_id'] question_row idquestion['code'] question_row codequestion['name'] s3db survey_qstn_name_represent question_row name question['type'] question_row typequestion['posn'] row survey_question_list posnquestion['section'] row survey_section namequestions append question return questions
def _duplicate_hits_generator timestamps **kwargs while True yield generate_hits timestamps **kwargs
def _find_matching_button category component_type buttons world css_find 'div new-component-{}button' format category matched_buttons [btn for btn in buttons if btn text component_type ]assert_equal len matched_buttons 1 return matched_buttons[0]
def _get_time time tzinfo None if time is None time datetime utcnow elif isinstance time number_types time datetime utcfromtimestamp time if time tzinfo is None time time replace tzinfo UTC if isinstance time datetime if tzinfo is not None time time astimezone tzinfo if hasattr tzinfo 'normalize' time tzinfo normalize time time time timetz elif tzinfo is not None time time replace tzinfo tzinfo return time
def print_actions actions sys stdout write _dumps actions encode 'utf-8' + '\n'
def http_content_security_policy http_server return "default-src'self''unsafe-inline''unsafe-eval' img-src'self'data "
def apply_units string units inter None final float blank_reg _BLANK_RE value_reg _VALUE_RE if inter is None inter finalfstring _BLANK_RE sub '' string if not fstring and _VALIDATION_RE match fstring raise ValueError 'Invalidunitstring %r ' % string values []for match in value_reg finditer fstring dic match groupdict lit unit dic['value'] dic get 'unit' value inter lit if unit is not None try value * units[unit lower ]except KeyError raise KeyError 'invalidunit%s validunitsare%s' % unit units keys values append value return final sum values
def apply_units string units inter None final float blank_reg _BLANK_RE value_reg _VALUE_RE if inter is None inter finalfstring _BLANK_RE sub '' string if not fstring and _VALIDATION_RE match fstring raise ValueError 'Invalidunitstring %r ' % string values []for match in value_reg finditer fstring dic match groupdict lit unit dic['value'] dic get 'unit' value inter lit if unit is not None try value * units[unit lower ]except KeyError raise KeyError 'invalidunit%s validunitsare%s' % unit units keys values append value return final sum values
@world absorbdef css_contains_text css_selector partial_text index 0 if partial_text wait_for lambda _ css_html css_selector index index timeout 8 actual_text css_html css_selector index index return partial_text in actual_text
@world absorbdef css_contains_text css_selector partial_text index 0 if partial_text wait_for lambda _ css_html css_selector index index timeout 8 actual_text css_html css_selector index index return partial_text in actual_text
def single_source_bellman_ford G source target None cutoff None weight 'weight' if source target return {source 0} {source [source]} weight _weight_function G weight paths {source [source]}return _bellman_ford G [source] weight paths paths cutoff cutoff target target paths
def _get_admin_info command host None core_name None url _format_url 'admin/{0}' format command host core_name core_name resp _http_request url return resp
def _get_admin_info command host None core_name None url _format_url 'admin/{0}' format command host core_name core_name resp _http_request url return resp
def check_estimator Estimator name Estimator __name__check_parameters_default_constructible name Estimator for check in _yield_all_checks name Estimator try check name Estimator except SkipTest as message warnings warn message SkipTestWarning
def check_estimator Estimator name Estimator __name__check_parameters_default_constructible name Estimator for check in _yield_all_checks name Estimator try check name Estimator except SkipTest as message warnings warn message SkipTestWarning
def combine_common_sections data sections []sections_dict {}for each in data if each[u'label'] not in sections_dict sections_dict[each[u'label']] eachsections append each else sections_dict[each[u'label']][u'items'] + each[u'items']return sections
def __get_size conn vm_ size config get_cloud_config_value 'size' vm_ __opts__ default 'n1-standard-1' search_global False return conn ex_get_size size __get_location conn vm_
def __get_size conn vm_ size config get_cloud_config_value 'size' vm_ __opts__ default 'n1-standard-1' search_global False return conn ex_get_size size __get_location conn vm_
def __get_size conn vm_ size config get_cloud_config_value 'size' vm_ __opts__ default 'n1-standard-1' search_global False return conn ex_get_size size __get_location conn vm_
def htmldiff_tokens html1_tokens html2_tokens s InsensitiveSequenceMatcher a html1_tokens b html2_tokens commands s get_opcodes result []for command i1 i2 j1 j2 in commands if command 'equal' result extend expand_tokens html2_tokens[j1 j2] equal True continueif command 'insert' or command 'replace' ins_tokens expand_tokens html2_tokens[j1 j2] merge_insert ins_tokens result if command 'delete' or command 'replace' del_tokens expand_tokens html1_tokens[i1 i2] merge_delete del_tokens result result cleanup_delete result return result
def _cluster_has_pending_steps steps return any step status state 'PENDING' for step in steps
def create_ma deg_f deg_g row1 row2 col_num if deg_g - deg_f > 1 print 'Reversedegrees' returnm zeros deg_f - deg_g + 2 col_num for i in range deg_f - deg_g + 1 m[i ] rotate_r row1 i m[ deg_f - deg_g + 1 ] row2return m
def group path use_sudo False func use_sudo and run_as_root or run with settings hide 'running' 'stdout' warn_only True result func 'stat-c%%G"% path s"' % locals if result failed and 'stat illegaloption' in result return func 'stat-f%%Sg"% path s"' % locals else return result
def ClusterController *args **kwargs controller Controller *args **kwargs Intf 'eth0' node controller updateIP return controller
def ClusterController *args **kwargs controller Controller *args **kwargs Intf 'eth0' node controller updateIP return controller
def add_ngram sequences token_indice ngram_range 2 new_sequences []for input_list in sequences new_list input_list[ ]for i in range len new_list - ngram_range + 1 for ngram_value in range 2 ngram_range + 1 ngram tuple new_list[i i + ngram_value ] if ngram in token_indice new_list append token_indice[ngram] new_sequences append new_list return new_sequences
def block_device_mapping_get_all_by_volume_id context volume_id columns_to_join None return IMPL block_device_mapping_get_all_by_volume_id context volume_id columns_to_join
def drop_empty_translationprojects apps schema_editor Directory apps get_model u'pootle_app Directory' TP apps get_model u'pootle_translationproject TranslationProject' for tp in TP objects all if not Directory objects filter pootle_path tp pootle_path exists tp delete
def setup_platform hass config add_devices discovery_info None from panasonic_viera import RemoteControlmac config get CONF_MAC name config get CONF_NAME port config get CONF_PORT if discovery_info _LOGGER debug '%s' discovery_info vals discovery_info split ' ' if len vals > 1 port vals[1]host vals[0]remote RemoteControl host port add_devices [PanasonicVieraTVDevice mac name remote ] return Truehost config get CONF_HOST remote RemoteControl host port add_devices [PanasonicVieraTVDevice mac name remote ] return True
def restore_disks job restore False disk_list None if restore and disk_list is not None prepare_disks job 'ext2/-q-i20480-m1//restore_ext2' disk1_only False disk_list disk_list
def restore_disks job restore False disk_list None if restore and disk_list is not None prepare_disks job 'ext2/-q-i20480-m1//restore_ext2' disk1_only False disk_list disk_list
def delete connect_spec dn l connect connect_spec log info 'deletingentry dn {0}' format repr dn try l c delete_s dn except ldap LDAPError as e _convert_exception e return True
@preprocess engine coerce_string_to_eng def downgrade engine desired_version with engine begin as conn metadata sa MetaData conn metadata reflect version_info_table metadata tables['version_info']starting_version sa select version_info_table c version scalar if starting_version < desired_version raise AssetDBImpossibleDowngrade db_version starting_version desired_version desired_version if starting_version desired_version returnctx MigrationContext configure conn op Operations ctx downgrade_keys range desired_version starting_version [ -1 ]_pragma_foreign_keys conn False for downgrade_key in downgrade_keys _downgrade_methods[downgrade_key] op conn version_info_table _pragma_foreign_keys conn True
def add_extra_output_destination writeable_object important_level 0 name None global extra_print_destsextra_print_dests append {'dest' writeable_object 'name' name 'important_level' important_level}
def record_played_exploration_in_collection_context user_id collection_id exploration_id progress_model user_models CollectionProgressModel get_or_create user_id collection_id if exploration_id not in progress_model completed_explorations progress_model completed_explorations append exploration_id progress_model put
def record_played_exploration_in_collection_context user_id collection_id exploration_id progress_model user_models CollectionProgressModel get_or_create user_id collection_id if exploration_id not in progress_model completed_explorations progress_model completed_explorations append exploration_id progress_model put
def assert_rpm_headers test_case expected_headers rpm_path output check_output ['rpm' '--query' '--info' '--package' rpm_path path] actual_headers parse_colon_dict output assert_dict_contains test_case expected_headers actual_headers 'MissingRPMHeaders '
def list_remove_repeat l None l2 [][l2 append i for i in l if not i in l2 ]return l2
def merge file feature_layers coord layers []for layer in feature_layers layers append get_feature_layer layer['name'] layer['features'] data mapbox_vector_tile encode layers file write data
def _keys_match attribute def key_match_invariant pmap if pmap is None return True '' for key value in pmap items if key getattr value attribute return False '{}isnotcorrectkeyfor{}' format key value return True '' return key_match_invariant
def _keys_match attribute def key_match_invariant pmap if pmap is None return True '' for key value in pmap items if key getattr value attribute return False '{}isnotcorrectkeyfor{}' format key value return True '' return key_match_invariant
def camelcase_to_snakecase string_to_convert s1 re sub ' [A-Z][a-z]+ ' '\\1_\\2' string_to_convert return re sub ' [a-z0-9] [A-Z] ' '\\1_\\2' s1 lower
def camelcase_to_snakecase string_to_convert s1 re sub ' [A-Z][a-z]+ ' '\\1_\\2' string_to_convert return re sub ' [a-z0-9] [A-Z] ' '\\1_\\2' s1 lower
def CreateUserSecret user secret base64 b32encode os urandom _SECRET_BYTES secrets PutSecret _SecretName user secret DisplayUserSecret user
def recursive_set_rng_kwarg expr rng None if rng is None rng np random RandomState lrng as_apply rng for node in dfs expr if node name in implicit_stochastic_symbols for ii name arg in enumerate list node named_args if name 'rng' node named_args[ii] 'rng' lrng breakelse node named_args append 'rng' lrng return expr
def recursive_set_rng_kwarg expr rng None if rng is None rng np random RandomState lrng as_apply rng for node in dfs expr if node name in implicit_stochastic_symbols for ii name arg in enumerate list node named_args if name 'rng' node named_args[ii] 'rng' lrng breakelse node named_args append 'rng' lrng return expr
def recursive_set_rng_kwarg expr rng None if rng is None rng np random RandomState lrng as_apply rng for node in dfs expr if node name in implicit_stochastic_symbols for ii name arg in enumerate list node named_args if name 'rng' node named_args[ii] 'rng' lrng breakelse node named_args append 'rng' lrng return expr
def get_default_support_url request None force_is_admin False siteconfig SiteConfiguration objects get_current if siteconfig get u'send_support_usage_stats' support_data serialize_support_data request force_is_admin else support_data u''return settings DEFAULT_SUPPORT_URL % {u'support_data' support_data}
def _extract_files_to_lint file_diffs if not file_diffs return []lint_files [f name for f in file_diffs if f status upper in 'ACMRT' ]return lint_files
def _extract_files_to_lint file_diffs if not file_diffs return []lint_files [f name for f in file_diffs if f status upper in 'ACMRT' ]return lint_files
def expand_tabs file0 str_file_contents open file0 'rb' read str_pep_contents str_file_contents replace ' DCTB ' 4 * '' open file0 'wb' write str_pep_contents return None
def test_hashed_install_failure script data tmpdir with requirements_file 'simple2 1 0--hash sha256 9336af72ca661e6336eb87bc7de3e8844d853e3848c2b9bbd2e8bf01db88c2c\n' tmpdir as reqs_file result script pip_install_local '-r' reqs_file abspath expect_error True assert len result files_created 0
def test_hashed_install_failure script data tmpdir with requirements_file 'simple2 1 0--hash sha256 9336af72ca661e6336eb87bc7de3e8844d853e3848c2b9bbd2e8bf01db88c2c\n' tmpdir as reqs_file result script pip_install_local '-r' reqs_file abspath expect_error True assert len result files_created 0
def wait_for_scrub path http httplib2 Http wait_for 300check_every 15for _ in xrange wait_for / check_every time sleep check_every response content http request path 'HEAD' if response['x-image-meta-status'] 'deleted' and response['x-image-meta-deleted'] 'True' breakelse continueelse self fail 'imagewasneverscrubbed'
def wait_for_scrub path http httplib2 Http wait_for 300check_every 15for _ in xrange wait_for / check_every time sleep check_every response content http request path 'HEAD' if response['x-image-meta-status'] 'deleted' and response['x-image-meta-deleted'] 'True' breakelse continueelse self fail 'imagewasneverscrubbed'
def wait_for_scrub path http httplib2 Http wait_for 300check_every 15for _ in xrange wait_for / check_every time sleep check_every response content http request path 'HEAD' if response['x-image-meta-status'] 'deleted' and response['x-image-meta-deleted'] 'True' breakelse continueelse self fail 'imagewasneverscrubbed'
def synchronized func def caller *params **kparams _lock acquire True try return func *params **kparams finally _lock release return caller
def synchronized func def caller *params **kparams _lock acquire True try return func *params **kparams finally _lock release return caller
def getFunctionsWithStringByFileName fileName searchString fileText archive getFileText fileName functions []lines archive getTextLines fileText for line in lines lineStripped line strip if lineStripped startswith 'def' and searchString in lineStripped if ' self ' not in lineStripped or lineStripped count ' ' > 1 functions append lineStripped[len 'def' ] strip functions sort return functions
@transaction non_atomic_requests@require_POST@ensure_csrf_cookie@cache_control no_cache True no_store True must_revalidate True @require_level 'staff' def problem_grade_report request course_id course_key SlashSeparatedCourseKey from_deprecated_string course_id try lms djangoapps instructor_task api submit_problem_grade_report request course_key success_status _ 'Theproblemgradereportisbeingcreated Toviewthestatusofthereport seePendingTasksbelow ' return JsonResponse {'status' success_status} except AlreadyRunningError already_running_status _ 'Aproblemgradereportisalreadybeinggenerated Toviewthestatusofthereport seePendingTasksbelow Youwillbeabletodownloadthereportwhenitiscomplete ' return JsonResponse {'status' already_running_status}
@transaction non_atomic_requests@require_POST@ensure_csrf_cookie@cache_control no_cache True no_store True must_revalidate True @require_level 'staff' def problem_grade_report request course_id course_key SlashSeparatedCourseKey from_deprecated_string course_id try lms djangoapps instructor_task api submit_problem_grade_report request course_key success_status _ 'Theproblemgradereportisbeingcreated Toviewthestatusofthereport seePendingTasksbelow ' return JsonResponse {'status' success_status} except AlreadyRunningError already_running_status _ 'Aproblemgradereportisalreadybeinggenerated Toviewthestatusofthereport seePendingTasksbelow Youwillbeabletodownloadthereportwhenitiscomplete ' return JsonResponse {'status' already_running_status}
def update_cache_slug cache_slug get_unique_id with open 'cache_slug yaml' 'r' as f content f read os remove 'cache_slug yaml' content content replace 'default' cache_slug with open 'cache_slug yaml' 'w+' as d d write content
def enable_api_key apiKey region None key None keyid None profile None try conn _get_conn region region key key keyid keyid profile profile response _api_key_patch_replace conn apiKey '/enabled' 'True' return {'apiKey' _convert_datetime_str response }except ClientError as e return {'error' salt utils boto3 get_error e }
def from_file f if sys hexversion > 33751040 str_type basestringopts 'rU'else str_type stropts 'r'if isinstance f str_type f file f opts want_close Trueelse want_close Falsetry m from_text f finally if want_close f close return m
def setup_platform hass config add_devices discovery_info None import steam as steamodsteamod api key set config get CONF_API_KEY add_devices [SteamSensor account steamod for account in config get CONF_ACCOUNTS ]
def get_trash_interval return get_conf get _CNF_TRASH_INTERVAL
def get_trash_interval return get_conf get _CNF_TRASH_INTERVAL
def _partition_estimators n_estimators n_jobs n_jobs min _get_n_jobs n_jobs n_estimators n_estimators_per_job n_estimators // n_jobs * np ones n_jobs dtype np int n_estimators_per_job[ n_estimators % n_jobs ] + 1starts np cumsum n_estimators_per_job return n_jobs n_estimators_per_job tolist [0] + starts tolist
def detect_range line None if '[' in line return Trueelse return False
def is_meta_resource_type_mutable context meta_resource_type if context is_admin return Trueif context owner is None return Falseif meta_resource_type namespace return meta_resource_type namespace owner context owner else return False
def resample_to_csv barFeed frequency csvFile assert frequency > 0 'Invalidfrequency'resample_impl barFeed frequency csvFile
def resample_to_csv barFeed frequency csvFile assert frequency > 0 'Invalidfrequency'resample_impl barFeed frequency csvFile
def istextfile fp_ blocksize 512 int2byte lambda x bytes x if six PY3 else chr text_characters '' join int2byte i for i in range 32 127 + '\n\r DCTB \x0c\x08' try block fp_ read blocksize except AttributeError try with fopen fp_ 'rb' as fp2_ block fp2_ read blocksize except IOError return Falseif '\x00' in block return Falseelif not block return Truetry block decode 'utf-8' return Trueexcept UnicodeDecodeError passnontext block translate None text_characters return float len nontext / len block < 0 3
def istextfile fp_ blocksize 512 int2byte lambda x bytes x if six PY3 else chr text_characters '' join int2byte i for i in range 32 127 + '\n\r DCTB \x0c\x08' try block fp_ read blocksize except AttributeError try with fopen fp_ 'rb' as fp2_ block fp2_ read blocksize except IOError return Falseif '\x00' in block return Falseelif not block return Truetry block decode 'utf-8' return Trueexcept UnicodeDecodeError passnontext block translate None text_characters return float len nontext / len block < 0 3
def get_issues_list project auth False **params params setdefault 'state' 'closed' url 'https //api github com/repos/{project}/issues' format project project if auth headers make_auth_header else headers Nonepages get_paged_request url headers headers **params return pages
def animal_pre_save_check signal sender instance **kwargs pre_save_checks append 'Count %s %s ' % instance count type instance count 'Weight %s %s ' % instance weight type instance weight
def animal_pre_save_check signal sender instance **kwargs pre_save_checks append 'Count %s %s ' % instance count type instance count 'Weight %s %s ' % instance weight type instance weight
def _SkipVarint buffer pos end while ord buffer[pos] & 128 pos + 1pos + 1if pos > end raise _DecodeError 'Truncatedmessage ' return pos
def get_voted user_or_id model obj_type apps get_model 'contenttypes' 'ContentType' objects get_for_model model conditions 'votes_vote content_type_id %s' '%s id votes_vote object_id' % model _meta db_table 'votes_vote user_id %s' if isinstance user_or_id get_user_model user_id user_or_id idelse user_id user_or_idreturn model objects extra where conditions tables 'votes_vote' params obj_type id user_id
def _script_names dist script_name is_gui if dist_in_usersite dist bin_dir bin_userelse bin_dir bin_pyexe_name os path join bin_dir script_name paths_to_remove [exe_name]if WINDOWS paths_to_remove append exe_name + ' exe' paths_to_remove append exe_name + ' exe manifest' if is_gui paths_to_remove append exe_name + '-script pyw' else paths_to_remove append exe_name + '-script py' return paths_to_remove
def get_ha1_dict_plain user_password_dict def get_ha1 realm username password user_password_dict get username if password return md5_hex '%s %s %s' % username realm password return Nonereturn get_ha1
def config_read_reseller_options conf defaults reseller_prefix_opt conf get 'reseller_prefix' 'AUTH' split ' ' reseller_prefixes []for prefix in [pre strip for pre in reseller_prefix_opt if pre strip ] if prefix "''" prefix ''prefix append_underscore prefix if prefix not in reseller_prefixes reseller_prefixes append prefix if len reseller_prefixes 0 reseller_prefixes append '' associated_options {}for prefix in reseller_prefixes associated_options[prefix] dict defaults associated_options[prefix] update config_read_prefixed_options conf '' defaults prefix_name prefix if prefix '' else "''" associated_options[prefix] update config_read_prefixed_options conf prefix_name defaults return reseller_prefixes associated_options
def plotHistogram freqCounts title 'On-TimesHistogram' xLabel 'On-Time' import pylabpylab ion pylab figure pylab bar numpy arange len freqCounts - 0 5 freqCounts pylab title title pylab xlabel xLabel
def plotHistogram freqCounts title 'On-TimesHistogram' xLabel 'On-Time' import pylabpylab ion pylab figure pylab bar numpy arange len freqCounts - 0 5 freqCounts pylab title title pylab xlabel xLabel
def plotHistogram freqCounts title 'On-TimesHistogram' xLabel 'On-Time' import pylabpylab ion pylab figure pylab bar numpy arange len freqCounts - 0 5 freqCounts pylab title title pylab xlabel xLabel
def collect_command host command dest_path logging info "Collecting'%s' " command devnull open '/dev/null' 'w' try result host run command stdout_tee devnull stdoututils open_write_close dest_path result except Exception as e logging warning "Collectionof'%s'failed \n%s" command e finally devnull close
def make_null_event_date_events all_sids timestamp return pd DataFrame {'sid' all_sids 'timestamp' timestamp 'event_date' pd Timestamp 'NaT' 'float' -9999 0 'int' -9999 'datetime' pd Timestamp '1980' 'string' 'shouldbeignored'}
def get_expr_params operator if operator type lo PARAM return operator data parameters else params []for arg in operator args params + get_expr_params arg if isinstance operator data lo LinOp params + get_expr_params operator data return params
@decorators memoizedef _get_version xpath _check_xbps version_string __salt__['cmd run'] '{0}--version' format xpath output_loglevel 'trace' if version_string is None return FalseVERSION_MATCH re compile ' ? XBPS [\\s]+ [\\d ]+ ? [\\s]+ * ' version_match VERSION_MATCH search version_string if not version_match return Falsereturn version_match group 1 split ' '
def BuildCampaignCriterionOperations campaign_operations criterion_operations [{'xsi_type' 'CampaignCriterionOperation' 'operand' {'xsi_type' 'NegativeCampaignCriterion' 'campaignId' campaign_operation['operand']['id'] 'criterion' {'xsi_type' 'Keyword' 'matchType' 'BROAD' 'text' 'venus'}} 'operator' 'ADD'} for campaign_operation in campaign_operations]return criterion_operations
def open_logfile filename mode 'a' filename os path expanduser filename filename os path abspath filename dirname os path dirname filename if not os path exists dirname os makedirs dirname exists os path exists filename log_fp open filename mode if exists log_fp write '%s\n' % '-' * 60 log_fp write '%srunon%s\n' % sys argv[0] time strftime '%c' return log_fp
def open_logfile filename mode 'a' filename os path expanduser filename filename os path abspath filename dirname os path dirname filename if not os path exists dirname os makedirs dirname exists os path exists filename log_fp open filename mode if exists log_fp write '%s\n' % '-' * 60 log_fp write '%srunon%s\n' % sys argv[0] time strftime '%c' return log_fp
def _expand_node node ret {}ret update node __dict__ try del ret['extra']['boot_disk']except Exception passzone ret['extra']['zone']ret['extra']['zone'] {}ret['extra']['zone'] update zone __dict__ return ret
def validate cls instance_or_dict trusted_data None partial False strict False convert True context None **kwargs context context or get_validation_context partial partial strict strict convert convert errors {}try data import_loop cls instance_or_dict trusted_data trusted_data context context **kwargs except DataError as exc errors exc messagesdata exc partial_dataerrors update _validate_model cls data context if errors raise DataError errors data return data
def validate cls instance_or_dict trusted_data None partial False strict False convert True context None **kwargs context context or get_validation_context partial partial strict strict convert convert errors {}try data import_loop cls instance_or_dict trusted_data trusted_data context context **kwargs except DataError as exc errors exc messagesdata exc partial_dataerrors update _validate_model cls data context if errors raise DataError errors data return data
def translate x y rv eye 3 rv[ 2 0 ] xrv[ 2 1 ] yreturn rv
def translate x y rv eye 3 rv[ 2 0 ] xrv[ 2 1 ] yreturn rv
@dec onlyif has_pywin32 'Thistestrequireswin32apitorun' def test_find_cmd_pythonw path find_cmd 'pythonw' assert path lower endswith 'pythonw exe' path
def make_simple_equity_info sids start_date end_date symbols None num_assets len sids if symbols is None symbols list ascii_uppercase[ num_assets] return pd DataFrame {'symbol' list symbols 'start_date' pd to_datetime [start_date] * num_assets 'end_date' pd to_datetime [end_date] * num_assets 'exchange' 'TEST' 'exchange_full' 'TESTFULL'} index sids columns 'start_date' 'end_date' 'symbol' 'exchange' 'exchange_full'
def make_simple_equity_info sids start_date end_date symbols None num_assets len sids if symbols is None symbols list ascii_uppercase[ num_assets] return pd DataFrame {'symbol' list symbols 'start_date' pd to_datetime [start_date] * num_assets 'end_date' pd to_datetime [end_date] * num_assets 'exchange' 'TEST' 'exchange_full' 'TESTFULL'} index sids columns 'start_date' 'end_date' 'symbol' 'exchange' 'exchange_full'
def make_simple_equity_info sids start_date end_date symbols None num_assets len sids if symbols is None symbols list ascii_uppercase[ num_assets] return pd DataFrame {'symbol' list symbols 'start_date' pd to_datetime [start_date] * num_assets 'end_date' pd to_datetime [end_date] * num_assets 'exchange' 'TEST' 'exchange_full' 'TESTFULL'} index sids columns 'start_date' 'end_date' 'symbol' 'exchange' 'exchange_full'
def get_non_generated_file_lines lines_to_copy []flag_found Falsewith open ' /plotly/graph_objs/graph_objs py' 'r' as f for line_to_copy in f if line_to_copy startswith FLAG flag_found Truebreaklines_to_copy append line_to_copy if not flag_found raise ValueError 'Failedtofindflag \n"{}"\ningraph_objs_tools py ' format FLAG return lines_to_copy
def get_non_generated_file_lines lines_to_copy []flag_found Falsewith open ' /plotly/graph_objs/graph_objs py' 'r' as f for line_to_copy in f if line_to_copy startswith FLAG flag_found Truebreaklines_to_copy append line_to_copy if not flag_found raise ValueError 'Failedtofindflag \n"{}"\ningraph_objs_tools py ' format FLAG return lines_to_copy
def force_implementation modname global implementationfor name spec in [ e[0] e for e in _modules] if name modname implementation _JsonImplementation spec returnraise ImportError 'Nomodulenamed %s' % modname
def force_implementation modname global implementationfor name spec in [ e[0] e for e in _modules] if name modname implementation _JsonImplementation spec returnraise ImportError 'Nomodulenamed %s' % modname
def create_api_model restApiId modelName modelDescription schema contentType 'application/json' region None key None keyid None profile None try schema_json json dumps schema if isinstance schema dict else schema conn _get_conn region region key key keyid keyid profile profile model conn create_model restApiId restApiId name modelName description modelDescription schema schema_json contentType contentType return {'created' True 'model' _convert_datetime_str model }except ClientError as e return {'created' False 'error' salt utils boto3 get_error e }
def create_api_model restApiId modelName modelDescription schema contentType 'application/json' region None key None keyid None profile None try schema_json json dumps schema if isinstance schema dict else schema conn _get_conn region region key key keyid keyid profile profile model conn create_model restApiId restApiId name modelName description modelDescription schema schema_json contentType contentType return {'created' True 'model' _convert_datetime_str model }except ClientError as e return {'created' False 'error' salt utils boto3 get_error e }
def create_api_model restApiId modelName modelDescription schema contentType 'application/json' region None key None keyid None profile None try schema_json json dumps schema if isinstance schema dict else schema conn _get_conn region region key key keyid keyid profile profile model conn create_model restApiId restApiId name modelName description modelDescription schema schema_json contentType contentType return {'created' True 'model' _convert_datetime_str model }except ClientError as e return {'created' False 'error' salt utils boto3 get_error e }
def _get_block_types_from_json_file xblock_json_file if not os path isfile xblock_json_file print 'xBlockconfigurationfiledoesnotexist %s' % xblock_json_file sys exit 2 with open xblock_json_file 'r' as json_file type_set set try json_data json loads json_file read except ValueError as e print 'xBlockconfigurationfiledoesnotmatchtheexpectedlayoutandismissing"data"list %s' % xblock_json_file sys exit e message if 'data' in json_data xblock_type_list json_data['data']for xblock in xblock_type_list type_set add xblock['name'] return type_setelse print 'xBlockconfigurationfiledoesnotmatchtheexpectedlayoutandismissing"data"list %s' % xblock_json_file sys exit 2
def get_public_task_data request task_id try task state info get_task_data task_id except TaskNotFound raise TaskNoPermission task_id if not hasattr task 'check_permission' raise TaskNoPermission task_id context info get 'context' {} if not task check_permission request state context raise TaskNoPermission task_id public_name task public_namereturn public_name state info get 'public_data' {} info get 'error' None
def get_public_task_data request task_id try task state info get_task_data task_id except TaskNotFound raise TaskNoPermission task_id if not hasattr task 'check_permission' raise TaskNoPermission task_id context info get 'context' {} if not task check_permission request state context raise TaskNoPermission task_id public_name task public_namereturn public_name state info get 'public_data' {} info get 'error' None
def get_public_task_data request task_id try task state info get_task_data task_id except TaskNotFound raise TaskNoPermission task_id if not hasattr task 'check_permission' raise TaskNoPermission task_id context info get 'context' {} if not task check_permission request state context raise TaskNoPermission task_id public_name task public_namereturn public_name state info get 'public_data' {} info get 'error' None
def is_running_from_reloader return os environ get 'WERKZEUG_RUN_MAIN' 'true'
def can_browse_repository_reviews app user repository if user for review in repository reviews for component_review in review component_reviews if app security_agent user_can_browse_component_review app repository component_review user return Truereturn False
def can_browse_repository_reviews app user repository if user for review in repository reviews for component_review in review component_reviews if app security_agent user_can_browse_component_review app repository component_review user return Truereturn False
def remove_arg args arg has_param False for idx found_arg in enumerate args if found_arg arg if has_param slice_idx idx + 2 else slice_idx idx + 1 args args[ idx] + args[slice_idx ] breakreturn args
def count_from_1 index collection return index + 1
def create_tip_index tree if hasattr tree '_tip_index' returnelse tree _tip_index {n Name n for n in tree tips }
def generate_gjrgarch nobs ar ma mu 1 0 scale 0 1 varinnovation None if varinnovation is None eta scale * np random randn nobs else eta varinnovationetax np empty nobs 3 etax[ 0] muetax[ 1 ] eta ** 2 [ None]etax[ eta > 0 2 ] 0h miso_lfilter ar ma etax [0]err np sqrt h[ len eta ] * eta return err h etax
def generate_gjrgarch nobs ar ma mu 1 0 scale 0 1 varinnovation None if varinnovation is None eta scale * np random randn nobs else eta varinnovationetax np empty nobs 3 etax[ 0] muetax[ 1 ] eta ** 2 [ None]etax[ eta > 0 2 ] 0h miso_lfilter ar ma etax [0]err np sqrt h[ len eta ] * eta return err h etax
def train_student dataset nb_teachers assert input create_dir_if_needed FLAGS train_dir stdnt_dataset prepare_student_data dataset nb_teachers save True stdnt_data stdnt_labels stdnt_test_data stdnt_test_labels stdnt_datasetif FLAGS deeper ckpt_path FLAGS train_dir + '/' + str dataset + '_' + str nb_teachers + '_student_deeper ckpt' else ckpt_path FLAGS train_dir + '/' + str dataset + '_' + str nb_teachers + '_student ckpt' assert deep_cnn train stdnt_data stdnt_labels ckpt_path ckpt_path_final ckpt_path + '-' + str FLAGS max_steps - 1 student_preds deep_cnn softmax_preds stdnt_test_data ckpt_path_final precision metrics accuracy student_preds stdnt_test_labels print 'Precisionofstudentaftertraining ' + str precision return True
def train_student dataset nb_teachers assert input create_dir_if_needed FLAGS train_dir stdnt_dataset prepare_student_data dataset nb_teachers save True stdnt_data stdnt_labels stdnt_test_data stdnt_test_labels stdnt_datasetif FLAGS deeper ckpt_path FLAGS train_dir + '/' + str dataset + '_' + str nb_teachers + '_student_deeper ckpt' else ckpt_path FLAGS train_dir + '/' + str dataset + '_' + str nb_teachers + '_student ckpt' assert deep_cnn train stdnt_data stdnt_labels ckpt_path ckpt_path_final ckpt_path + '-' + str FLAGS max_steps - 1 student_preds deep_cnn softmax_preds stdnt_test_data ckpt_path_final precision metrics accuracy student_preds stdnt_test_labels print 'Precisionofstudentaftertraining ' + str precision return True
def ValidateActionsInTarget target target_dict build_file target_name target_dict get 'target_name' actions target_dict get 'actions' [] for action in actions action_name action get 'action_name' if not action_name raise GypError "Anonymousactionintarget%s Anactionmusthavean'action_name'field " % target_name inputs action get 'inputs' None if inputs is None raise GypError 'Actionintarget%shasnoinputs ' % target_name action_command action get 'action' if action_command and not action_command[0] raise GypError 'Emptyactionascommandintarget%s ' % target_name
def sort_sample_ids_by_mapping_value mapping_file field field_type_f float data headers comments parse_mapping_file mapping_file try column headers index field except ValueError raise ValueError 'Column %s notfoundinmappingfileheaders \n%s' % field '' join headers results [ e[0] field_type_f e[column] for e in data]results sort key itemgetter 1 return results
@task@timeitdef add_short_links doc_ids base_url 'https //{0}%s' format Site objects get_current domain docs Document objects filter id__in doc_ids try pin_this_thread for doc in docs endpoint django_reverse 'wiki document' args [doc slug] doc update share_link generate_short_url base_url % endpoint statsd incr 'wiki add_short_links success' except BitlyRateLimitException statsd incr 'wiki add_short_links rate_limited' passfinally unpin_this_thread
def test_nested_iteration t table Table [[0 1]] names ['a'] out []for r1 in t for r2 in t out append r1['a'] r2['a'] assert out [ 0 0 0 1 1 0 1 1 ]
def test_nested_iteration t table Table [[0 1]] names ['a'] out []for r1 in t for r2 in t out append r1['a'] r2['a'] assert out [ 0 0 0 1 1 0 1 1 ]
def absent name user None maintenance_db None db_password None db_host None db_port None db_user None ret {'name' name 'changes' {} 'result' True 'comment' ''}db_args {'maintenance_db' maintenance_db 'runas' user 'host' db_host 'user' db_user 'port' db_port 'password' db_password}if __salt__['postgres user_exists'] name **db_args if __opts__['test'] ret['result'] Noneret['comment'] 'Group{0}issettoberemoved' format name return retif __salt__['postgres group_remove'] name **db_args ret['comment'] 'Group{0}hasbeenremoved' format name ret['changes'][name] 'Absent'return retelse ret['comment'] 'Group{0}isnotpresent soitcannotberemoved' format name return ret
def absent name user None maintenance_db None db_password None db_host None db_port None db_user None ret {'name' name 'changes' {} 'result' True 'comment' ''}db_args {'maintenance_db' maintenance_db 'runas' user 'host' db_host 'user' db_user 'port' db_port 'password' db_password}if __salt__['postgres user_exists'] name **db_args if __opts__['test'] ret['result'] Noneret['comment'] 'Group{0}issettoberemoved' format name return retif __salt__['postgres group_remove'] name **db_args ret['comment'] 'Group{0}hasbeenremoved' format name ret['changes'][name] 'Absent'return retelse ret['comment'] 'Group{0}isnotpresent soitcannotberemoved' format name return ret
def allocate_pixels_buffer width height return _c_uint * width * height * 4
def allocate_pixels_buffer width height return _c_uint * width * height * 4
def detail device '/dev/md0' ret {}ret['members'] {}if not os path exists device msg "Device{0}doesn'texist "raise CommandExecutionError msg format device cmd ['mdadm' '--detail' device]for line in __salt__['cmd run_stdout'] cmd python_shell False splitlines if line startswith device continueif '' not in line continueif ' ' not in line if '/dev/' in line comps line split state comps[4 -1 ]ret['members'][comps[0]] {'device' comps[ -1 ] 'major' comps[1] 'minor' comps[2] 'number' comps[0] 'raiddevice' comps[3] 'state' '' join state }continuecomps line split ' ' comps[0] comps[0] lower comps[0] comps[0] strip comps[0] comps[0] replace '' '_' ret[comps[0]] comps[1] strip return ret
def _nova_to_osvif_subnet subnet dnsaddrs [ip['address'] for ip in subnet['dns']]obj objects subnet Subnet dns dnsaddrs ips _nova_to_osvif_ips subnet['ips'] routes _nova_to_osvif_routes subnet['routes'] if subnet['cidr'] is not None obj cidr subnet['cidr']if subnet['gateway'] is not None and subnet['gateway']['address'] is not None obj gateway subnet['gateway']['address']return obj
def tuple_eval source node ast parse source '<source>' mode 'eval' if not isinstance node body ast Tuple raise ValueError '%risnotatupleliteral' % source if not all isinstance el ast Str ast Num or isinstance el ast UnaryOp and isinstance el op ast UAdd ast USub and isinstance el operand ast Num for el in node body elts raise ValueError 'Canonlycontainnumbersorstrings' return literal_eval source
def tuple_eval source node ast parse source '<source>' mode 'eval' if not isinstance node body ast Tuple raise ValueError '%risnotatupleliteral' % source if not all isinstance el ast Str ast Num or isinstance el ast UnaryOp and isinstance el op ast UAdd ast USub and isinstance el operand ast Num for el in node body elts raise ValueError 'Canonlycontainnumbersorstrings' return literal_eval source
def get_meta_entry dist name meta get_dist_meta dist return meta get name
def fix_encoding ret Trueif sys platform 'win32' ret & fix_win_codec ret & fix_default_encoding if sys platform 'win32' encoding sys getdefaultencoding ret & fix_win_sys_argv encoding ret & fix_win_console encoding return ret
def leaves tree branches attrgetter 'branches' return node for node in postorder tree branches if node is_leaf
def leaves tree branches attrgetter 'branches' return node for node in postorder tree branches if node is_leaf
def disabled name ret {'name' name 'result' True 'comment' '' 'changes' {}}is_enabled __salt__['apache check_site_enabled'] name if is_enabled if __opts__['test'] msg 'Apachesite{0}issettobedisabled ' format name ret['comment'] msgret['changes']['old'] nameret['changes']['new'] Noneret['result'] Nonereturn retstatus __salt__['apache a2dissite'] name ['Status']if isinstance status string_types and 'disabled' in status ret['result'] Trueret['changes']['old'] nameret['changes']['new'] Noneelse ret['result'] Falseret['comment'] 'Failedtodisable{0}Apachesite' format name if isinstance status string_types ret['comment'] ret['comment'] + ' {0} ' format status return retelse ret['comment'] '{0}alreadydisabled ' format name return ret
def add_server protocol None service_address None server_address None packet_forward_method 'dr' weight 1 **kwargs cmd '{0}-a{1}' format __detect_os _build_cmd protocol protocol service_address service_address server_address server_address packet_forward_method packet_forward_method weight weight **kwargs out __salt__['cmd run_all'] cmd python_shell False if out['retcode'] ret out['stderr'] strip else ret Truereturn ret
def get_codon_alphabet alphabet gap '-' stop '*' from Bio Alphabet import NucleotideAlphabetif isinstance alphabet NucleotideAlphabet alpha alphabetif gap alpha Gapped alpha gap_char gap if stop alpha HasStopCodon alpha stop_symbol stop else raise TypeError 'OnlyNuclteotideAlphabetisaccepted ' return alpha
def get_ancestors x collection None if collection is None collection random_variables node_dict {node value node for node in collection}output set [] nodes set [x] while nodes node nodes pop if isinstance node RandomVariable node node value candidate_node node_dict get node None if candidate_node and candidate_node x output add candidate_node nodes update node op inputs return list output
def lookupNameservers name timeout None return getResolver lookupNameservers name timeout
def test_idempotent dirty u'<span>invalid&</span><extrahttp //link com<em>'clean bleach clean dirty eq_ clean bleach clean clean linked bleach linkify dirty eq_ linked bleach linkify linked
def test_idempotent dirty u'<span>invalid&</span><extrahttp //link com<em>'clean bleach clean dirty eq_ clean bleach clean clean linked bleach linkify dirty eq_ linked bleach linkify linked
def test_idempotent dirty u'<span>invalid&</span><extrahttp //link com<em>'clean bleach clean dirty eq_ clean bleach clean clean linked bleach linkify dirty eq_ linked bleach linkify linked
def test_idempotent dirty u'<span>invalid&</span><extrahttp //link com<em>'clean bleach clean dirty eq_ clean bleach clean clean linked bleach linkify dirty eq_ linked bleach linkify linked
def emit_via_redis event message room r get_redis_server try r publish u'events' frappe as_json {u'event' event u'message' message u'room' room} except redis exceptions ConnectionError pass
def mc2mnc mc n len mc mean mc[0]mc [1] + list mc mc[1] 0mnc [1 mean]for nn m in enumerate mc[2 ] n nn + 2 mnc append 0 for k in range n + 1 mnc[n] + comb n k exact 1 * mc[k] * mean ** n - k return mnc[1 ]
def mc2mnc mc n len mc mean mc[0]mc [1] + list mc mc[1] 0mnc [1 mean]for nn m in enumerate mc[2 ] n nn + 2 mnc append 0 for k in range n + 1 mnc[n] + comb n k exact 1 * mc[k] * mean ** n - k return mnc[1 ]
def device_writer queue while True device value queue get scaled rescale_value value[0] log debug 'Write %s %s' % device value if not device continue
def device_writer queue while True device value queue get scaled rescale_value value[0] log debug 'Write %s %s' % device value if not device continue
def device_writer queue while True device value queue get scaled rescale_value value[0] log debug 'Write %s %s' % device value if not device continue
def device_writer queue while True device value queue get scaled rescale_value value[0] log debug 'Write %s %s' % device value if not device continue
def uuid_bin_to_str uuid block1 block2 block3 struct unpack '<LHH' uuid[ 8] block4 block5 block6 struct unpack '>HHL' uuid[8 16] return '%08x-%04x-%04x-%04x-%04x%08x' % block1 block2 block3 block4 block5 block6
def _build_locale_table filename_or_file from xml dom minidom import parsedom parse filename_or_file reps dom getElementsByTagName 'representation' locs map lambda r r childNodes[0] data reps locale_map {}for loc in locs lang _ reg loc partition '_' lang_map locale_map setdefault lang {'regs' [] 'default' reg} lang_map['regs'] append reg locale_map['en']['default'] 'US'locale_map['es']['default'] 'LA'locale_map['zh']['default'] 'CN'locale_map['fr']['default'] 'FR'locale_map['pt']['default'] 'PT'return locale_map
def setup_pid_file config odoo tools configif not odoo evented and config['pidfile'] pid os getpid with open config['pidfile'] 'w' as fd fd write str pid atexit register rm_pid_file pid
def trial_division n prime_factors []if n < 2 return prime_factorsfor p in eratosthenes int n ** 0 5 + 1 if p * p > n breakwhile n % p 0 prime_factors append p n // pif n > 1 prime_factors append n return prime_factors
def trial_division n prime_factors []if n < 2 return prime_factorsfor p in eratosthenes int n ** 0 5 + 1 if p * p > n breakwhile n % p 0 prime_factors append p n // pif n > 1 prime_factors append n return prime_factors
def applyFilter data b a padding 100 bidir True try import scipy signalexcept ImportError raise Exception 'applyFilter requiresthepackagescipy signal ' d1 data view np ndarray if padding > 0 d1 np hstack [d1[ padding] d1 d1[ - padding ]] if bidir d1 scipy signal lfilter b a scipy signal lfilter b a d1 [ -1 ] [ -1 ]else d1 scipy signal lfilter b a d1 if padding > 0 d1 d1[padding - padding ]if hasattr data 'implements' and data implements 'MetaArray' return MetaArray d1 info data infoCopy else return d1
def printDecoderBriefs decoders printfor d in decoders values print 'Modulename ' d nameprint ' ' * 20 if d longdescription print d longdescriptionelse print d descriptionprint 'Defaultfilter %s' % d filter return
def accepts request media_type accept parse_accept_header request META get 'HTTP_ACCEPT' '' return media_type in [t for t p q in accept]
@db_api retry_if_session_inactive def remove_provisioning_component context object_id object_type entity standard_attr_id None with context session begin subtransactions True standard_attr_id standard_attr_id or _get_standard_attr_id context object_id object_type if not standard_attr_id return Falserecord context session query pb_model ProvisioningBlock filter_by standard_attr_id standard_attr_id entity entity first if record context session delete record return Truereturn False
def get_error_statistics return get_statistics 'E'
def create_folder folderName add_init_file True if os path exists folderName raise NinjaIOException u'Thefolderalreadyexist' os makedirs folderName if add_init_file create_init_file folderName
@core_helperdef get_pkg_dict_extra pkg_dict key default None extras pkg_dict['extras'] if 'extras' in pkg_dict else [] for extra in extras if extra['key'] key return extra['value']return default
def get_conn host None username None password None if not HAS_IMPACKET return Falseconn impacket smbconnection SMBConnection remoteName '*SMBSERVER' remoteHost host conn login user username password password return conn
def expand_sig sig if isinstance sig tuple if len sig 3 num_pos_only func keyword_only sigassert isinstance sig[ -1 ] tuple else num_pos_only func sigkeyword_only sigspec signature_or_spec func else func sigsigspec signature_or_spec func num_pos_only num_pos_args sigspec keyword_only keyword_exclude get_exclude_keywords num_pos_only sigspec return num_pos_only func keyword_only + keyword_exclude sigspec
def expand_sig sig if isinstance sig tuple if len sig 3 num_pos_only func keyword_only sigassert isinstance sig[ -1 ] tuple else num_pos_only func sigkeyword_only sigspec signature_or_spec func else func sigsigspec signature_or_spec func num_pos_only num_pos_args sigspec keyword_only keyword_exclude get_exclude_keywords num_pos_only sigspec return num_pos_only func keyword_only + keyword_exclude sigspec
def page_title_breadcrumbs *crumbs **kwargs platform_name get_value 'platform_name' settings PLATFORM_NAME separator kwargs get 'separator' ' ' crumbs [c for c in crumbs if c is not None ]if crumbs return u'{}{}{}' format separator join crumbs separator platform_name else return platform_name
def page_title_breadcrumbs *crumbs **kwargs platform_name get_value 'platform_name' settings PLATFORM_NAME separator kwargs get 'separator' ' ' crumbs [c for c in crumbs if c is not None ]if crumbs return u'{}{}{}' format separator join crumbs separator platform_name else return platform_name
def _parse_repo_file filename repos {}header ''repo ''with salt utils fopen filename 'r' as rfile for line in rfile if line startswith '[' repo line strip replace '[' '' replace ']' '' repos[repo] {}if not line if not repo header + lineif line startswith '#' if not repo header + lineelse if 'comments' not in repos[repo] repos[repo]['comments'] []repos[repo]['comments'] append line strip continueif ' ' in line try comps line strip split ' ' repos[repo][comps[0] strip ] ' ' join comps[1 ] except KeyError log error "Failedtoparselinein%s offendinglinewas'%s'" filename line rstrip if comps[0] strip 'enabled' repos[repo]['disabled'] comps[1] '1' return header repos
def welcome_osf4m email from website files models import OsfStorageFileNodeif email user date_last_login if email user date_last_login > timezone now - settings WELCOME_OSF4M_WAIT_TIME_GRACE return Falseupload OsfStorageFileNode load email data['fid'] if upload email data['downloads'] upload get_download_count else email data['downloads'] 0email save return True
def welcome_osf4m email from website files models import OsfStorageFileNodeif email user date_last_login if email user date_last_login > timezone now - settings WELCOME_OSF4M_WAIT_TIME_GRACE return Falseupload OsfStorageFileNode load email data['fid'] if upload email data['downloads'] upload get_download_count else email data['downloads'] 0email save return True
def is_prereg_admin user if user is not None return PREREG_ADMIN_TAG in getattr user 'system_tags' [] return False
def get_cython_version import Cython Compiler Mainmatch re search '^ [0-9]+ \\ [0-9]+ ' Cython Compiler Main Version version try return map int match groups except AttributeError raise ImportError
def get_cython_version import Cython Compiler Mainmatch re search '^ [0-9]+ \\ [0-9]+ ' Cython Compiler Main Version version try return map int match groups except AttributeError raise ImportError
def user_addmedia userids active mediatypeid period sendto severity **connection_args conn_args _login **connection_args try if conn_args method 'user addmedia'params {'users' []}if not isinstance userids list userids [userids]for user in userids params['users'] append {'userid' user} params['medias'] [{'active' active 'mediatypeid' mediatypeid 'period' period 'sendto' sendto 'severity' severity}]ret _query method params conn_args['url'] conn_args['auth'] return ret['result']['mediaids']else raise KeyErrorexcept KeyError return ret
def get_default_domain request get_name True domain_id request session get 'domain_context' None domain_name request session get 'domain_context_name' None if VERSIONS active > 3 and domain_id is None domain_id request user user_domain_iddomain_name request user user_domain_nameif get_name and not request user is_federated try domain domain_get request domain_id domain_name domain nameexcept exceptions NotAuthorized LOG debug 'Cannotretrievedomaininformationforuser %s thatdoesnothaveanadminroleonproject %s ' % request user username request user project_name except Exception LOG warning 'UnabletoretrieveDomain %s' % domain_id domain base APIDictWrapper {'id' domain_id 'name' domain_name} return domain
def run executor sql join False expanded False pgspecial None exception_formatter None results executor run sql pgspecial exception_formatter formatted []for title rows headers status sql success in results formatted extend format_output title rows headers status 'psql' dcmlfmt 'd' floatfmt 'g' expanded expanded if join formatted '\n' join formatted return formatted
def list_custom_images call None if call 'function' raise SaltCloudSystemExit 'Thelist_vlansfunctionmustbecalledwith-for--function ' ret {}conn get_conn 'SoftLayer_Account' response conn getBlockDeviceTemplateGroups for image in response if 'globalIdentifier' not in image continueret[image['name']] {'id' image['id'] 'name' image['name'] 'globalIdentifier' image['globalIdentifier']}if 'note' in image ret[image['name']]['note'] image['note']return ret
def locateNodes nodeList key value noNesting 1 returnList []if not isinstance nodeList type [] return locateNodes nodeList childNodes key value noNesting for childNode in nodeList if not hasattr childNode 'getAttribute' continueif str childNode getAttribute key value returnList append childNode if noNesting continuereturnList extend locateNodes childNode key value noNesting return returnList
def locateNodes nodeList key value noNesting 1 returnList []if not isinstance nodeList type [] return locateNodes nodeList childNodes key value noNesting for childNode in nodeList if not hasattr childNode 'getAttribute' continueif str childNode getAttribute key value returnList append childNode if noNesting continuereturnList extend locateNodes childNode key value noNesting return returnList
def _fit_eval rd B B2 fwd_svd None fwd_data None whitener None if fwd_svd is None fwd _dipole_forwards fwd_data whitener rd[np newaxis ] [0] uu sing vv linalg svd fwd overwrite_a True full_matrices False else uu sing vv fwd_svdgof _dipole_gof uu sing vv B B2 [0]return 1 0 - gof
def cosh x np import_module 'numpy' if isinstance x int float return interval np cosh x np cosh x elif isinstance x interval if x start < 0 and x end > 0 end max np cosh x start np cosh x end return interval 1 end is_valid x is_valid else start np cosh x start end np cosh x end return interval start end is_valid x is_valid else raise NotImplementedError
def start_standalone config args logger info 'Startstandalonemode' global standalonefrom glances standalone import GlancesStandalonestandalone GlancesStandalone config config args args standalone serve_forever
def find_nodes_with_matching_text page xpath regex xpath_matches page xpath xpath results []for xp in xpath_matches if re search regex xp text_content results append xp return results
def create_filters model filters from_dict partial from_dictionary model filters map from_dict filters return map methodcaller 'to_expression' filters
def rotate_axes xs ys zs zdir if zdir u'x' return ys zs xs elif zdir u'-x' return zs xs ys elif zdir u'y' return zs xs ys elif zdir u'-y' return ys zs xs else return xs ys zs
def add_service_port service port if service not in get_services permanent True raise CommandExecutionError 'Theservicedoesnotexist ' cmd '--permanent--service {0}--add-port {1}' format service port return __firewall_cmd cmd
def libvlc_get_fullscreen p_mi f _Cfunctions get 'libvlc_get_fullscreen' None or _Cfunction 'libvlc_get_fullscreen' 1 None ctypes c_int MediaPlayer return f p_mi
def test_close_process_when_exception exc Exception 'boom' with pytest raises Exception as e with pipeline get_cat_pipeline pipeline PIPE pipeline PIPE as pl assert len pl commands 1 assert pl commands[0] _process poll is None raise excassert e value is exc pipeline_wait pl
def clear_memo if available cupy clear_memo
def prepare_rows_as_nested_dicts query nested_dict_column_names all_dicts []for row in query select_related row_dict row get_object_dict for column in nested_dict_column_names if row_dict[column] is not None row_dict[column] getattr row column get_object_dict all_dicts append row_dict return prepare_for_serialization all_dicts
def _tessellate_sphere_surf level rad 1 0 rr tris _tessellate_sphere level npt len rr ntri len tris nn rr copy rr * rads dict rr rr np npt tris tris use_tris tris ntri ntri nuse np nn nn inuse np ones npt int return s
def project_info result project_path str result project project_slug os path split project_path [ -1 ]project_dir os path join project_path project_slug return project_path project_slug project_dir
def shor N a random randrange N - 2 + 2 if igcd N a 1 print 'gotluckywithrand' return igcd N a print 'a ' a print 'N ' N r period_find a N print 'r ' r if r % 2 1 print 'risnoteven beginagain' shor N answer igcd a ** r / 2 - 1 N igcd a ** r / 2 + 1 N return answer
def shor N a random randrange N - 2 + 2 if igcd N a 1 print 'gotluckywithrand' return igcd N a print 'a ' a print 'N ' N r period_find a N print 'r ' r if r % 2 1 print 'risnoteven beginagain' shor N answer igcd a ** r / 2 - 1 N igcd a ** r / 2 + 1 N return answer
def shor N a random randrange N - 2 + 2 if igcd N a 1 print 'gotluckywithrand' return igcd N a print 'a ' a print 'N ' N r period_find a N print 'r ' r if r % 2 1 print 'risnoteven beginagain' shor N answer igcd a ** r / 2 - 1 N igcd a ** r / 2 + 1 N return answer
def shor N a random randrange N - 2 + 2 if igcd N a 1 print 'gotluckywithrand' return igcd N a print 'a ' a print 'N ' N r period_find a N print 'r ' r if r % 2 1 print 'risnoteven beginagain' shor N answer igcd a ** r / 2 - 1 N igcd a ** r / 2 + 1 N return answer
def caller_name skip 2 stack inspect stack start 0 + skip if len stack < start + 1 return ''parentframe stack[start][0]name []module inspect getmodule parentframe if module name append module __name__ if 'self' in parentframe f_locals name append parentframe f_locals['self'] __class__ __name__ codename parentframe f_code co_nameif codename '<module>' name append codename del parentframereturn ' ' join name
def _get_user_partition_groups course_key user_partitions user partition_groups {}for partition in user_partitions group partition scheme get_group_for_user course_key user partition if group is not None partition_groups[partition id] groupreturn partition_groups
def filesystem_absent name force False recursive False return _absent name 'filesystem' force recursive
def xor input key output '' join [chr ord c ^ key for c in input] return output
@require_contextdef _volume_get_query context session None project_only False joined_load True if not joined_load return model_query context models Volume session session project_only project_only if is_admin_context context return model_query context models Volume session session project_only project_only options joinedload 'volume_metadata' options joinedload 'volume_admin_metadata' options joinedload 'volume_type' options joinedload 'volume_attachment' options joinedload 'consistencygroup' options joinedload 'group' else return model_query context models Volume session session project_only project_only options joinedload 'volume_metadata' options joinedload 'volume_type' options joinedload 'volume_attachment' options joinedload 'consistencygroup' options joinedload 'group'
def backwards apps schema_editor certificate_html_view_configuration_model apps get_model u'certificates' u'CertificateHtmlViewConfiguration' certificate_html_view_configuration_model objects all delete
def test_correctness model MLP layers [Linear dim 10 layer_name 'linear' irange 1 0 Softmax n_classes 2 layer_name 'softmax' irange 1 0 ] batch_size 10 nvis 10 cost LpPenalty variables model get_params p 2 penalty cost expr model None penalty_function theano function inputs [] outputs penalty p penalty_function actual_p 0for param in model get_params actual_p + numpy sum param get_value ** 2 assert numpy allclose p actual_p
def test_correctness model MLP layers [Linear dim 10 layer_name 'linear' irange 1 0 Softmax n_classes 2 layer_name 'softmax' irange 1 0 ] batch_size 10 nvis 10 cost LpPenalty variables model get_params p 2 penalty cost expr model None penalty_function theano function inputs [] outputs penalty p penalty_function actual_p 0for param in model get_params actual_p + numpy sum param get_value ** 2 assert numpy allclose p actual_p
def do_format value *args **kwargs if args and kwargs raise FilterArgumentError "can'thandlepositionalandkeywordargumentsatthesametime" return soft_unicode value % kwargs or args
def CentralMoment xs k mean RawMoment xs 1 return sum x - mean ** k for x in xs / len xs
def get_host_ref_for_vm session instance vm_ref get_vm_ref session instance return session _call_method vutil 'get_object_property' vm_ref 'runtime host'
def write_cache entries stream extension_data None ShaStreamCls IndexFileSHA1Writer stream ShaStreamCls stream tell stream tellwrite stream writeversion 2write 'DIRC' write pack '>LL' version len entries for entry in entries beginoffset tell write entry[4] write entry[5] path entry[3]path force_bytes path encoding defenc plen len path & CE_NAMEMASK assert plen len path 'Path%stoolongtofitintoindex' % entry[3] flags plen entry[2] & CE_NAMEMASK_INV write pack '>LLLLLL20sH' entry[6] entry[7] entry[0] entry[8] entry[9] entry[10] entry[1] flags write path real_size tell - beginoffset + 8 & ~ 7 write '\x00' * beginoffset + real_size - tell if extension_data is not None stream write extension_data stream write_sha
@constructordef sum input axis None dtype None keepdims False acc_dtype None out elemwise Sum axis axis dtype dtype acc_dtype acc_dtype input if keepdims out makeKeepDims input out axis return out
def site def prep r if r representation 'json' and r method not in 'search_ac' 'search_address_ac' 'site_contact_person' return Falses3db gis_location_filter r return Trues3 prep prepreturn s3_rest_controller
def site def prep r if r representation 'json' and r method not in 'search_ac' 'search_address_ac' 'site_contact_person' return Falses3db gis_location_filter r return Trues3 prep prepreturn s3_rest_controller
def site def prep r if r representation 'json' and r method not in 'search_ac' 'search_address_ac' 'site_contact_person' return Falses3db gis_location_filter r return Trues3 prep prepreturn s3_rest_controller
def site def prep r if r representation 'json' and r method not in 'search_ac' 'search_address_ac' 'site_contact_person' return Falses3db gis_location_filter r return Trues3 prep prepreturn s3_rest_controller
def call_tadm args if isinstance args compat string_types raise TypeError u'argsshouldbealistofstrings' if _tadm_bin is None config_tadm cmd [_tadm_bin] + args p subprocess Popen cmd stdout sys stdout stdout stderr p communicate if p returncode 0 print print stderr raise OSError u'tadmcommandfailed '
@pytest mark skipif "sys platform 'win32'" @pytest mark networkdef test_check_submodule_addition script module_path submodule_path _create_test_package_with_submodule script install_result script pip 'install' '-e' 'git+' + module_path + '#egg version_pkg' assert script venv / 'src/version-pkg/testpkg/static/testfile' in install_result files_created _change_test_package_submodule script submodule_path _pull_in_submodule_changes_to_module script module_path update_result script pip 'install' '-e' 'git+' + module_path + '#egg version_pkg' '--upgrade' expect_error True assert script venv / 'src/version-pkg/testpkg/static/testfile2' in update_result files_created
@pytest mark skipif "sys platform 'win32'" @pytest mark networkdef test_check_submodule_addition script module_path submodule_path _create_test_package_with_submodule script install_result script pip 'install' '-e' 'git+' + module_path + '#egg version_pkg' assert script venv / 'src/version-pkg/testpkg/static/testfile' in install_result files_created _change_test_package_submodule script submodule_path _pull_in_submodule_changes_to_module script module_path update_result script pip 'install' '-e' 'git+' + module_path + '#egg version_pkg' '--upgrade' expect_error True assert script venv / 'src/version-pkg/testpkg/static/testfile2' in update_result files_created
@pytest mark skipif "sys platform 'win32'" @pytest mark networkdef test_check_submodule_addition script module_path submodule_path _create_test_package_with_submodule script install_result script pip 'install' '-e' 'git+' + module_path + '#egg version_pkg' assert script venv / 'src/version-pkg/testpkg/static/testfile' in install_result files_created _change_test_package_submodule script submodule_path _pull_in_submodule_changes_to_module script module_path update_result script pip 'install' '-e' 'git+' + module_path + '#egg version_pkg' '--upgrade' expect_error True assert script venv / 'src/version-pkg/testpkg/static/testfile2' in update_result files_created
def delete_network_acl network_acl_id None network_acl_name None disassociate False region None key None keyid None profile None if disassociate network_acl _get_resource 'network_acl' name network_acl_name region region key key keyid keyid profile profile if network_acl and network_acl associations subnet_id network_acl associations[0] subnet_idtry conn _get_conn region region key key keyid keyid profile profile conn disassociate_network_acl subnet_id except BotoServerError passreturn _delete_resource resource 'network_acl' name network_acl_name resource_id network_acl_id region region key key keyid keyid profile profile
def delete_network_acl network_acl_id None network_acl_name None disassociate False region None key None keyid None profile None if disassociate network_acl _get_resource 'network_acl' name network_acl_name region region key key keyid keyid profile profile if network_acl and network_acl associations subnet_id network_acl associations[0] subnet_idtry conn _get_conn region region key key keyid keyid profile profile conn disassociate_network_acl subnet_id except BotoServerError passreturn _delete_resource resource 'network_acl' name network_acl_name resource_id network_acl_id region region key key keyid keyid profile profile
def long_to_datetime x days x // 86400000000 x - days * 86400000000 seconds x // 1000000 x - seconds * 1000000 return datetime min + timedelta days days seconds seconds microseconds x
def long_to_datetime x days x // 86400000000 x - days * 86400000000 seconds x // 1000000 x - seconds * 1000000 return datetime min + timedelta days days seconds seconds microseconds x
def is_entrance_exams_enabled return settings FEATURES get 'ENTRANCE_EXAMS'
def is_entrance_exams_enabled return settings FEATURES get 'ENTRANCE_EXAMS'
def is_entrance_exams_enabled return settings FEATURES get 'ENTRANCE_EXAMS'
def process_autosummary_toc app doctree env app builder envcrawled {}def crawl_toc node depth 1 crawled[node] Truefor j subnode in enumerate node try if isinstance subnode autosummary_toc and isinstance subnode[0] addnodes toctree env note_toctree env docname subnode[0] continueexcept IndexError continueif not isinstance subnode nodes section continueif subnode not in crawled crawl_toc subnode depth + 1 crawl_toc doctree
def preseed_package pkg_name preseed for q_name _ in preseed items q_type q_answer _run_as_root 'echo"% pkg_name s% q_name s% q_type s% q_answer s" debconf-set-selections' % locals
def _sub_labels axis subs fmt axis get_minor_formatter minor_tlocs axis get_minorticklocs fmt set_locs minor_tlocs coefs minor_tlocs / 10 ** np floor np log10 minor_tlocs label_expected [ np round c in subs for c in coefs]label_test [ fmt x u'' for x in minor_tlocs]assert label_test label_expected
def init n_workers 1 global workersif n_workers 0 n_workers 1for i in range n_workers worker StyleTransfer 'googlenet' use_pbar False workers update {Lock worker}
def write_block_summary_report course_data block_summary_counts unique_course_counts _get_block_summary_totals course_data with open 'xblock_summary_counts csv' 'wb' as csvfile summary_writer csv writer csvfile delimiter ' ' quotechar '"' quoting csv QUOTE_MINIMAL summary_writer writerow ['XBLOCK_NAME' 'UNIQUE_COURSES' 'NUM_TOTAL_INSTANCES'] for block_type in sorted block_summary_counts block_count block_summary_counts get block_type summary_writer writerow [block_type str unique_course_counts[block_type] str block_count ] csvfile close
def write_block_summary_report course_data block_summary_counts unique_course_counts _get_block_summary_totals course_data with open 'xblock_summary_counts csv' 'wb' as csvfile summary_writer csv writer csvfile delimiter ' ' quotechar '"' quoting csv QUOTE_MINIMAL summary_writer writerow ['XBLOCK_NAME' 'UNIQUE_COURSES' 'NUM_TOTAL_INSTANCES'] for block_type in sorted block_summary_counts block_count block_summary_counts get block_type summary_writer writerow [block_type str unique_course_counts[block_type] str block_count ] csvfile close
def _footer_css_urls request package_name package settings PIPELINE_CSS get package_name {} paths [package['output_filename']] if not settings DEBUG else package['source_filenames'] return [_footer_static_url request path for path in paths]
def get_year book return int book['date'] split [1]
def get_year book return int book['date'] split [1]
def get_year book return int book['date'] split [1]
def _cmp_by_origin path1 path2 def get_origin_pref origin if origin value BGP_ATTR_ORIGIN_IGP return 3elif origin value BGP_ATTR_ORIGIN_EGP return 2elif origin value BGP_ATTR_ORIGIN_INCOMPLETE return 1else LOG error 'Invalidoriginvalueencountered%s ' origin return 0origin1 path1 get_pattr BGP_ATTR_TYPE_ORIGIN origin2 path2 get_pattr BGP_ATTR_TYPE_ORIGIN assert origin1 is not None and origin2 is not None if origin1 value origin2 value return Noneorigin1 get_origin_pref origin1 origin2 get_origin_pref origin2 if origin1 origin2 return Noneelif origin1 > origin2 return path1return path2
def dihedral n if n 1 yield Permutation [0 1] yield Permutation [1 0] elif n 2 yield Permutation [0 1 2 3] yield Permutation [1 0 3 2] yield Permutation [2 3 0 1] yield Permutation [3 2 1 0] else gen list range n for i in range n yield Permutation gen yield Permutation gen[ -1 ] gen rotate_left gen 1
@_assure_identitydef auth_with_token token tenant_id None tenant_name None region None global regions servicesidentity auth_with_token token tenant_id tenant_id tenant_name tenant_name regions tuple identity regions services tuple identity services keys connect_to_services region region
def security_group_exists context project_id group_name return IMPL security_group_exists context project_id group_name
def get_actor tr chamber actor tr[0] text_content strip if actor startswith 'H' or actor startswith 'S' actor actor[0]return {'H' 'lower' 'S' 'upper'}[actor]else h_or_s tr xpath 'ancestor table[1]/preceding-sibling ' + 'table/tr/td/b[contains text "TRANSMITTO" ]' if h_or_s h_or_s h_or_s[ -1 ] text_content strip actor 'upper' if h_or_s endswith 'SENATE ' else 'lower' else actor chamberreturn actor
def _parse_file descriptor_file is_bridge False validate False **kwargs while True if not is_bridge extrainfo_content _read_until_keywords 'router-signature' descriptor_file block_end_prefix PGP_BLOCK_END split '' 1 [0]extrainfo_content + _read_until_keywords block_end_prefix descriptor_file True else extrainfo_content _read_until_keywords 'router-digest' descriptor_file True if extrainfo_content if extrainfo_content[0] startswith '@type' extrainfo_content extrainfo_content[1 ]if is_bridge yield BridgeExtraInfoDescriptor bytes join '' extrainfo_content validate **kwargs else yield RelayExtraInfoDescriptor bytes join '' extrainfo_content validate **kwargs else break
@depends HAS_PYVMOMI def get_vmotion_enabled host username password protocol None port None host_names None service_instance salt utils vmware get_service_instance host host username username password password protocol protocol port port host_names _check_hosts service_instance host host_names ret {}for host_name in host_names host_ref _get_host_ref service_instance host host_name host_name vmotion_vnic host_ref configManager vmotionSystem netConfig selectedVnicif vmotion_vnic ret update {host_name {'VMotionEnabled' True}} else ret update {host_name {'VMotionEnabled' False}} return ret
def _get_conf_tpls image_name kube_annotations None kube_pod_name None kube_container_name None return [ x y for x y in copy deepcopy TestServiceDiscovery mock_templates get image_name [0] ]
def update_session_auth_hash request user request session cycle_key if hasattr user 'get_session_auth_hash' and request user user request session[HASH_SESSION_KEY] user get_session_auth_hash
def update_session_auth_hash request user request session cycle_key if hasattr user 'get_session_auth_hash' and request user user request session[HASH_SESSION_KEY] user get_session_auth_hash
def _has_access_error_desc user action descriptor course_key def check_for_staff return _has_staff_access_to_descriptor user descriptor course_key checkers {'load' check_for_staff 'staff' check_for_staff 'instructor' lambda _has_instructor_access_to_descriptor user descriptor course_key }return _dispatch checkers action user descriptor
def _has_access_error_desc user action descriptor course_key def check_for_staff return _has_staff_access_to_descriptor user descriptor course_key checkers {'load' check_for_staff 'staff' check_for_staff 'instructor' lambda _has_instructor_access_to_descriptor user descriptor course_key }return _dispatch checkers action user descriptor
def fisher_z_transform r if abs r > 1 return nanreturn 0 5 * log 1 0 + r / 1 0 - r
def get_network_timezone network if network is None return sr_timezonetry return tz gettz network_dict[network] or sr_timezone except Exception return sr_timezone
def gjrconvertparams self params nar nma p q nar nma ar np concatenate [1] params[ p] ma np zeros q + 1 3 ma[ 0 0 ] params[ -1 ]ma[ 1] np concatenate [0] params[p p + q ] ma[ 2] np concatenate [0] params[ p + q p + 2 * q ] mu params[ -1 ]params2 ar ma return paramsclass
def gjrconvertparams self params nar nma p q nar nma ar np concatenate [1] params[ p] ma np zeros q + 1 3 ma[ 0 0 ] params[ -1 ]ma[ 1] np concatenate [0] params[p p + q ] ma[ 2] np concatenate [0] params[ p + q p + 2 * q ] mu params[ -1 ]params2 ar ma return paramsclass
def write_tfrs fname tfr overwrite False out []if not isinstance tfr list tuple tfr [tfr]for ii tfr_ in enumerate tfr comment ii if tfr_ comment is None else tfr_ comment out append _prepare_write_tfr tfr_ condition comment write_hdf5 fname out overwrite overwrite title 'mnepython'
def twitter_inbox if not auth s3_logged_in session error T 'RequiresLogin ' redirect URL c 'default' f 'user' args 'login' tablename 'msg_twitter'table s3db msg_twitters3 filter table inbound True table inbound readable Falses3 crud_strings[tablename] Storage title_display T 'TweetDetails' title_list T 'TwitterInBox' label_list_button T 'ViewTwitterInBox' label_delete_button T 'DeleteTweet' msg_record_deleted T 'Tweetdeleted' msg_list_empty T 'NoTweetscurrentlyinInBox' s3db configure tablename editable False insertable False list_fields ['id' 'date' 'from_address' 'body'] return s3_rest_controller module 'twitter'
def natural_ipv4_netmask ip fmt 'prefixlen' bits _ipv4_to_bits ip if bits startswith '11' mask '24'elif bits startswith '1' mask '16'else mask '8'if fmt 'netmask' return cidr_to_ipv4_netmask mask else return '/' + mask
def is_writable path if os path isfile path return bool os stat path st_mode & stat S_IWUSR else return True
def make_comparison_tests klass kwargs1 kwargs2 def instance kwargs if isinstance kwargs dict return klass **kwargs return klass **kwargs class Tests TestCase def test_equality self '\nInstanceswiththesameargumentsareequal \n'self assertTrue instance kwargs1 instance kwargs1 self assertFalse instance kwargs1 instance kwargs2 def test_notequality self '\nInstancewithdifferentargumentsarenotequal \n'self assertTrue instance kwargs1 instance kwargs2 self assertFalse instance kwargs1 instance kwargs1 Tests __name__ klass __name__ + 'ComparisonTests' return Tests
def ec2_id_to_id ec2_id try return int ec2_id split '-' [ -1 ] 16 except ValueError raise exception InvalidEc2Id ec2_id ec2_id
def check_dns_name bucket_name if ' ' in bucket_name return Falsen len bucket_name if n < 3 or n > 63 return Falseif n 1 if not bucket_name isalnum return Falsematch LABEL_RE match bucket_name if match is None or match end len bucket_name return Falsereturn True
def absent name export False force False ret {'name' name 'changes' {} 'result' None 'comment' ''}log debug 'zpool absent {0} config force {1}' format name force log debug 'zpool absent {0} config export {1}' format name export if __salt__['zpool exists'] name ret['result'] Falseif export if __opts__['test'] ret['result'] Trueelse ret['result'] __salt__['zpool export'] name force force ret['result'] ret['result'] get name 'exported' elif __opts__['test'] ret['result'] Trueelse ret['result'] __salt__['zpool destroy'] name force force ret['result'] ret['result'] get name 'destroyed' if ret['result'] ret['changes'][name] 'exported' if export else 'destroyed' ret['comment'] 'storagepool{0}was{1}' format name ret['changes'][name] else ret['result'] Trueret['comment'] 'storagepool{0}isabsent' format name return ret
def generate_next_dates start_date offset 0 start offset * 7 for i in itertools count start yield start_date + timedelta i
def GetNodes node match_tag return child for child in node getchildren if GetTag child match_tag
def solc_parse_output compiler_output result yaml safe_load compiler_output ['contracts']if 'bin' in tuple result values [0] for value in result values value['bin_hex'] value['bin']try value['bin'] decode_hex value['bin_hex'] except TypeError passfor json_data in 'abi' 'devdoc' 'userdoc' if json_data not in tuple result values [0] continuefor value in result values value[json_data] yaml safe_load value[json_data] return result
def migrate print green '%s PerformingMigration' % env host with cd '/home/web2py' run 'sudo-H-uweb2pypythonweb2py py-N-Seden-M-Rapplications/eden/static/scripts/tools/noop py' pty True
def has_handshake_pyrit target capfile if not program_exists 'pyrit' return Falsecmd ['pyrit' '-r' capfile 'analyze']proc Popen cmd stdout PIPE stderr DN proc wait hit_essid Falsefor line in proc communicate [0] split '\n' if line '' or line None continueif line find 'AccessPoint' -1 hit_essid line find " '" + target ssid + "' " -1 and line lower find target bssid lower -1 elif hit_essid and line find ' good ' -1 or line find ' workable ' -1 return Truereturn False
def attachment_upload_to instance filename now datetime now return 'attachments/% date s/% id s/% md5 s/% filename s' % {'date' now strftime '%Y/%m/%d' 'id' instance attachment id 'md5' hashlib md5 str now hexdigest 'filename' filename}
def set_channel_access channel 14 access_update_mode 'non_volatile' alerting False per_msg_auth False user_level_auth False access_mode 'always' privilege_update_mode 'non_volatile' privilege_level 'administrator' **kwargs with _IpmiCommand **kwargs as s return s set_channel_access channel access_update_mode alerting per_msg_auth user_level_auth access_mode privilege_update_mode privilege_level
def get_checksum content encoding 'utf8' block_size 8192 md hashlib md5 def safe_update txt try md update txt except UnicodeEncodeError md update txt encode encoding try isfile os path isfile content except TypeError isfile Falseif isfile with open content 'rb' as ff txt ff read block_size while txt safe_update txt txt ff read block_size elif hasattr content 'read' pos content tell content seek 0 txt content read block_size while txt safe_update txt txt content read block_size content seek pos else safe_update content return md hexdigest
def direct_get_account node part account marker None limit None prefix None delimiter None conn_timeout 5 response_timeout 15 end_marker None reverse None path '/' + account return _get_direct_account_container path 'Account' node part marker marker limit limit prefix prefix delimiter delimiter end_marker end_marker reverse reverse conn_timeout conn_timeout response_timeout response_timeout
def ExpandXcodeVariables string expansions matches _xcode_variable_re findall string if matches None return stringmatches reverse for match in matches to_replace variable matchif not variable in expansions continuereplacement expansions[variable]string re sub re escape to_replace replacement string return string
def ExpandXcodeVariables string expansions matches _xcode_variable_re findall string if matches None return stringmatches reverse for match in matches to_replace variable matchif not variable in expansions continuereplacement expansions[variable]string re sub re escape to_replace replacement string return string
def ExpandXcodeVariables string expansions matches _xcode_variable_re findall string if matches None return stringmatches reverse for match in matches to_replace variable matchif not variable in expansions continuereplacement expansions[variable]string re sub re escape to_replace replacement string return string
def get_current_db from calibre gui2 ui import get_guigui get_gui if gui is not None and gui current_db is not None return gui current_dbfrom calibre library import dbreturn db
def get_current_db from calibre gui2 ui import get_guigui get_gui if gui is not None and gui current_db is not None return gui current_dbfrom calibre library import dbreturn db
def get_current_db from calibre gui2 ui import get_guigui get_gui if gui is not None and gui current_db is not None return gui current_dbfrom calibre library import dbreturn db
def config_to_kodi MASTER_SETTINGS config extracted_settings_for_kodi {}for setting protocols in MASTER_SETTINGS iteritems value general_config_get config **protocols extracted_settings_for_kodi[setting] valuereturn extracted_settings_for_kodi
def config_to_kodi MASTER_SETTINGS config extracted_settings_for_kodi {}for setting protocols in MASTER_SETTINGS iteritems value general_config_get config **protocols extracted_settings_for_kodi[setting] valuereturn extracted_settings_for_kodi
def config_to_kodi MASTER_SETTINGS config extracted_settings_for_kodi {}for setting protocols in MASTER_SETTINGS iteritems value general_config_get config **protocols extracted_settings_for_kodi[setting] valuereturn extracted_settings_for_kodi
def _have_socket_rds try s socket socket socket PF_RDS socket SOCK_SEQPACKET 0 except AttributeError OSError return Falseelse s close return True
def track_voted_event request course obj vote_value undo_vote False if isinstance obj cc Thread obj_type 'thread'else obj_type 'response'event_name _EVENT_NAME_TEMPLATE format obj_type obj_type action_name 'voted' event_data {'commentable_id' obj commentable_id 'target_username' obj get 'username' 'undo_vote' undo_vote 'vote_value' vote_value}track_forum_event request event_name course obj event_data
def build_single_handler_application path argv None argv argv or [] path os path abspath path if os path isdir path handler DirectoryHandler filename path argv argv elif path endswith ' ipynb' handler NotebookHandler filename path argv argv elif path endswith ' py' if path endswith 'main py' warnings warn DIRSTYLE_MAIN_WARNING handler ScriptHandler filename path argv argv else raise ValueError "Expecteda' py'scriptor' ipynb'notebook got '%s'" % path if handler failed raise RuntimeError 'Errorloading%s \n\n%s\n%s' % path handler error handler error_detail application Application handler return application
def reblock_2x2 B if not isinstance B BlockMatrix or not all d > 2 for d in B blocks shape return BBM BlockMatrixreturn BM [[B blocks[ 0 0 ] BM B blocks[0 1 ] ] [BM B blocks[1 0] BM B blocks[1 1 ] ]]
def reblock_2x2 B if not isinstance B BlockMatrix or not all d > 2 for d in B blocks shape return BBM BlockMatrixreturn BM [[B blocks[ 0 0 ] BM B blocks[0 1 ] ] [BM B blocks[1 0] BM B blocks[1 1 ] ]]
@utils arg 'server' metavar '<server>' help _ 'NameorIDofserver ' @utils arg 'console_type' metavar '<console-type>' help _ 'Typeofrdpconsole "rdp-html5" ' def do_get_rdp_console cs args server _find_server cs args server data server get_rdp_console args console_type print_console cs data
def random_variables return tf get_collection RANDOM_VARIABLE_COLLECTION
def zap target None **kwargs if target is not None log warning 'Depricateduseoffunction usekwargs' target kwargs get 'dev' target kwargs['dev'] targetreturn ceph_cfg zap **kwargs
def totals cls interval time_points get_time_points interval q Session query cls date sum cls pageview_count label 'sum' filter cls interval interval filter cls date in_ time_points filter cls codename startswith Link _type_prefix group_by cls date order_by desc cls date return fill_gaps time_points q 'sum'
def _iter_modules path import osimport pkgutilif hasattr pkgutil 'iter_modules' for importer modname ispkg in pkgutil iter_modules path yield modname ispkg returnfrom inspect import getmodulenamefrom pydoc import ispackagefound set for path in path for filename in os listdir path p os path join path filename modname getmodulename filename if modname and modname '__init__' if modname not in found found add modname yield modname ispackage modname
def asnumpy a stream None if isinstance a ndarray return a get stream stream else return numpy asarray a
def asnumpy a stream None if isinstance a ndarray return a get stream stream else return numpy asarray a
def xframe_allow_whitelisted view_func def wrapped_view request *args **kwargs 'ModifytheresponsewiththecorrectX-Frame-Options 'resp view_func request *args **kwargs x_frame_option 'DENY'if settings FEATURES['ENABLE_THIRD_PARTY_AUTH'] referer request META get 'HTTP_REFERER' if referer is not None parsed_url urlparse referer hostname parsed_url hostnameif LTIProviderConfig objects current_set filter lti_hostname hostname enabled True exists x_frame_option 'ALLOW'resp['X-Frame-Options'] x_frame_optionreturn respreturn wraps view_func assigned available_attrs view_func wrapped_view
def _trivial_gcd f g ring f ringif not f or g return ring zero ring zero ring zero elif not f if g LC < ring domain zero return - g ring zero - ring one else return g ring zero ring one elif not g if f LC < ring domain zero return - f - ring one ring zero else return f ring one ring zero return None
def _graded_scorable_blocks_to_header course_key scorable_blocks_map OrderedDict grading_context grading_context_for_course course_key for assignment_type_name subsection_infos in grading_context['all_graded_subsections_by_type'] iteritems for subsection_index subsection_info in enumerate subsection_infos start 1 for scorable_block in subsection_info['scored_descendants'] header_name u'{assignment_type}{subsection_index} {subsection_name}-{scorable_block_name}' format scorable_block_name scorable_block display_name assignment_type assignment_type_name subsection_index subsection_index subsection_name subsection_info['subsection_block'] display_name scorable_blocks_map[scorable_block location] [ header_name + ' Earned ' header_name + ' Possible ' ]return scorable_blocks_map
def _graded_scorable_blocks_to_header course_key scorable_blocks_map OrderedDict grading_context grading_context_for_course course_key for assignment_type_name subsection_infos in grading_context['all_graded_subsections_by_type'] iteritems for subsection_index subsection_info in enumerate subsection_infos start 1 for scorable_block in subsection_info['scored_descendants'] header_name u'{assignment_type}{subsection_index} {subsection_name}-{scorable_block_name}' format scorable_block_name scorable_block display_name assignment_type assignment_type_name subsection_index subsection_index subsection_name subsection_info['subsection_block'] display_name scorable_blocks_map[scorable_block location] [ header_name + ' Earned ' header_name + ' Possible ' ]return scorable_blocks_map
def get_account_id_from_arn trail_arn return trail_arn split ' ' [4]
def send_suggestion_email exploration_title exploration_id author_id recipient_list email_subject 'Newsuggestionfor"%s"' % exploration_title email_body_template 'Hi%s <br>%shassubmittedanewsuggestionforyourOppiaexploration <ahref "https //www oppia org/create/%s">"%s"</a> <br>Youcanacceptorrejectthissuggestionbyvisitingthe<ahref "https //www oppia org/create/%s#/feedback">feedbackpage</a>foryourexploration <br><br>Thanks <br>-TheOppiaTeam<br><br>%s'if not feconf CAN_SEND_EMAILS log_new_error 'Thisappcannotsendemailstousers ' returnif not feconf CAN_SEND_FEEDBACK_MESSAGE_EMAILS log_new_error 'Thisappcannotsendfeedbackmessageemailstousers ' returnauthor_settings user_services get_user_settings author_id can_users_receive_email can_users_receive_thread_email recipient_list exploration_id True for index recipient_id in enumerate recipient_list recipient_user_settings user_services get_user_settings recipient_id if can_users_receive_email[index] email_body email_body_template % recipient_user_settings username author_settings username exploration_id exploration_title exploration_id EMAIL_FOOTER value _send_email recipient_id feconf SYSTEM_COMMITTER_ID feconf EMAIL_INTENT_SUGGESTION_NOTIFICATION email_subject email_body feconf NOREPLY_EMAIL_ADDRESS
def send_suggestion_email exploration_title exploration_id author_id recipient_list email_subject 'Newsuggestionfor"%s"' % exploration_title email_body_template 'Hi%s <br>%shassubmittedanewsuggestionforyourOppiaexploration <ahref "https //www oppia org/create/%s">"%s"</a> <br>Youcanacceptorrejectthissuggestionbyvisitingthe<ahref "https //www oppia org/create/%s#/feedback">feedbackpage</a>foryourexploration <br><br>Thanks <br>-TheOppiaTeam<br><br>%s'if not feconf CAN_SEND_EMAILS log_new_error 'Thisappcannotsendemailstousers ' returnif not feconf CAN_SEND_FEEDBACK_MESSAGE_EMAILS log_new_error 'Thisappcannotsendfeedbackmessageemailstousers ' returnauthor_settings user_services get_user_settings author_id can_users_receive_email can_users_receive_thread_email recipient_list exploration_id True for index recipient_id in enumerate recipient_list recipient_user_settings user_services get_user_settings recipient_id if can_users_receive_email[index] email_body email_body_template % recipient_user_settings username author_settings username exploration_id exploration_title exploration_id EMAIL_FOOTER value _send_email recipient_id feconf SYSTEM_COMMITTER_ID feconf EMAIL_INTENT_SUGGESTION_NOTIFICATION email_subject email_body feconf NOREPLY_EMAIL_ADDRESS
def remove_result_ranges_diffs result_list file_dict result_diff_dict_dict {}for original_result in result_list mod_file_dict copy deepcopy file_dict source_ranges []previous Nonefor source_range in sorted original_result affected_code reverse True if previous is not None and source_range overlaps previous combined_sr SourceRange join previous source_range previous combined_srelif previous is None previous source_rangeelse source_ranges append previous previous source_rangeif previous source_ranges append previous for source_range in source_ranges file_name source_range filenew_file remove_range mod_file_dict[file_name] source_range mod_file_dict[file_name] new_filediff_dict {}for file_name in file_dict diff_dict[file_name] Diff from_string_arrays file_dict[file_name] mod_file_dict[file_name] result_diff_dict_dict[original_result] diff_dictreturn result_diff_dict_dict
def remove_result_ranges_diffs result_list file_dict result_diff_dict_dict {}for original_result in result_list mod_file_dict copy deepcopy file_dict source_ranges []previous Nonefor source_range in sorted original_result affected_code reverse True if previous is not None and source_range overlaps previous combined_sr SourceRange join previous source_range previous combined_srelif previous is None previous source_rangeelse source_ranges append previous previous source_rangeif previous source_ranges append previous for source_range in source_ranges file_name source_range filenew_file remove_range mod_file_dict[file_name] source_range mod_file_dict[file_name] new_filediff_dict {}for file_name in file_dict diff_dict[file_name] Diff from_string_arrays file_dict[file_name] mod_file_dict[file_name] result_diff_dict_dict[original_result] diff_dictreturn result_diff_dict_dict
def remove_result_ranges_diffs result_list file_dict result_diff_dict_dict {}for original_result in result_list mod_file_dict copy deepcopy file_dict source_ranges []previous Nonefor source_range in sorted original_result affected_code reverse True if previous is not None and source_range overlaps previous combined_sr SourceRange join previous source_range previous combined_srelif previous is None previous source_rangeelse source_ranges append previous previous source_rangeif previous source_ranges append previous for source_range in source_ranges file_name source_range filenew_file remove_range mod_file_dict[file_name] source_range mod_file_dict[file_name] new_filediff_dict {}for file_name in file_dict diff_dict[file_name] Diff from_string_arrays file_dict[file_name] mod_file_dict[file_name] result_diff_dict_dict[original_result] diff_dictreturn result_diff_dict_dict
def get_info_for_reshard stream_details min_hash_key 0max_hash_key 0stream_details['OpenShards'] []for shard in stream_details['Shards'] shard_id shard['ShardId']if 'EndingSequenceNumber' in shard['SequenceNumberRange'] log debug 'skippingclosedshard{0}' format shard_id continuestream_details['OpenShards'] append shard shard['HashKeyRange']['StartingHashKey'] long_int shard['HashKeyRange']['StartingHashKey'] shard['HashKeyRange']['EndingHashKey'] long_int shard['HashKeyRange']['EndingHashKey'] if shard['HashKeyRange']['StartingHashKey'] < min_hash_key min_hash_key shard['HashKeyRange']['StartingHashKey']if shard['HashKeyRange']['EndingHashKey'] > max_hash_key max_hash_key shard['HashKeyRange']['EndingHashKey']stream_details['OpenShards'] sort key lambda shard long_int shard['HashKeyRange']['StartingHashKey'] return min_hash_key max_hash_key stream_details
def logging_level runlevel if runlevel return LOG_VALUES[runlevel]else return logging FATAL + 5
def get_deployment_engine_class provider engine_path PROVIDER_MAPPING get provider None logger debug 'LoadingDeploymentEnginefor%s %s' provider engine_path if engine_path is None return None module engine engine_path rsplit ' ' 1 try _mod __import__ module fromlist [engine] except ImportError as e logger error 'Unabletoimportrequestedengine %s forprovider%s' engine provider logger error 'Arequiredlibrarywasmissing %s' e message logger error 'Pleaseinstallthelibraryandtryagain' else return getattr _mod engine
def return_traceback_in_unhandled_exceptions global get_fault_string_from_exceptionimport tracebackdef _get_fault_string_from_exception e return traceback format_exc get_fault_string_from_exception _get_fault_string_from_exception
def sha_encode t s hashlib sha1 t return s hexdigest
def _approx_fprime_helper xk f epsilon args f0 None if f0 is None f0 f * xk + args grad numpy zeros len xk float ei numpy zeros len xk float for k in range len xk ei[k] 1 0d epsilon * ei grad[k] f * xk + d + args - f0 / d[k] ei[k] 0 0return grad
def substitute_indices t *index_tuples if not isinstance t TensExpr return tfree t freefree1 []for j ipos cpos in free for i v in index_tuples if i _name j _name and i _tensortype j _tensortype if i _is_up j _is_up free1 append v ipos cpos else free1 append - v ipos cpos breakelse free1 append j ipos cpos t TensMul from_data t coeff t components free1 t dum return t
def substitute_indices t *index_tuples if not isinstance t TensExpr return tfree t freefree1 []for j ipos cpos in free for i v in index_tuples if i _name j _name and i _tensortype j _tensortype if i _is_up j _is_up free1 append v ipos cpos else free1 append - v ipos cpos breakelse free1 append j ipos cpos t TensMul from_data t coeff t components free1 t dum return t
def test_defn cant_compile u' defnif*[]1 ' cant_compile u' defn"hy"[]1 ' cant_compile u' defn hy[]1 ' can_compile u' defn&hy[]1 '
def for_dtypes_combination types names 'dtype' full None if full is None full int os environ get 'CUPY_TEST_FULL_COMBINATION' '0' 0 if full combination parameterized product {name types for name in names} else ts []for _ in range len names t list types random shuffle t ts append t combination [dict zip names typs for typs in zip *ts ]def decorator impl @functools wraps impl def test_func self *args **kw for dtypes in combination kw_copy kw copy kw_copy update dtypes try impl self *args **kw_copy except Exception print dtypes raisereturn test_funcreturn decorator
def test_find_number_false_exponent s '2e'r find_number s assert s[r[0] r[1]] '2' s[r[0] r[1]]
def test_find_number_false_exponent s '2e'r find_number s assert s[r[0] r[1]] '2' s[r[0] r[1]]
def test_find_number_false_exponent s '2e'r find_number s assert s[r[0] r[1]] '2' s[r[0] r[1]]
def edmonds_karp G s t capacity 'capacity' residual None value_only False cutoff None R edmonds_karp_impl G s t capacity residual cutoff R graph['algorithm'] 'edmonds_karp'return R
def edmonds_karp G s t capacity 'capacity' residual None value_only False cutoff None R edmonds_karp_impl G s t capacity residual cutoff R graph['algorithm'] 'edmonds_karp'return R
def all_bit_strings bits dtype 'uint8' return np array [[int x for x in np binary_repr i width bits ] for i in xrange 0 2 ** bits ] dtype dtype
def median_absolute_deviation x x asarray x median_x median x median_abs_deviation median abs x - median_x return median_abs_deviation median_x
def initialize_report report_type start_date end_date start_letter None end_letter None for item in REPORT_TYPES if report_type in item return item[1] start_date end_date start_letter end_letter raise ReportTypeDoesNotExistException
def _load_coverage F header_length 6 dtype np int16 header [F readline for i in range header_length ]make_tuple lambda t t split [0] float t split [1] header dict [make_tuple line for line in header] M np loadtxt F dtype dtype nodata int header['NODATA_value'] if nodata -9999 M[nodata] -9999 return M
def _load_coverage F header_length 6 dtype np int16 header [F readline for i in range header_length ]make_tuple lambda t t split [0] float t split [1] header dict [make_tuple line for line in header] M np loadtxt F dtype dtype nodata int header['NODATA_value'] if nodata -9999 M[nodata] -9999 return M
def stop container timeout 10 client _get_client status base_status copy try dcontainer _get_container_infos container ['Id']if is_running dcontainer client stop dcontainer timeout timeout if not is_running dcontainer _valid status comment 'Container{0}wasstopped' format container id_ container else _invalid status else _valid status comment 'Container{0}wasalreadystopped' format container id_ container except Exception _invalid status id_ container out traceback format_exc comment 'Anexceptionoccurredwhilestoppingyourcontainer{0}' format container __salt__['mine send'] 'dockerng ps' verbose True all True host True return status
def setup_platform hass config add_devices discovery_info None import myuspstry cookie hass config path COOKIE session myusps get_session config get CONF_USERNAME config get CONF_PASSWORD cookie_path cookie except myusps USPSError _LOGGER exception 'CouldnotconnecttoMyUSPS' return Falseadd_devices [USPSSensor session config get CONF_UPDATE_INTERVAL ]
def timeit func before time time res func return time time - before res
def get_leaking_objects objects None if objects is None gc collect objects gc get_objects try ids set id i for i in objects for i in objects ids difference_update id j for j in gc get_referents i return [i for i in objects if id i in ids ]finally del objects i
def get_leaking_objects objects None if objects is None gc collect objects gc get_objects try ids set id i for i in objects for i in objects ids difference_update id j for j in gc get_referents i return [i for i in objects if id i in ids ]finally del objects i
def get_leaking_objects objects None if objects is None gc collect objects gc get_objects try ids set id i for i in objects for i in objects ids difference_update id j for j in gc get_referents i return [i for i in objects if id i in ids ]finally del objects i
def OSVersion flavor urlbase path basename isoURLs get flavor 'unknown' return path splitext urlbase [0]
def setup_platform hass config add_devices discovery_info None if discovery_info is None or zwave NETWORK is None returnnode zwave NETWORK nodes[discovery_info[zwave const ATTR_NODE_ID]]value node values[discovery_info[zwave const ATTR_VALUE_ID]]if not node has_command_class zwave const COMMAND_CLASS_SWITCH_BINARY returnif value type zwave const TYPE_BOOL or value genre zwave const GENRE_USER returnvalue set_change_verified False add_devices [ZwaveSwitch value ]
def imread_qt filename qtimg QImage if not qtimg load filename raise IOError 'Unabletoloadfile%s' % filename if qtimg depth 1 raise IOError '1-bitimagescurrentlynotsupported' arrayptr qtimg bits bytes_per_pixel qtimg depth // 8 pixels_per_line qtimg bytesPerLine // bytes_per_pixel img_size pixels_per_line * qtimg height * bytes_per_pixel arrayptr setsize img_size img np array arrayptr if bytes_per_pixel > 1 img img reshape qtimg height pixels_per_line bytes_per_pixel img img[ qtimg width ]else img img reshape qtimg height pixels_per_line img img[ qtimg width ]if bytes_per_pixel 4 and not qtimg hasAlphaChannel img img[ 2 -1 ]elif bytes_per_pixel 4 img[ 0 3] img[ 2 -1 ]return img
def imread_qt filename qtimg QImage if not qtimg load filename raise IOError 'Unabletoloadfile%s' % filename if qtimg depth 1 raise IOError '1-bitimagescurrentlynotsupported' arrayptr qtimg bits bytes_per_pixel qtimg depth // 8 pixels_per_line qtimg bytesPerLine // bytes_per_pixel img_size pixels_per_line * qtimg height * bytes_per_pixel arrayptr setsize img_size img np array arrayptr if bytes_per_pixel > 1 img img reshape qtimg height pixels_per_line bytes_per_pixel img img[ qtimg width ]else img img reshape qtimg height pixels_per_line img img[ qtimg width ]if bytes_per_pixel 4 and not qtimg hasAlphaChannel img img[ 2 -1 ]elif bytes_per_pixel 4 img[ 0 3] img[ 2 -1 ]return img
def example_number region_code return example_number_for_type region_code PhoneNumberType FIXED_LINE
def create_txt_response name txt_records return dns rrset from_text_list name 60 'IN' 'TXT' txt_records
def create_txt_response name txt_records return dns rrset from_text_list name 60 'IN' 'TXT' txt_records
def preprocess_on_cluster sff_fps log_fp fasta_fp None out_fp '/tmp/' squeeze False verbose False primer STANDARD_BACTERIAL_PRIMER cmd 'denoiser_preprocess py-i%s-l%s-o%s' % ' ' join sff_fps log_fp out_fp if fasta_fp cmd + '-f%s' % fasta_fp if squeeze cmd + '-s'if verbose cmd + '-v'if primer cmd + '-p%s' % primer submit_jobs [cmd] 'pp_' + make_tmp_name 6 wait_for_file out_fp + '/prefix_mapping txt' 10
def preprocess_on_cluster sff_fps log_fp fasta_fp None out_fp '/tmp/' squeeze False verbose False primer STANDARD_BACTERIAL_PRIMER cmd 'denoiser_preprocess py-i%s-l%s-o%s' % ' ' join sff_fps log_fp out_fp if fasta_fp cmd + '-f%s' % fasta_fp if squeeze cmd + '-s'if verbose cmd + '-v'if primer cmd + '-p%s' % primer submit_jobs [cmd] 'pp_' + make_tmp_name 6 wait_for_file out_fp + '/prefix_mapping txt' 10
def preprocess_on_cluster sff_fps log_fp fasta_fp None out_fp '/tmp/' squeeze False verbose False primer STANDARD_BACTERIAL_PRIMER cmd 'denoiser_preprocess py-i%s-l%s-o%s' % ' ' join sff_fps log_fp out_fp if fasta_fp cmd + '-f%s' % fasta_fp if squeeze cmd + '-s'if verbose cmd + '-v'if primer cmd + '-p%s' % primer submit_jobs [cmd] 'pp_' + make_tmp_name 6 wait_for_file out_fp + '/prefix_mapping txt' 10
def assertUndefinedComparison testCase s1 s2 testCase assertFalse s1 s2 testCase assertFalse s1 < s2 testCase assertFalse s1 < s2 testCase assertFalse s1 > s2 testCase assertFalse s1 > s2
def _ems_diff data0 data1 return np mean data0 axis 0 - np mean data1 axis 0
def obtain proxy if not isproxy proxy raise TypeError 'objectmustbeaproxy' if isinstance proxy function globals getconn proxy _local_namespacereturn _load_function _dump_function proxy globals else return pickle loads getconn proxy modules cPickle dumps proxy pickle HIGHEST_PROTOCOL
def osf_storage_root addon_config node_settings auth **kwargs node node_settings ownerroot rubeus build_addon_root node_settings node_settings name u'' permissions auth user auth user nodeUrl node url nodeApiUrl node api_url return [root]
def format_track track show_url True out track['title']out + 'by\x02{}\x02' format track['user']['username'] if track['genre'] out + '-\x02{}\x02' format track['genre'] out + '-\x02{ }\x02plays \x02{ }\x02favorites \x02{ }\x02comments' format track['playback_count'] track['favoritings_count'] track['comment_count'] if show_url out + '-{}' format web try_shorten track['permalink_url'] return out
def refresh_db full False if full return __salt__['cmd retcode'] '/bin/pkgrefresh--full' 0 else return __salt__['cmd retcode'] '/bin/pkgrefresh' 0
def parse_ml_headers headers attrs {}attrs['List-Archive'] headers get 'List-Archive' attrs['List-Help'] headers get 'List-Help' attrs['List-Id'] headers get 'List-Id' attrs['List-Owner'] headers get 'List-Owner' attrs['List-Post'] headers get 'List-Post' attrs['List-Subscribe'] headers get 'List-Subscribe' attrs['List-Unsubscribe'] headers get 'List-Unsubscribe' return attrs
def parse_ml_headers headers attrs {}attrs['List-Archive'] headers get 'List-Archive' attrs['List-Help'] headers get 'List-Help' attrs['List-Id'] headers get 'List-Id' attrs['List-Owner'] headers get 'List-Owner' attrs['List-Post'] headers get 'List-Post' attrs['List-Subscribe'] headers get 'List-Subscribe' attrs['List-Unsubscribe'] headers get 'List-Unsubscribe' return attrs
def get_fontconfig_fonts fontext 'ttf' try import commandsexcept ImportError return {}fontext get_fontext_synonyms fontext fontfiles {} status output commands getstatusoutput 'fc-listfile' if status 0 for line in output split '\n' fname line split ' ' [0]if os path splitext fname [1][1 ] in fontext and os path exists fname fontfiles[fname] 1return fontfiles
def get_fontconfig_fonts fontext 'ttf' try import commandsexcept ImportError return {}fontext get_fontext_synonyms fontext fontfiles {} status output commands getstatusoutput 'fc-listfile' if status 0 for line in output split '\n' fname line split ' ' [0]if os path splitext fname [1][1 ] in fontext and os path exists fname fontfiles[fname] 1return fontfiles
@no_debug_modedef train_yaml yaml_file train yaml_parse load yaml_file train main_loop
def _get_presser fig callbacks fig canvas callbacks callbacks['button_press_event']func Nonefor key val in callbacks items if val func __class__ __name__ 'partial' func val funcbreakassert func is not None return func
def load_fips_ecdsa_key_pair_vectors vector_data vectors []key_data Nonefor line in vector_data line line strip if not line or line startswith '#' continueif line[1 -1 ] in _ECDSA_CURVE_NAMES curve_name _ECDSA_CURVE_NAMES[line[1 -1 ]]elif line startswith 'd ' if key_data is not None vectors append key_data key_data {'curve' curve_name 'd' int line split ' ' [1] 16 }elif key_data is not None if line startswith 'Qx ' key_data['x'] int line split ' ' [1] 16 elif line startswith 'Qy ' key_data['y'] int line split ' ' [1] 16 assert key_data is not None vectors append key_data return vectors
def crc string return '%08x' % binascii crc32 string encode 'utf-8' & 4294967295
def setdefaulttimeout timeout if timeout is None timeout -1 0 else try timeout 0 0 + timeout except TypeError raise TypeError 'afloatisrequired' if timeout < 0 0 raise ValueError 'Timeoutvalueoutofrange' global _GLOBAL_TIMEOUT_VALUE_GLOBAL_TIMEOUT_VALUE timeout
def build_single_handler_applications paths argvs None applications {}argvs {} or argvs for path in paths application build_single_handler_application path argvs get path [] route application handlers[0] url_path if not route if '/' in applications raise RuntimeError "Don'tknowtheURLpathtousefor%s" % path route '/'applications[route] applicationreturn applications
@register filterdef to_js value return mark_safe 'JSON parse "%s" ' % escapejs jsonify value
def create_files paths chroot dirs files set set for path in paths path osp join chroot path filename osp basename path if filename '' dirs add path else dirs add osp dirname path files add path for dirpath in dirs if not osp isdir dirpath os makedirs dirpath for filepath in files open filepath 'w' close
def build_tool_dependencies_select_field app tool_shed_repository name multiple True display 'checkboxes' uninstalled_only False tool_dependencies_select_field SelectField name name multiple multiple display display for tool_dependency in tool_shed_repository tool_dependencies if uninstalled_only if tool_dependency status not in [app install_model ToolDependency installation_status NEVER_INSTALLED app install_model ToolDependency installation_status UNINSTALLED] continueelif tool_dependency status in [app install_model ToolDependency installation_status NEVER_INSTALLED app install_model ToolDependency installation_status UNINSTALLED] continueoption_label '%sversion%s' % str tool_dependency name str tool_dependency version option_value app security encode_id tool_dependency id tool_dependencies_select_field add_option option_label option_value return tool_dependencies_select_field
def multi_params schema return {'type' 'array' 'items' schema}
def _render_file file request file restat False if file type is None file type file encoding getTypeAndEncoding file basename file contentTypes file contentEncodings file defaultType if not file exists return file childNotFound render request if file isdir return file redirect request request setHeader 'accept-ranges' 'bytes' try fileForReading file openForReading except IOError as e import errnoif e[0] errno EACCES return ForbiddenResource render request else raiseproducer file makeProducer request fileForReading if request method 'HEAD' return ''producer start return NOT_DONE_YET
def _render_file file request file restat False if file type is None file type file encoding getTypeAndEncoding file basename file contentTypes file contentEncodings file defaultType if not file exists return file childNotFound render request if file isdir return file redirect request request setHeader 'accept-ranges' 'bytes' try fileForReading file openForReading except IOError as e import errnoif e[0] errno EACCES return ForbiddenResource render request else raiseproducer file makeProducer request fileForReading if request method 'HEAD' return ''producer start return NOT_DONE_YET
def add_user_permission doctype name user with_message False if name not in frappe defaults get_user_permissions user get doctype [] if not frappe db exists doctype name frappe throw _ u'{0}{1}notfound' format _ doctype name frappe DoesNotExistError frappe defaults add_default doctype name user u'UserPermission' elif with_message msgprint _ u'Permissionalreadyset'
def get_current_kernel_arch return platform machine
def run_child_process test_info test_info start_server sys exit 0
def run_child_process test_info test_info start_server sys exit 0
def clearNode node node childNodes[ ] []
def make_commodity_future_info first_sid root_symbols years month_codes None nineteen_days pd Timedelta days 19 one_year pd Timedelta days 365 return make_future_info first_sid first_sid root_symbols root_symbols years years notice_date_func lambda dt dt - MonthBegin 2 + nineteen_days expiration_date_func lambda dt dt - MonthBegin 1 + nineteen_days start_date_func lambda dt dt - one_year month_codes month_codes
def make_commodity_future_info first_sid root_symbols years month_codes None nineteen_days pd Timedelta days 19 one_year pd Timedelta days 365 return make_future_info first_sid first_sid root_symbols root_symbols years years notice_date_func lambda dt dt - MonthBegin 2 + nineteen_days expiration_date_func lambda dt dt - MonthBegin 1 + nineteen_days start_date_func lambda dt dt - one_year month_codes month_codes
def make_commodity_future_info first_sid root_symbols years month_codes None nineteen_days pd Timedelta days 19 one_year pd Timedelta days 365 return make_future_info first_sid first_sid root_symbols root_symbols years years notice_date_func lambda dt dt - MonthBegin 2 + nineteen_days expiration_date_func lambda dt dt - MonthBegin 1 + nineteen_days start_date_func lambda dt dt - one_year month_codes month_codes
def make_commodity_future_info first_sid root_symbols years month_codes None nineteen_days pd Timedelta days 19 one_year pd Timedelta days 365 return make_future_info first_sid first_sid root_symbols root_symbols years years notice_date_func lambda dt dt - MonthBegin 2 + nineteen_days expiration_date_func lambda dt dt - MonthBegin 1 + nineteen_days start_date_func lambda dt dt - one_year month_codes month_codes
def create_permission_grant_for_resource_db role_db resource_db permission_types permission_types _validate_permission_types resource_db resource_db permission_types permission_types resource_uid resource_db get_uid resource_type resource_db get_resource_type result create_permission_grant role_db role_db resource_uid resource_uid resource_type resource_type permission_types permission_types return result
def test_pickle_globals def f x return np sin x + np cos x assert set ['np' '__builtins__'] set _loads _dumps f __globals__ keys
def test_pickle_globals def f x return np sin x + np cos x assert set ['np' '__builtins__'] set _loads _dumps f __globals__ keys
def walk_modules path load False mods []mod __import__ path {} {} [''] mods append mod if hasattr mod '__path__' for _ subpath ispkg in iter_modules mod __path__ fullpath path + ' ' + subpath if ispkg mods + walk_modules fullpath else submod __import__ fullpath {} {} [''] mods append submod return mods
def home_directory name with settings hide 'running' 'stdout' return run 'echo~' + name
def validate *simple_vals **param_vals def val fn @wraps fn def newfn self *a **env try kw _make_validated_kw fn simple_vals param_vals env except RedditError as err self on_validation_error err for err in c errors self on_validation_error c errors[err] try return fn self *a **kw except RedditError as err self on_validation_error err set_api_docs newfn simple_vals param_vals newfn handles_csrf _validators_handle_csrf simple_vals param_vals return newfnreturn val
def validate *simple_vals **param_vals def val fn @wraps fn def newfn self *a **env try kw _make_validated_kw fn simple_vals param_vals env except RedditError as err self on_validation_error err for err in c errors self on_validation_error c errors[err] try return fn self *a **kw except RedditError as err self on_validation_error err set_api_docs newfn simple_vals param_vals newfn handles_csrf _validators_handle_csrf simple_vals param_vals return newfnreturn val
def writeElementNode derivation fileNames target xmlObject target xmlObjectif xmlObject None print 'Warning writeTargetinwritecouldnotgetxmlObjectfor 'print targetprint derivation elementNodereturnparserDirectory os path dirname derivation elementNode getOwnerDocument fileName absoluteFolderDirectory os path abspath os path join parserDirectory derivation folderName if '/models' not in absoluteFolderDirectory print 'Warning models/wasnotintheabsolutefilepath soforsecuritynothingwillbedonefor 'print derivation elementNodeprint 'Forwhichtheabsolutefolderpathis 'print absoluteFolderDirectoryprint 'Thewritetoolcanonlywriteafilewhichhasmodels/inthefilepath 'print 'Towritethefile movethefileintoafoldercalledmodel/orasubfolderwhichisinsidethemodelfoldertree 'returnquantity evaluate getEvaluatedInt 1 target 'quantity' for itemIndex in xrange quantity writeXMLObject absoluteFolderDirectory derivation fileNames target xmlObject
def login_user sender user request **kwargs backend get_backends [0]user backend '%s %s' % backend __module__ backend __class__ __name__ login request user request session['REGISTRATION_AUTO_LOGIN'] Truerequest session modified True
def ChiNoncentral name k l return rv name ChiNoncentralDistribution k l
def _get_vm_by_name name allDetails False vms get_resources_vms includeConfig allDetails if name in vms return vms[name]log info 'VMwithname"{0}"couldnotbefound ' format name return False
def _get_vm_by_name name allDetails False vms get_resources_vms includeConfig allDetails if name in vms return vms[name]log info 'VMwithname"{0}"couldnotbefound ' format name return False
def _get_vm_by_name name allDetails False vms get_resources_vms includeConfig allDetails if name in vms return vms[name]log info 'VMwithname"{0}"couldnotbefound ' format name return False
def test_scenario_matches_tags_excluding_when_scenario_has_no_tags scenario Scenario from_string SCENARIO1 original_string SCENARIO1 strip assert scenario matches_tags ['-nope' '-neither']
def extrema input labels None index None dims numpy array numpy asarray input shape dim_prod numpy cumprod [1] + list dims[ 0 -1 ] [ -1 ] minimums min_positions maximums max_positions _select input labels index find_min True find_max True find_min_positions True find_max_positions True if numpy isscalar minimums return minimums maximums tuple min_positions // dim_prod % dims tuple max_positions // dim_prod % dims min_positions [tuple v for v in min_positions reshape -1 1 // dim_prod % dims ]max_positions [tuple v for v in max_positions reshape -1 1 // dim_prod % dims ]return minimums maximums min_positions max_positions
def setup_platform hass config add_devices discovery_info None known_devices hass data get KNOWN_DEVICES_KEY if known_devices is None known_devices set hass data[KNOWN_DEVICES_KEY] known_devicesif config get CONF_HOST is not None host config get CONF_HOST port config get CONF_PORT name config get CONF_NAME timeout config get CONF_TIMEOUT elif discovery_info is not None tv_name model host discovery_infoname '{} {} ' format tv_name model port DEFAULT_PORTtimeout DEFAULT_TIMEOUTelse _LOGGER warning 'Internalerroronsamsungtvcomponent Cannotdeterminedevice' returnip_addr socket gethostbyname host if ip_addr not in known_devices known_devices add ip_addr add_devices [SamsungTVDevice host port name timeout ] _LOGGER info "SamsungTV%s %daddedas'%s'" host port name else _LOGGER info 'IgnoringduplicateSamsungTV%s %d' host port
def run_le_auto venv_dir base_url **kwargs env environ copy d dict XDG_DATA_HOME venv_dir LE_AUTO_JSON_URL base_url + 'certbot/json' LE_AUTO_DIR_TEMPLATE base_url + '%s/' LE_AUTO_PUBLIC_KEY '-----BEGINPUBLICKEY-----\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAsMoSzLYQ7E1sdSOkwelg\ntzKIh2qi3bpXuYtcfFC0XrvWig071NwIj+dZiT0OLZ2hPispEH0B7ISuuWg1ll7G\nhFW0VdbxL6JdGzS2ShNWkX9hE9z+j8VqwDPOBn3ZHm03qwpYkBDwQib3KqOdYbTT\nuUtJmmGcuk3a9Aq/sCT6DdfmTSdP5asdQYwIcaQreDrOosaS84DTWI3IU+UYJVgl\nLsIVPBuy9IcgHidUQ96hJnoPsDCWsHwX62495QKEarauyKQrJzFes0EY95orDM47\nZ5o/NDiQB11m91yNB0MmPYY9QSbnOA9j7IaaC97AwRLuwXY+/R2ablTcxurWou68\niQIDAQAB\n-----ENDPUBLICKEY-----' **kwargs env update d return out_and_err join venv_dir 'letsencrypt-auto' + '--version' shell True env env
def _find_boundaries_subpixel label_img ndim label_img ndimmax_label np iinfo label_img dtype maxlabel_img_expanded np zeros [ 2 * s - 1 for s in label_img shape] label_img dtype pixels [slice None None 2 ] * ndim label_img_expanded[pixels] label_imgedges np ones label_img_expanded shape dtype bool edges[pixels] Falselabel_img_expanded[edges] max_labelwindows view_as_windows np pad label_img_expanded 1 mode 'constant' constant_values 0 3 * ndim boundaries np zeros_like edges for index in np ndindex label_img_expanded shape if edges[index] values np unique windows[index] ravel if len values > 2 boundaries[index] Truereturn boundaries
def wlPen wl l1 400l2 700hue np clip l2 - l1 - wl - l1 * 0 8 / l2 - l1 0 0 8 val 1 0if wl > 700 val 1 0 * 700 - wl / 700 0 + 1 elif wl < 400 val wl * 1 0 / 400 0 color pg hsvColor hue 1 0 val pen pg mkPen color return pen
def wlPen wl l1 400l2 700hue np clip l2 - l1 - wl - l1 * 0 8 / l2 - l1 0 0 8 val 1 0if wl > 700 val 1 0 * 700 - wl / 700 0 + 1 elif wl < 400 val wl * 1 0 / 400 0 color pg hsvColor hue 1 0 val pen pg mkPen color return pen
def check_state_result running recurse False if not isinstance running dict return Falseif not running return Falseret Truefor state_result in six itervalues running if not recurse and not isinstance state_result dict ret Falseif ret and isinstance state_result dict result state_result get 'result' _empty if result is False ret Falseelif result is _empty and isinstance state_result dict and ret ret check_state_result state_result recurse True if not ret breakreturn ret
def stonith_create stonith_id stonith_device_type stonith_device_options None cibfile None return item_create item 'stonith' item_id stonith_id item_type stonith_device_type extra_args stonith_device_options cibfile cibfile
def stonith_create stonith_id stonith_device_type stonith_device_options None cibfile None return item_create item 'stonith' item_id stonith_id item_type stonith_device_type extra_args stonith_device_options cibfile cibfile
def generateCookieSecret return base64 b64encode uuid uuid4 bytes + uuid uuid4 bytes
def get_markdown_extensions return _markdown_extensions
def source_hashed source_filename prepared_options thumbnail_extension **kwargs source_sha hashlib sha1 source_filename encode u'utf-8' digest source_hash base64 urlsafe_b64encode source_sha[ 9] decode u'utf-8' parts u' ' join prepared_options[1 ] parts_sha hashlib sha1 parts encode u'utf-8' digest options_hash base64 urlsafe_b64encode parts_sha[ 6] decode u'utf-8' return u'%s_%s_%s %s' % source_hash prepared_options[0] options_hash thumbnail_extension
def checked response status_code int response['status'][ 3] if status_code 401 raise Unauthorizedif status_code 406 raise NoSessionif status_code 407 raise DownloadLimitReachedif status_code 413 raise InvalidImdbidif status_code 414 raise UnknownUserAgentif status_code 415 raise DisabledUserAgentif status_code 503 raise ServiceUnavailableif status_code 200 raise OpenSubtitlesError response['status'] return response
def checked response status_code int response['status'][ 3] if status_code 401 raise Unauthorizedif status_code 406 raise NoSessionif status_code 407 raise DownloadLimitReachedif status_code 413 raise InvalidImdbidif status_code 414 raise UnknownUserAgentif status_code 415 raise DisabledUserAgentif status_code 503 raise ServiceUnavailableif status_code 200 raise OpenSubtitlesError response['status'] return response
def get_permalink_path self try first_permalink_id self get_permalink_ids_iter next except StopIteration return Nonereturn '/{settings[PERMALINK_PATH]}/{first_permalink}' format settings self settings first_permalink first_permalink_id
def ord char if isinstance char unicodeT return __builtin__ ord char return __builtin__ ord to_unicode char 'utf-8'
def get_uuid prefix '' if settings DEBUG return prefix + str uuid uuid4 return str uuid uuid4
def get_next_downloadable_changeset_revision repository repo after_changeset_revision changeset_revisions [revision[1] for revision in get_metadata_revisions repository repo ]if len changeset_revisions 1 changeset_revision changeset_revisions[0]if changeset_revision after_changeset_revision return after_changeset_revisionfound_after_changeset_revision Falsefor changeset in repo changelog changeset_revision str repo changectx changeset if found_after_changeset_revision if changeset_revision in changeset_revisions return changeset_revisionelif not found_after_changeset_revision and changeset_revision after_changeset_revision found_after_changeset_revision Truereturn None
def get_next_downloadable_changeset_revision repository repo after_changeset_revision changeset_revisions [revision[1] for revision in get_metadata_revisions repository repo ]if len changeset_revisions 1 changeset_revision changeset_revisions[0]if changeset_revision after_changeset_revision return after_changeset_revisionfound_after_changeset_revision Falsefor changeset in repo changelog changeset_revision str repo changectx changeset if found_after_changeset_revision if changeset_revision in changeset_revisions return changeset_revisionelif not found_after_changeset_revision and changeset_revision after_changeset_revision found_after_changeset_revision Truereturn None
def get_next_downloadable_changeset_revision repository repo after_changeset_revision changeset_revisions [revision[1] for revision in get_metadata_revisions repository repo ]if len changeset_revisions 1 changeset_revision changeset_revisions[0]if changeset_revision after_changeset_revision return after_changeset_revisionfound_after_changeset_revision Falsefor changeset in repo changelog changeset_revision str repo changectx changeset if found_after_changeset_revision if changeset_revision in changeset_revisions return changeset_revisionelif not found_after_changeset_revision and changeset_revision after_changeset_revision found_after_changeset_revision Truereturn None
def get_extension srcurl if 'youtu' in srcurl return 'video/youtube'else disassembled urlparse srcurl file_ext splitext basename disassembled path [1]return 'video/' + file_ext replace ' ' ''
def flowgram_id_to_seq_id_map seqs result {}for id_ seq in seqs fields id_ split seq_id id_flowgram_id fields[1]result[flowgram_id] seq_idreturn result
def linspace start stop num 50 chunks None dtype None num int num if chunks is None raise ValueError 'Mustsupplyachunks keywordargument' chunks normalize_chunks chunks num range_ stop - start space float range_ / num - 1 if dtype is None dtype np linspace 0 1 1 dtypename 'linspace-' + tokenize start stop num chunks dtype dsk {}blockstart startfor i bs in enumerate chunks[0] blockstop blockstart + bs - 1 * space task partial np linspace dtype dtype blockstart blockstop bs blockstart blockstart + space * bs dsk[ name i ] taskreturn Array dsk name chunks dtype dtype
def ext_functest_builder method func def do_test self if not isinstance func types BuiltinFunctionType raise SkipTest '%sextensionnotfound' % func method self func return do_test
def ext_functest_builder method func def do_test self if not isinstance func types BuiltinFunctionType raise SkipTest '%sextensionnotfound' % func method self func return do_test
def requires_nltk_corpus func @wraps func def decorated *args **kwargs try return func *args **kwargs except LookupError as err print errraise MissingCorpusError return decorated
@world absorbdef css_click css_selector index 0 wait_time GLOBAL_WAIT_FOR_TIMEOUT dismiss_alert False wait_for_clickable css_selector timeout wait_time wait_for_visible css_selector index index timeout wait_time assert_true css_visible css_selector index index msg 'Element{}[{}]ispresentbutnotvisible' format css_selector index retry_on_exception lambda css_find css_selector [index] click if dismiss_alert world browser get_alert accept wait_for_js_to_load return True
def getHorizontalSegmentListsFromLoopLists alreadyFilledArounds front numberOfLines rotatedFillLoops width xIntersectionIndexLists []yList []frontOverWidth getFrontOverWidthAddXListYList front alreadyFilledArounds numberOfLines xIntersectionIndexLists width yList addXIntersectionIndexesFromLoops frontOverWidth rotatedFillLoops -1 xIntersectionIndexLists width yList horizontalSegmentLists []for xIntersectionIndexListIndex in xrange len xIntersectionIndexLists xIntersectionIndexList xIntersectionIndexLists[xIntersectionIndexListIndex]lineSegments getSegmentsFromXIntersectionIndexes xIntersectionIndexList yList[xIntersectionIndexListIndex] horizontalSegmentLists append lineSegments return horizontalSegmentLists
def parse_hl_lines expr if not expr return []try return list map int expr split except ValueError return []
def _link_to text *args **kwargs assert len args < 2 'Toomanyunnamedarguments'def _link_class kwargs 'createsclassesforthelink_tocalls'suppress_active_class kwargs pop 'suppress_active_class' False if not suppress_active_class and _link_active kwargs active 'active'else active ''kwargs pop 'highlight_actions' '' return kwargs pop 'class_' '' + active or None def _create_link_text text **kwargs 'Updatelinktexttoaddaiconorspanifspecifiedinthe\nkwargs'if kwargs pop 'inner_span' None text literal '<span>' + text + literal '</span>' if icon text literal '<iclass "icon-%s"></i>' % icon + text return texticon kwargs pop 'icon' None class_ _link_class kwargs return tags link_to _create_link_text text **kwargs url_for *args **kwargs class_ class_
def _link_to text *args **kwargs assert len args < 2 'Toomanyunnamedarguments'def _link_class kwargs 'createsclassesforthelink_tocalls'suppress_active_class kwargs pop 'suppress_active_class' False if not suppress_active_class and _link_active kwargs active 'active'else active ''kwargs pop 'highlight_actions' '' return kwargs pop 'class_' '' + active or None def _create_link_text text **kwargs 'Updatelinktexttoaddaiconorspanifspecifiedinthe\nkwargs'if kwargs pop 'inner_span' None text literal '<span>' + text + literal '</span>' if icon text literal '<iclass "icon-%s"></i>' % icon + text return texticon kwargs pop 'icon' None class_ _link_class kwargs return tags link_to _create_link_text text **kwargs url_for *args **kwargs class_ class_
def xstrip filename while xisabs filename if re match '\\w [\\\\/]' filename filename re sub '^\\w+ [\\\\/]+' '' filename elif re match '[\\\\/]' filename filename re sub '^[\\\\/]+' '' filename return filename
def xstrip filename while xisabs filename if re match '\\w [\\\\/]' filename filename re sub '^\\w+ [\\\\/]+' '' filename elif re match '[\\\\/]' filename filename re sub '^[\\\\/]+' '' filename return filename
@click command @click option '-f' '--filename' help 'FixturesJSONpath' default ' /etc/fixtures/initial_data json' @click option '-b' '--baseurl' help 'baseurltouse' default None @click option '--reset/--no-reset' default False def populate filename baseurl None reset False populator Populate db filepath filename baseurl baseurl app app if not reset try populator except RuntimeError with app test_request_context base_url baseurl or 'http //localhost 5000/' populator else populator reset
@contextlib contextmanagerdef captured_cuda_stdout sys stdout flush if config ENABLE_CUDASIM with captured_stdout as stream yield PythonTextCapture stream else from numba import cudafd sys __stdout__ fileno with redirect_fd fd as stream yield CUDATextCapture stream cuda synchronize
@contextlib contextmanagerdef captured_cuda_stdout sys stdout flush if config ENABLE_CUDASIM with captured_stdout as stream yield PythonTextCapture stream else from numba import cudafd sys __stdout__ fileno with redirect_fd fd as stream yield CUDATextCapture stream cuda synchronize
def matchOnlyAtCol n def verifyCol strg locn toks if col locn strg n raise ParseException strg locn 'matchedtokennotatcolumn%d' % n return verifyCol
def matchOnlyAtCol n def verifyCol strg locn toks if col locn strg n raise ParseException strg locn 'matchedtokennotatcolumn%d' % n return verifyCol
def max_heapify seq i n l 2 * i + 1 r 2 * i + 2 if l < n and seq[l] > seq[i] largest lelse largest iif r < n and seq[r] > seq[largest] largest rif largest i seq[i] seq[largest] seq[largest] seq[i] max_heapify seq largest n
def max_heapify seq i n l 2 * i + 1 r 2 * i + 2 if l < n and seq[l] > seq[i] largest lelse largest iif r < n and seq[r] > seq[largest] largest rif largest i seq[i] seq[largest] seq[largest] seq[i] max_heapify seq largest n
def max_heapify seq i n l 2 * i + 1 r 2 * i + 2 if l < n and seq[l] > seq[i] largest lelse largest iif r < n and seq[r] > seq[largest] largest rif largest i seq[i] seq[largest] seq[largest] seq[i] max_heapify seq largest n
def max_heapify seq i n l 2 * i + 1 r 2 * i + 2 if l < n and seq[l] > seq[i] largest lelse largest iif r < n and seq[r] > seq[largest] largest rif largest i seq[i] seq[largest] seq[largest] seq[i] max_heapify seq largest n
def dup_integrate f m K if m < 0 or not f return fg [K zero] * m for i c in enumerate reversed f n i + 1 for j in range 1 m n * i + j + 1 g insert 0 K exquo c K n return g
def _rename_node graph node_id copy_id graph _add_node_silent copy_id graph node[copy_id] graph node[node_id]for nbr in graph neighbors node_id wt graph[node_id][nbr]['weight']graph add_edge nbr copy_id {'weight' wt} graph remove_node node_id
def _rename_node graph node_id copy_id graph _add_node_silent copy_id graph node[copy_id] graph node[node_id]for nbr in graph neighbors node_id wt graph[node_id][nbr]['weight']graph add_edge nbr copy_id {'weight' wt} graph remove_node node_id
def analyze_modules project task_handle taskhandle NullTaskHandle resources project get_python_files job_set task_handle create_jobset 'AnalyzingModules' len resources for resource in resources job_set started_job resource path analyze_module project resource job_set finished_job
def repolist status '' media None manager MANAGERwith settings hide 'running' 'stdout' if media repos run_as_root "% manager srepolist% status s sed'$d' sed-n'/repoid/ $p'" % locals else repos run_as_root "% manager srepolist% status s sed'/Media\\ Debug/d' sed'$d' sed-n'/repoid/ $p'" % locals return map lambda line line split '' [0] repos splitlines [1 ]
@raise_eventdef send_response resource response if isinstance response Response return responseelse return _prepare_response resource * response if response else [None]
def abcd_normalize A None B None C None D None A B C D map _atleast_2d_or_none A B C D MA NA _shape_or_none A MB NB _shape_or_none B MC NC _shape_or_none C MD ND _shape_or_none D p _choice_not_none MA MB NC q _choice_not_none NB ND r _choice_not_none MC MD if p is None or q is None or r is None raise ValueError 'Notenoughinformationonthesystem ' A B C D map _none_to_empty_2d A B C D A _restore A p p B _restore B p q C _restore C r p D _restore D r q return A B C D
def NamedTupleRow cursor from collections import namedtupleattr_names [x[0] for x in cursor _ColBufferList]class Row namedtuple 'Row' attr_names rename True cursor_description cursor descriptiondef __new__ cls iterable return super Row cls __new__ cls *iterable return Row
def NamedTupleRow cursor from collections import namedtupleattr_names [x[0] for x in cursor _ColBufferList]class Row namedtuple 'Row' attr_names rename True cursor_description cursor descriptiondef __new__ cls iterable return super Row cls __new__ cls *iterable return Row
def NamedTupleRow cursor from collections import namedtupleattr_names [x[0] for x in cursor _ColBufferList]class Row namedtuple 'Row' attr_names rename True cursor_description cursor descriptiondef __new__ cls iterable return super Row cls __new__ cls *iterable return Row
def relaxed_average var_name_suffix rx_step relaxed_vars []for l in xrange rx_step with tf variable_scope 'RX%d' % l reuse True try relaxed_vars append tf get_variable var_name_suffix except ValueError passdsum tf add_n relaxed_vars avg dsum / len relaxed_vars diff [ v - avg for v in relaxed_vars]davg tf add_n [ d * d for d in diff] return avg tf reduce_sum davg
def relaxed_average var_name_suffix rx_step relaxed_vars []for l in xrange rx_step with tf variable_scope 'RX%d' % l reuse True try relaxed_vars append tf get_variable var_name_suffix except ValueError passdsum tf add_n relaxed_vars avg dsum / len relaxed_vars diff [ v - avg for v in relaxed_vars]davg tf add_n [ d * d for d in diff] return avg tf reduce_sum davg
def relaxed_average var_name_suffix rx_step relaxed_vars []for l in xrange rx_step with tf variable_scope 'RX%d' % l reuse True try relaxed_vars append tf get_variable var_name_suffix except ValueError passdsum tf add_n relaxed_vars avg dsum / len relaxed_vars diff [ v - avg for v in relaxed_vars]davg tf add_n [ d * d for d in diff] return avg tf reduce_sum davg
def _chunks l n for i in range 0 len l n yield l[i i + n ]
def direct_to_template request template extra_context None **kwargs context extra_context or {} context[u'params'] kwargsfor key value in context items if callable value context[key] value return TemplateResponse request template context
def direct_to_template request template extra_context None **kwargs context extra_context or {} context[u'params'] kwargsfor key value in context items if callable value context[key] value return TemplateResponse request template context
def direct_to_template request template extra_context None **kwargs context extra_context or {} context[u'params'] kwargsfor key value in context items if callable value context[key] value return TemplateResponse request template context
def receive_message sock operation request_id max_message_size MAX_MESSAGE_SIZE header _receive_data_on_socket sock 16 length _UNPACK_INT header[ 4] [0]actual_op _UNPACK_INT header[12 ] [0]if operation actual_op raise ProtocolError 'Gotopcode%rbutexpected%r' % actual_op operation if request_id is not None response_id _UNPACK_INT header[8 12] [0]if request_id response_id raise ProtocolError 'Gotresponseid%rbutexpected%r' % response_id request_id if length < 16 raise ProtocolError 'Messagelength %r notlongerthanstandardmessageheadersize 16 ' % length if length > max_message_size raise ProtocolError 'Messagelength %r islargerthanservermaxmessagesize %r ' % length max_message_size return _receive_data_on_socket sock length - 16
def list_master_symlinks saltenv 'base' prefix '' return __context__['fileclient'] symlink_list saltenv prefix
def RemoveLinkDependenciesFromNoneTargets targets for target_name target_dict in targets iteritems for dependency_key in dependency_sections dependencies target_dict get dependency_key [] if dependencies for t in dependencies if target_dict get 'type' None 'none' if targets[t] get 'variables' {} get 'link_dependency' 0 target_dict[dependency_key] Filter target_dict[dependency_key] t
def remove_markup html return HTML bleach clean html tags [] strip True
def chess_pgn_graph pgn_file 'chess_masters_WCC pgn bz2' import bz2G nx MultiDiGraph game {}datafile bz2 BZ2File pgn_file lines line decode rstrip '\r\n' for line in datafile for line in lines if line startswith '[' tag value line[1 -1 ] split '' 1 game[str tag ] value strip '"' elif game white game pop 'White' black game pop 'Black' G add_edge white black **game game {}return G
def chess_pgn_graph pgn_file 'chess_masters_WCC pgn bz2' import bz2G nx MultiDiGraph game {}datafile bz2 BZ2File pgn_file lines line decode rstrip '\r\n' for line in datafile for line in lines if line startswith '[' tag value line[1 -1 ] split '' 1 game[str tag ] value strip '"' elif game white game pop 'White' black game pop 'Black' G add_edge white black **game game {}return G
def chess_pgn_graph pgn_file 'chess_masters_WCC pgn bz2' import bz2G nx MultiDiGraph game {}datafile bz2 BZ2File pgn_file lines line decode rstrip '\r\n' for line in datafile for line in lines if line startswith '[' tag value line[1 -1 ] split '' 1 game[str tag ] value strip '"' elif game white game pop 'White' black game pop 'Black' G add_edge white black **game game {}return G
def SetHelpMenuOtherHelp mainMenu global helpIDMapif helpIDMap is None helpIDMap {}cmdID win32ui ID_HELP_OTHERexcludeList ['MainPythonDocumentation' 'PythonwinReference']firstList ListAllHelpFiles excludeFnames []for desc fname in firstList if desc in excludeList excludeFnames append fname helpDescs []for desc fname in firstList if fname not in excludeFnames helpIDMap[cmdID] desc fname win32ui GetMainFrame HookCommand HandleHelpOtherCommand cmdID cmdID cmdID + 1 helpMenu mainMenu GetSubMenu mainMenu GetMenuItemCount - 1 otherHelpMenuPos 2otherMenu helpMenu GetSubMenu otherHelpMenuPos while otherMenu GetMenuItemCount otherMenu DeleteMenu 0 win32con MF_BYPOSITION if helpIDMap for id desc fname in helpIDMap iteritems otherMenu AppendMenu win32con MF_ENABLED win32con MF_STRING id desc else helpMenu EnableMenuItem otherHelpMenuPos win32con MF_BYPOSITION win32con MF_GRAYED
def redirect post_save_redirect obj if post_save_redirect return HttpResponseRedirect post_save_redirect % obj __dict__ elif hasattr obj 'get_absolute_url' return HttpResponseRedirect obj get_absolute_url else raise ImproperlyConfigured 'NoURLtoredirectto Eitherpassapost_save_redirectparametertothegenericviewordefineaget_absolute_urlmethodontheModel '
def redirect post_save_redirect obj if post_save_redirect return HttpResponseRedirect post_save_redirect % obj __dict__ elif hasattr obj 'get_absolute_url' return HttpResponseRedirect obj get_absolute_url else raise ImproperlyConfigured 'NoURLtoredirectto Eitherpassapost_save_redirectparametertothegenericviewordefineaget_absolute_urlmethodontheModel '
def write_fft fft_features fn base_fn ext os path splitext fn data_fn base_fn + ' fft' np save data_fn fft_features print 'Written' % data_fn
def get_osarch ret subprocess Popen 'rpm--eval"%{_host_cpu}"' shell True close_fds True stdout subprocess PIPE stderr subprocess PIPE communicate [0]return ret or 'unknown'
def package_minifest request if not settings MARKETPLACE_GUID return HttpResponseNotFound return mini_manifest request settings MARKETPLACE_GUID
def render template context None **kwargs renderer Renderer return renderer render template context **kwargs
def _serialize_rules rules result [ rule_name str rule for rule_name rule in rules items ]return sorted result key lambda rule rule[0]
def _serialize_rules rules result [ rule_name str rule for rule_name rule in rules items ]return sorted result key lambda rule rule[0]
def fix_return node if isinstance node datetime datetime node_value str node elif isinstance node list node_value [fix_return item for item in node]elif isinstance node dict node_value dict [ item fix_return node[item] for item in node keys ] else node_value nodereturn node_value
def add_lock packages **kwargs locks list_locks added []try packages list __salt__['pkg_resource parse_targets'] packages [0] keys except MinionError as exc raise CommandExecutionError exc for pkg in packages if not locks get pkg added append pkg if added __zypper__ call 'al' *added return {'added' len added 'packages' added}
def allcap_differential words is_different Falseallcap_words 0for word in words if word isupper allcap_words + 1cap_differential len words - allcap_words if cap_differential > 0 and cap_differential < len words is_different Truereturn is_different
@after each_scenariodef stop_stubs _scenario for name in SERVICES keys stub_server getattr world name None if stub_server is not None stub_server shutdown
def _filter_bultins module name module __name__return not name startswith 'django contrib' and name 'lettuce django'
def _filter_bultins module name module __name__return not name startswith 'django contrib' and name 'lettuce django'
def _filter_bultins module name module __name__return not name startswith 'django contrib' and name 'lettuce django'
def get_name_levels node visitor _NodeNameCollector ast walk node visitor return visitor names
@simple_decoratordef check_local_site_access view_func def _check request local_site_name None *args **kwargs if local_site_name if not request local_site raise Http404local_site request local_siteif not local_site is_accessible_by request user if local_site public or request user is_authenticated response render_to_response 'permission_denied html' RequestContext request response status_code 403return responseelse return HttpResponseRedirect '%s?next_page %s' % reverse 'login' request get_full_path else local_site Nonereturn view_func request local_site local_site *args **kwargs return _check
def _getLastMessage acc path os path expanduser '~/Library/ApplicationSupport/Skype/' + getUserName + '/main db' with contextlib closing sqlite3 connect path cursor as db db execute 'SELECTtimestamp body_xmlFROMMessagesWHEREauthor ?ORDERBYtimestampDESCLIMIT1' acc return db fetchone
def render_link_tag url rel u'stylesheet' media None attrs {u'href' url u'rel' rel}if media attrs[u'media'] mediareturn render_tag u'link' attrs attrs close False
def strFromDate date return time strftime '%Y_%m_%d%H %M' date
def EscapeMakeVariableExpansion s return s replace '$' '$$'
def EscapeMakeVariableExpansion s return s replace '$' '$$'
def EscapeMakeVariableExpansion s return s replace '$' '$$'
def group def prep r if r interactive or r representation 'aadata' resource r resourcelist_fields ['name' 'description' 'team_status_team status_id' 'comments']resource configure list_fields list_fields if r interactive from s3 import S3SQLCustomForm S3SQLInlineComponentcrud_fields ['name' 'description' S3SQLInlineComponent 'team_status_team' fields [ '' 'status_id' ] label T 'Status' multiple False 'comments']crud_form S3SQLCustomForm *crud_fields r resource configure crud_form crud_form return Trues3 prep prepreturn s3_rest_controller 'pr' 'group'
def saveNameCacheToDb cache_db_con db DBConnection 'cache db' for name indexer_id in nameCache iteritems cache_db_con action 'INSERTORREPLACEINTOscene_names indexer_id name VALUES ? ? ' [indexer_id name]
def _read_hc directory fname _make_ctf_name directory 'hc' raise_error False if fname is None logger info 'hcdatanotpresent' return Nones list with open fname 'rb' as fid while True p _read_one_coil_point fid if p is None if len s 0 logger info 'hcfileempty nodatapresent' return Nonelogger info 'hcdataread ' return sif p['valid'] s append p
def _read_hc directory fname _make_ctf_name directory 'hc' raise_error False if fname is None logger info 'hcdatanotpresent' return Nones list with open fname 'rb' as fid while True p _read_one_coil_point fid if p is None if len s 0 logger info 'hcfileempty nodatapresent' return Nonelogger info 'hcdataread ' return sif p['valid'] s append p
def getDistanceToLine begin end point pointMinusBegin point - begin if begin end return abs pointMinusBegin endMinusBegin end - begin return abs endMinusBegin cross pointMinusBegin / abs endMinusBegin
def run_traffic_step emr_connection step jobflow_name wait True sleeptime 60 retries 1 **jobflow_kw jobflowid _add_step emr_connection step jobflow_name **jobflow_kw if not wait returnattempts 1exit_state _wait_for_step emr_connection step jobflowid sleeptime while attempts < retries and exit_state COMPLETED jobflowid _add_step emr_connection step jobflow_name **jobflow_kw exit_state _wait_for_step emr_connection step jobflowid sleeptime attempts + 1if exit_state COMPLETED msg '%sfailed exit %s ' % step name exit_state if retries msg + 'retried%stimes' % retries raise EmrException msg
def cms_to_token cms_text start_delim '-----BEGINCMS-----'end_delim '-----ENDCMS-----'signed_text cms_textsigned_text signed_text replace '/' '-' signed_text signed_text replace start_delim '' signed_text signed_text replace end_delim '' signed_text signed_text replace '\n' '' return signed_text
def is_int s if not s return Truetry int s return Trueexcept ValueError return False
def _get_conn socket '/var/run/haproxy sock' assert os path exists socket '{0}doesnotexist ' format socket issock os stat socket st_modeassert stat S_ISSOCK issock '{0}isnotasocket ' format socket ha_conn haproxy conn HaPConn socket return ha_conn
def lint_citations tool_xml lint_ctx root tool_xml getroot citations root findall 'citations' if len citations > 1 lint_ctx error 'Morethanonecitationsectionfound behaviorundefined ' returnif len citations 0 lint_ctx warn 'Nocitationsfound consideraddingcitationstoyourtool ' returnvalid_citations 0for citation in citations[0] if citation tag 'citation' lint_ctx warn 'Unknowntagdiscoveredincitationsblock[%s] willbeignored ' % citation tag if 'type' in citation attrib citation_type citation attrib get 'type' if citation_type not in ['doi' 'bibtex'] lint_ctx warn 'Unknowncitationtypediscovered[%s] willbeignored ' citation_type else valid_citations + 1if valid_citations > 0 lint_ctx valid 'Found%dlikelyvalidcitations ' valid_citations
def get_snapshot artifactory_url repository group_id artifact_id packaging version snapshot_version None target_dir '/tmp' target_file None classifier None username None password None log debug ' MODULEFUNCTION artifactory get_snapshot artifactory_url %s repository %s group_id %s artifact_id %s packaging %s version %s target_dir %s classifier %s ' artifactory_url repository group_id artifact_id packaging version target_dir classifier headers {}if username and password headers['Authorization'] 'Basic{0}' format base64 encodestring '{0} {1}' format username password replace '\n' '' snapshot_url file_name _get_snapshot_url artifactory_url artifactory_url repository repository group_id group_id artifact_id artifact_id version version packaging packaging snapshot_version snapshot_version classifier classifier headers headers target_file __resolve_target_file file_name target_dir target_file return __save_artifact snapshot_url target_file headers
def returns_None function def call_and_assert *args **kwargs original_args copy deepcopy args original_kwargs copy deepcopy kwargs result function *args **kwargs assert result is None 'ShouldreturnNonewhencalledwithargs {args}andkwargs {kwargs}' format args original_args kwargs original_kwargs return resultreturn call_and_assert
def delete_demo exploration_id if not exp_domain Exploration is_demo_exploration_id exploration_id raise Exception 'Invaliddemoexplorationid%s' % exploration_id exploration get_exploration_by_id exploration_id strict False if not exploration logging info 'Explorationwithid%swasnotdeleted becauseitdoesnotexist ' % exploration_id else delete_exploration feconf SYSTEM_COMMITTER_ID exploration_id force_deletion True
def hypersimp f k f sympify f g f subs k k + 1 / f g g rewrite gamma g expand_func g g powsimp g deep True combine 'exp' if g is_rational_function k return simplify g ratio S Infinity else return None
def setup_platform hass config add_devices discovery_info None from xboxapi import xbox_apiapi xbox_api XboxApi config get CONF_API_KEY devices []for xuid in config get CONF_XUID new_device XboxSensor hass api xuid if new_device success_init devices append new_device if len devices > 0 add_devices devices else return False
@contextlib contextmanagerdef _refcounting type_ gc collect refcount len objgraph by_type type_ yield refcount gc collect assert len objgraph by_type type_ < refcount 'More{0 r}objectsstillinmemorythanbefore '
def exception_handler type value tb msg ''if hasattr value 'filename' and value filename None msg value filename + ' ' if hasattr value 'strerror' and value strerror None msg + value strerrorelse msg + str value if len msg error_msg_qt msg
def exception_handler type value tb msg ''if hasattr value 'filename' and value filename None msg value filename + ' ' if hasattr value 'strerror' and value strerror None msg + value strerrorelse msg + str value if len msg error_msg_qt msg
def _lombscargle_cdf z N normalization dH 1 dK 3 if dK - dH 2 raise NotImplementedError 'Degreesoffreedom 2' Nk N - dK if normalization 'psd' return 1 - np exp - z elif normalization 'standard' return 1 - 1 + z ** -0 5 * Nk elif normalization 'model' return 1 - 1 - z ** 0 5 * Nk elif normalization 'log' return 1 - np exp -0 5 * Nk * z else raise ValueError "normalization '{0}'isnotrecognized" format normalization
def _lombscargle_cdf z N normalization dH 1 dK 3 if dK - dH 2 raise NotImplementedError 'Degreesoffreedom 2' Nk N - dK if normalization 'psd' return 1 - np exp - z elif normalization 'standard' return 1 - 1 + z ** -0 5 * Nk elif normalization 'model' return 1 - 1 - z ** 0 5 * Nk elif normalization 'log' return 1 - np exp -0 5 * Nk * z else raise ValueError "normalization '{0}'isnotrecognized" format normalization
def test_close_process_when_normal with pipeline get_cat_pipeline pipeline PIPE pipeline PIPE as pl assert len pl commands 1 assert pl commands[0] _process poll is None pipeline_wait pl
def test_close_process_when_normal with pipeline get_cat_pipeline pipeline PIPE pipeline PIPE as pl assert len pl commands 1 assert pl commands[0] _process poll is None pipeline_wait pl
def test_close_process_when_normal with pipeline get_cat_pipeline pipeline PIPE pipeline PIPE as pl assert len pl commands 1 assert pl commands[0] _process poll is None pipeline_wait pl
def deprecatedDeferredGenerator f return runWithWarningsSuppressed [SUPPRESS message 'twisted internet defer deferredGeneratorwasdeprecated' ] deferredGenerator f
def value_for_key tuple_of_tuples key for t in tuple_of_tuples if t[0] key return t[1]else return key
def memory_usage pid if pid 0 return 0 0 start_time parameter time time 'memoryusage' status_path '/proc/%s/status' % pid mem_lines _get_lines status_path 'VmRSS ' 'VmSize ' parameter try residentSize int mem_lines['VmRSS '] split [1] * 1024 virtualSize int mem_lines['VmSize '] split [1] * 1024 _log_runtime parameter '%s[VmRSS VmSize]' % status_path start_time return residentSize virtualSize except exc IOError 'unabletoparsethe%sVmRSSandVmSizeentries %s' % status_path ' ' join mem_lines _log_failure parameter exc raise exc
def convert_background_to_jpg background_url file_path get_path_of_temp_url background_url im Image open file_path out_im file_path replace 'png' 'jpg' bg Image new 'RGB' im size 255 255 255 bg paste im 0 0 im bg save out_im quality 55 return out_im
def convert_background_to_jpg background_url file_path get_path_of_temp_url background_url im Image open file_path out_im file_path replace 'png' 'jpg' bg Image new 'RGB' im size 255 255 255 bg paste im 0 0 im bg save out_im quality 55 return out_im
def convert_background_to_jpg background_url file_path get_path_of_temp_url background_url im Image open file_path out_im file_path replace 'png' 'jpg' bg Image new 'RGB' im size 255 255 255 bg paste im 0 0 im bg save out_im quality 55 return out_im
def clean_dir directory sh 'find{dir}-typef-delete' format dir directory
def gaussian x mean stddev tmp -0 5 * sum x - mean / stddev ** 2 return np exp tmp / np power 2 0 * np pi 0 5 * len x * stddev
def ls dir None recurse False indent 0 dir dir or os getcwd fns os listdir dir fns sort for fn in fns full os path join dir fn if os path isdir full fn fn + '/' print '' * indent + fn if os path isdir full and recurse ls dir full recurse True indent indent + 2
def get_user return _store user
def get_file_name type test platform None return '_' join get_platform_string platform type __module__ get_class_name type test + ' log'
def __virtual__ if salt utils is_sunos and _check_fmadm and _check_fmdump return __virtualname__return False '{0}modulecanonlybeloadedonSolariswiththefaultmanagementinstalled' format __virtualname__
def gidcounter global GCOUNTGCOUNT + 1return '%s-%s' % time strftime DATESTRING GCOUNT
def log_to_file filename level DEBUG l logging getLogger 'paramiko' if len l handlers > 0 returnl setLevel level f open filename 'w' lh logging StreamHandler f lh setFormatter logging Formatter '% levelname - 3s[% asctime s % msecs 03d]thr % _threadid -3d% name s % message s' '%Y%m%d-%H %M %S' l addHandler lh
def _makeLineNumbers howMany width len str howMany labels [ '%*d' % width i for i in range 1 howMany + 1 ]p dom Element 'p' p setAttribute 'class' 'py-linenumber' t dom Text t data '\n' join labels + '\n' p appendChild t return p
def modelControlFinishLearningCb model model finishLearning return
def resample obj kind None **kwds tg TimeGrouper **kwds return tg _get_resampler obj kind kind
def resample obj kind None **kwds tg TimeGrouper **kwds return tg _get_resampler obj kind kind
def server_enable s_name **connection_args ret Trueserver _server_get s_name **connection_args if server is None return Falseif server get_state 'ENABLED' return Truenitro _connect **connection_args if nitro is None return Falsetry NSServer enable nitro server except NSNitroError as error log debug 'netscalermoduleerror-NSServer enable failed {0}' format error ret False_disconnect nitro return ret
def _linesearch_powell func p xi tol 0 001 def myfunc alpha return func p + alpha * xi alpha_min fret iter num brent myfunc full_output 1 tol tol xi alpha_min * xi return squeeze fret p + xi xi
def _linesearch_powell func p xi tol 0 001 def myfunc alpha return func p + alpha * xi alpha_min fret iter num brent myfunc full_output 1 tol tol xi alpha_min * xi return squeeze fret p + xi xi
def stopTouchApp if EventLoop is None returnif EventLoop status 'started' returnLogger info 'Base Leavingapplicationinprogress ' EventLoop close
def stopTouchApp if EventLoop is None returnif EventLoop status 'started' returnLogger info 'Base Leavingapplicationinprogress ' EventLoop close
def show_hc kwargs None call None if call 'function' raise SaltCloudSystemExit 'Theshow_hcfunctionmustbecalledwith-for--function ' if not kwargs or 'name' not in kwargs log error 'Mustspecifynameofhealthcheck ' return Falseconn get_conn return _expand_item conn ex_get_healthcheck kwargs['name']
def get_default_keychain user None domain 'user' cmd 'securitydefault-keychain-d{0}' format domain return __salt__['cmd run'] cmd runas user
def get_default_keychain user None domain 'user' cmd 'securitydefault-keychain-d{0}' format domain return __salt__['cmd run'] cmd runas user
def get_default_keychain user None domain 'user' cmd 'securitydefault-keychain-d{0}' format domain return __salt__['cmd run'] cmd runas user
def _wrapper orig pre '' post '' err_ None run_args None **kwargs ret ''cmd _cmd **kwargs cmd_str '' join [x for x in pre cmd post orig ] if run_args and isinstance run_args dict res __salt__['cmd run_all'] cmd_str **run_args else res __salt__['cmd run_all'] cmd_str if isinstance err_ dict for k v in six itermitems res err_[k] vif 'retcode' in res and res['retcode'] 0 msg '' join [x for x in res['stdout'] res['stderr'] if x] ret 'Unabletorun"{0}"withrun_args "{1}" Error {2}' format cmd_str run_args msg log error ret else try ret res['stdout']except KeyError log error "cmd run_alldidnotreturnadictionarywithakeynamed'stdout'" return ret
def test_nvidia_driver3 var cuda fvector f theano function [var] var + 1 mode mode_with_gpu profile False topo f maker fgraph toposort assert any [isinstance node op cuda GpuElemwise for node in topo] assert theano sandbox cuda use device_number is not None
def test_nvidia_driver3 var cuda fvector f theano function [var] var + 1 mode mode_with_gpu profile False topo f maker fgraph toposort assert any [isinstance node op cuda GpuElemwise for node in topo] assert theano sandbox cuda use device_number is not None
@app route '/hello' def hello response make_response json dumps {'hello' current_app config['HELLO']} response headers['Content-Type'] 'application/json'return response
@app route '/hello' def hello response make_response json dumps {'hello' current_app config['HELLO']} response headers['Content-Type'] 'application/json'return response
def rogerstanimoto u v u _validate_vector u v _validate_vector v nff nft ntf ntt _nbool_correspond_all u v return float 2 0 * ntf + nft / float ntt + nff + 2 0 * ntf + nft
def boot_time time_format None try b_time int psutil boot_time except AttributeError b_time int psutil boot_time if time_format b_time datetime datetime fromtimestamp b_time try return b_time strftime time_format except TypeError as exc raise SaltInvocationError 'Invalidformatstring {0}' format exc return b_time
def boot_time time_format None try b_time int psutil boot_time except AttributeError b_time int psutil boot_time if time_format b_time datetime datetime fromtimestamp b_time try return b_time strftime time_format except TypeError as exc raise SaltInvocationError 'Invalidformatstring {0}' format exc return b_time
def _find_files root_dir should_include paths []is_module lambda path path endswith ' py' for dir_path dir_names file_names in os walk root_dir new_paths [os path join dir_path file_name for file_name in file_names]new_paths filter is_module new_paths new_paths filter should_include new_paths paths extend new_paths return paths
def notify_about_aggregate_update context event_suffix aggregate_payload aggregate_identifier aggregate_payload get 'aggregate_id' None if not aggregate_identifier aggregate_identifier aggregate_payload get 'name' None if not aggregate_identifier LOG debug 'Noaggregateidornamespecifiedforthisnotificationanditwillbeignored' returnnotifier rpc get_notifier service 'aggregate' host aggregate_identifier notifier info context 'aggregate %s' % event_suffix aggregate_payload
@register inclusion_tag 'zinnia/tags/dummy html' takes_context True def get_categories_tree context template 'zinnia/tags/categories_tree html' return {'template' template 'categories' Category objects all annotate count_entries Count 'entries' 'context_category' context get 'category' }
def get_configuration_dict name default None default default or {} output default copy output update get_configuration_value name {} or {} return output
def random_objs objs album number 1 time None equal_chance False if equal_chance perm _equal_chance_permutation objs else perm objsrandom shuffle perm if time return _take_time perm time * 60 album else return _take perm number
def _to_full_path item path_prefix if not item return itemreturn item[0] item[1] path_prefix + item[2]
def letter_form_to_array_form array_form group a list array_form[ ] new_array []n 1symbols group symbolsfor i in range len a if i len a - 1 if a[i] a[ i - 1 ] if - a[i] in symbols new_array append - a[i] - n else new_array append a[i] n elif - a[i] in symbols new_array append - a[i] -1 else new_array append a[i] 1 return new_arrayelif a[i] a[ i + 1 ] n + 1else if - a[i] in symbols new_array append - a[i] - n else new_array append a[i] n n 1
def letter_form_to_array_form array_form group a list array_form[ ] new_array []n 1symbols group symbolsfor i in range len a if i len a - 1 if a[i] a[ i - 1 ] if - a[i] in symbols new_array append - a[i] - n else new_array append a[i] n elif - a[i] in symbols new_array append - a[i] -1 else new_array append a[i] 1 return new_arrayelif a[i] a[ i + 1 ] n + 1else if - a[i] in symbols new_array append - a[i] - n else new_array append a[i] n n 1
def create_ssh_wrapper ssh_wrapper ssh_file SSH_WRAPPER with open ssh_wrapper u'w' as handle handle write SSH_WRAPPER_TEMPLATE format known_hosts ssh_file KNOWN_HOSTS identity ssh_file RSA_KEY os chmod ssh_wrapper 493
def hostname dnsname if not isinstance dnsname basestring raise CX 'Invalidinput dnsnamemustbeastring' else dnsname dnsname strip if dnsname '' return dnsnameif not RE_HOSTNAME match dnsname raise CX 'Invalidhostnameformat %s ' % dnsname return dnsname
def RekallStringRenderer x try return x['str']except KeyError return x['b64']
def energy H q p return H pot energy p - H logp q
def get_login_failed_count name ret _get_account_policy_data_value name 'failedLoginCount' return salt utils mac_utils parse_return ret
def list_bindings site ret dict sites list_sites if site not in sites _LOG warning 'Sitenotfound %s' site return retret sites[site]['bindings']if not ret _LOG warning 'Nobindingsfoundforsite %s' site return ret
def protected_resource scopes None validator_cls OAuth2Validator server_cls Server _scopes scopes or [] def decorator view_func @wraps view_func def _validate request *args **kwargs validator validator_cls core OAuthLibCore server_cls validator valid oauthlib_req core verify_request request scopes _scopes if valid request resource_owner oauthlib_req userreturn view_func request *args **kwargs return HttpResponseForbidden return _validatereturn decorator
def cpu_times procfs_path get_procfs_path set_scputimes_ntuple procfs_path with open_binary '%s/stat' % procfs_path as f values f readline split fields values[1 len scputimes _fields + 1 ]fields [ float x / CLOCK_TICKS for x in fields]return scputimes *fields
def cpu_times procfs_path get_procfs_path set_scputimes_ntuple procfs_path with open_binary '%s/stat' % procfs_path as f values f readline split fields values[1 len scputimes _fields + 1 ]fields [ float x / CLOCK_TICKS for x in fields]return scputimes *fields
def cpu_times procfs_path get_procfs_path set_scputimes_ntuple procfs_path with open_binary '%s/stat' % procfs_path as f values f readline split fields values[1 len scputimes _fields + 1 ]fields [ float x / CLOCK_TICKS for x in fields]return scputimes *fields
def cpu_times procfs_path get_procfs_path set_scputimes_ntuple procfs_path with open_binary '%s/stat' % procfs_path as f values f readline split fields values[1 len scputimes _fields + 1 ]fields [ float x / CLOCK_TICKS for x in fields]return scputimes *fields
def cpu_times procfs_path get_procfs_path set_scputimes_ntuple procfs_path with open_binary '%s/stat' % procfs_path as f values f readline split fields values[1 len scputimes _fields + 1 ]fields [ float x / CLOCK_TICKS for x in fields]return scputimes *fields
def _create_module name module new module name sys modules[name] modulereturn module
def is_unresponsive url host urlparse url hostnamereturn host in unresponsive_hosts
def numeric_collator global _numeric_collatorif _numeric_collator is None _numeric_collator collator clone _numeric_collator strength _icu UCOL_SECONDARY_numeric_collator numeric Truereturn _numeric_collator
def numeric_collator global _numeric_collatorif _numeric_collator is None _numeric_collator collator clone _numeric_collator strength _icu UCOL_SECONDARY_numeric_collator numeric Truereturn _numeric_collator
def _encode_text name value dummy0 dummy1 value _utf_8_encode value [0]return '\x02' + name + _PACK_INT len value + 1 + value + '\x00'
def generate_task_families task_class n ret {}for i in range n class_name '{}_{}' format task_class task_family i ret[class_name] type class_name task_class {} return ret
def _get_corrected_exif image original if 'exif' in original info image_exif image info get 'exif' piexif dump {} original_exif original info['exif']image_exif _update_exif_orientation image_exif _get_exif_orientation original_exif return image_exif
def save_to_disk db ids root opts None callback None root opts length sanitize_args root opts failures []for x in ids tb ''try failed id title save_book_to_disk x db root opts length tb _ 'Requestedformatsnotavailable' except failed id title True x db title x index_is_id True tb traceback format_exc if failed failures append id title tb if callable callback if not callback int id title failed tb breakreturn failures
def asmodule module if isinstance module types ModuleType return moduleelif isinstance module str return __import__ module fromlist [''] else raise TypeError type module
def test_suffix formatter hug output_format suffix {' js' hug output_format json ' html' hug output_format text} class FakeRequest object path 'endpoint js'request FakeRequest response FakeRequest converted hug input_format json formatter BytesIO hug output_format json {'name' 'name'} request response assert converted {'name' 'name'} request path 'endpoint html'assert formatter 'hi' request response 'hi' with pytest raises hug HTTPNotAcceptable request path 'undefined always'formatter 'hi' request response
def test_suffix formatter hug output_format suffix {' js' hug output_format json ' html' hug output_format text} class FakeRequest object path 'endpoint js'request FakeRequest response FakeRequest converted hug input_format json formatter BytesIO hug output_format json {'name' 'name'} request response assert converted {'name' 'name'} request path 'endpoint html'assert formatter 'hi' request response 'hi' with pytest raises hug HTTPNotAcceptable request path 'undefined always'formatter 'hi' request response
def transcribe dna if isinstance dna Seq return dna transcribe elif isinstance dna MutableSeq return dna toseq transcribe else return dna replace 'T' 'U' replace 't' 'u'
def getBoolean value return bool value
@contextmanagerdef temporary_file root_dir None cleanup True suffix u'' permissions None with tempfile NamedTemporaryFile suffix suffix dir root_dir delete False as fd try if permissions is not None os chmod fd name permissions yield fd finally if cleanup safe_delete fd name
def get_scanner hass config for cls in [Tplink4DeviceScanner Tplink3DeviceScanner Tplink2DeviceScanner TplinkDeviceScanner] scanner cls config[DOMAIN] if scanner success_init return scannerreturn None
def enable_monitor_mode iface global RUN_CONFIGprint GR + '[+]' + W + 'enablingmonitormodeon%s ' % G + iface + W stdout flush call ['airmon-ng' 'start' iface] stdout DN stderr DN print 'done'RUN_CONFIG IFACE_TO_TAKE_DOWN get_iface if RUN_CONFIG TX_POWER > 0 print GR + '[+]' + W + 'settingTxpowerto%s%s%s ' % G RUN_CONFIG TX_POWER W call ['iw' 'reg' 'set' 'BO'] stdout OUTLOG stderr ERRLOG call ['iwconfig' iface 'txpower' RUN_CONFIG TX_POWER] stdout OUTLOG stderr ERRLOG print 'done'return RUN_CONFIG IFACE_TO_TAKE_DOWN
def enable_monitor_mode iface global RUN_CONFIGprint GR + '[+]' + W + 'enablingmonitormodeon%s ' % G + iface + W stdout flush call ['airmon-ng' 'start' iface] stdout DN stderr DN print 'done'RUN_CONFIG IFACE_TO_TAKE_DOWN get_iface if RUN_CONFIG TX_POWER > 0 print GR + '[+]' + W + 'settingTxpowerto%s%s%s ' % G RUN_CONFIG TX_POWER W call ['iw' 'reg' 'set' 'BO'] stdout OUTLOG stderr ERRLOG call ['iwconfig' iface 'txpower' RUN_CONFIG TX_POWER] stdout OUTLOG stderr ERRLOG print 'done'return RUN_CONFIG IFACE_TO_TAKE_DOWN
def _GetMimeType file_name extension_index file_name rfind ' ' if extension_index -1 extension ''else extension file_name[ extension_index + 1 ] lower if extension in EXTENSION_BLACKLIST raise InvalidAttachmentTypeError 'Extension%sisnotsupported ' % extension mime_type EXTENSION_MIME_MAP get extension None if mime_type is None mime_type 'application/octet-stream'return mime_type
def test_sobel_horizontal i j np mgrid[ -5 6 -5 6]image i > 0 astype float result filters sobel image * np sqrt 2 i[ np abs j 5 ] 10000assert_allclose result[ i 0 ] 1 assert np all result[ np abs i > 1 ] 0
def setup hass config conf config[DOMAIN]token conf get CONF_TOKEN le_wh '{}{}' format DEFAULT_HOST token def logentries_event_listener event 'ListenfornewmessagesonthebusandsendsthemtoLogentries 'state event data get 'new_state' if state is None returntry _state state_helper state_as_number state except ValueError _state state statejson_body [{'domain' state domain 'entity_id' state object_id 'attributes' dict state attributes 'time' str event time_fired 'value' _state}]try payload {'host' le_wh 'event' json_body}requests post le_wh data json dumps payload timeout 10 except requests exceptions RequestException as error _LOGGER exception 'ErrorsendingtoLogentries %s' error hass bus listen EVENT_STATE_CHANGED logentries_event_listener return True
def partition_entropy_by inputs attribute partitions partition_by inputs attribute return partition_entropy partitions values
def upgrade migrate_engine if migrate_engine name in 'sqlite' 'postgresql' for table_name index_name column_names in INDEXES ensure_index_exists migrate_engine table_name index_name column_names elif migrate_engine name 'mysql' ensure_index_removed migrate_engine 'dns_domains' 'project_id' ensure_index_exists migrate_engine 'dns_domains' 'dns_domains_project_id_idx' ['project_id'] ensure_index_removed migrate_engine 'virtual_interfaces' 'network_id' ensure_index_exists migrate_engine 'virtual_interfaces' 'virtual_interfaces_network_id_idx' ['network_id']
def upgrade migrate_engine if migrate_engine name in 'sqlite' 'postgresql' for table_name index_name column_names in INDEXES ensure_index_exists migrate_engine table_name index_name column_names elif migrate_engine name 'mysql' ensure_index_removed migrate_engine 'dns_domains' 'project_id' ensure_index_exists migrate_engine 'dns_domains' 'dns_domains_project_id_idx' ['project_id'] ensure_index_removed migrate_engine 'virtual_interfaces' 'network_id' ensure_index_exists migrate_engine 'virtual_interfaces' 'virtual_interfaces_network_id_idx' ['network_id']
def runTestSlow test_name mode output errors ec run_one_command mode test_name return ec errors
def inject **k return InjectionFactory k
def inject **k return InjectionFactory k
def inject **k return InjectionFactory k
def inject **k return InjectionFactory k
def inject **k return InjectionFactory k
def push location source None version None force False cmd ['bzr' 'push']if source cmd extend ['-d' source] if version cmd extend ['-r' version] if force cmd extend ['--create-prefix' '--use-existing-dir' '--overwrite'] cmd append location cmd '' join cmd local cmd
def rischDE fa fd ga gd DE _ fa fd weak_normalizer fa fd DE a ba bd ca cd hn normal_denom fa fd ga gd DE A B C hs special_denom a ba bd ca cd DE try n bound_degree A B C DE except NotImplementedError n oo B C m alpha beta spde A B C n DE if C is_zero y Celse y solve_poly_rde B C m DE return alpha * y + beta hn * hs
def is_mounted mount_path source None try check_cmd ['findmnt' '--target' mount_path]if source check_cmd extend ['--source' source] utils execute *check_cmd return Trueexcept processutils ProcessExecutionError return Falseexcept OSError as exc if exc errno errno ENOENT LOG info _LI 'findmnttoolisnotinstalled' return False
def is_mounted mount_path source None try check_cmd ['findmnt' '--target' mount_path]if source check_cmd extend ['--source' source] utils execute *check_cmd return Trueexcept processutils ProcessExecutionError return Falseexcept OSError as exc if exc errno errno ENOENT LOG info _LI 'findmnttoolisnotinstalled' return False
@task@timeddef i18n_clean sh 'gitclean-fdXconf/locale'
def test_install_package_conflict_prefix_and_user script data prefix_path script scratch_path / 'prefix' result script pip 'install' '-f' data find_links '--no-index' '--user' '--prefix' prefix_path 'simple 1 0' expect_error True quiet True assert "Cannotcombine'--user'and'--prefix'" in result stderr
def get_request_and_user_id from framework sessions import get_sessionreq get_cache_key user_id Noneif isinstance req FlaskRequest session get_session user_id session data get 'auth_user_id' elif hasattr req 'user' user_id getattr req user '_id' None return req user_id
def get_request_and_user_id from framework sessions import get_sessionreq get_cache_key user_id Noneif isinstance req FlaskRequest session get_session user_id session data get 'auth_user_id' elif hasattr req 'user' user_id getattr req user '_id' None return req user_id
def authenticate_lti_user request lti_user_id lti_consumer try lti_user LtiUser objects get lti_user_id lti_user_id lti_consumer lti_consumer except LtiUser DoesNotExist lti_user create_lti_user lti_user_id lti_consumer if not request user is_authenticated and request user lti_user edx_user switch_user request lti_user lti_consumer
def authenticate_lti_user request lti_user_id lti_consumer try lti_user LtiUser objects get lti_user_id lti_user_id lti_consumer lti_consumer except LtiUser DoesNotExist lti_user create_lti_user lti_user_id lti_consumer if not request user is_authenticated and request user lti_user edx_user switch_user request lti_user lti_consumer
def eigvalsh a b None lower True overwrite_a False overwrite_b False turbo True eigvals None type 1 check_finite True return eigh a b b lower lower eigvals_only True overwrite_a overwrite_a overwrite_b overwrite_b turbo turbo eigvals eigvals type type check_finite check_finite
def _conflict_bail VC_err version conflict_tmpl textwrap dedent "\nTherequiredversionofsetuptools > {version} isnotavailable \nandcan'tbeinstalledwhilethisscriptisrunning Please\ninstallamorerecentversionfirst using\n'easy_install-Usetuptools' \n\n Currentlyusing{VC_err args[0] r} \n" msg conflict_tmpl format **locals sys stderr write msg sys exit 2
def get_color_scheme name name name lower scheme {}for key in COLOR_SCHEME_KEYS try scheme[key] CONF get 'color_schemes' name + '/' + key except scheme[key] CONF get 'color_schemes' 'spyder/' + key return scheme
def add_traits base names trait_type None if trait_type is None trait_type traits Anyundefined_traits {}for key in names base add_trait key trait_type undefined_traits[key] Undefinedbase trait_set trait_change_notify False **undefined_traits for key in names _ getattr base key return base
def lastsave host None port None db None password None server _connect host port db password return int server lastsave strftime '%s'
def lastsave host None port None db None password None server _connect host port db password return int server lastsave strftime '%s'
@receiver post_save sender Release def promote_latest_release sender instance **kwargs if kwargs get 'raw' False returnif instance is_latest Release objects filter version instance version exclude pk instance pk update is_latest False
def create_message request message assert hasattr request 'session' "django-session-messagesrequiressessionmiddlewaretobeinstalled EdityourMIDDLEWARE_CLASSESsettingtoinsert'django contrib sessions middleware SessionMiddleware' "try request session['messages'] append message except KeyError request session['messages'] [message]
def create_profile_images image_file profile_image_names storage get_profile_image_storage original Image open image_file image _set_color_mode_to_rgb original image _crop_image_to_square image for size name in profile_image_names items scaled _scale_image image size exif _get_corrected_exif scaled original with closing _create_image_file scaled exif as scaled_image_file storage save name scaled_image_file
def create_profile_images image_file profile_image_names storage get_profile_image_storage original Image open image_file image _set_color_mode_to_rgb original image _crop_image_to_square image for size name in profile_image_names items scaled _scale_image image size exif _get_corrected_exif scaled original with closing _create_image_file scaled exif as scaled_image_file storage save name scaled_image_file
def __virtual__ if __grains__ get 'init' 'runit' if __grains__['os'] 'Void' add_svc_avail_path '/etc/sv' return __virtualname__return False
def test_flattener_layer_state_separation_for_softmax soft1 Softmax 5 'sf1' 0 1 soft2 Softmax 5 'sf2' 0 1 mlp MLP layers [FlattenerLayer CompositeLayer 'comp' [soft1 soft2] ] nvis 2 X np random rand 20 2 astype theano config floatX y np random rand 20 10 astype theano config floatX dataset DenseDesignMatrix X X y y train Train dataset mlp SGD 0 1 batch_size 5 monitoring_dataset dataset train algorithm termination_criterion EpochCounter 1 train main_loop
def stash registry xml_parent data top XML SubElement xml_parent 'org jenkinsci plugins stashNotifier StashNotifier' XML SubElement top 'stashServerBaseUrl' text data get 'url' '' if data get 'credentials-id' is not None XML SubElement top 'credentialsId' text str data get 'credentials-id' else XML SubElement top 'stashUserName' text helpers get_value_from_yaml_or_config_file 'username' 'stash' data registry jjb_config XML SubElement top 'stashUserPassword' text helpers get_value_from_yaml_or_config_file 'password' 'stash' data registry jjb_config XML SubElement top 'ignoreUnverifiedSSLPeer' text str data get 'ignore-ssl' False lower XML SubElement top 'commitSha1' text data get 'commit-sha1' '' XML SubElement top 'includeBuildNumberInKey' text str data get 'include-build-number' False lower
def _systemctl_status name contextkey 'systemd _systemctl_status %s' % name if contextkey in __context__ return __context__[contextkey]__context__[contextkey] __salt__['cmd run_all'] _systemctl_cmd 'status' name python_shell False redirect_stderr True ignore_retcode True return __context__[contextkey]
def _systemctl_status name contextkey 'systemd _systemctl_status %s' % name if contextkey in __context__ return __context__[contextkey]__context__[contextkey] __salt__['cmd run_all'] _systemctl_cmd 'status' name python_shell False redirect_stderr True ignore_retcode True return __context__[contextkey]
def generate_completions event b event current_bufferif b complete_state b complete_next else event cli start_completion insert_common_part True select_first False
def _get_self_href response data jsonutils loads response body for link in data['versions'][0]['links'] if link['rel'] 'self' return link['href']return ''
def twobyte val assert isinstance val int return divmod val 256
def reverse_lex ustring newstr ''for ii in ustring ordinance ord ii new_byte 255 - ordinance char chr new_byte newstr + charreturn newstr
def reverse_lex ustring newstr ''for ii in ustring ordinance ord ii new_byte 255 - ordinance char chr new_byte newstr + charreturn newstr
def assert_dict_contains test_case expected actual message '' missing_items []mismatch_items []no_value object for key expected_value in expected items actual_value actual get key no_value if actual_value is no_value missing_items append key elif actual_value expected_value mismatch_items append '{} {} {}' format key expected_value actual_value if missing_items or mismatch_items test_case fail '{}\nMissingitems {}\nMismatchitems {}\nActualitems {}' format message missing_items mismatch_items actual
@receiver post_migrate def sync_create_groups sender **kwargs if sender label u'accounts' create_groups False
def parse_process_statistics statistics if statistics is None statistics DEFAULT_STATISTICSstatistics util listify statistics statistics [_tuplize_statistic _ for _ in statistics]for statistic in statistics if statistic[0] not in STATISTIC_TYPES raise Exception 'Unknownstatistictypeencountered%s' % statistic[0] if statistic[1] not in PROCESS_COLUMNS raise Exception 'Unknownprocesscolumnencountered%s' % statistic[1] return statistics
def parse_process_statistics statistics if statistics is None statistics DEFAULT_STATISTICSstatistics util listify statistics statistics [_tuplize_statistic _ for _ in statistics]for statistic in statistics if statistic[0] not in STATISTIC_TYPES raise Exception 'Unknownstatistictypeencountered%s' % statistic[0] if statistic[1] not in PROCESS_COLUMNS raise Exception 'Unknownprocesscolumnencountered%s' % statistic[1] return statistics
def parse_process_statistics statistics if statistics is None statistics DEFAULT_STATISTICSstatistics util listify statistics statistics [_tuplize_statistic _ for _ in statistics]for statistic in statistics if statistic[0] not in STATISTIC_TYPES raise Exception 'Unknownstatistictypeencountered%s' % statistic[0] if statistic[1] not in PROCESS_COLUMNS raise Exception 'Unknownprocesscolumnencountered%s' % statistic[1] return statistics
def getpackage filename src_file src filename if os path isdir src_file or not src_file endswith ' py' and not ispackage src_file return None base ext os path splitext os path basename src_file if base '__init__' mod_parts []else mod_parts [base] path part os path split os path split src_file [0] while part if ispackage os path join path part mod_parts append part else break path part os path split path mod_parts reverse return ' ' join mod_parts
def adapt_obj obj blob sqlite3 Binary cPickle dumps obj if len blob > config max_blob_size warnings warn 'largeobjectsstoredinSQLite' + LARGE_BLOB_WARNING format type obj len blob warnings filterwarnings 'ignore' 'largeobjects *' return blob
def __clean_tmp sfn if sfn startswith os path join tempfile gettempdir salt utils files TEMPFILE_PREFIX all_roots itertools chain from_iterable six itervalues __opts__['file_roots'] in_roots any sfn startswith root for root in all_roots if os path exists sfn and not in_roots os remove sfn
def set_output fp import commands browsercommands OUT browser OUT fp
def flatten seq for x in seq if isinstance x types GeneratorType list tuple for y in flatten x yield smart_str y else yield smart_str x
def bilinear_kernel_1D ratio normalize True T theano tensorhalf_kern T arange 1 ratio + 1 dtype theano config floatX kern T concatenate [half_kern half_kern[ -2 -1 ]] if normalize kern / ratioreturn kern
def _find_review_request_object review_request_id local_site q ReviewRequest objects all if local_site q q filter local_site local_site local_id review_request_id else q q filter pk review_request_id try q q select_related u'submitter' u'repository' return q get except ReviewRequest DoesNotExist raise Http404
def test_adjust_gamma_greater_one image np arange 0 255 4 np uint8 reshape 8 8 expected np array [[0 0 0 0 1 1 2 3] [4 5 6 7 9 10 12 14] [16 18 20 22 25 27 30 33] [36 39 42 45 49 52 56 60] [64 68 72 76 81 85 90 95] [100 105 110 116 121 127 132 138] [144 150 156 163 169 176 182 189] [196 203 211 218 225 233 241 249]] dtype np uint8 result exposure adjust_gamma image 2 assert_array_equal result expected
def generate_clone_url_for_installed_repository app repository tool_shed_url get_tool_shed_url_from_tool_shed_registry app str repository tool_shed return util build_url tool_shed_url pathspec ['repos' str repository owner str repository name ]
def headers_url_generator resp fuzzable_req resp_headers resp get_headers for parser header_names in URL_HEADERS iteritems for header_name in header_names header_value _ resp_headers iget header_name None if header_value is not None header_value smart_unicode header_value encoding resp charset for ref in parser resp header_name header_value yield ref fuzzable_req resp False
def _item_to_entry iterator resource loggers return entry_from_resource resource iterator client loggers
def manual_search session task artist input_ u'Artist ' strip name input_ u'Album ' if task is_album else u'Track ' strip if task is_album _ _ prop autotag tag_album task items artist name return propelse return autotag tag_item task item artist name
def delete_stream client stream_name wait False wait_timeout 300 check_mode False success Falsechanged Falseerr_msg ''results dict stream_found stream_msg current_stream find_stream client stream_name check_mode check_mode if stream_found success err_msg stream_action client stream_name action 'delete' check_mode check_mode if success changed Trueif wait success err_msg results wait_for_status client stream_name 'DELETING' wait_timeout check_mode check_mode err_msg 'Stream{0}deletedsuccessfully' format stream_name if not success return success True err_msg results else err_msg 'Stream{0}isintheprocessofbeingdeleted' format stream_name else success Truechanged Falseerr_msg 'Stream{0}doesnotexist' format stream_name return success changed err_msg results
def _get_site_profile self local_site if not hasattr self u'_site_profiles' self _site_profiles {}if local_site pk not in self _site_profiles site_profile LocalSiteProfile objects get user self local_site local_site site_profile user selfsite_profile local_site local_siteself _site_profiles[local_site pk] site_profilereturn self _site_profiles[local_site pk]
def create_prereqs_cache_dir try os makedirs PREREQS_STATE_DIR except OSError if not os path isdir PREREQS_STATE_DIR raise
def webapp_add_wsgi_middleware app app I18nMiddleware app return app
def get_all_bears from coalib settings Section import Sectionprinter LogPrinter NullPrinter local_bears global_bears collect_bears Section '' bear_dirs ['**'] [BEAR_KIND LOCAL BEAR_KIND GLOBAL] printer warn_if_unused_glob False return list itertools chain local_bears global_bears
def oo_collect data attribute None filters None if not isinstance data list raise errors AnsibleFilterError ' failedexpectstofilteronaList' if not attribute raise errors AnsibleFilterError ' failedexpectsattributetobeset' if filters is not None if not isinstance filters dict raise errors AnsibleFilterError ' failedexpectsfiltertobeadict' retval [get_attr d attribute for d in data if all [ d get key None filters[key] for key in filters] ]else retval [get_attr d attribute for d in data]retval [val for val in retval if val is not None ]return retval
def oo_collect data attribute None filters None if not isinstance data list raise errors AnsibleFilterError ' failedexpectstofilteronaList' if not attribute raise errors AnsibleFilterError ' failedexpectsattributetobeset' if filters is not None if not isinstance filters dict raise errors AnsibleFilterError ' failedexpectsfiltertobeadict' retval [get_attr d attribute for d in data if all [ d get key None filters[key] for key in filters] ]else retval [get_attr d attribute for d in data]retval [val for val in retval if val is not None ]return retval
def test_grad X T matrix y X sum G T grad y [X] assert isinstance G list G T grad y X assert not isinstance G list
def test_grad X T matrix y X sum G T grad y [X] assert isinstance G list G T grad y X assert not isinstance G list
def make_overload_attribute_template typ attr overload_func base _OverloadAttributeTemplate assert isinstance typ types Type or issubclass typ types Type name 'OverloadTemplate_%s_%s' % typ attr dct dict key typ _attr attr _impl_cache {} _overload_func staticmethod overload_func return type base name base dct
def prefixed_userid request authn_type getattr request 'authn_type' None if authn_type is not None return authn_type + ' ' + request selected_userid
def is_git_repo sourcepath os path realpath os path join os path dirname nipype __file__ os path pardir gitpathgit os path join sourcepath u' git' if os path exists gitpathgit return Trueelse return False
def is_git_repo sourcepath os path realpath os path join os path dirname nipype __file__ os path pardir gitpathgit os path join sourcepath u' git' if os path exists gitpathgit return Trueelse return False
def tag *tags def decorate f if not hasattr f 'tags' f tags []f tags + tagsreturn freturn decorate
def contract_creation_exceptions return {sa Column [ '%s binding_index' % ROUTER_L3_AGENT_BINDING ]}
def contract_creation_exceptions return {sa Column [ '%s binding_index' % ROUTER_L3_AGENT_BINDING ]}
def snapshot_get_all_for_volume context volume_id return IMPL snapshot_get_all_for_volume context volume_id
def enable_merge_strategies *merge_strategies return _EnableMergeStrategies *merge_strategies
def assert_no_validation_errors validation if hasattr validation 'task_error' error validation task_errorelse error validation['error']if error print '-' * 70 print errorprint '-' * 70 raise AssertionError 'Unexpectedtaskerror %s' % error rstrip split '\n' [ -1 ]
def assert_no_validation_errors validation if hasattr validation 'task_error' error validation task_errorelse error validation['error']if error print '-' * 70 print errorprint '-' * 70 raise AssertionError 'Unexpectedtaskerror %s' % error rstrip split '\n' [ -1 ]
def assert_no_validation_errors validation if hasattr validation 'task_error' error validation task_errorelse error validation['error']if error print '-' * 70 print errorprint '-' * 70 raise AssertionError 'Unexpectedtaskerror %s' % error rstrip split '\n' [ -1 ]
def get_prompt_tokens cli now datetime datetime now return [ Token Prompt u'%s %s %s' % now hour now minute now second Token Prompt u'Entersomething ' ]
def normalize_time_unit s s s lower strip if s in units return sif s in unit_aliases return unit_aliases[s]if s[ -1 ] 's' return normalize_time_unit s rstrip 's' raise ValueError 'Donotunderstandtimeunit%s' % s
def _process_emerge_err stdout stderr ret {}rexp re compile '^[<> ][^]+/[^]+[^\\n]+' re M slot_conflicts re compile '^[^\\n]+/[^]+ [^]' re M findall stderr if slot_conflicts ret['slotconflicts'] slot_conflictsblocked re compile ' ?m ^\\[blocks +\\] [^]+/[^]+-[0-9]+[^]+ *$' findall stdout unsatisfied re compile 'Error Theabovepackagelistcontains' findall stderr if blocked and unsatisfied ret['blocked'] blockedsections re split '\n\n' stderr for section in sections if 'Thefollowingkeywordchanges' in section ret['keywords'] rexp findall section elif 'Thefollowinglicensechanges' in section ret['license'] rexp findall section elif 'ThefollowingUSEchanges' in section ret['use'] rexp findall section elif 'Thefollowingmaskchanges' in section ret['mask'] rexp findall section return ret
def _process_emerge_err stdout stderr ret {}rexp re compile '^[<> ][^]+/[^]+[^\\n]+' re M slot_conflicts re compile '^[^\\n]+/[^]+ [^]' re M findall stderr if slot_conflicts ret['slotconflicts'] slot_conflictsblocked re compile ' ?m ^\\[blocks +\\] [^]+/[^]+-[0-9]+[^]+ *$' findall stdout unsatisfied re compile 'Error Theabovepackagelistcontains' findall stderr if blocked and unsatisfied ret['blocked'] blockedsections re split '\n\n' stderr for section in sections if 'Thefollowingkeywordchanges' in section ret['keywords'] rexp findall section elif 'Thefollowinglicensechanges' in section ret['license'] rexp findall section elif 'ThefollowingUSEchanges' in section ret['use'] rexp findall section elif 'Thefollowingmaskchanges' in section ret['mask'] rexp findall section return ret
def facility_geojson s3db org_facility_geojson
def facility_geojson s3db org_facility_geojson
def set_dags_paused_state is_paused session settings Session dms session query DagModel filter DagModel dag_id in_ DAG_IDS for dm in dms logging info 'SettingDAG {}is_paused {}' format dm is_paused dm is_paused is_pausedsession commit
def find_changes accounts monitor_names debug True for account_name in accounts monitors get_monitors account_name monitor_names debug for mon in monitors cw mon watcher items exception_map cw slurp cw find_changes current items exception_map exception_map cw save audit_changes accounts monitor_names False debug db session close
def hostinterface_update interfaceid **connection_args conn_args _login **connection_args try if conn_args method 'hostinterface update'params {'interfaceid' interfaceid}params _params_extend params **connection_args ret _query method params conn_args['url'] conn_args['auth'] return ret['result']['interfaceids']else raise KeyErrorexcept KeyError return ret
def get_nginx_configurator config_path config_dir work_dir version 1 6 2 backups os path join work_dir 'backups' with mock patch 'certbot_nginx configurator NginxConfigurator config_test' with mock patch 'certbot_nginx configurator util exe_exists' as mock_exe_exists mock_exe_exists return_value Trueconfig configurator NginxConfigurator config mock MagicMock nginx_server_root config_path le_vhost_ext '-le-ssl conf' config_dir config_dir work_dir work_dir backup_dir backups temp_checkpoint_dir os path join work_dir 'temp_checkpoints' in_progress_dir os path join backups 'IN_PROGRESS' server 'https //acme-server org 443/new' tls_sni_01_port 5001 name 'nginx' version version config prepare nsconfig configuration NamespaceConfig config config zope component provideUtility nsconfig return config
def critical title message None details None if message is None message titlembox ResizeableMessageBox active_window mbox setWindowTitle title mbox setTextFormat Qt PlainText mbox setText message mbox setIcon QtWidgets QMessageBox Critical mbox setStandardButtons QtWidgets QMessageBox Close mbox setDefaultButton QtWidgets QMessageBox Close if details mbox setDetailedText details mbox exec_
def getPathByList vertexList if len vertexList < 1 return Vector3 if vertexList[0] __class__ list vertexList [vertexList]path []for floatList in vertexList vector3 getVector3ByFloatList floatList Vector3 path append vector3 return path
def read_po fileobj locale None domain None ignore_obsolete False charset None catalog Catalog locale locale domain domain charset charset parser PoFileParser catalog ignore_obsolete parser parse fileobj return catalog
def rotate_90_clockwise request fileobjects transpose_image request fileobjects 4
def register_deep_copy_op_c_code typ code version DeepCopyOp c_code_and_version[typ] code version
def IncludeUtilitiesInUserAgent value with _UTILITY_LOCK _utility_registry SetEnabled value
def check_header_match_180_or_later header1 header2 header1 header1 split ' ' header2 header2 split ' ' for e1 e2 in zip header1 header2 if e1 split '' [0] e2 split '' [0] return Falsereturn True
def UnregisterModule modName try win32api RegDeleteKey GetRootKey BuildDefaultPythonKey + '\\Modules\\%s' % modName except win32api error as code fn desc import winerrorif code winerror ERROR_FILE_NOT_FOUND raise win32api error code fn desc
def eigvals_banded a_band lower False overwrite_a_band False select 'a' select_range None check_finite True return eig_banded a_band lower lower eigvals_only 1 overwrite_a_band overwrite_a_band select select select_range select_range check_finite check_finite
def _build_text_msg message _LOGGER debug 'Buildingplaintextemail' return MIMEText message
def sort_nicely l l sort key alphanum_key
def _update_epoch_data params start params['t_start']n_epochs params['n_epochs']end start + n_epochs * len params['epochs'] times data params['orig_data'][ start end]types params['types']for pick ind in enumerate params['inds'] params['data'][pick] data[ind] / params['scalings'][types[pick]] params['plot_fun']
def function_simple a b c return a b c
def returner ret opts _get_options {} try with salt utils flopen opts['filename'] 'a' as logfile logfile write str ret + '\n' except log error 'Couldnotwritetorawdata_jsonfile{0}' format opts['filename'] raise
def scanNetwork devices []for remoteAddress in range 2 6 print 'Tryingaddress' + str remoteAddress p snap Packet remoteAddress snap localAddress 0 1 [CMD_GETMODULETYPE] p send rep p getReply print 'dev' remoteAddresstime sleep 0 5 for d in devices print 'device' d
def resolve_one_media file_id resource _file app media get file_id resource if _file if config RETURN_MEDIA_AS_BASE64_STRING ret_file base64 encodestring _file read elif config RETURN_MEDIA_AS_URL prefix config MEDIA_BASE_URL if config MEDIA_BASE_URL is not None else app api_prefix ret_file '%s/%s/%s' % prefix config MEDIA_ENDPOINT file_id else ret_file Noneif config EXTENDED_MEDIA_INFO ret {'file' ret_file}for attribute in config EXTENDED_MEDIA_INFO if hasattr _file attribute ret update {attribute getattr _file attribute } else abort 500 description debug_error_message 'Invalidextendedmediaattributerequested' return retelse return ret_fileelse return None
def Start servers None print 'inbabysitter'options parse_command_line babysitter admin_server AdminServer handlers [ '/' _MainHandler ] port options options babysitter_port print 'connecttobabysitterviahttps //{0} {1}/' format 'localhost' options options babysitter_port ioloop IOLoop instance start
def Start servers None print 'inbabysitter'options parse_command_line babysitter admin_server AdminServer handlers [ '/' _MainHandler ] port options options babysitter_port print 'connecttobabysitterviahttps //{0} {1}/' format 'localhost' options options babysitter_port ioloop IOLoop instance start
def nopackage pkg_name if is_installed pkg_name uninstall pkg_name
def get_info_file filename data ''errors re compile '\\b error fail failed \\b' re IGNORECASE if os path isfile filename f open '%s' % filename 'r' lines f readlines f close rx re compile ' \' " ' for line in lines new_line rx sub '' line errors_found errors findall new_line if len errors_found > 0 data + '<fontcolor red>%s</font><br>' % str new_line else data + '%s<br>' % str new_line if not data data 'NoInformationFound <br>'else data 'Filenotfound <br>'return data
def qflags_key base value add_base False klass None if klass is None klass value __class__if klass int raise TypeError "Can'tguessenumclassofanint " bits []names []mask 1value int value while mask < value if value & mask bits append mask mask << 1for bit in bits names append qenum_key base klass bit add_base return ' ' join names
def get_all_types ctxt None inactive False filters None if ctxt is None ctxt context get_admin_context inst_types db instance_type_get_all ctxt inactive inactive filters filters inst_type_dict {}for inst_type in inst_types inst_type_dict[inst_type['name']] inst_typereturn inst_type_dict
def getDoubleForLetter letter splitLine return getDoubleAfterFirstLetter splitLine[getIndexOfStartingWithSecond letter splitLine ]
def main prepare_to_run failed_modules 0modules get_modules for module in modules try run_module_tests module except FailedSystemTestModule failed_modules + 1sys exit failed_modules
def fitLine pts n len pts if n < 1 return 0 0 0 0 a np zeros n - 1 2 for i in range n - 1 v pts[i] - pts[ i + 1 ] a[i] v / np linalg norm v direction np mean a[1 -1 ] axis 0 start np mean pts[1 -1 ] axis 0 return start start + direction
def fitLine pts n len pts if n < 1 return 0 0 0 0 a np zeros n - 1 2 for i in range n - 1 v pts[i] - pts[ i + 1 ] a[i] v / np linalg norm v direction np mean a[1 -1 ] axis 0 start np mean pts[1 -1 ] axis 0 return start start + direction
def fitLine pts n len pts if n < 1 return 0 0 0 0 a np zeros n - 1 2 for i in range n - 1 v pts[i] - pts[ i + 1 ] a[i] v / np linalg norm v direction np mean a[1 -1 ] axis 0 start np mean pts[1 -1 ] axis 0 return start start + direction
def fitLine pts n len pts if n < 1 return 0 0 0 0 a np zeros n - 1 2 for i in range n - 1 v pts[i] - pts[ i + 1 ] a[i] v / np linalg norm v direction np mean a[1 -1 ] axis 0 start np mean pts[1 -1 ] axis 0 return start start + direction
def _ValidateContacts contact_dicts for contact in contact_dicts if 'identity' in contact Identity ValidateKey contact['identity']
def _ValidateContacts contact_dicts for contact in contact_dicts if 'identity' in contact Identity ValidateKey contact['identity']
def p_boolean_document corpus segmented_topics top_ids _ret_top_ids segmented_topics per_topic_postings {}for id in top_ids id_list set for n document in enumerate corpus if id in frozenset x[0] for x in document id_list add n per_topic_postings[id] id_listnum_docs len corpus return per_topic_postings num_docs
def die signal frame os _exit 1
def iter_tracebacks logdir crash_files sorted glob op join logdir '* pkl*' for cf in crash_files yield cf load_pklz_traceback cf
def iter_tracebacks logdir crash_files sorted glob op join logdir '* pkl*' for cf in crash_files yield cf load_pklz_traceback cf
def group_type_specs_delete context group_type_id key return IMPL group_type_specs_delete context group_type_id key
def _maybe_reset_index data if data index equals Index lrange 1 len data + 1 data data reset_index drop True return data
def _maybe_reset_index data if data index equals Index lrange 1 len data + 1 data data reset_index drop True return data
def image_description shape colormaped False **metadata if colormaped shape shape + 3 metadata update {'shape' shape} return json dumps metadata encode 'utf-8'
def test_start_detached_error fake_proc message_mock caplog argv ['foo' 'bar']fake_proc _proc startDetached return_value False 0 fake_proc _proc error return_value 'Errormessage'with caplog at_level logging ERROR fake_proc start_detached *argv msg message_mock getmsg usertypes MessageLevel error assert msg text 'Errorwhilespawningtestprocess Errormessage '
@LocalContextdef line raw_bytes *a **kw return encode raw_bytes expr re_whitespace *a **kw
@LocalContextdef line raw_bytes *a **kw return encode raw_bytes expr re_whitespace *a **kw
@contextlib contextmanagerdef MockVimBuffers buffers current_buffer cursor_position 1 1 if current_buffer not in buffers raise RuntimeError u'Currentbuffermustbepartofthebufferslist ' with patch u'vim buffers' buffers with patch u'vim current buffer' current_buffer with patch u'vim current window cursor' cursor_position yield
@contextlib contextmanagerdef MockVimBuffers buffers current_buffer cursor_position 1 1 if current_buffer not in buffers raise RuntimeError u'Currentbuffermustbepartofthebufferslist ' with patch u'vim buffers' buffers with patch u'vim current buffer' current_buffer with patch u'vim current window cursor' cursor_position yield
@contextlib contextmanagerdef MockVimBuffers buffers current_buffer cursor_position 1 1 if current_buffer not in buffers raise RuntimeError u'Currentbuffermustbepartofthebufferslist ' with patch u'vim buffers' buffers with patch u'vim current buffer' current_buffer with patch u'vim current window cursor' cursor_position yield
def minkowski_distance x y p 2 x np asarray x y np asarray y if p np inf or p 1 return minkowski_distance_p x y p else return minkowski_distance_p x y p ** 1 0 / p
def command methodName cmdClass WrapperCommand def wrapper obj journal *args **kwargs return journal executeCommand cmdClass methodName obj args kwargs return wrapper
def requireAttrs obj *attributes missing [name for name in attributes if not hasattr obj name ]return skipWithClientIf missing "don'thave" + ' ' join name for name in missing
@docfillerdef generic_filter1d input function filter_size axis -1 output None mode 'reflect' cval 0 0 origin 0 extra_arguments extra_keywords None if extra_keywords is None extra_keywords {}input numpy asarray input if numpy iscomplexobj input raise TypeError 'Complextypenotsupported' output return_value _ni_support _get_output output input if filter_size < 1 raise RuntimeError 'invalidfiltersize' axis _ni_support _check_axis axis input ndim if filter_size // 2 + origin < 0 or filter_size // 2 + origin > filter_size raise ValueError 'invalidorigin' mode _ni_support _extend_mode_to_code mode _nd_image generic_filter1d input function filter_size axis output mode cval origin extra_arguments extra_keywords return return_value
def format_slice key_val dim if isinstance key_val slice key_val slice to_int key_val start to_int key_val stop to_int key_val step return key_valelse key_val to_int key_val key_val wrap_neg_index key_val dim if 0 < key_val < dim return slice key_val key_val + 1 1 else raise IndexError 'Index/sliceoutofbounds '
def getCraftTypePluginModule craftTypeName '' if craftTypeName '' craftTypeName getCraftTypeName profilePluginsDirectoryPath getPluginsDirectoryPath return archive getModuleWithDirectoryPath profilePluginsDirectoryPath craftTypeName
@content_git_object_init connectdef git_permalink content git_content if not content settings['GIT_GENERATE_PERMALINK'] returnif not string_to_bool content metadata get 'git_permalink' 'yes' returnif not git_content is_committed returnpermalink_hash hashlib sha1 permalink_hash update str git_content get_oldest_commit permalink_hash update str git_content get_oldest_filename git_permalink_id base64 urlsafe_b64encode permalink_hash digest permalink_id_metadata_key content settings['PERMALINK_ID_METADATA_KEY']if permalink_id_metadata_key in content metadata content metadata[permalink_id_metadata_key] ' ' join content metadata[permalink_id_metadata_key] git_permalink_id else content metadata[permalink_id_metadata_key] git_permalink_id
def create_verifier_for_pss signature hash_method public_key mgf padding MGF1 hash_method salt_length padding PSS MAX_LENGTHreturn public_key verifier signature padding PSS mgf mgf salt_length salt_length hash_method
def create_verifier_for_pss signature hash_method public_key mgf padding MGF1 hash_method salt_length padding PSS MAX_LENGTHreturn public_key verifier signature padding PSS mgf mgf salt_length salt_length hash_method
def put content path log_content_filter identity return Effect Put content content path path log_content_filter log_content_filter
def moderator_required function None def decorator request *args **kwargs group get_object_or_404 Group slug kwargs['slug'] if request user is_anonymous return HttpResponseRedirect reverse 'django contrib auth views login' if GroupMember objects is_moderator group request user return function request *args **kwargs else raise Http404return decorator
def send_birthday_reminders if int frappe db get_single_value u'HRSettings' u'stop_birthday_reminders' or 0 returnfrom frappe utils user import get_enabled_system_usersusers Nonebirthdays get_employees_who_are_born_today if birthdays if not users users [ u email_id or u name for u in get_enabled_system_users ]for e in birthdays frappe sendmail recipients filter lambda u u not in e company_email e personal_email e user_id users subject _ u'BirthdayReminderfor{0}' format e employee_name message _ u"Todayis{0}'sbirthday " format e employee_name reply_to e company_email or e personal_email or e user_id
def process_message_notification request messages_path if not messages_path returnglobal _MESSAGES_CACHEglobal _MESSAGES_MTIMEif _MESSAGES_CACHE is None or _MESSAGES_MTIME os path getmtime messages_path _MESSAGES_CACHE _get_processed_messages messages_path _MESSAGES_MTIME os path getmtime messages_path for msg in _MESSAGES_CACHE msg send_message request
def process_message_notification request messages_path if not messages_path returnglobal _MESSAGES_CACHEglobal _MESSAGES_MTIMEif _MESSAGES_CACHE is None or _MESSAGES_MTIME os path getmtime messages_path _MESSAGES_CACHE _get_processed_messages messages_path _MESSAGES_MTIME os path getmtime messages_path for msg in _MESSAGES_CACHE msg send_message request
def _chinese_remainder_reconstruction_univariate hp hq p q n hp degree x hp ring gens[0]hpq hp ring zerofor i in range n + 1 hpq[ i ] crt [p q] [hp coeff x ** i hq coeff x ** i ] symmetric True [0]hpq strip_zero return hpq
def wait share timeout 0 0 delay 0 01 start time time while True msg receive share if msg return msgtime sleep delay if timeout > 0 0 and time time - start > timeout return None
def wait share timeout 0 0 delay 0 01 start time time while True msg receive share if msg return msgtime sleep delay if timeout > 0 0 and time time - start > timeout return None
def machines return [name for name state in _status ]
def initialize_scheduler from headphones import updater searcher librarysync postprocessor torrentfinishedwith SCHED_LOCK start_jobs not len SCHED get_jobs minutes CONFIG SEARCH_INTERVALschedule_job searcher searchforalbum 'SearchforWanted' hours 0 minutes minutes minutes CONFIG DOWNLOAD_SCAN_INTERVALschedule_job postprocessor checkFolder 'DownloadScan' hours 0 minutes minutes hours CONFIG LIBRARYSCAN_INTERVALschedule_job librarysync libraryScan 'LibraryScan' hours hours minutes 0 hours CONFIG UPDATE_DB_INTERVALschedule_job updater dbUpdate 'MusicBrainzUpdate' hours hours minutes 0 if CONFIG CHECK_GITHUB if CONFIG CHECK_GITHUB_INTERVAL minutes CONFIG CHECK_GITHUB_INTERVALelse minutes 0schedule_job versioncheck checkGithub 'CheckGitHubforupdates' hours 0 minutes minutes minutes CONFIG TORRENT_REMOVAL_INTERVALschedule_job torrentfinished checkTorrentFinished 'Torrentremovalcheck' hours 0 minutes minutes if start_jobs and len SCHED get_jobs try SCHED start except Exception as e logger info e
def list_nodes_full kwargs None call None if call 'action' raise SaltCloudSystemExit 'Thelist_nodes_fullfunctionmustbecalledwith-for--function ' machines {}for machine in vb_list_machines name machine get 'name' if name machines[name] treat_machine_dict machine del machine['name']return machines
def join_nonshared_inputs xs vars shared make_shared False joined tt concatenate [var ravel for var in vars] if not make_shared tensor_type joined typeinarray tensor_type 'inarray' else inarray theano shared joined tag test_value 'inarray' ordering ArrayOrdering vars inarray tag test_value joined tag test_valueget_var {var name var for var in vars}replace {get_var[var] reshape_t inarray[slc] shp astype dtyp for var slc shp dtyp in ordering vmap}replace update shared xs_special [theano clone x replace strict False for x in xs]return xs_special inarray
def join_nonshared_inputs xs vars shared make_shared False joined tt concatenate [var ravel for var in vars] if not make_shared tensor_type joined typeinarray tensor_type 'inarray' else inarray theano shared joined tag test_value 'inarray' ordering ArrayOrdering vars inarray tag test_value joined tag test_valueget_var {var name var for var in vars}replace {get_var[var] reshape_t inarray[slc] shp astype dtyp for var slc shp dtyp in ordering vmap}replace update shared xs_special [theano clone x replace strict False for x in xs]return xs_special inarray
def override_bytes_from_content_type listing_dict logger None listing_dict['content_type'] swift_bytes extract_swift_bytes listing_dict['content_type'] if swift_bytes is not None try listing_dict['bytes'] int swift_bytes except ValueError if logger logger exception _ 'Invalidswift_bytes'
def property_name property index statuses ['' '' 'Open' 'Pending' 'Resolved' 'Closed' 'WaitingonCustomer' 'JobApplication' 'Monthly']priorities ['' 'Low' 'Medium' 'High' 'Urgent']if property 'status' return statuses[index] if index < len statuses else str index elif property 'priority' return priorities[index] if index < len priorities else str index else raise ValueError 'Unknownproperty'
def find_apple_crash_report_referenced_images binary_images threads image_map {}for image in binary_images image_map[image['image_addr']] image['uuid']to_load set for thread in threads if 'backtrace' not in thread continuefor frame in thread['backtrace']['contents'] img_uuid image_map get frame['object_addr'] if img_uuid is not None to_load add img_uuid return list to_load
def get_chimeras_from_Nast_aligned seqs_fp ref_db_aligned_fp None ref_db_fasta_fp None HALT_EXEC False min_div_ratio None keep_intermediates False files_to_remove []seqs_fp str seqs_fp seqs_fp seqs_fp rstrip '"' seqs_fp seqs_fp lstrip '"' seqs_dir new_seqs_fp split seqs_fp if seqs_dir '' seqs_dir ' /'params {'--query_NAST' new_seqs_fp '--exec_dir' seqs_dir}if ref_db_aligned_fp is None and ref_db_fasta_fp is None passelse if not ref_db_fasta_fp ref_db_fasta_fp write_degapped_fasta_to_file parse_fasta open ref_db_aligned_fp files_to_remove append ref_db_fasta_fp params update {'--db_NAST' abspath ref_db_aligned_fp '--db_FASTA' abspath ref_db_fasta_fp } if min_div_ratio is not None params update {'-R' min_div_ratio} app ChimeraSlayer params params HALT_EXEC HALT_EXEC app_results app chimeras parse_CPS_file app_results['CPS'] if not keep_intermediates app remove_intermediate_files remove_files files_to_remove return chimeras
def get_chimeras_from_Nast_aligned seqs_fp ref_db_aligned_fp None ref_db_fasta_fp None HALT_EXEC False min_div_ratio None keep_intermediates False files_to_remove []seqs_fp str seqs_fp seqs_fp seqs_fp rstrip '"' seqs_fp seqs_fp lstrip '"' seqs_dir new_seqs_fp split seqs_fp if seqs_dir '' seqs_dir ' /'params {'--query_NAST' new_seqs_fp '--exec_dir' seqs_dir}if ref_db_aligned_fp is None and ref_db_fasta_fp is None passelse if not ref_db_fasta_fp ref_db_fasta_fp write_degapped_fasta_to_file parse_fasta open ref_db_aligned_fp files_to_remove append ref_db_fasta_fp params update {'--db_NAST' abspath ref_db_aligned_fp '--db_FASTA' abspath ref_db_fasta_fp } if min_div_ratio is not None params update {'-R' min_div_ratio} app ChimeraSlayer params params HALT_EXEC HALT_EXEC app_results app chimeras parse_CPS_file app_results['CPS'] if not keep_intermediates app remove_intermediate_files remove_files files_to_remove return chimeras
def _get_instance_info identity boto utils get_instance_identity ['document']return identity['instanceId'] identity['region']
def test dummy_function
def test dummy_function
def get_latest_repository_metadata_if_it_includes_invalid_tools trans repository repository_metadata get_latest_repository_metadata trans repository if repository_metadata is not None metadata repository_metadata metadataif metadata is not None and 'invalid_tools' in metadata return repository_metadatareturn None
def casoratian seqs n zero True from dense import Matrixseqs list map sympify seqs if not zero f lambda i j seqs[j] subs n n + i else f lambda i j seqs[j] subs n i k len seqs return Matrix k k f det
def casoratian seqs n zero True from dense import Matrixseqs list map sympify seqs if not zero f lambda i j seqs[j] subs n n + i else f lambda i j seqs[j] subs n i k len seqs return Matrix k k f det
def drop n seq return itertools islice seq n None
def filename_match filename patterns default True if not patterns return defaultreturn any fnmatch filename pattern for pattern in patterns
def filename_match filename patterns default True if not patterns return defaultreturn any fnmatch filename pattern for pattern in patterns
def snapshot domain name None suffix None if name and name lower domain lower raise CommandExecutionError 'VirtualMachine{name}isalreadydefined Pleasechooseanothernameforthesnapshot' format name name if not name name '{domain}-{tsnap}' format domain domain tsnap time strftime '%Y%m%d-%H%M%S' time localtime if suffix name '{name}-{suffix}' format name name suffix suffix doc ElementTree Element 'domainsnapshot' n_name ElementTree SubElement doc 'name' n_name text name_get_domain domain snapshotCreateXML ElementTree tostring doc return {'name' name}
def libvlc_video_set_spu p_mi i_spu f _Cfunctions get 'libvlc_video_set_spu' None or _Cfunction 'libvlc_video_set_spu' 1 1 None ctypes c_int MediaPlayer ctypes c_int return f p_mi i_spu
def UINT value if value is None raise ValueError u'Noneisnotavalidinteger' if not value isdigit raise ValueError u'Onlypositivenumbersareallowed' return int value
def get_precision doctype fieldname currency None doc None from frappe model meta import get_field_precisionreturn get_field_precision get_meta doctype get_field fieldname doc currency
def _GetUserSecret user secret secrets GetSecret _SecretName user if not secret raise LookupError 'nosecrethasbeencreatedfor{0}' format user return secret
def colorize_ansi msg color None style None if color is None and style is None return msgescape_code _get_ansi_code color style if escape_code return '%s%s%s' % escape_code msg ANSI_RESET return msg
def colorize_ansi msg color None style None if color is None and style is None return msgescape_code _get_ansi_code color style if escape_code return '%s%s%s' % escape_code msg ANSI_RESET return msg
@jingo register functiondef showing query pager format_opts pager start_index pager end_index pager paginator count query escape query if query showing _ u'Showing{0}-{1}of{2}resultsfor<strong>{3}</strong>' format * format_opts + query else showing _ u'Showing{0}-{1}of{2}results' format *format_opts return jinja2 Markup showing
@jingo register functiondef showing query pager format_opts pager start_index pager end_index pager paginator count query escape query if query showing _ u'Showing{0}-{1}of{2}resultsfor<strong>{3}</strong>' format * format_opts + query else showing _ u'Showing{0}-{1}of{2}results' format *format_opts return jinja2 Markup showing
def _vode_banded_jac_wrapper jacfunc ml jac_params def jac_wrapper t y jac asarray jacfunc t y *jac_params padded_jac vstack jac zeros ml jac shape[1] return padded_jacreturn jac_wrapper
def metadef_tag_count context namespace_name session None session session or get_session return metadef_tag_api count context namespace_name session
def split_on_groups string groups if not groups return [string]boundaries sorted set functools reduce lambda l x l + list x groups [] if boundaries[0] 0 boundaries insert 0 0 if boundaries[ -1 ] len string boundaries append len string groups [string[start end] for start end in zip boundaries[ -1 ] boundaries[1 ] ]return [g for g in groups if g]
def test_threading_limit db folder_sync_engine monkeypatch from inbox models import Message ThreadMAX_THREAD_LENGTH 10monkeypatch setattr 'inbox mailsync backends imap generic MAX_THREAD_LENGTH' MAX_THREAD_LENGTH namespace_id folder_sync_engine namespace_idmsg MockRawMessage [] for i in range 3 * MAX_THREAD_LENGTH m Message m namespace_id namespace_idm received_date datetime datetime utcnow m references []m size 0m body ''m from_addr [ 'KarimHamidou' 'karim@nilas com' ]m to_addr [ 'EbenFreeman' 'eben@nilas com' ]m snippet ''m subject 'uniquesubject'db session add m folder_sync_engine add_message_to_thread db session m msg db session commit new_threads db session query Thread filter Thread subject 'uniquesubject' all assert len new_threads 3 assert all len thread messages MAX_THREAD_LENGTH for thread in new_threads
def test_threading_limit db folder_sync_engine monkeypatch from inbox models import Message ThreadMAX_THREAD_LENGTH 10monkeypatch setattr 'inbox mailsync backends imap generic MAX_THREAD_LENGTH' MAX_THREAD_LENGTH namespace_id folder_sync_engine namespace_idmsg MockRawMessage [] for i in range 3 * MAX_THREAD_LENGTH m Message m namespace_id namespace_idm received_date datetime datetime utcnow m references []m size 0m body ''m from_addr [ 'KarimHamidou' 'karim@nilas com' ]m to_addr [ 'EbenFreeman' 'eben@nilas com' ]m snippet ''m subject 'uniquesubject'db session add m folder_sync_engine add_message_to_thread db session m msg db session commit new_threads db session query Thread filter Thread subject 'uniquesubject' all assert len new_threads 3 assert all len thread messages MAX_THREAD_LENGTH for thread in new_threads
def test_threading_limit db folder_sync_engine monkeypatch from inbox models import Message ThreadMAX_THREAD_LENGTH 10monkeypatch setattr 'inbox mailsync backends imap generic MAX_THREAD_LENGTH' MAX_THREAD_LENGTH namespace_id folder_sync_engine namespace_idmsg MockRawMessage [] for i in range 3 * MAX_THREAD_LENGTH m Message m namespace_id namespace_idm received_date datetime datetime utcnow m references []m size 0m body ''m from_addr [ 'KarimHamidou' 'karim@nilas com' ]m to_addr [ 'EbenFreeman' 'eben@nilas com' ]m snippet ''m subject 'uniquesubject'db session add m folder_sync_engine add_message_to_thread db session m msg db session commit new_threads db session query Thread filter Thread subject 'uniquesubject' all assert len new_threads 3 assert all len thread messages MAX_THREAD_LENGTH for thread in new_threads
def equal_fields matchdict field return equal m[field] for m in matchdict values
def sysrq vm action 'nmi' key 'uuid' ret {}vmadm _check_vmadm if key not in ['uuid' 'alias' 'hostname'] ret['Error'] 'Keymustbeeitheruuid aliasorhostname'return retif action not in ['nmi' 'screenshot'] ret['Error'] 'Actionmustbeeithernmiorscreenshot'return retvm lookup '{0} {1}' format key vm one True if 'Error' in vm return vmcmd '{vmadm}sysrq{uuid}{action}' format vmadm vmadm uuid vm action action res __salt__['cmd run_all'] cmd retcode res['retcode']if retcode 0 ret['Error'] res['stderr'] if 'stderr' in res else _exit_status retcode return retreturn True
def AnyBut s ranges chars_to_ranges s ranges insert 0 - maxint ranges append maxint result CodeRanges ranges result str 'AnyBut %s ' % repr s return result
def unit_propagate clauses symbol output []for c in clauses if c func Or output append c continuefor arg in c args if arg ~ symbol output append Or *[x for x in c args if x ~ symbol ] breakif arg symbol breakelse output append c return output
def unit_propagate clauses symbol output []for c in clauses if c func Or output append c continuefor arg in c args if arg ~ symbol output append Or *[x for x in c args if x ~ symbol ] breakif arg symbol breakelse output append c return output
def extract_components_from_tuple repository_components_tuple toolshed repository_components_tuple[0]name repository_components_tuple[1]owner repository_components_tuple[2]changeset_revision repository_components_tuple[3]components_list [toolshed name owner changeset_revision]if len repository_components_tuple 5 toolshed name owner changeset_revision prior_installation_required repository_components_tuplecomponents_list [toolshed name owner changeset_revision prior_installation_required]elif len repository_components_tuple 6 toolshed name owner changeset_revision prior_installation_required only_if_compiling_contained_td repository_components_tuplecomponents_list [toolshed name owner changeset_revision prior_installation_required only_if_compiling_contained_td]return components_list
def extract_components_from_tuple repository_components_tuple toolshed repository_components_tuple[0]name repository_components_tuple[1]owner repository_components_tuple[2]changeset_revision repository_components_tuple[3]components_list [toolshed name owner changeset_revision]if len repository_components_tuple 5 toolshed name owner changeset_revision prior_installation_required repository_components_tuplecomponents_list [toolshed name owner changeset_revision prior_installation_required]elif len repository_components_tuple 6 toolshed name owner changeset_revision prior_installation_required only_if_compiling_contained_td repository_components_tuplecomponents_list [toolshed name owner changeset_revision prior_installation_required only_if_compiling_contained_td]return components_list
def get_doctypes_for_user_permissions return list set [p parent for p in get_valid_perms if p set_user_permissions]
def get_doctypes_for_user_permissions return list set [p parent for p in get_valid_perms if p set_user_permissions]
def get_doctypes_for_user_permissions return list set [p parent for p in get_valid_perms if p set_user_permissions]
def absolute_symlink source_path target_path if not os path isabs source_path raise ValueError u'Pathforsource {}mustbeabsolute' format source_path if not os path isabs target_path raise ValueError u'Pathforlink {}mustbeabsolute' format target_path if source_path target_path raise ValueError u'Pathforlinkisidenticaltosource {}' format source_path try if os path lexists target_path if os path islink target_path or os path isfile target_path os unlink target_path else shutil rmtree target_path safe_mkdir_for target_path os symlink source_path target_path except OSError as e if not e errno errno EEXIST or e errno errno ENOENT raise
def absolute_symlink source_path target_path if not os path isabs source_path raise ValueError u'Pathforsource {}mustbeabsolute' format source_path if not os path isabs target_path raise ValueError u'Pathforlink {}mustbeabsolute' format target_path if source_path target_path raise ValueError u'Pathforlinkisidenticaltosource {}' format source_path try if os path lexists target_path if os path islink target_path or os path isfile target_path os unlink target_path else shutil rmtree target_path safe_mkdir_for target_path os symlink source_path target_path except OSError as e if not e errno errno EEXIST or e errno errno ENOENT raise
def srs_output func argtypes func argtypes argtypesfunc restype c_void_pfunc errcheck check_srsreturn func
def srs_output func argtypes func argtypes argtypesfunc restype c_void_pfunc errcheck check_srsreturn func
def setDefaultFetcher fetcher wrap_exceptions True global _default_fetcherif fetcher is None or not wrap_exceptions _default_fetcher fetcherelse _default_fetcher ExceptionWrappingFetcher fetcher
def remove_wsgi_intercept host port key host port if _wsgi_intercept has_key key del _wsgi_intercept[key]
def __virtual__ if get_configured_provider is False return Falsereturn __virtualname__
def CreateRPC return RPC
def get_year_and_month_format locale return locale_year_and_month_formats get locale language lower u'MMMy'
def get_year_and_month_format locale return locale_year_and_month_formats get locale language lower u'MMMy'
def estimate_beta_ridge x y alpha beta_initial [random random for x_i in x[0]]return minimize_stochastic partial squared_error_ridge alpha alpha partial squared_error_ridge_gradient alpha alpha x y beta_initial 0 001
@ignore_warningsdef test_sensitivity_specificity_ignored_labels y_true [1 1 2 3]y_pred [1 3 3 3]specificity_13 partial specificity_score y_true y_pred labels [1 3] specificity_all partial specificity_score y_true y_pred labels None assert_allclose [1 0 0 33] specificity_13 average None rtol R_TOL assert_allclose np mean [1 0 0 33] specificity_13 average 'macro' rtol R_TOL assert_allclose np average [1 0 0 33] weights [2 0 1 0] specificity_13 average 'weighted' rtol R_TOL assert_allclose 3 0 / 3 0 + 2 0 specificity_13 average 'micro' rtol R_TOL for average in ['macro' 'weighted' 'micro'] assert_not_equal specificity_13 average average specificity_all average average
def gauss_chebyshev_u n n_digits xi []w []for i in range 1 n + 1 xi append cos i / n + S One * S Pi n n_digits w append S Pi / n + S One * sin i * S Pi / n + S One ** 2 n n_digits return xi w
def write_pnm file width height pixels meta bitdepth meta['bitdepth']maxval 2 ** bitdepth - 1 planes meta['planes']assert planes in 1 2 3 4 if planes in 1 3 if 1 planes fmt 'P5'else fmt 'P6'file write '%s%d%d%d\n' % fmt width height maxval if planes in 2 4 if 2 planes tupltype 'GRAYSCALE_ALPHA'else tupltype 'RGB_ALPHA'file write 'P7\nWIDTH%d\nHEIGHT%d\nDEPTH%d\nMAXVAL%d\nTUPLTYPE%s\nENDHDR\n' % width height planes maxval tupltype vpr planes * width fmt '>%d' % vpr if maxval > 255 fmt fmt + 'H' else fmt fmt + 'B' for row in pixels file write struct pack fmt *row file flush
def create_access_port network_switch port_name vlan debug Falsenew_port SwitchPort objects get_or_create port_name port_name mode 'access' access_vlan vlan network_switch network_switch if debug print new_port
def maybe_s_to_ms v return int float v * 1000 0 if v is not None else v
def vector_add v w return [ v_i + w_i for v_i w_i in zip v w ]
def _get_terminal_size_tput try cols int subprocess check_call shlex split 'tputcols' rows int subprocess check_call shlex split 'tputlines' return cols rows except pass
def _get_terminal_size_tput try cols int subprocess check_call shlex split 'tputcols' rows int subprocess check_call shlex split 'tputlines' return cols rows except pass
def _get_terminal_size_tput try cols int subprocess check_call shlex split 'tputcols' rows int subprocess check_call shlex split 'tputlines' return cols rows except pass
def _decode_preferred_encoding s enc locale getpreferredencoding try try return s decode enc except LookupError enc _DEFAULT_ENCODINGreturn s decode enc except UnicodeDecodeError return s decode u'latin-1'
def matrix_tensor_product *product if isinstance product[0] Matrix return _sympy_tensor_product *product elif isinstance product[0] numpy_ndarray return _numpy_tensor_product *product elif isinstance product[0] scipy_sparse_matrix return _scipy_sparse_tensor_product *product
@hug post def post_here body return body
def build_profile_map parsed_ini_config parsed_config copy deepcopy parsed_ini_config profiles {}final_config {}for key values in parsed_config items if key startswith 'profile' try parts shlex split key except ValueError continueif len parts 2 profiles[parts[1]] valueselif key 'default' profiles[key] valueselse final_config[key] valuesfinal_config['profiles'] profilesreturn final_config
def test_zookeeper client logged_in_client client click id 'ccs-zookeeper-menu' client waits forElement classname 'CCS-zookeeper' timeout '2000'
@pytest mark networkdef test_upgrade_vcs_req_with_dist_found script req '%s#egg pretend' % 'git+git //github com/alex/pretend@e7f26ad7dbcb4a02a4995aade4743aad47656b27' script pip 'install' req expect_stderr True result script pip 'install' '-U' req expect_stderr True assert 'pypi python org' not in result stdout result stdout
@pytest mark networkdef test_upgrade_vcs_req_with_dist_found script req '%s#egg pretend' % 'git+git //github com/alex/pretend@e7f26ad7dbcb4a02a4995aade4743aad47656b27' script pip 'install' req expect_stderr True result script pip 'install' '-U' req expect_stderr True assert 'pypi python org' not in result stdout result stdout
def _warning_for_deprecated_user_based_rules rules for rule in rules if [resource for resource in USER_BASED_RESOURCES if resource in rule[0] ] continueif 'user_id' in KEY_EXPR findall rule[1] LOG warning _LW "Theuser_idattributeisn'tsupportedintherule'%s' Alltheuser_idbasedpolicyenforcementwillberemovedinthefuture " rule[0]
def _warning_for_deprecated_user_based_rules rules for rule in rules if [resource for resource in USER_BASED_RESOURCES if resource in rule[0] ] continueif 'user_id' in KEY_EXPR findall rule[1] LOG warning _LW "Theuser_idattributeisn'tsupportedintherule'%s' Alltheuser_idbasedpolicyenforcementwillberemovedinthefuture " rule[0]
def make_decreasing_ohlc open high low close dates **kwargs flat_decrease_x flat_decrease_y text_decrease _OHLC open high low close dates get_decrease kwargs setdefault 'line' dict color _DEFAULT_DECREASING_COLOR width 1 kwargs setdefault 'text' text_decrease kwargs setdefault 'showlegend' False kwargs setdefault 'name' 'Decreasing' ohlc_decr dict type 'scatter' x flat_decrease_x y flat_decrease_y mode 'lines' **kwargs return ohlc_decr
def make_decreasing_ohlc open high low close dates **kwargs flat_decrease_x flat_decrease_y text_decrease _OHLC open high low close dates get_decrease kwargs setdefault 'line' dict color _DEFAULT_DECREASING_COLOR width 1 kwargs setdefault 'text' text_decrease kwargs setdefault 'showlegend' False kwargs setdefault 'name' 'Decreasing' ohlc_decr dict type 'scatter' x flat_decrease_x y flat_decrease_y mode 'lines' **kwargs return ohlc_decr
@verbosedef deactivate_proj projs copy True verbose None if copy projs deepcopy projs for proj in projs proj['active'] Falselogger info '%dprojectionitemsdeactivated' % len projs return projs
@verbosedef deactivate_proj projs copy True verbose None if copy projs deepcopy projs for proj in projs proj['active'] Falselogger info '%dprojectionitemsdeactivated' % len projs return projs
def validate_xml xsd_filename xml_filename with open xsd_filename 'r' as f schema_root etree XML f read schema etree XMLSchema schema_root xmlparser etree XMLParser schema schema with open xml_filename 'r' as f etree fromstring f read xmlparser
def validate_xml xsd_filename xml_filename with open xsd_filename 'r' as f schema_root etree XML f read schema etree XMLSchema schema_root xmlparser etree XMLParser schema schema with open xml_filename 'r' as f etree fromstring f read xmlparser
def _compute_diff configured expected diff {'add' {} 'update' {} 'remove' {}}configured_users set configured keys expected_users set expected keys add_usernames expected_users - configured_users remove_usernames configured_users - expected_users common_usernames expected_users & configured_users add dict username expected get username for username in add_usernames remove dict username configured get username for username in remove_usernames update {}for username in common_usernames user_configuration configured get username user_expected expected get username if user_configuration user_expected continueupdate[username] {}for field field_value in six iteritems user_expected if user_configuration get field field_value update[username][field] field_valuediff update {'add' add 'update' update 'remove' remove} return diff
def _blockdevice_volume_from_datasetid volumes dataset_id for volume in volumes if volume dataset_id dataset_id return volume
@pytest mark installeddef test_activate_does_not_leak_echo_setting shell if not on_win or shell u'cmd exe' pytest skip u"echoleakingisonlyrelevantonWindow'sCMD EXE" shell_vars _format_vars shell with TemporaryDirectory prefix u'envs' dir os path dirname __file__ as envs env_dirs env_vars gen_test_env_paths envs shell scripts []src_activate shell_vars[u'source'] format u'{syspath}{binpath}activate{suffix_executable}' scripts + [dedent u'@ECHOON\n{}"{{env_dirs[0]}}"\n@ECHO\n' ]for script in scripts script script format src_activate script script format env_vars env_vars env_dirs env_dirs **shell_vars commands shell_vars[u'command_setup'] + script stdout stderr run_in commands shell print u'commands ' commands print u'stdout ' stdout print u'stderr ' stderr assert_equals stdout u'ECHOison ' stderr assert_equals stderr u''
@pytest mark installeddef test_activate_does_not_leak_echo_setting shell if not on_win or shell u'cmd exe' pytest skip u"echoleakingisonlyrelevantonWindow'sCMD EXE" shell_vars _format_vars shell with TemporaryDirectory prefix u'envs' dir os path dirname __file__ as envs env_dirs env_vars gen_test_env_paths envs shell scripts []src_activate shell_vars[u'source'] format u'{syspath}{binpath}activate{suffix_executable}' scripts + [dedent u'@ECHOON\n{}"{{env_dirs[0]}}"\n@ECHO\n' ]for script in scripts script script format src_activate script script format env_vars env_vars env_dirs env_dirs **shell_vars commands shell_vars[u'command_setup'] + script stdout stderr run_in commands shell print u'commands ' commands print u'stdout ' stdout print u'stderr ' stderr assert_equals stdout u'ECHOison ' stderr assert_equals stderr u''
@pytest mark installeddef test_activate_does_not_leak_echo_setting shell if not on_win or shell u'cmd exe' pytest skip u"echoleakingisonlyrelevantonWindow'sCMD EXE" shell_vars _format_vars shell with TemporaryDirectory prefix u'envs' dir os path dirname __file__ as envs env_dirs env_vars gen_test_env_paths envs shell scripts []src_activate shell_vars[u'source'] format u'{syspath}{binpath}activate{suffix_executable}' scripts + [dedent u'@ECHOON\n{}"{{env_dirs[0]}}"\n@ECHO\n' ]for script in scripts script script format src_activate script script format env_vars env_vars env_dirs env_dirs **shell_vars commands shell_vars[u'command_setup'] + script stdout stderr run_in commands shell print u'commands ' commands print u'stdout ' stdout print u'stderr ' stderr assert_equals stdout u'ECHOison ' stderr assert_equals stderr u''
@pytest mark installeddef test_activate_does_not_leak_echo_setting shell if not on_win or shell u'cmd exe' pytest skip u"echoleakingisonlyrelevantonWindow'sCMD EXE" shell_vars _format_vars shell with TemporaryDirectory prefix u'envs' dir os path dirname __file__ as envs env_dirs env_vars gen_test_env_paths envs shell scripts []src_activate shell_vars[u'source'] format u'{syspath}{binpath}activate{suffix_executable}' scripts + [dedent u'@ECHOON\n{}"{{env_dirs[0]}}"\n@ECHO\n' ]for script in scripts script script format src_activate script script format env_vars env_vars env_dirs env_dirs **shell_vars commands shell_vars[u'command_setup'] + script stdout stderr run_in commands shell print u'commands ' commands print u'stdout ' stdout print u'stderr ' stderr assert_equals stdout u'ECHOison ' stderr assert_equals stderr u''
@pytest mark installeddef test_activate_does_not_leak_echo_setting shell if not on_win or shell u'cmd exe' pytest skip u"echoleakingisonlyrelevantonWindow'sCMD EXE" shell_vars _format_vars shell with TemporaryDirectory prefix u'envs' dir os path dirname __file__ as envs env_dirs env_vars gen_test_env_paths envs shell scripts []src_activate shell_vars[u'source'] format u'{syspath}{binpath}activate{suffix_executable}' scripts + [dedent u'@ECHOON\n{}"{{env_dirs[0]}}"\n@ECHO\n' ]for script in scripts script script format src_activate script script format env_vars env_vars env_dirs env_dirs **shell_vars commands shell_vars[u'command_setup'] + script stdout stderr run_in commands shell print u'commands ' commands print u'stdout ' stdout print u'stderr ' stderr assert_equals stdout u'ECHOison ' stderr assert_equals stderr u''
def NetshStaticIp interface ip u'127 0 0 9' subnet u'255 255 255 255' gw u'127 0 0 1' args ['/c' 'netsh' 'interface' 'ip' 'set' 'address' interface 'static' ip subnet gw '1']res client_utils_common Execute 'cmd' args time_limit -1 bypass_whitelist True return res
def Zero dtype None return Constant 0 0 dtype dtype
def segment sequence aliases if not sequence or aliases returnfor alias parts in aliases items variants {alias OrderByTuple parts OrderBy alias opposite OrderByTuple parts opposite}for valias vparts in variants items if list sequence[ len vparts ] list vparts tail_aliases dict aliases del tail_aliases[alias]tail_sequence sequence[len vparts ]if tail_sequence for tail in segment tail_sequence tail_aliases yield tuple chain [valias] tail else continueelse yield tuple [valias]
def filter_query_params url remove_params if not url return urlremove_params dict p None if isinstance p basestring else p for p in remove_params parsed_url urlparse urlparse url parsed_qsl urlparse parse_qsl parsed_url query keep_blank_values True filtered_qsl [ p remove_params get p v for p v in parsed_qsl]filtered_qsl [ p v for p v in filtered_qsl if v is not None ]filtered_url urlparse ParseResult scheme parsed_url scheme netloc parsed_url netloc path parsed_url path params parsed_url params query urllib urlencode filtered_qsl fragment parsed_url fragment return urlparse urlunparse filtered_url
def _sss l1 l2 l3 c1 Circle 0 0 l3 c2 Circle l1 0 l2 inter [a for a in c1 intersection c2 if a y is_nonnegative]if not inter return Nonept inter[0]return Triangle 0 0 l1 0 pt
def _sss l1 l2 l3 c1 Circle 0 0 l3 c2 Circle l1 0 l2 inter [a for a in c1 intersection c2 if a y is_nonnegative]if not inter return Nonept inter[0]return Triangle 0 0 l1 0 pt
def get_c_cleanup r name sub post '\n{Py_XDECREF py_% name s }\n' % locals return r type c_cleanup name sub + post
def lookup_lastlines lastlines_dirpath path underscored path replace '/' '_' try lastlines_file open os path join lastlines_dirpath underscored except OSError IOError returnlastlines lastlines_file read lastlines_file close os remove lastlines_file name if not lastlines returntry target_file open path except OSError IOError returntarget_data target_file read target_file close loc target_data rfind lastlines if loc -1 returnreverse_lineno target_data count '\n' loc + len lastlines return reverse_lineno
def closed_issue issue after None if issue['state'] 'closed' if after is None or parse_timestamp issue['closed_at'] > after return Truereturn False
def clean_path path split_path path rsplit '/' 2 [ -1 ]if len split_path 2 split_path[1] PATH_TOKENreturn '/' join split_path + '/'
def validate_no_arg_method interface method_name for name method in interface namesAndDescriptions if name method_name if len method getSignatureInfo ['required'] > 0 raise InvalidMethod 'Method{ r}requiresparameters' format method_name returnraise InvalidMethod 'Method{ r}notfoundininterface{}' format method_name interface __name__
def validate_no_arg_method interface method_name for name method in interface namesAndDescriptions if name method_name if len method getSignatureInfo ['required'] > 0 raise InvalidMethod 'Method{ r}requiresparameters' format method_name returnraise InvalidMethod 'Method{ r}notfoundininterface{}' format method_name interface __name__
def background f t Thread target f t start return t
def warnings_to_stdout showwarning_orig warnings showwarningdef showwarning msg cat fname lno file None line 0 showwarning_orig msg cat os path basename fname line sys stdout warnings showwarning showwarning
def unique_values func @wraps func def wrapper *args **kwargs return unique_everseen func *args **kwargs return wrapper
def _str_extract_noexpand arr pat flags 0 from pandas import DataFrame Indexregex re compile pat flags flags groups_or_na _groups_or_na_fun regex if regex groups 1 result np array [groups_or_na val [0] for val in arr] dtype object name _get_single_group_name regex else if isinstance arr Index raise ValueError 'onlyoneregexgroupissupportedwithIndex' name Nonenames dict zip regex groupindex values regex groupindex keys columns [names get 1 + i i for i in range regex groups ]if arr empty result DataFrame columns columns dtype object else result DataFrame [groups_or_na val for val in arr] columns columns index arr index dtype object return result name
def drop_unique_constraint migrate_engine table_name uc_name *columns **col_name_col_instance if migrate_engine name in ['mysql' 'postgresql'] meta MetaData meta bind migrate_enginet Table table_name meta autoload True uc UniqueConstraint table t name uc_name *columns uc drop else _drop_unique_constraint_in_sqlite migrate_engine table_name uc_name **col_name_col_instance
def key_text_to_keyinfo keytext if keytext startswith '"' return keyseq_to_keyinfo keytext[1 -1 ] else return keyname_to_keyinfo keytext
def system name ret {'name' name 'changes' {} 'result' None 'comment' ''}if __salt__['keyboard get_sys'] name ret['result'] Trueret['comment'] 'Systemlayout{0}alreadyset' format name return retif __opts__['test'] ret['comment'] 'Systemlayout{0}needstobeset' format name return retif __salt__['keyboard set_sys'] name ret['changes'] {'layout' name}ret['result'] Trueret['comment'] 'Setsystemkeyboardlayout{0}' format name return retelse ret['result'] Falseret['comment'] 'Failedtosetsystemkeyboardlayout'return ret
def react main argv _reactor None if _reactor is None from twisted internet import reactor as _reactorfinished main _reactor *argv codes [0]stopping []_reactor addSystemEventTrigger 'before' 'shutdown' stopping append True def stop result stopReactor if stopReactor try _reactor stop except ReactorNotRunning passif isinstance result Failure if result check SystemExit is not None code result value codeelse log err result 'mainfunctionencounterederror' code 1codes[0] codedef cbFinish result if stopping stop result False else _reactor callWhenRunning stop result True finished addBoth cbFinish _reactor run sys exit codes[0]
def react main argv _reactor None if _reactor is None from twisted internet import reactor as _reactorfinished main _reactor *argv codes [0]stopping []_reactor addSystemEventTrigger 'before' 'shutdown' stopping append True def stop result stopReactor if stopReactor try _reactor stop except ReactorNotRunning passif isinstance result Failure if result check SystemExit is not None code result value codeelse log err result 'mainfunctionencounterederror' code 1codes[0] codedef cbFinish result if stopping stop result False else _reactor callWhenRunning stop result True finished addBoth cbFinish _reactor run sys exit codes[0]
def combine_game_stats games return reduce lambda ps1 ps2 ps1 + ps2 [g players for g in games if g is not None ]
def get_all_access_keys user_name marker None max_items None region None key None keyid None profile None conn _get_conn region region key key keyid keyid profile profile try return conn get_all_access_keys user_name marker max_items except boto exception BotoServerError as e log debug e log error "Failedtogetuser's{0}accesskeys " format user_name return str e
def make_sshnode test_case server create_ssh_server FilePath test_case mktemp test_case addCleanup server restore return ProcessNode using_ssh host unicode server ip encode 'ascii' port server port username 'root' private_key server key_path
def make_sshnode test_case server create_ssh_server FilePath test_case mktemp test_case addCleanup server restore return ProcessNode using_ssh host unicode server ip encode 'ascii' port server port username 'root' private_key server key_path
def entropy density if isinstance density Density density represent density if isinstance density scipy_sparse_matrix density to_numpy density if isinstance density Matrix eigvals density eigenvals keys return expand - sum e * log e for e in eigvals elif isinstance density numpy_ndarray import numpy as npeigvals np linalg eigvals density return - np sum eigvals * np log eigvals else raise ValueError 'numpy ndarray scipy sparseorsympymatrixexpected'
def pretty_use_unicode flag None global _use_unicodeglobal unicode_warningsif flag is None return _use_unicodeif unicode_warnings known [ 'LATINSUBSCRIPTSMALLLETTER%s' % i for i in 'HKLMNPST']unicode_warnings '\n' join [l for l in unicode_warnings splitlines if not any i in l for i in known ] if flag and unicode_warnings warnings warn unicode_warnings unicode_warnings ''use_unicode_prev _use_unicode_use_unicode flagreturn use_unicode_prev
def pretty_use_unicode flag None global _use_unicodeglobal unicode_warningsif flag is None return _use_unicodeif unicode_warnings known [ 'LATINSUBSCRIPTSMALLLETTER%s' % i for i in 'HKLMNPST']unicode_warnings '\n' join [l for l in unicode_warnings splitlines if not any i in l for i in known ] if flag and unicode_warnings warnings warn unicode_warnings unicode_warnings ''use_unicode_prev _use_unicode_use_unicode flagreturn use_unicode_prev
def test_nvidia_driver2 a numpy random rand 10000 astype 'float32' cuda shared_constructor a assert theano sandbox cuda use device_number is not None
def arma_impulse_response ar ma nobs 100 impulse np zeros nobs impulse[0] 1 0return signal lfilter ma ar impulse
def init state cmd ['shutdown' '-i' state '-g' '0' '-y']ret __salt__['cmd run'] cmd python_shell False return ret
def _get_cycles graph_dict path visited result vertice if vertice in path cycle [vertice]for node in path[ -1 ] if node vertice breakcycle insert 0 node start_from min cycle index cycle index start_from cycle cycle[index ] + cycle[0 index] if not cycle in result result append cycle returnpath append vertice try for node in graph_dict[vertice] if node not in visited _get_cycles graph_dict path visited result node visited add node except KeyError passpath pop
def noHint str return re sub ' ^ *? ?\\ +?\\ ?$' '\\1' str
@track_state_change device_tracker ENTITY_ID_ALL_DEVICES def track_devices hass entity_id old_state new_state if not TARGET_ID returnif new_state state STATE_HOME and not core is_on hass TARGET_ID core turn_on hass TARGET_ID elif new_state state STATE_NOT_HOME and core is_on hass TARGET_ID core turn_off hass TARGET_ID
def absent name database None user None password None host None port None ret {'name' name 'changes' {} 'result' True 'comment' ''}if __salt__['influxdb08 user_exists'] name database user password host port if __opts__['test'] ret['result'] Noneret['comment'] 'User{0}ispresentandneedstoberemoved' format name return retif __salt__['influxdb08 user_remove'] name database user password host port ret['comment'] 'User{0}hasbeenremoved' format name ret['changes'][name] 'Absent'return retelse ret['comment'] 'Failedtoremoveuser{0}' format name ret['result'] Falsereturn retret['comment'] 'User{0}isnotpresent soitcannotberemoved' format name return ret
def generate_verification_key verification_type None token security random_string 30 if not verification_type return tokenexpires timezone now + dt timedelta minutes settings EXPIRATION_TIME_DICT[verification_type] return {'token' token 'expires' expires}
def test_non_existing_unknown_ext with pytest raises IOError data Table read u'non-existing-file-with-unknown ext'
def test_non_existing_unknown_ext with pytest raises IOError data Table read u'non-existing-file-with-unknown ext'
@set_databasedef get_download_youtube_ids paths None downloaded False **kwargs if paths youtube_ids dict for path in paths selector Item kind 'Topic' & Item path contains path & Item youtube_id is_null False if downloaded selector & Item files_complete > 0 else selector & Item files_complete 0 youtube_ids update dict [item for item in Item select Item youtube_id Item title where selector tuples if item[0]] return youtube_ids
def check_token code tokens lx XonshLexer tks list lx get_tokens code for tk in tokens while tks if tk tks[0] breaktks tks[1 ]else msg 'Token{ r}missing { r}' format tk list lx get_tokens code pytest fail msg break
def precision_recall_curve y_true probas_pred pos_label None sample_weight None fps tps thresholds _binary_clf_curve y_true probas_pred pos_label pos_label sample_weight sample_weight precision tps / tps + fps recall tps / tps[ -1 ] last_ind tps searchsorted tps[ -1 ] sl slice last_ind None -1 return np r_[ precision[sl] 1 ] np r_[ recall[sl] 0 ] thresholds[sl]
def woeid_search query query u'q select*fromgeo placeswheretext "%s"' % query body web get u'http //query yahooapis com/v1/public/yql?' + query dont_decode True parsed xmltodict parse body get u'query' results parsed get u'results' if results is None or results get u'place' is None return Noneif type results get u'place' is list return results get u'place' [0]return results get u'place'
def merge_similar_guesses guesses prop choose similar [guess for guess in guesses if prop in guess ]if len similar < 2 returnif len similar 2 _merge_similar_guesses_nocheck guesses prop choose if len similar > 2 log debug u'complexmerge tryingourbest ' before len guesses _merge_similar_guesses_nocheck guesses prop choose after len guesses if after < before merge_similar_guesses guesses prop choose
def merge_similar_guesses guesses prop choose similar [guess for guess in guesses if prop in guess ]if len similar < 2 returnif len similar 2 _merge_similar_guesses_nocheck guesses prop choose if len similar > 2 log debug u'complexmerge tryingourbest ' before len guesses _merge_similar_guesses_nocheck guesses prop choose after len guesses if after < before merge_similar_guesses guesses prop choose
def merge_similar_guesses guesses prop choose similar [guess for guess in guesses if prop in guess ]if len similar < 2 returnif len similar 2 _merge_similar_guesses_nocheck guesses prop choose if len similar > 2 log debug u'complexmerge tryingourbest ' before len guesses _merge_similar_guesses_nocheck guesses prop choose after len guesses if after < before merge_similar_guesses guesses prop choose
def inject_search_after_save output if 'form' in output id 'search_after_save'label LABEL '%s ' % T 'SearchAfterSave?' _for 'msg_twitter_search' widget INPUT _name 'search_after_save' _type 'checkbox' value 'on' _id id _class 'boolean' comment ''if s3_formstyle 'bootstrap' _controls DIV widget comment _class 'controls' row DIV label _controls _class 'control-group' _id '%s__row' % id elif callable s3_formstyle row s3_formstyle id label widget comment else raiseoutput['form'][0][ -2 ] append row
def getSymmetricXLoop path vertexes x loop []for point in path vector3Index Vector3Index len vertexes x point real point imag loop append vector3Index vertexes append vector3Index return loop
def _cast_params_from params context schemas result {}for name in params param_schema {}for schema in schemas if name in schema param_schema schema[name]result[name] _cast context[name] param_schema return result
def _cast_params_from params context schemas result {}for name in params param_schema {}for schema in schemas if name in schema param_schema schema[name]result[name] _cast context[name] param_schema return result
def _cast_params_from params context schemas result {}for name in params param_schema {}for schema in schemas if name in schema param_schema schema[name]result[name] _cast context[name] param_schema return result
def is_known_type item return isinstance item MaskedArray or get_type_string item is not None
def _parse_date_columns data_frame parse_dates if parse_dates is True or parse_dates is None or parse_dates is False parse_dates []if not hasattr parse_dates '__iter__' parse_dates [parse_dates]for col_name in parse_dates df_col data_frame[col_name]try fmt parse_dates[col_name]except TypeError fmt Nonedata_frame[col_name] _handle_date_column df_col format fmt for col_name df_col in data_frame iteritems if is_datetime64tz_dtype df_col data_frame[col_name] _handle_date_column df_col return data_frame
def _parse_date_columns data_frame parse_dates if parse_dates is True or parse_dates is None or parse_dates is False parse_dates []if not hasattr parse_dates '__iter__' parse_dates [parse_dates]for col_name in parse_dates df_col data_frame[col_name]try fmt parse_dates[col_name]except TypeError fmt Nonedata_frame[col_name] _handle_date_column df_col format fmt for col_name df_col in data_frame iteritems if is_datetime64tz_dtype df_col data_frame[col_name] _handle_date_column df_col return data_frame
def _read_config conf_file None if conf_file is None paths '/etc/supervisor/supervisord conf' '/etc/supervisord conf' for path in paths if os path exists path conf_file pathbreakif conf_file is None raise CommandExecutionError 'Nosuitableconfigfilefound' config configparser ConfigParser try config read conf_file except IOError OSError as exc raise CommandExecutionError 'Unabletoreadfrom{0} {1}' format conf_file exc return config
def listen_fds unset_environment True fds int os environ get 'LISTEN_FDS' 0 listen_pid int os environ get 'LISTEN_PID' 0 if listen_pid os getpid return 0if unset_environment os environ pop 'LISTEN_PID' None os environ pop 'LISTEN_FDS' None return fds
def _make_masked_array data mask if len data return ma array np array data mask np array mask dtype u'bool' else return ma array np array data
def _make_masked_array data mask if len data return ma array np array data mask np array mask dtype u'bool' else return ma array np array data
def BeginThreadsSimpleMarshal numThreads cookie ret []for i in range numThreads hEvent win32event CreateEvent None 0 0 None thread start_new TestInterpInThread hEvent cookie ret append hEvent return ret
def __virtual__ if __grains__ get 'os_family' False 'Alpine' return __virtualname__return False 'ModuleapkonlyworksonAlpineLinuxbasedsystems'
def twinx ax None if ax is None ax gca ax1 ax twinx draw_if_interactive return ax1
def validate_target func *args **kwargs def inner self *args **kwargs target_id Noneif 'target_id' in kwargs and kwargs['target_id'] None target_id kwargs['target_id']else target_id 0if not self target_is_valid target_id raise NoSuchTargetException return func self *args **kwargs return inner
def action_events_get context action_id return IMPL action_events_get context action_id
def main try opts args getopt getopt sys argv[1 ] '' ['query '] except getopt error as msg print 'pythonCodesearchExample py--query[query_text]'sys exit 2 query ''for o a in opts if o '--query' query aif query '' print 'pythonCodesearchExample py--query[query]'sys exit 2 sample CodesearchExample sample PrintCodeSnippets query
def main try opts args getopt getopt sys argv[1 ] '' ['query '] except getopt error as msg print 'pythonCodesearchExample py--query[query_text]'sys exit 2 query ''for o a in opts if o '--query' query aif query '' print 'pythonCodesearchExample py--query[query]'sys exit 2 sample CodesearchExample sample PrintCodeSnippets query
def nested_view request c Client c get '/no_template_view/' return render request 'base html' {'nested' 'yes'}
def add_channel_arguments parser parser add_argument '--repo-data' dest 'repo_data' default None help 'Publishedrepositorydata willbefetchedfrom--channelifnotavailableandwritten Defaultsto[channel_name]-repodata json ' parser add_argument '--diff-hours' dest 'diff_hours' default '25' help 'Iffindingallrecentlychangedrecipes usethisnumberofhours ' parser add_argument '--recipes-dir' dest 'recipes_dir' default ' /bioconda-recipes'
def get_profile_from_user user for field in user _meta get_fields try if hasattr user field name attribute getattr user field name if get_profile_model type attribute return attributeexcept Exception logger exception 'Errorgettingprofileattributefromuser ' logger info 'Couldnotfindprofileattribute ' return None
def get_profile_from_user user for field in user _meta get_fields try if hasattr user field name attribute getattr user field name if get_profile_model type attribute return attributeexcept Exception logger exception 'Errorgettingprofileattributefromuser ' logger info 'Couldnotfindprofileattribute ' return None
def check_strict_xfail pyfuncitem evalxfail pyfuncitem _evalxfailif evalxfail istrue strict_default pyfuncitem config getini 'xfail_strict' is_strict_xfail evalxfail get 'strict' strict_default if is_strict_xfail del pyfuncitem _evalxfailexplanation evalxfail getexplanation pytest fail '[XPASS strict ]' + explanation pytrace False
def MakeUniformPmf low high n pmf Pmf for x in np linspace low high n pmf Set x 1 pmf Normalize return pmf
def runmodule name '__main__' **kw main defaultTest name **kw
def runmodule name '__main__' **kw main defaultTest name **kw
def mimedata_from_paths paths abspaths [core abspath path for path in paths]urls [QtCore QUrl fromLocalFile path for path in abspaths]mimedata QtCore QMimeData mimedata setUrls urls paths_text core list2cmdline abspaths encoding gitcfg current get u'cola dragencoding' u'utf-16' moz_text core encode paths_text encoding encoding mimedata setData u'text/x-moz-url' moz_text return mimedata
def get_projection_names return projection_registry get_projection_names
def cloud tgt provider None if not isinstance tgt six string_types return {}ret {}opts salt config cloud_config '/etc/salt/cloud' cloud_cache __utils__['cloud list_cache_nodes_full'] opts opts provider provider for driver providers in cloud_cache items for provider servers in providers items for name data in servers items if fnmatch fnmatch name tgt ret[name] dataret['name']['provider'] providerreturn ret
def is_shopping_cart_enabled enable_paid_course_registration configuration_helpers get_value 'ENABLE_PAID_COURSE_REGISTRATION' settings FEATURES get 'ENABLE_PAID_COURSE_REGISTRATION' enable_shopping_cart configuration_helpers get_value 'ENABLE_SHOPPING_CART' settings FEATURES get 'ENABLE_SHOPPING_CART' return enable_paid_course_registration and enable_shopping_cart
def treetypes root w NodeTypeWriter w visit root return u'\n' join [u''] + w result + [u'']
def _process_quantiles x dim x np asarray x dtype float if x ndim 0 x x[np newaxis]elif x ndim 1 if dim 1 x x[ np newaxis]else x x[np newaxis ]return x
def test_cae_basic with open os path join yaml_dir_path 'cae yaml' as f yaml_string f read train yaml_parse load yaml_string train main_loop
@core_helperdef view_resource_url resource_view resource package **kw return resource['url']
def scan_multilang tokens module_elem tokenizer perl_lexer PerlMultiLangLexer tokens parser perl_parser Parser tokenizer lang 'PerlHTML' provide_full_docs gProvideFullDocs parser moduleName ''parser parse parse_tree parser produce_CIX_NoHeader module_elem csl_tokens tokenizer get_csl_tokens return csl_tokens tokenizer has_perl_code
def test_found with pytest raises falcon http_status HTTPStatus as redirect hug redirect found '/' assert '302' in redirect value status
def generate_mtime_map opts path_map file_map {}for saltenv path_list in six iteritems path_map for path in path_list for directory dirnames filenames in os walk path dirnames[ ] [d for d in dirnames if not is_file_ignored opts d ]for item in filenames try file_path os path join directory item file_map[file_path] os path getmtime file_path except OSError IOError log info 'Failedtogetmtimeon{0} danglingsymlink?' format file_path continuereturn file_map
def stop *args **kwargs return _query 'server/halt'
def extract_line text index p text rfind '\n' 0 index + 1 q text find '\n' index if q < 0 q len text return text[p q]
def extract_line text index p text rfind '\n' 0 index + 1 q text find '\n' index if q < 0 q len text return text[p q]
def create_realistic_servicepair test from_pool StoragePool reactor create_zfs_pool test FilePath test mktemp from_config FilePath test mktemp from_service VolumeService from_config from_pool reactor Clock from_service startService test addCleanup from_service stopService to_pool StoragePool reactor create_zfs_pool test FilePath test mktemp to_config FilePath test mktemp to_service VolumeService to_config to_pool reactor Clock to_service startService test addCleanup to_service stopService remote RemoteVolumeManager MutatingProcessNode to_service to_config origin_remote LocalVolumeManager from_service return ServicePair from_service from_service to_service to_service remote remote origin_remote origin_remote
def create_realistic_servicepair test from_pool StoragePool reactor create_zfs_pool test FilePath test mktemp from_config FilePath test mktemp from_service VolumeService from_config from_pool reactor Clock from_service startService test addCleanup from_service stopService to_pool StoragePool reactor create_zfs_pool test FilePath test mktemp to_config FilePath test mktemp to_service VolumeService to_config to_pool reactor Clock to_service startService test addCleanup to_service stopService remote RemoteVolumeManager MutatingProcessNode to_service to_config origin_remote LocalVolumeManager from_service return ServicePair from_service from_service to_service to_service remote remote origin_remote origin_remote
def absent name ret {'name' name 'changes' {} 'result' False 'comment' ''}if not __salt__['marathon has_app'] name ret['result'] Trueret['comment'] 'App{0}alreadyabsent' format name return retif __opts__['test'] ret['result'] Noneret['comment'] 'App{0}issettoberemoved' format name return retif __salt__['marathon rm_app'] name ret['changes'] {'app' name}ret['result'] Trueret['comment'] 'Removedapp{0}' format name return retelse ret['result'] Falseret['comment'] 'Failedtoremoveapp{0}' format name return ret
def read_cache_entry f beginoffset f tell ctime read_cache_time f mtime read_cache_time f dev ino mode uid gid size sha flags struct unpack '>LLLLLL20sH' f read 20 + 4 * 6 + 2 name f read flags & 4095 real_size f tell - beginoffset + 8 & ~ 7 f read beginoffset + real_size - f tell return name ctime mtime dev ino mode uid gid size sha_to_hex sha flags & ~ 4095
def _get_count queryset try return queryset count except AttributeError TypeError return len queryset
@pytest fixturedef small_push_dir tmpdir contents 'abcdefghijlmnopqrstuvwxyz\n' * 10000 push_dir tmpdir join 'push-from' ensure dir True push_dir join 'arbitrary-file' write contents push_dir join 'pg_xlog' mksymlinkto '/tmp/wal-e-test-must-not-exist' push_dir join 'holy-smokes' ensure return push_dir
def get_data users exclude None exclude exclude if exclude else set emails u email for u in users return {'EMAIL' e} for e in emails if e not in exclude
def extract_entities template if template is None or _RE_NONE_ENTITIES search template return MATCH_ALLextraction _RE_GET_ENTITIES findall template if len extraction > 0 return list set extraction return MATCH_ALL
@bdd then bdd parsers re 'the ?P<category>error message warning " ?P<message> * "shouldbeshown' def expect_message quteproc httpbin category message category_to_loglevel {'message' logging INFO 'error' logging ERROR 'warning' logging WARNING}message message replace ' port ' str httpbin port quteproc mark_expected category 'message' loglevel category_to_loglevel[category] message message
def load_doctype_module doctype module None prefix u'' suffix u'' if not module module get_doctype_module doctype app get_module_app module key app doctype prefix suffix module_name get_module_name doctype module prefix suffix try if key not in doctype_python_modules doctype_python_modules[key] frappe get_module module_name except ImportError raise ImportError u'Moduleimportfailedfor{0} {1} ' format doctype module_name return doctype_python_modules[key]
def memoize for_each_device False if available return cupy memoize for_each_device def dummy_decorator f @functools wraps f def ret *args **kwargs return f *args **kwargs return retreturn dummy_decorator
def memoize for_each_device False if available return cupy memoize for_each_device def dummy_decorator f @functools wraps f def ret *args **kwargs return f *args **kwargs return retreturn dummy_decorator
def tensordot a b axes 2 return _tensordot_as_dot a b axes dot dot batched False
def _create_char_spinner while True for c in ' /-\\' yield c
def _create_char_spinner while True for c in ' /-\\' yield c
def _constrain_sv_less_than_one_python unconstrained order None k_endog None from scipy import linalgconstrained []if order is None order len unconstrained if k_endog is None k_endog unconstrained[0] shape[0]eye np eye k_endog for i in range order A unconstrained[i] B lower linalg cho_factor eye + np dot A A T lower True constrained append linalg solve_triangular B A lower lower return constrained
def _constrain_sv_less_than_one_python unconstrained order None k_endog None from scipy import linalgconstrained []if order is None order len unconstrained if k_endog is None k_endog unconstrained[0] shape[0]eye np eye k_endog for i in range order A unconstrained[i] B lower linalg cho_factor eye + np dot A A T lower True constrained append linalg solve_triangular B A lower lower return constrained
def unlink_paths_older_than filepaths mtime for fpath in filepaths try if os path getmtime fpath < mtime os unlink fpath except OSError pass
def read_console_output_character x 0 y 0 fd 1 buf None bufsize 1024 raw False hcon STDHANDLES[fd]if buf is None if raw buf ctypes c_char_p '' * bufsize else buf ctypes c_wchar_p '' * bufsize coord COORD x y n DWORD if raw ReadConsoleOutputCharacterA hcon buf bufsize coord byref n else ReadConsoleOutputCharacterW hcon buf bufsize coord byref n return buf value[ n value]
def renew_cert config domains le_client lineage renewal_params lineage configuration['renewalparams']original_server renewal_params get 'server' cli flag_default 'server' _avoid_invalidating_lineage config lineage original_server if not domains domains lineage names new_certr new_chain new_key _ le_client obtain_certificate domains if config dry_run logger debug 'Dryrun skippingupdatinglineageat%s' os path dirname lineage cert else prior_version lineage latest_common_version new_cert OpenSSL crypto dump_certificate OpenSSL crypto FILETYPE_PEM new_certr body wrapped new_chain crypto_util dump_pyopenssl_chain new_chain lineage save_successor prior_version new_cert new_key pem new_chain config lineage update_all_links_to lineage latest_common_version hooks renew_hook config domains lineage live_dir
def _get_deleted_exploration_change_list exploration_id return [{'cmd' collection_domain CMD_DELETE_COLLECTION_NODE 'exploration_id' exploration_id}]
def add overlay ret list old_overlays list_local cmd 'layman--quietness 0--add{0}' format overlay add_attempt __salt__['cmd run_all'] cmd python_shell False stdin 'y' if add_attempt['retcode'] 0 raise salt exceptions CommandExecutionError add_attempt['stdout'] new_overlays list_local if len old_overlays 0 and len new_overlays > 0 srcline 'source/var/lib/layman/make conf'makeconf _get_makeconf if not __salt__['file contains'] makeconf 'layman' __salt__['file append'] makeconf srcline ret [overlay for overlay in new_overlays if overlay not in old_overlays ]return ret
def PosixShutdown dev_process GlobalProcess children dev_process Children for term_signal in signal SIGTERM signal SIGKILL for child in children if child process is None continueif child process returncode is not None continuepid child process pidtry logging debug 'posixkill%dwithsignal%d' pid term_signal os kill pid term_signal except OSError as err logging error 'Errorencounteredsendingpid%dsignal%d %s\n' pid term_signal err breaktime sleep 0 2 for child in children if child process is None continueif child process returncode is not None continuetry child process wait except OSError as e if e errno errno ECHILD raise e
def libvlc_audio_equalizer_get_band_frequency u_index f _Cfunctions get 'libvlc_audio_equalizer_get_band_frequency' None or _Cfunction 'libvlc_audio_equalizer_get_band_frequency' 1 None ctypes c_float ctypes c_uint return f u_index
@cache_permissiondef can_overwrite_translation user project return check_permission user project 'trans overwrite_translation'
def wordrelationships relationshiplist relationships etree fromstring '<Relationshipsxmlns "http //schemas openxmlformats org/package/2006/relationships"></Relationships>' count 0for relationship in relationshiplist rel_elm makeelement 'Relationship' nsprefix None attributes {'Id' 'rId' + str count + 1 'Type' relationship[0] 'Target' relationship[1]} relationships append rel_elm count + 1return relationships
def get_datetime_utc_now dt datetime datetime utcnow dt add_utc_tz dt return dt
def get_datetime_utc_now dt datetime datetime utcnow dt add_utc_tz dt return dt
def is_switchport_default existing c1 existing['access_vlan'] '1' c2 existing['native_vlan'] '1' c3 existing['trunk_vlans'] '1-4094' c4 existing['mode'] 'access' default c1 and c2 and c3 and c4 return default
def findlabels code labels []n len code i 0while i < n c code[i]op ord c i i + 1 if op > HAVE_ARGUMENT oparg ord code[i] + ord code[ i + 1 ] * 256 i i + 2 label -1 if op in hasjrel label i + oparg elif op in hasjabs label opargif label > 0 if label not in labels labels append label return labels
def get_taxes_of_effective_rules taxing_context tax_rules matching_rules tax_rule for tax_rule in tax_rules if tax_rule matches taxing_context grouped_by_override groupby matching_rules attrgetter 'override_group' highest_override_group first grouped_by_override None [] [1]grouped_rules groupby highest_override_group attrgetter 'priority' tax_groups [[rule tax for rule in rules] for _ rules in grouped_rules]return tax_groups
def get_external_ip return search_upnp_device addCallback lambda x x get_external_ip
def _report_unknown_attribute name logging error 'unknownCookieattribute %r' name
def create_from_template zone template ret {'status' True}res __salt__['cmd run_all'] 'zonecfg-z{zone}create-t{tmpl}-F' format zone zone tmpl template ret['status'] res['retcode'] 0 ret['message'] res['stdout'] if ret['status'] else res['stderr'] ret['message'] ret['message'] replace 'zonecfg ' '' if ret['message'] '' del ret['message']return ret
def get_fg_bg name if requires_special_home_display name return Color HOME_FG Color HOME_BG return Color PATH_FG Color PATH_BG
def _check_density density n_features if density 'auto' density 1 / np sqrt n_features elif density < 0 or density > 1 raise ValueError 'Expecteddensityinrange]0 1] got %r' % density return density
def create_clones config model_fn args None kwargs None clones []args args or [] kwargs kwargs or {} with slim arg_scope [slim model_variable slim variable] device config variables_device for i in range 0 config num_clones with tf name_scope config clone_scope i as clone_scope clone_device config clone_device i with tf device clone_device with tf variable_scope tf get_variable_scope reuse True if i > 0 else None outputs model_fn *args **kwargs clones append Clone outputs clone_scope clone_device return clones
def service_for_pool test pool service VolumeService FilePath test mktemp pool None service startService test addCleanup service stopService return service
def service_for_pool test pool service VolumeService FilePath test mktemp pool None service startService test addCleanup service stopService return service
def service_for_pool test pool service VolumeService FilePath test mktemp pool None service startService test addCleanup service stopService return service
def make_future_info first_sid root_symbols years notice_date_func expiration_date_func start_date_func month_codes None if month_codes is None month_codes CME_CODE_TO_MONTHyear_strs list map str years years [pd Timestamp s tz 'UTC' for s in year_strs]contract_suffix_to_beginning_of_month tuple month_code + year_str[ -2 ] year + MonthBegin month_num for year year_str month_code month_num in product zip years year_strs iteritems month_codes contracts []parts product root_symbols contract_suffix_to_beginning_of_month for sid root_sym suffix month_begin in enumerate parts first_sid contracts append {'sid' sid 'root_symbol' root_sym 'symbol' root_sym + suffix 'start_date' start_date_func month_begin 'notice_date' notice_date_func month_begin 'expiration_date' notice_date_func month_begin 'multiplier' 500 'exchange' 'TEST' 'exchange_full' 'TESTFULL'} return pd DataFrame from_records contracts index 'sid'
def consistencygroup_destroy context consistencygroup_id return IMPL consistencygroup_destroy context consistencygroup_id
def execute *cmd **kwargs process_input kwargs pop 'process_input' None run_as_root kwargs pop 'run_as_root' True ret 0try if len cmd > 3 and cmd[0] 'raidcom' and cmd[1] '-login' stdout stderr cinder_utils execute process_input process_input run_as_root run_as_root loglevel base_logging NOTSET *cmd [ 2]else stdout stderr cinder_utils execute process_input process_input run_as_root run_as_root *cmd [ 2]except putils ProcessExecutionError as ex ret ex exit_codestdout ex stdoutstderr ex stderrLOG debug 'cmd %s' mask_password cmd LOG debug 'from %s' inspect stack [2] LOG debug 'ret %s' ret LOG debug 'stdout %s' '' join stdout splitlines LOG debug 'stderr %s' '' join stderr splitlines return ret stdout stderr
@shared_taskdef sleeping i **_ sleep i
def now request return {'now' datetime now }
def config if 'conf_file' not in __opts__ return {}if os path isdir __opts__['conf_file'] gfn os path join __opts__['conf_file'] 'grains' else gfn os path join os path dirname __opts__['conf_file'] 'grains' if os path isfile gfn with salt utils fopen gfn 'rb' as fp_ try return yaml safe_load fp_ read except Exception log warning 'Badsyntaxingrainsfile Skipping ' return {}return {}
def org_service_root_service service_id db current dbtable current s3db org_servicerecord db table id service_id select table id table root_service table parent first try parent record parentcurrent_root record root_serviceexcept AttributeError current log error 'Cannotfindrecordwithservice_id %s' % service_id raiseif parent if parent service_id raise KeyError 'Service#%sshowingwithparent#%s' % service_id parent new_root org_service_root_service parent else new_root service_idif current_root new_root def descendants ids rows db table parent belongs ids select table id children set row id for row in rows if children children descendants children return ids children else return idsnodes descendants set [service_id] db table id belongs nodes update root_service new_root return new_root
def test_longer jobs bg BackgroundJobManager j jobs new sleeper 0 1 nt assert_equal len jobs running 1 nt assert_equal len jobs completed 0 j join nt assert_equal len jobs running 0 nt assert_equal len jobs completed 1
def _introspectRoute route exampleByIdentifier schema_store result {}try user_documentation route attributes['user_documentation']except KeyError raise SphinxError 'Undocumentedroute {}' format route result['description'] prepare_docstring user_documentation text result['header'] user_documentation headerresult['section'] user_documentation sectioninputSchema route attributes get 'inputSchema' None outputSchema route attributes get 'outputSchema' None if inputSchema result['input'] _parseSchema inputSchema schema_store result['input_schema'] inputSchemaif outputSchema result['output'] _parseSchema outputSchema schema_store result['output_schema'] outputSchemaexamples user_documentation examplesresult['examples'] list Example fromDictionary exampleByIdentifier identifier for identifier in examples return result
def dependency_check should_exit mdep_check dependency_set GUI exit_on_failure False try import pygtkpygtk require '2 0' import gtkimport gobjectassert gtk gtk_version > 2 12 assert gtk pygtk_version > 2 12 except msg 'TheGTKpackagerequirementsarenotmet pleasemakesureyoursystemmeetstheserequirements \n-PyGTK> 2 12\n-GTK> 2 12\n'print msgshould_exit Trueif not which 'dot' msg 'Therequired"dot"binaryismissing pleaseinstallthe"graphviz"packageinyouroperatingsystem 'print msgshould_exit Trueif should_exit sys exit 1
def dependency_check should_exit mdep_check dependency_set GUI exit_on_failure False try import pygtkpygtk require '2 0' import gtkimport gobjectassert gtk gtk_version > 2 12 assert gtk pygtk_version > 2 12 except msg 'TheGTKpackagerequirementsarenotmet pleasemakesureyoursystemmeetstheserequirements \n-PyGTK> 2 12\n-GTK> 2 12\n'print msgshould_exit Trueif not which 'dot' msg 'Therequired"dot"binaryismissing pleaseinstallthe"graphviz"packageinyouroperatingsystem 'print msgshould_exit Trueif should_exit sys exit 1
def dependency_check should_exit mdep_check dependency_set GUI exit_on_failure False try import pygtkpygtk require '2 0' import gtkimport gobjectassert gtk gtk_version > 2 12 assert gtk pygtk_version > 2 12 except msg 'TheGTKpackagerequirementsarenotmet pleasemakesureyoursystemmeetstheserequirements \n-PyGTK> 2 12\n-GTK> 2 12\n'print msgshould_exit Trueif not which 'dot' msg 'Therequired"dot"binaryismissing pleaseinstallthe"graphviz"packageinyouroperatingsystem 'print msgshould_exit Trueif should_exit sys exit 1
def dependency_check should_exit mdep_check dependency_set GUI exit_on_failure False try import pygtkpygtk require '2 0' import gtkimport gobjectassert gtk gtk_version > 2 12 assert gtk pygtk_version > 2 12 except msg 'TheGTKpackagerequirementsarenotmet pleasemakesureyoursystemmeetstheserequirements \n-PyGTK> 2 12\n-GTK> 2 12\n'print msgshould_exit Trueif not which 'dot' msg 'Therequired"dot"binaryismissing pleaseinstallthe"graphviz"packageinyouroperatingsystem 'print msgshould_exit Trueif should_exit sys exit 1
@csrf_exempt@facebook_required_lazydef connect request graph backend get_registration_backend try response _connect request graph except open_facebook_exceptions FacebookUnreachable as e warning_format u'%s oftencausedbyFacebookslowdown error%s'warn_message warning_format % type e str e send_warning warn_message e e additional_params dict fb_error_or_cancel 1 response backend post_error request additional_params return response
@csrf_exempt@facebook_required_lazydef connect request graph backend get_registration_backend try response _connect request graph except open_facebook_exceptions FacebookUnreachable as e warning_format u'%s oftencausedbyFacebookslowdown error%s'warn_message warning_format % type e str e send_warning warn_message e e additional_params dict fb_error_or_cancel 1 response backend post_error request additional_params return response
@csrf_exempt@facebook_required_lazydef connect request graph backend get_registration_backend try response _connect request graph except open_facebook_exceptions FacebookUnreachable as e warning_format u'%s oftencausedbyFacebookslowdown error%s'warn_message warning_format % type e str e send_warning warn_message e e additional_params dict fb_error_or_cancel 1 response backend post_error request additional_params return response
@csrf_exempt@facebook_required_lazydef connect request graph backend get_registration_backend try response _connect request graph except open_facebook_exceptions FacebookUnreachable as e warning_format u'%s oftencausedbyFacebookslowdown error%s'warn_message warning_format % type e str e send_warning warn_message e e additional_params dict fb_error_or_cancel 1 response backend post_error request additional_params return response
@csrf_exempt@facebook_required_lazydef connect request graph backend get_registration_backend try response _connect request graph except open_facebook_exceptions FacebookUnreachable as e warning_format u'%s oftencausedbyFacebookslowdown error%s'warn_message warning_format % type e str e send_warning warn_message e e additional_params dict fb_error_or_cancel 1 response backend post_error request additional_params return response
def getattrs value attrs default _no_default try for attr in attrs value getattr value attr except AttributeError if default is _no_default raisevalue defaultreturn value
def prep_jid nocache False passed_jid None if passed_jid is None jid salt utils jid gen_jid else jid passed_jidcb_ _get_connection try cb_ add str jid {'nocache' nocache} ttl _get_ttl except couchbase exceptions KeyExistsError if passed_jid is None return prep_jid nocache nocache return jid
def prep_jid nocache False passed_jid None if passed_jid is None jid salt utils jid gen_jid else jid passed_jidcb_ _get_connection try cb_ add str jid {'nocache' nocache} ttl _get_ttl except couchbase exceptions KeyExistsError if passed_jid is None return prep_jid nocache nocache return jid
def provides_csp_features response return len retrieve_csp_policies response + len retrieve_csp_policies response True > 0
def ordlist s return map ord s
def distorted_bounding_box_crop image bbox min_object_covered 0 1 aspect_ratio_range 0 75 1 33 area_range 0 05 1 0 max_attempts 100 scope None with tf name_scope scope 'distorted_bounding_box_crop' [image bbox] sample_distorted_bounding_box tf image sample_distorted_bounding_box tf shape image bounding_boxes bbox min_object_covered min_object_covered aspect_ratio_range aspect_ratio_range area_range area_range max_attempts max_attempts use_image_if_no_bounding_boxes True bbox_begin bbox_size distort_bbox sample_distorted_bounding_boxcropped_image tf slice image bbox_begin bbox_size return cropped_image distort_bbox
def dfs_labeled_edges G source None if source is None nodes Gelse nodes [source]visited set for start in nodes if start in visited continue yield start start 'forward' visited add start stack [ start iter G[start] ]while stack parent children stack[ -1 ]try child next children if child in visited yield parent child 'nontree' else yield parent child 'forward' visited add child stack append child iter G[child] except StopIteration stack pop if stack yield stack[ -1 ][0] parent 'reverse' yield start start 'reverse'
@unbox types Array def unbox_array typ obj c nativearycls c context make_array typ nativeary nativearycls c context c builder aryptr nativeary _getpointer ptr c builder bitcast aryptr c pyapi voidptr if c context enable_nrt errcode c pyapi nrt_adapt_ndarray_from_python obj ptr else errcode c pyapi numba_array_adaptor obj ptr failed cgutils is_not_null c builder errcode return NativeValue c builder load aryptr is_error failed
def _update_lut cmap colors cmap _lut[ 256] colorscmap _set_extremes
def _update_lut cmap colors cmap _lut[ 256] colorscmap _set_extremes
def getSharedFace firstEdge faces secondEdge for firstEdgeFaceIndex in firstEdge faceIndexes for secondEdgeFaceIndex in secondEdge faceIndexes if firstEdgeFaceIndex secondEdgeFaceIndex return faces[firstEdgeFaceIndex]return None
def deflate_long n add_sign_padding True s bytes n long n while n 0 and n -1 s struct pack '>I' n & xffffffff + s n >> 32for i in enumerate s if n 0 and i[1] deflate_zero breakif n -1 and i[1] deflate_ff breakelse i 0 if n 0 s zero_byteelse s max_bytes s[i[0] ]if add_sign_padding if n 0 and byte_ord s[0] > 128 s zero_byte + s if n -1 and byte_ord s[0] < 128 s max_byte + s return s
def pluralize count item_type def pluralize_string x if x endswith u's' return x + u'es' else return x + u's' text u'{}{}' format count item_type if count 1 else pluralize_string item_type return text
def pluralize count item_type def pluralize_string x if x endswith u's' return x + u'es' else return x + u's' text u'{}{}' format count item_type if count 1 else pluralize_string item_type return text
def pluralize count item_type def pluralize_string x if x endswith u's' return x + u'es' else return x + u's' text u'{}{}' format count item_type if count 1 else pluralize_string item_type return text
def pluralize count item_type def pluralize_string x if x endswith u's' return x + u'es' else return x + u's' text u'{}{}' format count item_type if count 1 else pluralize_string item_type return text
def _dummy_closure x return lambda x
def _dummy_closure x return lambda x
def _dummy_closure x return lambda x
def aggregate_host_get_all context aggregate_id return IMPL aggregate_host_get_all context aggregate_id
def _build_tree_structure queryset all_nodes {}mptt_opts queryset model _mptt_metaitems queryset order_by mptt_opts tree_id_attr mptt_opts left_attr values_list u'pk' u'%s_id' % mptt_opts parent_attr for p_id parent_id in items all_nodes setdefault str parent_id if parent_id else 0 [] append p_id return all_nodes
@intercept_errors FakeOutputException ignore_errors [ValueError] def intercepted_function raise_error None if raise_error is not None raise raise_error
def get_pool config volume if volume volume_type metadata {}type_id volume volume_type_idif type_id is not None metadata volume_types get_volume_type_extra_specs type_id if metadata get 'service_label' if metadata['service_label'] in config['services'] keys return metadata['service_label']return 'default'
def create_user_contributions user_id created_exploration_ids edited_exploration_ids user_contributions get_user_contributions user_id strict False if user_contributions raise Exception 'Usercontributionsmodelforuser%salreadyexists ' % user_id else user_contributions UserContributions user_id created_exploration_ids edited_exploration_ids _save_user_contributions user_contributions return user_contributions
def create_user_contributions user_id created_exploration_ids edited_exploration_ids user_contributions get_user_contributions user_id strict False if user_contributions raise Exception 'Usercontributionsmodelforuser%salreadyexists ' % user_id else user_contributions UserContributions user_id created_exploration_ids edited_exploration_ids _save_user_contributions user_contributions return user_contributions
def main epilog 'Formorehelpusingthesafetemplatelinter includingdetailsonhow\n'epilog + 'tounderstandandfixanyviolations readthedocshere \n'epilog + '\n'epilog + 'http //edx readthedocs org/projects/edx-developer-guide/en/latest/conventions/safe_templates html#safe-template-linter\n'parser argparse ArgumentParser formatter_class argparse RawDescriptionHelpFormatter description 'Checksthattemplatesaresafe ' epilog epilog parser add_argument '--list-files' dest 'list_files' action 'store_true' help 'Onlydisplaythefilenamesthatcontainviolations ' parser add_argument '--rule-totals' dest 'rule_totals' action 'store_true' help 'Displaythetotalsforeachrule ' parser add_argument '--verbose' dest 'verbose' action 'store_true' help 'Printmultiplelineswherepossibleforadditionalcontextofviolations ' parser add_argument 'path' nargs '?' default None help 'Afiletolintordirectorytorecursivelylint ' args parser parse_args options {'list_files' args list_files 'rule_totals' args rule_totals 'verbose' args verbose}template_linters [MakoTemplateLinter UnderscoreTemplateLinter JavaScriptLinter PythonLinter ]summary_results SummaryResults _lint args path template_linters options summary_results out sys stdout
def precedence_traditional item from sympy import Integral Sum Product Limit Derivativefrom sympy core expr import UnevaluatedExprif isinstance item Integral Sum Product Limit Derivative return PRECEDENCE['Mul']elif isinstance item UnevaluatedExpr return precedence_traditional item args[0] else return precedence item
def gf_normal f p K return gf_trunc list map K f p
def processlist **connection_args ret []dbc _connect **connection_args if dbc is None return []cur dbc cursor _execute cur 'SHOWFULLPROCESSLIST' hdr [c[0] for c in cur description]for _ in range cur rowcount row cur fetchone idx_r {}for idx_j in range len hdr idx_r[hdr[idx_j]] row[idx_j]ret append idx_r cur close return ret
def processlist **connection_args ret []dbc _connect **connection_args if dbc is None return []cur dbc cursor _execute cur 'SHOWFULLPROCESSLIST' hdr [c[0] for c in cur description]for _ in range cur rowcount row cur fetchone idx_r {}for idx_j in range len hdr idx_r[hdr[idx_j]] row[idx_j]ret append idx_r cur close return ret
def SSLeay_version type return _ffi string _lib SSLeay_version type
def max_pool_2d_same_size input patch_size output Pool True input patch_size outs MaxPoolGrad True input output output patch_size return outs
def getGeometryOutputByNestedRing derivation nestedRing portionDirections loopLists getLoopListsByPath derivation None nestedRing vector3Loop portionDirections outsideOutput triangle_mesh getPillarsOutput loopLists if len nestedRing innerNestedRings < 1 return outsideOutputshapes [outsideOutput]for nestedRing innerNestedRing in nestedRing innerNestedRings loopLists getLoopListsByPath derivation 1 000001 nestedRing innerNestedRing vector3Loop portionDirections shapes append triangle_mesh getPillarsOutput loopLists return {'difference' {'shapes' shapes}}
def reset_terminal if not mswin subprocess call ['tset' '-c']
def run_process cmd out_log None err_log None return run_multi_processes [cmd] out_log out_log err_log err_log
def _listdir root res []root os path expanduser root try for name in os listdir root path os path join root name if os path isdir path name + os sepres append name except passreturn res
def _listdir root res []root os path expanduser root try for name in os listdir root path os path join root name if os path isdir path name + os sepres append name except passreturn res
def require_initialized exception def decorator method @wraps method def wrapped_method self *args **kwargs if not self initialized raise exceptionreturn method self *args **kwargs return wrapped_methodreturn decorator
def atoms cls instance_or_dict field_getter serializable_getter instance_or_dict gettry field_getter instance_or_dict _data getexcept AttributeError passsequences cls _field_list field_getter cls _serializables items serializable_getter for sequence get in sequences for field_name field in sequence yield field_name field get field_name Undefined
def atoms cls instance_or_dict field_getter serializable_getter instance_or_dict gettry field_getter instance_or_dict _data getexcept AttributeError passsequences cls _field_list field_getter cls _serializables items serializable_getter for sequence get in sequences for field_name field in sequence yield field_name field get field_name Undefined
def test_doc_string_contains_models for kind in 'mean' 'apparent' for model in SIDEREAL_TIME_MODELS[kind] assert model in Time sidereal_time __doc__
def numsplit text result []for group in re split ' \\d+ ' text if group try group int group except ValueError passresult append group return result
def record_chosen_plugins config plugins auth inst cn config namespacecn authenticator plugins find_init auth name if auth else 'None' cn installer plugins find_init inst name if inst else 'None'
def record_chosen_plugins config plugins auth inst cn config namespacecn authenticator plugins find_init auth name if auth else 'None' cn installer plugins find_init inst name if inst else 'None'
def record_chosen_plugins config plugins auth inst cn config namespacecn authenticator plugins find_init auth name if auth else 'None' cn installer plugins find_init inst name if inst else 'None'
def arguments_format_string args types if len args 0 return ''parts [printf_format_for_type x['type'] types for x in args]return ' ' join parts
def arguments_format_string args types if len args 0 return ''parts [printf_format_for_type x['type'] types for x in args]return ' ' join parts
def make_inmemorystatepersister test_case state_persister InMemoryStatePersister return state_persister state_persister get_state
def PrePlot num None rows None cols None if num _Brewer InitializeIter num if rows is None and cols is None returnif rows is not None and cols is None cols 1if cols is not None and rows is None rows 1size_map { 1 1 8 6 1 2 14 6 1 3 14 6 2 2 10 10 2 3 16 10 3 1 8 10 }if rows cols in size_map fig pyplot gcf fig set_size_inches *size_map[ rows cols ] if rows > 1 or cols > 1 pyplot subplot rows cols 1 global SUBPLOT_ROWS SUBPLOT_COLSSUBPLOT_ROWS rowsSUBPLOT_COLS cols
def get_order_source_modifier_modules return load_module_instances u'SHUUP_ORDER_SOURCE_MODIFIER_MODULES' u'order_source_modifier_module'
def tupleize func def wrapper *args **kargs return func *args **kargs return wrapper
def get_bom fn compression None boms set codecs BOM_UTF16 codecs BOM_UTF16_BE codecs BOM_UTF16_LE with open fn mode 'rb' compression compression as f f seek 0 bom f read 2 f seek 0 if bom in boms return bomelse return ''
def map_view request mapid snapshot None template 'maps/map_view html' map_obj _resolve_map request mapid 'base view_resourcebase' _PERMISSION_MSG_VIEW if 'access_token' in request session access_token request session['access_token']else access_token Noneif snapshot is None config map_obj viewer_json request user access_token else config snapshot_config snapshot map_obj request user access_token return render_to_response template RequestContext request {'config' json dumps config 'map' map_obj 'preview' getattr settings 'LAYER_PREVIEW_LIBRARY' '' }
def read_file_unix_endings file_name encoding 'utf8' ignore True if _ST3 errors 'ignore' if ignore else 'strict' with open file_name 'rt' encoding encoding errors errors as f file_content f read else file_content _read_file_content file_name encoding ignore file_content file_content replace '\r\n' '\n' return file_content
def read_file_unix_endings file_name encoding 'utf8' ignore True if _ST3 errors 'ignore' if ignore else 'strict' with open file_name 'rt' encoding encoding errors errors as f file_content f read else file_content _read_file_content file_name encoding ignore file_content file_content replace '\r\n' '\n' return file_content
def get_ctx_rev app tool_shed_url name owner changeset_revision tool_shed_url common_util get_tool_shed_url_from_tool_shed_registry app tool_shed_url params dict name name owner owner changeset_revision changeset_revision pathspec ['repository' 'get_ctx_rev']ctx_rev util url_get tool_shed_url password_mgr app tool_shed_registry url_auth tool_shed_url pathspec pathspec params params return ctx_rev
def get_ctx_rev app tool_shed_url name owner changeset_revision tool_shed_url common_util get_tool_shed_url_from_tool_shed_registry app tool_shed_url params dict name name owner owner changeset_revision changeset_revision pathspec ['repository' 'get_ctx_rev']ctx_rev util url_get tool_shed_url password_mgr app tool_shed_registry url_auth tool_shed_url pathspec pathspec params params return ctx_rev
def is_sevenfile path return SEVEN_COMMAND and os path splitext path [1] lower ' 7z'
def get_install_server_profiles install_server Noneinstall_server_info get_install_server_info install_server_type install_server_info get 'type' None install_server_url install_server_info get 'xmlrpc_url' None if install_server_type 'cobbler' and install_server_url install_server xmlrpclib ServerProxy install_server_url if install_server is None return Nonereturn install_server get_item_names 'profile'
def getSimplifiedInsetFromClockwiseLoop loop radius inset []for pointIndex begin in enumerate loop center loop[ pointIndex + 1 % len loop ]end loop[ pointIndex + 2 % len loop ]addInsetPointFromClockwiseTriple begin center end inset radius return getWithoutIntersections euclidean getSimplifiedLoop inset radius
def merged_cached_query fn def merge_wrapper *args **kwargs queries fn *args **kwargs return MergedCachedQuery queries return merge_wrapper
def merged_cached_query fn def merge_wrapper *args **kwargs queries fn *args **kwargs return MergedCachedQuery queries return merge_wrapper
def create_process progname sensor return subprocess Popen CMDLINE % {'progname' progname 'sensor' SENSORS get sensor sensor } shell True stdin subprocess PIPE
def create_process progname sensor return subprocess Popen CMDLINE % {'progname' progname 'sensor' SENSORS get sensor sensor } shell True stdin subprocess PIPE
def test_against_pyephem obstime Time u'2011-09-1808 50 00' location EarthLocation lon Angle u'-109d24m53 1s' lat Angle u'33d41m46 0s' height 30000 0 * u m altaz_frame AltAz obstime obstime location location temperature 15 * u deg_C pressure 1 01 * u bar altaz SkyCoord u'6 8927d-60 7665d' frame altaz_frame radec_actual altaz transform_to u'icrs' radec_expected SkyCoord u'196 497518d-4 569323d' frame u'icrs' distance radec_actual separation radec_expected to u'arcsec' assert distance < 1000 0 * u arcsec radec_expected SkyCoord u'196 495372d-4 560694d' frame u'icrs' distance radec_actual separation radec_expected to u'arcsec' assert distance < 1 * u arcsec
def _TestUnlinkIdentity tester user_cookie request_dict validator tester validator user_id device_id tester GetIdsFromCookie user_cookie request_dict deepcopy request_dict actual_dict tester SendRequest 'unlink_identity' user_cookie request_dict op_dict tester _DeriveNotificationOpDict user_id device_id request_dict validator ValidateUnlinkIdentity op_dict request_dict['identity'] tester _CompareResponseDicts 'unlink_identity' user_id request_dict {} actual_dict return actual_dict
@contextmanagerdef gc_state state orig_state gc isenabled set_gc_state state yield set_gc_state orig_state
@contextmanagerdef gc_state state orig_state gc isenabled set_gc_state state yield set_gc_state orig_state
def get_ctx_file_path_from_manifest filename repo changeset_revision stripped_filename basic_util strip_path filename for changeset in reversed_upper_bounded_changelog repo changeset_revision manifest_ctx repo changectx changeset for ctx_file in manifest_ctx files ctx_file_name basic_util strip_path ctx_file if ctx_file_name stripped_filename return manifest_ctx ctx_file return None None
def test_ast_expression_basics code can_compile u' foobar ' body[0]tree ast Expr value ast Call func ast Name id u'foo' ctx ast Load args [ast Name id u'bar' ctx ast Load ] keywords [] starargs None kwargs None _ast_spotcheck u'value func id' code tree
def flatten l types list if not isinstance l types return lreturn list flattened_iterator l types
def convert_id32 id_64 out ['STEAM_0 ']final id_64 - ID_BASE if final % 2 0 out append '0 ' else out append '1 ' out append str final // 2 return '' join out
def assertLogMessage testCase expectedMessages callable *args **kwargs loggedMessages []log addObserver loggedMessages append testCase addCleanup log removeObserver loggedMessages append callable *args **kwargs testCase assertEqual [m['message'][0] for m in loggedMessages] expectedMessages
def clear_limit key limits get_limits to_clear [key] if isinstance key basestring else key for key in to_clear if key in limits del limits[key]update_site_config u'limits' limits validate False frappe conf limits limits
def cbrt arg return Pow arg Rational 1 3
def get path value '' load_path None load_path _check_load_paths load_path aug _Augeas loadpath load_path ret {}path path rstrip '/' if value path + '/{0}' format value strip '/' try _match aug match path except RuntimeError as err return {'error' str err }if _match ret[path] aug get path else ret[path] ''return ret
def repeat_func func *args **kwargs if kwargs return starmap lambda args kwargs func *args **kwargs repeat args kwargs else return starmap func repeat args
def group_follower_list context data_dict _check_access 'group_follower_list' context data_dict return _follower_list context data_dict ckan logic schema default_follow_group_schema context['model'] UserFollowingGroup
def _set_attributes_clean test data test_type {'client' 1 'server' 2}test_time {'short' 1 'medium' 2 'long' 3}test test_type test_type[data test_type lower ]test test_time test_time[data time lower ]string_attributes 'name' 'author' 'test_class' 'test_category' 'test_category' 'sync_count' for attribute in string_attributes setattr test attribute getattr data attribute test description data doctest dependencies ' ' join data dependencies int_attributes 'experimental' 'run_verify' for attribute in int_attributes setattr test attribute int getattr data attribute
def _set_attributes_clean test data test_type {'client' 1 'server' 2}test_time {'short' 1 'medium' 2 'long' 3}test test_type test_type[data test_type lower ]test test_time test_time[data time lower ]string_attributes 'name' 'author' 'test_class' 'test_category' 'test_category' 'sync_count' for attribute in string_attributes setattr test attribute getattr data attribute test description data doctest dependencies ' ' join data dependencies int_attributes 'experimental' 'run_verify' for attribute in int_attributes setattr test attribute int getattr data attribute
def push_note device None title None body None spb _SaltPushbullet device res spb push_note title body return res
def org_site_has_inv row tablename 'org_facility' if not current deployment_settings has_module 'inv' return Falseif hasattr row tablename row row[tablename]try id row idexcept AttributeError return Nones3db current s3dbitable s3db inv_inv_itemstable s3db[tablename]query itable deleted True & stable id id & itable site_id stable site_id & itable quantity > 0 inv current db query select itable id limitby 0 1 first if inv return Trueelse return False
@memoizeddef get_project_tree options pants_ignore options pants_ignore or [] if options build_file_rev return ScmProjectTree get_buildroot get_scm options build_file_rev pants_ignore else return FileSystemProjectTree get_buildroot pants_ignore
@memoizeddef get_project_tree options pants_ignore options pants_ignore or [] if options build_file_rev return ScmProjectTree get_buildroot get_scm options build_file_rev pants_ignore else return FileSystemProjectTree get_buildroot pants_ignore
def _schema_builder urn_nid method attributes soap ET Element 'soap Body' {'xmlns m' 'https //durabledns com/services/dns/%s' % method } urn ET SubElement soap 'urn %s %s' % urn_nid method for attribute in attributes ET SubElement urn 'urn %s %s' % urn_nid attribute return soap
def _schema_builder urn_nid method attributes soap ET Element 'soap Body' {'xmlns m' 'https //durabledns com/services/dns/%s' % method } urn ET SubElement soap 'urn %s %s' % urn_nid method for attribute in attributes ET SubElement urn 'urn %s %s' % urn_nid attribute return soap
def connect_to_cloud_dns region None return _create_client ep_name 'dns' region region
def generate_token length 30 chars UNICODE_ASCII_CHARACTER_SET rand random SystemRandom return u'' join rand choice chars for x in range length
def generate_token length 30 chars UNICODE_ASCII_CHARACTER_SET rand random SystemRandom return u'' join rand choice chars for x in range length
def generate_token length 30 chars UNICODE_ASCII_CHARACTER_SET rand random SystemRandom return u'' join rand choice chars for x in range length
def ordered_permutation_regex sentence sentence _RE_REF sub '\\1' sentence sentence _RE_REF_LANG sub '\\1' sentence sentence _RE_SELF_REF sub '' sentence words sentence split combinations itertools product True False repeat len words solution []for combination in combinations comb []for iword word in enumerate words if combination[iword] comb append word elif comb breakif comb solution append _PREFIX + '[0-9]*%s*%s ? \\W $ +' % _NUM_SEP re_escape '' join comb rstrip '\\' regex ' ' join sorted set solution key lambda o len o reverse True return regex
def get_collapse_fns return {'median' _collapse_to_median 'first' _collapse_to_first 'random' _collapse_to_random 'sum' _collapse_to_sum 'mean' _collapse_to_mean}
def test_importorskip_module_level testdir testdir makepyfile '\nimportpytest\nfoobarbaz pytest importorskip "foobarbaz" \n\ndeftest_foo \npass\n' result testdir runpytest result stdout fnmatch_lines ['*collected0items/1skipped*']
def test_importorskip_module_level testdir testdir makepyfile '\nimportpytest\nfoobarbaz pytest importorskip "foobarbaz" \n\ndeftest_foo \npass\n' result testdir runpytest result stdout fnmatch_lines ['*collected0items/1skipped*']
def test_importorskip_module_level testdir testdir makepyfile '\nimportpytest\nfoobarbaz pytest importorskip "foobarbaz" \n\ndeftest_foo \npass\n' result testdir runpytest result stdout fnmatch_lines ['*collected0items/1skipped*']
def latex_highlight classified_text title 'python' commands default_latex_commands document default_latex_document macros '\n' join '\\newcommand{\\py%s}[1]{%s}' % c for c in commands items result []for kind text in classified_text if kind result append '\\py%s{' % kind result append alltt_escape text if kind result append '}' return default_latex_document % dict title title macros macros body '' join result
def test_predict1 sp SequencePattern ts2s TFLearnSeq2Seq sp verbose 1 wfn 'test_%s' % ts2s canonical_weights_fn 0 print 'usingweightsfilename%s' % wfn tf reset_default_graph prediction y ts2s predict Xin range 10 weights_input_fn wfn assert len prediction 10
def runner name **kwargs jid kwargs pop '__orchestration_jid__' None saltenv kwargs pop '__env__' 'base' full_return kwargs pop 'full_return' False kwargs salt utils clean_kwargs **kwargs if 'master_job_cache' not in __opts__ master_config os path join os path dirname __opts__['conf_file'] 'master' master_opts salt config master_config master_config rclient salt runner RunnerClient master_opts else rclient salt runner RunnerClient __opts__ if name in rclient functions aspec salt utils args get_function_argspec rclient functions[name] if 'saltenv' in aspec args kwargs['saltenv'] saltenvif jid salt utils event fire_args __opts__ jid {'type' 'runner' 'name' name 'args' kwargs} prefix 'run' return rclient cmd name kwarg kwargs print_event False full_return full_return
def get_elements_count N N1 W2 thresh 0 9max_count 0total_counts 0 0for i in xrange N w W2[ i]wa np abs w total wa sum s np asarray sorted wa count 1while s[ - count ] sum < thresh * total count + 1if count > max_count max_count counttotal_counts + countave total_counts / float N print 'averageneededfilters' ave count max_countprint 'Ittakes' count 'of' N1 'elementstoaccountfor' thresh * 100 0 '\\%oftheweightinatleastonefilter' lim 10if count > lim count limprint 'Onlydisplaying' count 'elementsthough ' if count > N1 count N1return count
def _find_get_notepad_pages if config WEB_GET_NOTEPAD_PAGES is None return Noneif type config WEB_GET_NOTEPAD_PAGES is not tuple config WEB_GET_NOTEPAD_PAGES config WEB_GET_NOTEPAD_PAGES return functools partial GET_NOTEPAD_PAGES[config WEB_GET_NOTEPAD_PAGES[0]] *config WEB_GET_NOTEPAD_PAGES[1]
def ref_sort_key ref return len ref ref
def ref_sort_key ref return len ref ref
@step u'Iinputananswerona" [^"]* "problem" [^"]* ly"' def input_problem_answer _ problem_type correctness assert correctness in ['correct' 'incorrect'] assert problem_type in PROBLEM_DICT answer_problem world scenario_dict['COURSE'] number problem_type correctness
def hex_to_sha hex assert len hex 40 'Incorrectlengthofhexsha %s' % hex try return binascii unhexlify hex except TypeError as exc if not isinstance hex bytes raiseraise ValueError exc args[0]
def _define_atomic_inc_dec module op ordering ftype ir FunctionType _word_type [_word_type as_pointer ] fn_atomic ir Function module ftype name 'nrt_atomic_{0}' format op [ptr] fn_atomic argsbb fn_atomic append_basic_block builder ir IRBuilder bb ONE ir Constant _word_type 1 if not _disable_atomicity oldval builder atomic_rmw op ptr ONE ordering ordering res getattr builder op oldval ONE builder ret res else oldval builder load ptr newval getattr builder op oldval ONE builder store newval ptr builder ret oldval return fn_atomic
def list_terms type kwargs {'type' type}result util callm '%s/%s' % 'artist' 'list_terms' kwargs return result['response']['terms']
def test_settings_clean_revert env modified 'outer'env notmodified 'outer'with settings modified 'inner' notmodified 'inner' inner_only 'only' clean_revert True eq_ env modified 'inner' eq_ env notmodified 'inner' eq_ env inner_only 'only' env modified 'modifiedinternally'eq_ env modified 'modifiedinternally' ok_ 'inner_only' not in env
def _type_reconstructor reconstructor reconstructor_args state obj reconstructor *reconstructor_args if state obj __dict__ update state return type obj _intern obj
def is_missing data missing_value if is_float data and isnan missing_value return isnan data elif is_datetime data and isnat missing_value return isnat data return data missing_value
def is_missing data missing_value if is_float data and isnan missing_value return isnan data elif is_datetime data and isnat missing_value return isnat data return data missing_value
def success_installation domains z_util interfaces IDisplay notification 'Congratulations Youhavesuccessfullyenabled{0}{1}{1}Youshouldtestyourconfigurationat {1}{2}' format _gen_https_names domains os linesep os linesep join _gen_ssl_lab_urls domains pause False
def is_oozie_enabled return len [app for app in appmanager DESKTOP_MODULES if app name 'oozie' ] > 0
def get_metaschemas *args **kwargs count request args get 'count' 100 include request args get 'include' 'latest' meta_schemas []if include 'latest' schema_names list MetaSchema objects all values_list 'name' flat True distinct for name in schema_names meta_schema_set MetaSchema find Q 'name' 'eq' name & Q 'schema_version' 'eq' 2 meta_schemas meta_schemas + [s for s in meta_schema_set] else meta_schemas MetaSchema find meta_schemas [schema for schema in meta_schemas if schema name in ACTIVE_META_SCHEMAS ]meta_schemas sort key lambda a ACTIVE_META_SCHEMAS index a name return {'meta_schemas' [serialize_meta_schema ms for ms in meta_schemas[ count]]} http OK
def test_pushd_appends_current_dir_to_stack_if_empty mox Mox old_os io osio os mox CreateMockAnything class MyFs io FileSystem stack []@classmethoddef current_dir cls return 'shouldbecurrentdir'io os chdir 'somewhere' mox ReplayAll try assert len MyFs stack is 0 MyFs pushd 'somewhere' assert len MyFs stack is 2 assert_equals MyFs stack ['shouldbecurrentdir' 'somewhere'] mox VerifyAll finally io os old_os
def test_pushd_appends_current_dir_to_stack_if_empty mox Mox old_os io osio os mox CreateMockAnything class MyFs io FileSystem stack []@classmethoddef current_dir cls return 'shouldbecurrentdir'io os chdir 'somewhere' mox ReplayAll try assert len MyFs stack is 0 MyFs pushd 'somewhere' assert len MyFs stack is 2 assert_equals MyFs stack ['shouldbecurrentdir' 'somewhere'] mox VerifyAll finally io os old_os
def tauchen rho sigma_u m 3 n 7 std_y np sqrt sigma_u ** 2 / 1 - rho ** 2 x_max m * std_y x_min - x_max x np linspace x_min x_max n step x_max - x_min / n - 1 half_step 0 5 * step P np empty n n _fill_tauchen x P n rho sigma_u half_step mc MarkovChain P state_values x return mc
def getmro cls if hasattr cls '__mro__' return cls __mro__else result []_searchbases cls result return tuple result
def getmro cls if hasattr cls '__mro__' return cls __mro__else result []_searchbases cls result return tuple result
def to_timestamp val if isinstance val numbers Number return valelif isinstance val six string_types dt _parse_datetime_string val else dt valreturn time mktime dt timetuple
def test_doc_api_merge_children en_tokenizer text u'WKROplayedsongsbythebeachboysallnight'doc en_tokenizer text assert len doc 9 doc merge doc[4] idx doc[6] idx + len doc[6] u'NAMED' u'LEMMA' u'TYPE' for word in doc if word i < word head i assert word in list word head lefts elif word i > word head i assert word in list word head rights
def update_api_key_description apiKey description region None key None keyid None profile None try conn _get_conn region region key key keyid keyid profile profile response _api_key_patch_replace conn apiKey '/description' description return {'updated' True 'apiKey' _convert_datetime_str response }except ClientError as e return {'updated' False 'error' salt utils boto3 get_error e }
def prefix bp func class Operator TokenBase lbp bpdef nud self parser self first parser expression bp self second Nonereturn selfdef eval self context try return func context self first except Exception return Falsereturn Operator
def validate_pack_name name if not name raise ValueError 'Contentpacknamecannotbeempty' if name lower in USER_PACK_NAME_BLACKLIST raise ValueError 'Name"%s"isblacklistedandcan\'tbeused' % name lower return name
def get_tag_commit try return check_output ['git' 'describe' '--tags'] stderr STDOUT cwd os path dirname os path abspath __file__ except CalledProcessError as e logger error 'Errorcallinggit "{}"\noutput "{}"' format e e output return Noneexcept OSError as e logger error 'Couldnotcallgit isitinstalled?errormsg "{}"' format e return None
def main args get_args si SmartConnect host args host user args user pwd args password port args port atexit register Disconnect si content si RetrieveContent template Nonetemplate get_obj content [vim VirtualMachine] args template if template clone_vm content template args vm_name si args datacenter_name args vm_folder args datastore_name args cluster_name args resource_pool args power_on else print 'templatenotfound'
def get_easy_s3 config_file None cache False cfg get_config config_file cache return cfg get_easy_s3
def json_decode rsp try o json loads rsp object_hook _decode_dict except ValueError raise CoprHdError CoprHdError VALUE_ERR _ 'FailedtorecognizeJSONpayload \n[%s]' % rsp return o
def get_site_status_msg course_key try if not GlobalStatusMessage current enabled return Nonereturn GlobalStatusMessage current full_message course_key except log exception 'Errorwhilegettingastatusmessage ' return None
def random_tree n seed None if n 0 raise nx NetworkXPointlessConcept 'thenullgraphisnotatree' if n 1 return nx empty_graph 1 random seed seed sequence sample_with_replacement range n n - 2 return nx from_prufer_sequence sequence
def _make_stc raw src seed 42sfreq raw info['sfreq']tstep 1 0 / sfreq n_samples len raw times // 10 times np arange 0 n_samples * tstep stc simulate_sparse_stc src 10 times random_state seed return stc
@then 'thecommandoutputshouldnotcontainlogrecordsfromcategories' def step_command_output_should_not_contain_log_records_from_categories context assert context table 'REQUIRE context table'context table require_column 'category' record_schema context log_record_row_schemaLogRecordTable annotate_with_row_schema context table record_schema step_command_output_should_not_contain_log_records context context table remove_columns ['level' 'message']
@then 'thecommandoutputshouldnotcontainlogrecordsfromcategories' def step_command_output_should_not_contain_log_records_from_categories context assert context table 'REQUIRE context table'context table require_column 'category' record_schema context log_record_row_schemaLogRecordTable annotate_with_row_schema context table record_schema step_command_output_should_not_contain_log_records context context table remove_columns ['level' 'message']
@then 'thecommandoutputshouldnotcontainlogrecordsfromcategories' def step_command_output_should_not_contain_log_records_from_categories context assert context table 'REQUIRE context table'context table require_column 'category' record_schema context log_record_row_schemaLogRecordTable annotate_with_row_schema context table record_schema step_command_output_should_not_contain_log_records context context table remove_columns ['level' 'message']
def binary_crossentropy predictions targets predictions targets align_targets predictions targets return theano tensor nnet binary_crossentropy predictions targets
def post_account url token headers http_conn None response_dict None service_token None query_string None data None if http_conn parsed conn http_connelse parsed conn http_connection url method 'POST'path parsed pathif query_string path + '?' + query_string headers['X-Auth-Token'] tokenif service_token headers['X-Service-Token'] service_tokenconn request method path data headers resp conn getresponse body resp read http_log url method {'headers' headers} resp body store_response resp response_dict if resp status < 200 or resp status > 300 raise ClientException from_response resp 'AccountPOSTfailed' body resp_headers {}for header value in resp getheaders resp_headers[header lower ] valuereturn resp_headers body
def test_config_alterations_kwargs class LineConfig Config no_prefix Trueshow_legend Falsefill Truepretty_print Truex_labels ['a' 'b' 'c']config LineConfig line1 Line config line1 add '_' [1 2 3] l1 line1 render line1 stroke Falsel1bis line1 render assert l1 l1bis line2 Line config line2 add '_' [1 2 3] l2 line2 render assert l1 l2 assert l1bis l2 line3 Line config title 'Title' line3 add '_' [1 2 3] l3 line3 render assert l3 l2 l2bis line2 render assert l2 l2bis
def test_config_alterations_kwargs class LineConfig Config no_prefix Trueshow_legend Falsefill Truepretty_print Truex_labels ['a' 'b' 'c']config LineConfig line1 Line config line1 add '_' [1 2 3] l1 line1 render line1 stroke Falsel1bis line1 render assert l1 l1bis line2 Line config line2 add '_' [1 2 3] l2 line2 render assert l1 l2 assert l1bis l2 line3 Line config title 'Title' line3 add '_' [1 2 3] l3 line3 render assert l3 l2 l2bis line2 render assert l2 l2bis
def _get_reparse_data path if sys getwindowsversion major < 6 raise SaltInvocationError 'SymlinksareonlysupportedonWindowsVistaorlater ' path os path normpath path if not _is_reparse_point path return NonefileHandle Nonetry fileHandle win32file CreateFileW path 2147483648 1 None 3 2097152 33554432 reparseData win32file DeviceIoControl fileHandle 589992 None 16384 finally if fileHandle win32file CloseHandle fileHandle return reparseData
def json_handler obj if isinstance obj datetime date datetime timedelta datetime datetime return unicode obj elif isinstance obj LocalProxy return unicode obj elif isinstance obj frappe model document BaseDocument doc obj as_dict no_nulls True return docelif type obj type or isinstance obj Exception return repr obj else raise TypeError u'Objectoftype%swithvalueof%sisnotJSONserializable' % type obj repr obj
def makeService options svc service MultiService namespace options['namespace']if namespace is None namespace {}checker checkers FilePasswordDB options['passwd'] if options['telnetPort'] telnetRealm _StupidRealm telnet TelnetBootstrapProtocol insults ServerProtocol manhole ColoredManhole namespace telnetPortal portal Portal telnetRealm [checker] telnetFactory protocol ServerFactory telnetFactory protocol makeTelnetProtocol telnetPortal telnetService strports service options['telnetPort'] telnetFactory telnetService setServiceParent svc if options['sshPort'] sshRealm manhole_ssh TerminalRealm sshRealm chainedProtocolFactory chainedProtocolFactory namespace sshPortal portal Portal sshRealm [checker] sshFactory manhole_ssh ConchFactory sshPortal sshService strports service options['sshPort'] sshFactory sshService setServiceParent svc return svc
def _triangle2unit tb lower 0 if lower d tb[0] copy else d tb[ -1 ] copy if lower return d tb / d else l _upper2lower tb return d _lower2upper l / d
def _triangle2unit tb lower 0 if lower d tb[0] copy else d tb[ -1 ] copy if lower return d tb / d else l _upper2lower tb return d _lower2upper l / d
def volume_destroy context volume_id return IMPL volume_destroy context volume_id
def packets_for_stream fobj offset pcap dpkt pcap Reader fobj pcapiter iter pcap ts raw pcapiter next fobj seek offset for p in next_connection_packets pcapiter linktype pcap datalink yield p
def generate_model root_class ROOT_CLASS classes_result {}result {u'root' fqpn root_class u'classes' classes_result}classes {root_class}while classes klass classes pop klass_name fqpn klass if klass_name in classes_result continueif issubclass klass PRecord PClass to_model _precord_modelelif issubclass klass CheckedPMap to_model _pmap_modelelif issubclass klass CheckedPSet CheckedPVector to_model _psequence_modelelse to_model _default_model record further_classes to_model klass classes_result[klass_name] recordclasses further_classesreturn result
def generate_model root_class ROOT_CLASS classes_result {}result {u'root' fqpn root_class u'classes' classes_result}classes {root_class}while classes klass classes pop klass_name fqpn klass if klass_name in classes_result continueif issubclass klass PRecord PClass to_model _precord_modelelif issubclass klass CheckedPMap to_model _pmap_modelelif issubclass klass CheckedPSet CheckedPVector to_model _psequence_modelelse to_model _default_model record further_classes to_model klass classes_result[klass_name] recordclasses further_classesreturn result
def generate_model root_class ROOT_CLASS classes_result {}result {u'root' fqpn root_class u'classes' classes_result}classes {root_class}while classes klass classes pop klass_name fqpn klass if klass_name in classes_result continueif issubclass klass PRecord PClass to_model _precord_modelelif issubclass klass CheckedPMap to_model _pmap_modelelif issubclass klass CheckedPSet CheckedPVector to_model _psequence_modelelse to_model _default_model record further_classes to_model klass classes_result[klass_name] recordclasses further_classesreturn result
def ExpandWindowsUserEnvironmentVariables data_string knowledge_base sid None username None win_environ_regex re compile '% [^%]+? %' components []offset 0for match in win_environ_regex finditer data_string components append data_string[offset match start ] kb_user knowledge_base GetUser sid sid username username kb_value Noneif kb_user kb_value getattr kb_user match group 1 lower None if isinstance kb_value basestring and kb_value components append kb_value else components append '%%%s%%' % match group 1 offset match end components append data_string[offset ] return '' join components
def insert_meta_param_description *args **kwargs if not args return lambda f insert_meta_param_description f **kwargs f args[0]if f __doc__ indent '' * kwargs get 'pad' 8 body textwrap wrap _META_DESCRIPTION initial_indent indent subsequent_indent indent width 78 descr '{0}\n{1}' format _META_TYPES '\n' join body f __doc__ f __doc__ replace '$META' descr return f
def debounce seconds 1 def decorator func func timer None@wraps func def wrapper *args **kwargs def call func *args **kwargs func timer Noneif func timer func timer cancel func timer Timer seconds call func timer start return wrapperreturn decorator
@bdd given 'Ihaveafreshinstance' def fresh_instance quteproc quteproc terminate quteproc start
def getDjangoObjects context c context extrak 'django_objects'try return c[k]except KeyError c[k] DjangoReferenceCollection return c[k]
def getDjangoObjects context c context extrak 'django_objects'try return c[k]except KeyError c[k] DjangoReferenceCollection return c[k]
def HeatMap data x None y None values None stat 'count' xgrid False ygrid False hover_tool True hover_text None **kw kw['x'] xkw['y'] ykw['values'] valueskw['stat'] statchart create_and_build HeatMapBuilder data xgrid xgrid ygrid ygrid **kw if hover_tool tooltip build_agg_tooltip hover_text hover_text aggregated_col values agg_text stat chart add_tooltips [tooltip] return chart
def HeatMap data x None y None values None stat 'count' xgrid False ygrid False hover_tool True hover_text None **kw kw['x'] xkw['y'] ykw['values'] valueskw['stat'] statchart create_and_build HeatMapBuilder data xgrid xgrid ygrid ygrid **kw if hover_tool tooltip build_agg_tooltip hover_text hover_text aggregated_col values agg_text stat chart add_tooltips [tooltip] return chart
def HeatMap data x None y None values None stat 'count' xgrid False ygrid False hover_tool True hover_text None **kw kw['x'] xkw['y'] ykw['values'] valueskw['stat'] statchart create_and_build HeatMapBuilder data xgrid xgrid ygrid ygrid **kw if hover_tool tooltip build_agg_tooltip hover_text hover_text aggregated_col values agg_text stat chart add_tooltips [tooltip] return chart
def getargtxt obj one_arg_per_line True args getargs obj if args sep ' 'textlist Nonefor i_arg arg in enumerate args if textlist is None textlist ['']textlist[ -1 ] + argif i_arg < len args - 1 textlist[ -1 ] + sepif len textlist[ -1 ] > 32 or one_arg_per_line textlist append '' if inspect isclass obj or inspect ismethod obj if len textlist 1 return Noneif 'self' + sep in textlist textlist remove 'self' + sep return textlist
def dir_tree_find tree kind nodes []if isinstance tree list for t in tree nodes + dir_tree_find t kind else if tree['block'] kind nodes append tree for child in tree['children'] nodes + dir_tree_find child kind return nodes
def cpuThrottle value delay 1e-05 * value ** 2 time sleep delay
def delivery_pipeline registry xml_parent data pvc XML SubElement xml_parent 'se diabol jenkins pipeline PipelineVersionContributor' pvc set 'plugin' 'delivery-pipeline-plugin' mapping [ 'version-template' 'versionTemplate' '' 'set-display-name' 'updateDisplayName' False ]convert_mapping_to_xml pvc data mapping fail_required True
def get_equivalent_release_groups release_group for equivalent_release_group in equivalent_release_groups if release_group in equivalent_release_group return equivalent_release_groupreturn {release_group}
def show_reference_template request template try context {'disable_courseware_js' True 'uses_pattern_library' True}context update request GET dict return render_to_response template context except TopLevelLookupException return HttpResponseNotFound "Couldn'tfindtemplate{template}" format template template
def restart_app id restart False force True ret {'restarted' None}if not restart ret['restarted'] Falsereturn rettry response salt utils http query '{0}/v2/apps/{1}/restart?force {2}' format _base_url _app_id id force method 'POST' decode_type 'json' decode True header_dict {'Content-Type' 'application/json' 'Accept' 'application/json'} log debug 'restartresponse %s' response ret['restarted'] Trueret update response['dict'] return retexcept Exception as ex log error 'unabletorestartmarathonapp %s' ex message return {'exception' {'message' ex message}}
def setup_platform hass config add_devices discovery_info None from pmsensor import serial_pm as pmtry coll pm PMDataCollector config get CONF_SERIAL_DEVICE pm SUPPORTED_SENSORS[config get CONF_BRAND ] except KeyError _LOGGER error 'Brand%snotsupported\nsupportedbrands %s' config get CONF_BRAND pm SUPPORTED_SENSORS keys returnexcept OSError as err _LOGGER error 'Couldnotopenserialconnectionto%s %s ' config get CONF_SERIAL_DEVICE err returndev []for pmname in coll supported_values if config get CONF_NAME is None name '{}PM{}' format config get CONF_NAME pmname else name 'PM{}' format pmname dev append ParticulateMatterSensor coll name pmname add_devices dev
def num_cpus if hasattr os 'sysconf' if 'SC_NPROCESSORS_ONLN' in os sysconf_names ncpus os sysconf 'SC_NPROCESSORS_ONLN' if isinstance ncpus int and ncpus > 0 return ncpuselse return int _read_stdout ['sysctl' '-n' 'hw ncpu'] or 1 if 'NUMBER_OF_PROCESSORS' in os environ ncpus int os environ['NUMBER_OF_PROCESSORS'] if ncpus > 0 return ncpusreturn 1
def _get_url view_name backend_name auth_entry None redirect_url None extra_params None url_params None url_params url_params or {} url_params['backend'] backend_nameurl reverse view_name kwargs url_params query_params OrderedDict if auth_entry query_params[AUTH_ENTRY_KEY] auth_entryif redirect_url query_params[AUTH_REDIRECT_KEY] redirect_urlif extra_params query_params update extra_params return u'{url}?{params}' format url url params urllib urlencode query_params
def stringify pkgs try for key in pkgs pkgs[key] ' ' join pkgs[key] except AttributeError as exc log exception exc
def stringify pkgs try for key in pkgs pkgs[key] ' ' join pkgs[key] except AttributeError as exc log exception exc
def generate_json_file results_dir relative_links True results_data parse_results_dir results_dir relative_links json_path os path join results_dir 'status json' json_file open json_path 'w' json dump results_data json_file json_file close return json_path
def bannerParser banner xmlfile Noneif Backend isDbms DBMS MSSQL xmlfile paths MSSQL_XMLelif Backend isDbms DBMS MYSQL xmlfile paths MYSQL_XMLelif Backend isDbms DBMS ORACLE xmlfile paths ORACLE_XMLelif Backend isDbms DBMS PGSQL xmlfile paths PGSQL_XMLif not xmlfile returnif Backend isDbms DBMS MSSQL handler MSSQLBannerHandler banner kb bannerFp parseXmlFile xmlfile handler handler FingerprintHandler banner kb bannerFp parseXmlFile paths GENERIC_XML handler else handler FingerprintHandler banner kb bannerFp parseXmlFile xmlfile handler parseXmlFile paths GENERIC_XML handler
def bannerParser banner xmlfile Noneif Backend isDbms DBMS MSSQL xmlfile paths MSSQL_XMLelif Backend isDbms DBMS MYSQL xmlfile paths MYSQL_XMLelif Backend isDbms DBMS ORACLE xmlfile paths ORACLE_XMLelif Backend isDbms DBMS PGSQL xmlfile paths PGSQL_XMLif not xmlfile returnif Backend isDbms DBMS MSSQL handler MSSQLBannerHandler banner kb bannerFp parseXmlFile xmlfile handler handler FingerprintHandler banner kb bannerFp parseXmlFile paths GENERIC_XML handler else handler FingerprintHandler banner kb bannerFp parseXmlFile xmlfile handler parseXmlFile paths GENERIC_XML handler
@skip 'silverlight' 'multiple_execute' 'win32' def test_c1cs if not has_csc returnc1cs get_local_filename 'c1 cs' outp sys exec_prefixcompileAndRef 'c1' c1cs '/d BAR1' import Fooclass c1Child Foo Bar passo c1Child AreEqual o Method 'Inbar1' compileAndRef 'c1_b' c1cs import Fooclass c2Child Foo Bar passo c2Child AreEqual o Method 'Inbar2'
@skip 'silverlight' 'multiple_execute' 'win32' def test_c1cs if not has_csc returnc1cs get_local_filename 'c1 cs' outp sys exec_prefixcompileAndRef 'c1' c1cs '/d BAR1' import Fooclass c1Child Foo Bar passo c1Child AreEqual o Method 'Inbar1' compileAndRef 'c1_b' c1cs import Fooclass c2Child Foo Bar passo c2Child AreEqual o Method 'Inbar2'
@skip 'silverlight' 'multiple_execute' 'win32' def test_c1cs if not has_csc returnc1cs get_local_filename 'c1 cs' outp sys exec_prefixcompileAndRef 'c1' c1cs '/d BAR1' import Fooclass c1Child Foo Bar passo c1Child AreEqual o Method 'Inbar1' compileAndRef 'c1_b' c1cs import Fooclass c2Child Foo Bar passo c2Child AreEqual o Method 'Inbar2'
def get_enrolled return User objects raw 'SELECT*FROMauth_userwhereidnotin SELECTuser_idfromstudent_usersignupsource '
def set_creation_order instance global _creation_orderinstance _creation_order _creation_order_creation_order + 1
def psutil_phymem_usage import psutiltry percent psutil virtual_memory percentexcept percent psutil phymem_usage percentreturn percent
def update_module_store_settings module_store_setting doc_store_settings None module_store_options None xml_store_options None default_store None mappings None for store in module_store_setting['default']['OPTIONS']['stores'] if store['NAME'] 'xml' xml_store_options and store['OPTIONS'] update xml_store_options else module_store_options and store['OPTIONS'] update module_store_options doc_store_settings and store['DOC_STORE_CONFIG'] update doc_store_settings if default_store mixed_stores get_mixed_stores module_store_setting for store in mixed_stores if store['NAME'] default_store mixed_stores remove store mixed_stores insert 0 store returnraise Exception 'Couldnotfindsettingforrequesteddefaultstore {}' format default_store if mappings and 'mappings' in module_store_setting['default']['OPTIONS'] module_store_setting['default']['OPTIONS']['mappings'] mappings
def update_module_store_settings module_store_setting doc_store_settings None module_store_options None xml_store_options None default_store None mappings None for store in module_store_setting['default']['OPTIONS']['stores'] if store['NAME'] 'xml' xml_store_options and store['OPTIONS'] update xml_store_options else module_store_options and store['OPTIONS'] update module_store_options doc_store_settings and store['DOC_STORE_CONFIG'] update doc_store_settings if default_store mixed_stores get_mixed_stores module_store_setting for store in mixed_stores if store['NAME'] default_store mixed_stores remove store mixed_stores insert 0 store returnraise Exception 'Couldnotfindsettingforrequesteddefaultstore {}' format default_store if mappings and 'mappings' in module_store_setting['default']['OPTIONS'] module_store_setting['default']['OPTIONS']['mappings'] mappings
def params_from_strings params param_values app ignore_errors False rval dict param_values param_values or {} for key value in param_values items value json_fix safe_loads value if key in params value params[key] value_from_basic value app ignore_errors rval[key] valuereturn rval
def clone_plugin plugin dirname os path join plugin_cache_dir os path basename plugin print 'Cloning%s->%s' % plugin dirname if os path exists dirname print 'Skipcloningof%s Alreadythere ' % plugin returncreate_directory dirname subprocess call ['git' 'clone' '--recursive' '--depth' '1' 'https //github com/%s' % plugin dirname] if plugin 'Valloric/YouCompleteMe' subprocess call os path join dirname ' /install sh' cwd dirname
@repo_mgr_cli command 'delete' @click option '--repository' required True help 'Repositoryname' type str @click option '--yes' is_flag True callback delete_callback expose_value False prompt 'Areyousureyouwanttodeletetherepository?' @click pass_contextdef _delete ctx repository client get_client **ctx obj['client_args'] try logger info 'Deletingrepository{0} ' format repository client snapshot delete_repository repository repository except elasticsearch NotFoundError logger error 'Unabletodeleterepository {0}NotFound ' format repository sys exit 1
def make_totp secret skew 0 timestamp None timestamp timestamp or time time counter timestamp // PERIOD return make_hotp secret counter - skew
def CallWithRetryAsync retry_policy func *args **kwargs inner_callback kwargs get 'callback' None assert 'callback' in kwargs 'CallWithRetryAsyncrequiresanamed"callback"argumentthatisnotNone 'retry_manager retry_policy CreateManager def _OnCompletedCall *callback_args **callback_kwargs 'Calledwhentheoperationhascompleted Determinewhethertoretry \nbasedontheargumentstothecallback \n'retry_func functools partial func *args **kwargs if not retry_manager MaybeRetryOnResult retry_func *callback_args **callback_kwargs exception_context check_retry Falseinner_callback *callback_args **callback_kwargs def _OnException type value tb 'Callediftheoperationraisesanexception Determinewhethertoretry\norre-raisetheexception basedontheexceptiondetails \n'if exception_context check_retry retry_func functools partial func *args **kwargs return retry_manager MaybeRetryOnException retry_func type value tb kwargs['callback'] _OnCompletedCallexception_context ExceptionStackContext _OnException exception_context check_retry Truewith exception_context func *args **kwargs
def CallWithRetryAsync retry_policy func *args **kwargs inner_callback kwargs get 'callback' None assert 'callback' in kwargs 'CallWithRetryAsyncrequiresanamed"callback"argumentthatisnotNone 'retry_manager retry_policy CreateManager def _OnCompletedCall *callback_args **callback_kwargs 'Calledwhentheoperationhascompleted Determinewhethertoretry \nbasedontheargumentstothecallback \n'retry_func functools partial func *args **kwargs if not retry_manager MaybeRetryOnResult retry_func *callback_args **callback_kwargs exception_context check_retry Falseinner_callback *callback_args **callback_kwargs def _OnException type value tb 'Callediftheoperationraisesanexception Determinewhethertoretry\norre-raisetheexception basedontheexceptiondetails \n'if exception_context check_retry retry_func functools partial func *args **kwargs return retry_manager MaybeRetryOnException retry_func type value tb kwargs['callback'] _OnCompletedCallexception_context ExceptionStackContext _OnException exception_context check_retry Truewith exception_context func *args **kwargs
def CallWithRetryAsync retry_policy func *args **kwargs inner_callback kwargs get 'callback' None assert 'callback' in kwargs 'CallWithRetryAsyncrequiresanamed"callback"argumentthatisnotNone 'retry_manager retry_policy CreateManager def _OnCompletedCall *callback_args **callback_kwargs 'Calledwhentheoperationhascompleted Determinewhethertoretry \nbasedontheargumentstothecallback \n'retry_func functools partial func *args **kwargs if not retry_manager MaybeRetryOnResult retry_func *callback_args **callback_kwargs exception_context check_retry Falseinner_callback *callback_args **callback_kwargs def _OnException type value tb 'Callediftheoperationraisesanexception Determinewhethertoretry\norre-raisetheexception basedontheexceptiondetails \n'if exception_context check_retry retry_func functools partial func *args **kwargs return retry_manager MaybeRetryOnException retry_func type value tb kwargs['callback'] _OnCompletedCallexception_context ExceptionStackContext _OnException exception_context check_retry Truewith exception_context func *args **kwargs
def get_all context session query session query models MetadefResourceType resource_types query all resource_types_list []for rt in resource_types resource_types_list append rt to_dict return resource_types_list
@cli command 'paste' @click option '-l' '--left' default 0 help 'Offsetfromleft ' @click option '-r' '--right' default 0 help 'Offsetfromright ' @processordef paste_cmd images left right imageiter iter images image next imageiter None to_paste next imageiter None if to_paste is None if image is not None yield image returnclick echo 'Paste"%s"on"%s"' % to_paste filename image filename mask Noneif to_paste mode 'RGBA' or 'transparency' in to_paste info mask to_pasteimage paste to_paste left right mask image filename + '+' + to_paste filename yield image for image in imageiter yield image
@cli command 'paste' @click option '-l' '--left' default 0 help 'Offsetfromleft ' @click option '-r' '--right' default 0 help 'Offsetfromright ' @processordef paste_cmd images left right imageiter iter images image next imageiter None to_paste next imageiter None if to_paste is None if image is not None yield image returnclick echo 'Paste"%s"on"%s"' % to_paste filename image filename mask Noneif to_paste mode 'RGBA' or 'transparency' in to_paste info mask to_pasteimage paste to_paste left right mask image filename + '+' + to_paste filename yield image for image in imageiter yield image
def format_history_for_queue slotinfo []history_items get_active_history for item in history_items slot {'nzo_id' item['nzo_id'] 'bookmark' '' 'filename' xml_name item['name'] 'loaded' False 'stages' item['stage_log'] 'status' item['status'] 'bytes' item['bytes'] 'size' item['size']}slotinfo append slot return slotinfo
def extract query choices processor default_processor scorer default_scorer limit 5 sl extractWithoutOrder query choices processor scorer return heapq nlargest limit sl key lambda i i[1] if limit is not None else sorted sl key lambda i i[1] reverse True
def encode_missing object_hash ts_data ts_meta None ts_ctype None msg '%s%s' % urllib parse quote object_hash urllib parse quote ts_data internal if ts_meta and ts_meta ts_data delta ts_meta raw - ts_data raw msg '%sm %x' % msg delta if ts_ctype and ts_ctype ts_data delta ts_ctype raw - ts_data raw msg '%s t %x' % msg delta return msg
def convert_json_to_df results_url with closing urlopen results_url as resp res json loads resp read timings res get 'timings' if not timings returnres [x for x in timings if x get 'succeeded' ]df pd DataFrame res df df set_index 'name' return df
def convert_json_to_df results_url with closing urlopen results_url as resp res json loads resp read timings res get 'timings' if not timings returnres [x for x in timings if x get 'succeeded' ]df pd DataFrame res df df set_index 'name' return df
def readAudioFile path extension os path splitext path [1]try if extension lower ' wav' [Fs x] wavfile read path elif extension lower ' aif' or extension lower ' aiff' s aifc open path 'r' nframes s getnframes strsig s readframes nframes x numpy fromstring strsig numpy short byteswap Fs s getframerate else print 'ErrorinreadAudioFile Unknownfiletype 'return -1 -1 except IOError print 'Error filenotfoundorotherI/Oerror 'return -1 -1 return Fs x
def readAudioFile path extension os path splitext path [1]try if extension lower ' wav' [Fs x] wavfile read path elif extension lower ' aif' or extension lower ' aiff' s aifc open path 'r' nframes s getnframes strsig s readframes nframes x numpy fromstring strsig numpy short byteswap Fs s getframerate else print 'ErrorinreadAudioFile Unknownfiletype 'return -1 -1 except IOError print 'Error filenotfoundorotherI/Oerror 'return -1 -1 return Fs x
def assert_nD array ndim arg_name 'image' array np asanyarray array msg_incorrect_dim 'Theparameter`%s`mustbea%s-dimensionalarray'msg_empty_array 'Theparameter`%s`cannotbeanemptyarray'if isinstance ndim int ndim [ndim]if array size 0 raise ValueError msg_empty_array % arg_name if not array ndim in ndim raise ValueError msg_incorrect_dim % arg_name '-or-' join [str n for n in ndim]
def assert_nD array ndim arg_name 'image' array np asanyarray array msg_incorrect_dim 'Theparameter`%s`mustbea%s-dimensionalarray'msg_empty_array 'Theparameter`%s`cannotbeanemptyarray'if isinstance ndim int ndim [ndim]if array size 0 raise ValueError msg_empty_array % arg_name if not array ndim in ndim raise ValueError msg_incorrect_dim % arg_name '-or-' join [str n for n in ndim]
def floor x np import_module 'numpy' if isinstance x int float return interval np floor x elif isinstance x interval if x is_valid is False return interval - np inf np inf is_valid False else start np floor x start end np floor x end if start end return interval start end is_valid x is_valid else return interval start end is_valid None else return NotImplementedError
def get_display spec if spec is None return Noneelif isinstance spec six string_types return pyglet canvas Display spec else raise error Error 'Invaliddisplayspecification {} Mustbeastringlike 0orNone ' format spec
@statfuncdef quantiles x qlist 2 5 25 50 75 97 5 transform lambda x x x transform x copy if x ndim > 1 sx np sort x T Telse sx np sort x try quants [sx[int len sx * q / 100 0 ] for q in qlist]return dict zip qlist quants except IndexError _log warning 'Toofewelementsforquantilecalculation'
@statfuncdef quantiles x qlist 2 5 25 50 75 97 5 transform lambda x x x transform x copy if x ndim > 1 sx np sort x T Telse sx np sort x try quants [sx[int len sx * q / 100 0 ] for q in qlist]return dict zip qlist quants except IndexError _log warning 'Toofewelementsforquantilecalculation'
@statfuncdef quantiles x qlist 2 5 25 50 75 97 5 transform lambda x x x transform x copy if x ndim > 1 sx np sort x T Telse sx np sort x try quants [sx[int len sx * q / 100 0 ] for q in qlist]return dict zip qlist quants except IndexError _log warning 'Toofewelementsforquantilecalculation'
@statfuncdef quantiles x qlist 2 5 25 50 75 97 5 transform lambda x x x transform x copy if x ndim > 1 sx np sort x T Telse sx np sort x try quants [sx[int len sx * q / 100 0 ] for q in qlist]return dict zip qlist quants except IndexError _log warning 'Toofewelementsforquantilecalculation'
def _snapper_post opts jid pre_num try if not opts['test'] and __opts__ get 'snapper_states' and pre_num __salt__['snapper create_snapshot'] config __opts__ get 'snapper_states_config' 'root' snapshot_type 'post' pre_number pre_num description 'SaltStaterunforjid{0}' format jid __pub_jid jid except Exception log error 'Failedtocreatesnapperpresnapshotforjid {0}' format jid
def aws_output args aws_config environment os environ copy environment update aws_config return check_output ['aws'] + args env environment
@lru_cache def physical_memory start_time parameter time time 'systemphysicalmemory' mem_total_line _get_line '/proc/meminfo' 'MemTotal ' parameter try result int mem_total_line split [1] * 1024 _log_runtime parameter '/proc/meminfo[MemTotal]' start_time return resultexcept exc IOError 'unabletoparsethe/proc/meminfoMemTotalentry %s' % mem_total_line _log_failure parameter exc raise exc
def list_journals config sep u'\n'journal_list sep join config[u'journals'] return journal_list
def transformVector3Blindly tetragrid vector3 x getTransformedByList tetragrid[0] vector3 y getTransformedByList tetragrid[1] vector3 z getTransformedByList tetragrid[2] vector3 vector3 x xvector3 y yvector3 z z
def setup_platform hass config add_devices discovery_info None dev []for resource in config[CONF_RESOURCES] if 'arg' not in resource resource['arg'] ''dev append SystemMonitorSensor resource[CONF_TYPE] resource['arg'] add_devices dev
def TestInit global INIT_RANif stats STATS is None stats STATS stats StatsCollector flags FLAGS config config_lib Resource Filter 'install_data/etc/grr-server yaml' flags FLAGS secondary_configs [config_lib Resource Filter 'test_data/grr_test yaml@grr-response-test' ]extra_test_config config_lib CONFIG['Test additional_test_config']if os path exists extra_test_config flags FLAGS secondary_configs append extra_test_config config_lib CONFIG AddContext 'TestContext' 'Contextappliedwhenweruntests ' config_lib SetPlatformArchContext config_lib ParseConfigCommandLine if not INIT_RAN log ServerLoggingStartupInit registry TestInit INIT_RAN True
def bspline_basis_set d knots x n_splines len knots - d - 1 return [bspline_basis d knots i x for i in range n_splines ]
def make_anonymous_struct builder values struct_type None if struct_type is None struct_type ir LiteralStructType [v type for v in values] struct_val struct_type ir Undefined for i v in enumerate values struct_val builder insert_value struct_val v i return struct_val
def make_anonymous_struct builder values struct_type None if struct_type is None struct_type ir LiteralStructType [v type for v in values] struct_val struct_type ir Undefined for i v in enumerate values struct_val builder insert_value struct_val v i return struct_val
def make_anonymous_struct builder values struct_type None if struct_type is None struct_type ir LiteralStructType [v type for v in values] struct_val struct_type ir Undefined for i v in enumerate values struct_val builder insert_value struct_val v i return struct_val
def test_neg_type___new___args global calledcalled Falseclass MyType type def __new__ self passdef __clrtype__ self global calledcalled Truereturn super MyType self __clrtype__ try class X object __metaclass__ MyTypeFail 'type __new__signatureiswrong' except TypeError as e AreEqual e message '__new__ takesexactly1argument 4given ' finally AreEqual called False
def getvalue name default return getattr settings name default
@never_cachedef unsubscribe request hash None token None perm_setting None assert hash is not None and token is not None user Nonetry email UnsubscribeCode parse token hash user UserProfile objects get email email except ValueError UserProfile DoesNotExist passperm_settings []if user is not None unsubscribed Trueif not perm_setting perm_settings [l for l in notifications NOTIFICATIONS if not l mandatory ]else perm_setting notifications NOTIFICATIONS_BY_SHORT[perm_setting]UserNotification update_or_create update {'enabled' False} user user notification_id perm_setting id perm_settings [perm_setting]else unsubscribed Falseemail ''return render request 'users/unsubscribe html' {'unsubscribed' unsubscribed 'email' email 'perm_settings' perm_settings}
@never_cachedef unsubscribe request hash None token None perm_setting None assert hash is not None and token is not None user Nonetry email UnsubscribeCode parse token hash user UserProfile objects get email email except ValueError UserProfile DoesNotExist passperm_settings []if user is not None unsubscribed Trueif not perm_setting perm_settings [l for l in notifications NOTIFICATIONS if not l mandatory ]else perm_setting notifications NOTIFICATIONS_BY_SHORT[perm_setting]UserNotification update_or_create update {'enabled' False} user user notification_id perm_setting id perm_settings [perm_setting]else unsubscribed Falseemail ''return render request 'users/unsubscribe html' {'unsubscribed' unsubscribed 'email' email 'perm_settings' perm_settings}
def check_dna_chars_bcs header mapping_data errors has_barcodes True valid_dna_chars DNASequence iupac_standard_characters header_fields_to_check []if has_barcodes header_fields_to_check append 'BarcodeSequence' check_indices []for curr_field in range len header if header[curr_field] in header_fields_to_check check_indices append curr_field correction_ix 1for curr_data in range len mapping_data for curr_ix in check_indices if len mapping_data[curr_data][curr_ix] 0 errors append 'MissingexpectedDNAsequence DCTB %d %d' % curr_data + correction_ix curr_ix continuefor curr_nt in mapping_data[curr_data][curr_ix] if curr_nt not in valid_dna_chars errors append 'InvalidDNAsequencedetected %s DCTB %d %d' % mapping_data[curr_data][curr_ix] curr_data + correction_ix curr_ix continuereturn errors
def _get_client region key keyid profile session _get_session region key keyid profile if not session log error 'Failedtogetdatapipelineclient ' return Nonereturn session client 'datapipeline'
def libvlc_media_get_type p_md f _Cfunctions get 'libvlc_media_get_type' None or _Cfunction 'libvlc_media_get_type' 1 None MediaType Media return f p_md
def generate_objects_info_packs repo for pack in repo object_store packs yield 'P' + pack data filename encode sys getfilesystemencoding + '\n'
def update name maximum_version None required_version None flags [ u'Name' name ]if maximum_version is not None flags append u'MaximumVersion' maximum_version if required_version is not None flags append u'RequiredVersion' required_version params u''for flag value in flags params + u'-{0}{1}' format flag value cmd u'Update-Module{0}-Force' format params _pshell cmd return name in list_modules
def allow_serial_nos_with_different_item sle_serial_no sle allow_serial_nos Falseif sle voucher_type u'StockEntry' and sle actual_qty > 0 stock_entry frappe get_doc u'StockEntry' sle voucher_no if stock_entry purpose in u'Repack' u'Manufacture' for d in stock_entry get u'items' if d serial_no and d s_warehouse if sle is_cancelled u'No' else d t_warehouse serial_nos get_serial_nos d serial_no if sle_serial_no in serial_nos allow_serial_nos Truereturn allow_serial_nos
def allow_serial_nos_with_different_item sle_serial_no sle allow_serial_nos Falseif sle voucher_type u'StockEntry' and sle actual_qty > 0 stock_entry frappe get_doc u'StockEntry' sle voucher_no if stock_entry purpose in u'Repack' u'Manufacture' for d in stock_entry get u'items' if d serial_no and d s_warehouse if sle is_cancelled u'No' else d t_warehouse serial_nos get_serial_nos d serial_no if sle_serial_no in serial_nos allow_serial_nos Truereturn allow_serial_nos
def unescape text chars REV_HTML_ESCAPE_CHARS for s h in chars text text replace h s return text
def scoreatpercentile a per limit interpolation_method 'fraction' axis None a np asarray a if a size 0 if np isscalar per return np nanelse return np ones np asarray per shape dtype np float64 * np nan if limit a a[ limit[0] < a & a < limit[1] ]sorted np sort a axis axis if axis is None axis 0return _compute_qth_percentile sorted per interpolation_method axis
def easyxf strg_to_parse '' num_format_str None field_sep ' ' line_sep ' ' intro_sep ' ' esc_char '\\' debug False xfobj XFStyle if num_format_str is not None xfobj num_format_str num_format_strif strg_to_parse _parse_strg_to_obj strg_to_parse xfobj xf_dict field_sep field_sep line_sep line_sep intro_sep intro_sep esc_char esc_char debug debug return xfobj
@register inclusion_tag 'test_incl_tag_current_app html' takes_context True def inclusion_tag_current_app context return {}
def libvlc_video_set_track p_mi i_track f _Cfunctions get 'libvlc_video_set_track' None or _Cfunction 'libvlc_video_set_track' 1 1 None ctypes c_int MediaPlayer ctypes c_int return f p_mi i_track
def fire data tag try event salt utils event get_event 'minion' sock_dir __opts__['sock_dir'] transport __opts__['transport'] opts __opts__ listen False return event fire_event data tag except Exception exc_type exc_value exc_traceback sys exc_info lines traceback format_exception exc_type exc_value exc_traceback log debug lines return False
def _emit_problem_submitted_event kwargs root_type get_event_transaction_type if not root_type root_id get_event_transaction_id if not root_id root_id create_new_event_transaction_id set_event_transaction_type PROBLEM_SUBMITTED_EVENT_TYPE tracker emit unicode PROBLEM_SUBMITTED_EVENT_TYPE {'user_id' unicode kwargs['user_id'] 'course_id' unicode kwargs['course_id'] 'problem_id' unicode kwargs['usage_id'] 'event_transaction_id' unicode root_id 'event_transaction_type' unicode PROBLEM_SUBMITTED_EVENT_TYPE 'weighted_earned' kwargs get 'weighted_earned' 'weighted_possible' kwargs get 'weighted_possible' }
def preprocess_image img img img astype np float32 img imresize img 224 224 img img[ [2 1 0]]img img * 255 mean np array [103 939 116 779 123 68] for i in range 0 3 img[ i] img[ i] - mean[i] img np transpose img [2 0 1] return img
def no_type_check_decorator decorator @functools wraps decorator def wrapped_decorator *args **kwds func decorator *args **kwds func no_type_check func return funcreturn wrapped_decorator
def no_type_check_decorator decorator @functools wraps decorator def wrapped_decorator *args **kwds func decorator *args **kwds func no_type_check func return funcreturn wrapped_decorator
def get_readable_ctx_date ctx t tz ctx date date datetime *gmtime float t - tz [ 6] ctx_date date strftime '%Y-%m-%d' return ctx_date
def rotate_left x y if len x 0 return []y y % len x return x[y ] + x[ y]
def rotate_left x y if len x 0 return []y y % len x return x[y ] + x[ y]
@core_helperdef build_nav_icon menu_item title **kw return _make_menu_item menu_item title **kw
def PQa P_0 Q_0 D A_i_2 B_i_1 0A_i_1 B_i_2 1G_i_2 - P_0 G_i_1 Q_0P_i P_0Q_i Q_0while 1 a_i floor P_i + sqrt D / Q_i A_i a_i * A_i_1 + A_i_2 B_i a_i * B_i_1 + B_i_2 G_i a_i * G_i_1 + G_i_2 yield P_i Q_i a_i A_i B_i G_i A_i_1 A_i_2 A_i A_i_1 B_i_1 B_i_2 B_i B_i_1 G_i_1 G_i_2 G_i G_i_1 P_i a_i * Q_i - P_i Q_i D - P_i ** 2 / Q_i
def find_in_path fname path None if path is None path os environ get 'PATH' '' for dir in path split os pathsep fpath os path join dir fname if os path isfile fpath return fpathelse return None
def _rewrite1 f x recursive True fac po g _split_mul f x g _rewrite_single g x recursive if g return fac po g[0] g[1]
def _rewrite1 f x recursive True fac po g _split_mul f x g _rewrite_single g x recursive if g return fac po g[0] g[1]
@pytest fixture def default_groups database return create_default_groups
def void_output func argtypes errcheck True cpl False if argtypes func argtypes argtypesif errcheck func restype c_intfunc errcheck partial check_errcode cpl cpl else func restype Nonereturn func
def void_output func argtypes errcheck True cpl False if argtypes func argtypes argtypesif errcheck func restype c_intfunc errcheck partial check_errcode cpl cpl else func restype Nonereturn func
def void_output func argtypes errcheck True cpl False if argtypes func argtypes argtypesif errcheck func restype c_intfunc errcheck partial check_errcode cpl cpl else func restype Nonereturn func
def void_output func argtypes errcheck True cpl False if argtypes func argtypes argtypesif errcheck func restype c_intfunc errcheck partial check_errcode cpl cpl else func restype Nonereturn func
def void_output func argtypes errcheck True cpl False if argtypes func argtypes argtypesif errcheck func restype c_intfunc errcheck partial check_errcode cpl cpl else func restype Nonereturn func
def status **connection_args dbc _connect **connection_args if dbc is None return {}cur dbc cursor qry 'SHOWSTATUS'try _execute cur qry except MySQLdb OperationalError as exc err 'MySQLError{0} {1}' format *exc __context__['mysql error'] errlog error err return {}ret {}for _ in range cur rowcount row cur fetchone ret[row[0]] row[1]return ret
def convert_installed return _get_convert_command is not None
def strip_ansi source return re sub '\\033\\[ \\d +?m' '' source
def equalize_epoch_counts epochs_list method 'mintime' if not all isinstance e BaseEpochs for e in epochs_list raise ValueError 'AllinputsmustbeEpochsinstances' for e in epochs_list if not e _bad_dropped e drop_bad event_times [e events[ 0] for e in epochs_list]indices _get_drop_indices event_times method for e inds in zip epochs_list indices e drop inds reason 'EQUALIZED_COUNT'
@render_to 'distributed/loadtesting/load_test html' def load_test request nusers None username uuid uuid4 hex[ 12]if not Facility objects count fac Facility objects create name 'fac' fac Facility objects all [0] user _ FacilityUser get_or_initialize username username facility fac user set_password username user save return {'pct_videos' request GET get 'pct_videos' 0 3 'username' username}
@render_to 'distributed/loadtesting/load_test html' def load_test request nusers None username uuid uuid4 hex[ 12]if not Facility objects count fac Facility objects create name 'fac' fac Facility objects all [0] user _ FacilityUser get_or_initialize username username facility fac user set_password username user save return {'pct_videos' request GET get 'pct_videos' 0 3 'username' username}
def build_entrypoints prefix entrypoints result []for entrypoint_id rel_class_name in entrypoints if ' ' in rel_class_name sep ' 'else sep ' 'result append '%s %s%s%s' % entrypoint_id prefix sep rel_class_name return result
def build_entrypoints prefix entrypoints result []for entrypoint_id rel_class_name in entrypoints if ' ' in rel_class_name sep ' 'else sep ' 'result append '%s %s%s%s' % entrypoint_id prefix sep rel_class_name return result
def test_space_separator d iso8601 parse_date '2007-06-2306 40 34 00Z' assert d year 2007 assert d month 6 assert d day 23 assert d hour 6 assert d minute 40 assert d second 34 assert d microsecond 0 assert d tzinfo iso8601 UTC
def collect_unioned_set_field block_structure transformer merged_field_name filter_by for block_key in block_structure topological_traversal result_set {block_key} if filter_by block_key else set for parent in block_structure get_parents block_key result_set block_structure get_transformer_block_field parent transformer merged_field_name set block_structure set_transformer_block_field block_key transformer merged_field_name result_set
def collect_unioned_set_field block_structure transformer merged_field_name filter_by for block_key in block_structure topological_traversal result_set {block_key} if filter_by block_key else set for parent in block_structure get_parents block_key result_set block_structure get_transformer_block_field parent transformer merged_field_name set block_structure set_transformer_block_field block_key transformer merged_field_name result_set
@deprecated u'2 0' message _hold_msg def over func *args **kwargs ax gca h ax _holdax _hold Truefunc *args **kwargs ax _hold h
def copy_byte src dest byte src read 1 dest write byte val unpack 'B' byte return val
def setup_test_show_dir if not os path exists SHOW_DIR os makedirs SHOW_DIR
def get_api_servers api_servers []for api_server in FLAGS glance_api_servers if '//' not in api_server api_server 'http //' + api_server url urlparse urlparse api_server port url port or 80 host url netloc split ' ' 1 [0]use_ssl url scheme 'https' api_servers append host port use_ssl random shuffle api_servers return itertools cycle api_servers
def _ico_downsample surf dest_grade n_tri surf['ntri']found -1 bad_msg 'Asurfacewith%dtrianglescannotbeisomorphicwithasubdividedicosahedron ' % surf['ntri'] if n_tri % 20 0 raise RuntimeError bad_msg n_tri n_tri // 20 found int round np log n_tri / np log 4 if n_tri 4 ** found raise RuntimeError bad_msg del n_triif dest_grade > found raise RuntimeError 'Forthissurface decimationgradeshouldbe%dorless not%s ' % found dest_grade source _get_ico_surface found dest _get_ico_surface dest_grade patch_stats True del dest['tri_cent']del dest['tri_nn']del dest['neighbor_tri']del dest['tri_area']if not np array_equal source['tris'] surf['tris'] raise RuntimeError 'Thesourcesurfacehasamatchingnumberoftrianglesbutorderingiswrong' logger info 'Goingfrom%dthto%dthsubdivisionofanicosahedron n_tri %d->%d ' % found dest_grade surf['ntri'] dest['ntri'] dest['rr'] surf['rr'][_get_ico_map source dest ]return dest
def transitive_closure graph reflexive False if reflexive base_set lambda k set [k] else base_set lambda k set agenda_graph dict k graph[k] copy for k in graph closure_graph dict k base_set k for k in graph for i in graph agenda agenda_graph[i]closure closure_graph[i]while agenda j agenda pop closure add j closure closure_graph setdefault j base_set j agenda agenda_graph get j base_set j agenda - closurereturn closure_graph
def make_packing_list doc if doc get u'_action' and doc _action u'update_after_submit' returnparent_items []for d in doc get u'items' if frappe db get_value u'ProductBundle' {u'new_item_code' d item_code} for i in get_product_bundle_items d item_code update_packing_list_item doc i item_code flt i qty * flt d qty d i description if [d item_code d name] not in parent_items parent_items append [d item_code d name] cleanup_packing_list doc parent_items
def update_rollout_dict spec rollout_dict if should_skip_env_spec_for_tests spec logger info u'Skippingtestsfor{}' format spec id return Falseif spec nondeterministic logger info u'Skippingtestsfornondeterministicenv{}' format spec id return Falselogger info u'Generatingrolloutfor{}' format spec id try observations_hash actions_hash rewards_hash dones_hash generate_rollout_hash spec except logger warn u'Exception{}thrownwhilegeneratingrolloutfor{} Rolloutnotadded ' format sys exc_info [0] spec id return Falserollout {}rollout[u'observations'] observations_hashrollout[u'actions'] actions_hashrollout[u'rewards'] rewards_hashrollout[u'dones'] dones_hashexisting rollout_dict get spec id if existing differs Falsefor key new_hash in rollout items differs differs or existing[key] new_hash if not differs logger debug u'Hashesmatchwithexistingfor{}' format spec id return Falseelse logger warn u'Gotnewhashfor{} Overwriting ' format spec id rollout_dict[spec id] rolloutreturn True
@click command u'uninstall-app' @click argument u'app' @click option u'--yes' u'-y' help u'Tobypassconfirmationpromptforuninstallingtheapp' is_flag True default False multiple True @click option u'--dry-run' help u'Listalldoctypesthatwillbedeleted' is_flag True default False @pass_contextdef uninstall context app dry_run False yes False from frappe installer import remove_appfor site in context sites try frappe init site site frappe connect remove_app app dry_run yes finally frappe destroy
def hasExplorer if sys platform 'win32' or sys platform 'cygwin' or sys platform 'darwin' return Trueif sys platform 'linux2' if os path isfile '/usr/bin/xdg-open' return Truereturn False
def _rgb_vector color if isinstance color six string_types color color_dict[color]return np array color[ 3]
def from_tree expr namespace None if isinstance expr dict op args expr[u'op'] expr[u'args'] if op u'slice' return expr_utils _slice *[from_tree arg namespace for arg in args] if hasattr blaze expr op cls getattr blaze expr op else cls expression_from_name op if cls is Symbol cls symbolchildren [from_tree arg namespace for arg in args]return cls *children elif isinstance expr list tuple return tuple from_tree arg namespace for arg in expr if namespace and expr in namespace return namespace[expr]else return expr
def expandService service_element uris sortedURIs service_element if not uris uris [None]expanded []for uri in uris type_uris getTypeURIs service_element expanded append type_uris uri service_element return expanded
def merge_asof left right on None left_on None right_on None left_index False right_index False by None left_by None right_by None suffixes '_x' '_y' tolerance None allow_exact_matches True direction 'backward' op _AsOfMerge left right on on left_on left_on right_on right_on left_index left_index right_index right_index by by left_by left_by right_by right_by suffixes suffixes how 'asof' tolerance tolerance allow_exact_matches allow_exact_matches direction direction return op get_result
def GetOTP user return _ComputeOTP _GetUserSecret user long time time / _GRANULARITY
def GetOTP user return _ComputeOTP _GetUserSecret user long time time / _GRANULARITY
def GetOTP user return _ComputeOTP _GetUserSecret user long time time / _GRANULARITY
def GetOTP user return _ComputeOTP _GetUserSecret user long time time / _GRANULARITY
def GetOTP user return _ComputeOTP _GetUserSecret user long time time / _GRANULARITY
def tamper payload **kwargs return '%s%%00' % payload if payload else payload
def cpus with settings hide 'running' 'stdout' res run 'python-c"importmultiprocessing print multiprocessing cpu_count "' return int res
def _run_on_all_nodes nodes task return parallel list run_remotely username 'root' address node address commands task node for node in nodes
def __determine_list_kwargs options list_kwargs __determine_delete_kwargs options list_kwargs update __kwargs_option_to_dict options list_kwargs return list_kwargs
def tanm A A _asarray_square A return _maybe_real A solve cosm A sinm A
def rm_handlers app handlers key None for handler_name func in handlers iteritems rm_handler app handler_name func key key
def _create_modulename cdef_sources source sys_version key '\x00' join [sys_version[ 3] source cdef_sources] key key encode 'utf-8' k1 hex binascii crc32 key[0 2] & 4294967295 k1 k1 lstrip '0x' rstrip 'L' k2 hex binascii crc32 key[1 2] & 4294967295 k2 k2 lstrip '0' rstrip 'L' return '_Cryptography_cffi_{0}{1}' format k1 k2
def _create_modulename cdef_sources source sys_version key '\x00' join [sys_version[ 3] source cdef_sources] key key encode 'utf-8' k1 hex binascii crc32 key[0 2] & 4294967295 k1 k1 lstrip '0x' rstrip 'L' k2 hex binascii crc32 key[1 2] & 4294967295 k2 k2 lstrip '0' rstrip 'L' return '_Cryptography_cffi_{0}{1}' format k1 k2
def _create_modulename cdef_sources source sys_version key '\x00' join [sys_version[ 3] source cdef_sources] key key encode 'utf-8' k1 hex binascii crc32 key[0 2] & 4294967295 k1 k1 lstrip '0x' rstrip 'L' k2 hex binascii crc32 key[1 2] & 4294967295 k2 k2 lstrip '0' rstrip 'L' return '_Cryptography_cffi_{0}{1}' format k1 k2
def _create_modulename cdef_sources source sys_version key '\x00' join [sys_version[ 3] source cdef_sources] key key encode 'utf-8' k1 hex binascii crc32 key[0 2] & 4294967295 k1 k1 lstrip '0x' rstrip 'L' k2 hex binascii crc32 key[1 2] & 4294967295 k2 k2 lstrip '0' rstrip 'L' return '_Cryptography_cffi_{0}{1}' format k1 k2
def _get_zendesk_custom_field_context request context {}course_id request POST get 'course_id' if not course_id return contextcontext['course_id'] course_idif not request user is_authenticated return contextenrollment CourseEnrollment get_enrollment request user CourseKey from_string course_id if enrollment and enrollment is_active context['enrollment_mode'] enrollment modereturn context
@validate 'form' 'graph' 'tree' def valid_att_in_field arch return not arch xpath '//field[not @name ]'
@validate 'form' 'graph' 'tree' def valid_att_in_field arch return not arch xpath '//field[not @name ]'
def safe_add x y if x is not None and y is not None return x + y elif x is not None return xelif y is not None return yelse return None
def message_question text title informative_text None details None buttons None default_button None exc_info False parent None return message QMessageBox Question text title informative_text details buttons default_button exc_info parent
def message_question text title informative_text None details None buttons None default_button None exc_info False parent None return message QMessageBox Question text title informative_text details buttons default_button exc_info parent
def message_question text title informative_text None details None buttons None default_button None exc_info False parent None return message QMessageBox Question text title informative_text details buttons default_button exc_info parent
def _get_words_from_dataset dataset def tokenize words if isinstance words basestring return word_tokenize words include_punc False else return wordsall_words chain from_iterable tokenize words for words _ in dataset return set all_words
def _get_words_from_dataset dataset def tokenize words if isinstance words basestring return word_tokenize words include_punc False else return wordsall_words chain from_iterable tokenize words for words _ in dataset return set all_words
def make_events add_nulls def gen_date_interleavings for e1 e2 t1 t2 in product * [critical_dates] * 4 if e1 < e2 yield e1 e2 t1 t2 event_frames []for sid e1 e2 t1 t2 in enumerate gen_date_interleavings event_frames append make_events_for_sid sid [e1 e2] [t1 t2] if add_nulls for date in critical_dates event_frames append make_null_event_date_events np arange sid + 1 timestamp date return pd concat event_frames ignore_index True
def make_events add_nulls def gen_date_interleavings for e1 e2 t1 t2 in product * [critical_dates] * 4 if e1 < e2 yield e1 e2 t1 t2 event_frames []for sid e1 e2 t1 t2 in enumerate gen_date_interleavings event_frames append make_events_for_sid sid [e1 e2] [t1 t2] if add_nulls for date in critical_dates event_frames append make_null_event_date_events np arange sid + 1 timestamp date return pd concat event_frames ignore_index True
def rot_axis2 theta ct cos theta st sin theta lil ct 0 - st 0 1 0 st 0 ct return Matrix lil
def join_urls *urls if not urls returnurl urls[0]for u in urls[1 ] if not url endswith '/' url + '/'while u startswith '/' u utils lstrips u '/' url + ureturn url
def selfSimilarityMatrix featureVectors [nDims nVectors] featureVectors shape[featureVectors2 MEAN STD] aT normalizeFeatures [featureVectors T] featureVectors2 featureVectors2[0] TS 1 0 - distance squareform distance pdist featureVectors2 T 'cosine' return S
@treeio_login_required@handle_response_formatdef report_filter_remove request report_id field_name filter_index response_format 'html' report get_object_or_404 Report pk report_id if not request user profile has_permission report mode 'w' return user_denied request message "Youdon'thavewriteaccesstothisReport" model loads report model field model get_field field_name field filters pop int filter_index - 1 report model dumps model report save return HttpResponseRedirect reverse 'reports_report_edit' args [int report_id ]
@treeio_login_required@handle_response_formatdef report_filter_remove request report_id field_name filter_index response_format 'html' report get_object_or_404 Report pk report_id if not request user profile has_permission report mode 'w' return user_denied request message "Youdon'thavewriteaccesstothisReport" model loads report model field model get_field field_name field filters pop int filter_index - 1 report model dumps model report save return HttpResponseRedirect reverse 'reports_report_edit' args [int report_id ]
def run dataset_dir if not tf gfile Exists dataset_dir tf gfile MakeDirs dataset_dir if _dataset_exists dataset_dir print 'Datasetfilesalreadyexist Exitingwithoutre-creatingthem ' returndataset_utils download_and_uncompress_tarball _DATA_URL dataset_dir photo_filenames class_names _get_filenames_and_classes dataset_dir class_names_to_ids dict zip class_names range len class_names random seed _RANDOM_SEED random shuffle photo_filenames training_filenames photo_filenames[_NUM_VALIDATION ]validation_filenames photo_filenames[ _NUM_VALIDATION]_convert_dataset 'train' training_filenames class_names_to_ids dataset_dir _convert_dataset 'validation' validation_filenames class_names_to_ids dataset_dir labels_to_class_names dict zip range len class_names class_names dataset_utils write_label_file labels_to_class_names dataset_dir _clean_up_temporary_files dataset_dir print '\nFinishedconvertingtheFlowersdataset '
def dt_action row 1 action None column 1 tableID 'datatable' config current test_configbrowser config browserif action button " //*[@id '%s']/tbody/tr[%s]/td[%s]/a[contains text '%s' ]" % tableID row column action else button " //*[@id '%s']/tbody/tr[%s]/td[%s]/a" % tableID row column giveup 0 0sleeptime 0 2while giveup < 10 0 try element browser find_element_by_xpath button url element get_attribute 'href' if url browser get url return Trueexcept Exception as inst print '%swith%s' % type inst button time sleep sleeptime giveup + sleeptimereturn False
def dt_action row 1 action None column 1 tableID 'datatable' config current test_configbrowser config browserif action button " //*[@id '%s']/tbody/tr[%s]/td[%s]/a[contains text '%s' ]" % tableID row column action else button " //*[@id '%s']/tbody/tr[%s]/td[%s]/a" % tableID row column giveup 0 0sleeptime 0 2while giveup < 10 0 try element browser find_element_by_xpath button url element get_attribute 'href' if url browser get url return Trueexcept Exception as inst print '%swith%s' % type inst button time sleep sleeptime giveup + sleeptimereturn False
def hexdump s width 16 skip True hexii False begin 0 style None highlight None cyclic False s packing flat s return '\n' join hexdump_iter StringIO StringIO s width skip hexii begin style highlight cyclic
def hexdump s width 16 skip True hexii False begin 0 style None highlight None cyclic False s packing flat s return '\n' join hexdump_iter StringIO StringIO s width skip hexii begin style highlight cyclic
@memoizedef from_style key style QtGui QApplication instance style return style standardIcon key
@memoizedef from_style key style QtGui QApplication instance style return style standardIcon key
def setup_platform hass config add_devices discovery_info None digital_ocean get_component 'digital_ocean' droplets config get CONF_DROPLETS dev []for droplet in droplets droplet_id digital_ocean DIGITAL_OCEAN get_droplet_id droplet dev append DigitalOceanSwitch digital_ocean DIGITAL_OCEAN droplet_id add_devices dev
def test_complex entry tokenize ' 1j ' [0][0]assert entry HyComplex '1 0j' entry tokenize ' j ' [0][0]assert entry HySymbol 'j'
@cleanupdef test__EventCollection__get_linestyle _ coll _ generate_EventCollection_plot assert_equal coll get_linestyle [ None None ]
@cleanupdef test__EventCollection__get_linestyle _ coll _ generate_EventCollection_plot assert_equal coll get_linestyle [ None None ]
def _polyder p m if m 0 result pelse n len p if n < m result np zeros_like p[ 1 ] else dp p[ - m ] copy for k in range m rng np arange n - k - 1 m - k - 1 -1 dp * rng reshape n - m + 1 * p ndim - 1 result dpreturn result
def align alignment x return x + alignment - 1 // alignment * alignment
@mock_streams 'stdout' @with_patched_input p def test_prompt_appends_space s 'Thisismyprompt'prompt s eq_ sys stdout getvalue s + ''
def _process_bti_headshape fname convert True use_hpi True idx_points dig_points _read_head_shape fname if convert ctf_head_t _get_ctf_head_to_head_t idx_points else ctf_head_t Transform 'ctf_head' 'ctf_head' if dig_points is not None all_points np r_[ idx_points dig_points ]else all_points idx_pointsif convert all_points _convert_hs_points all_points ctf_head_t dig _points_to_dig all_points len idx_points use_hpi return dig ctf_head_t
def _extract_expressions node if isinstance node nodes CallFunc and isinstance node func nodes Name and node func name _TRANSIENT_FUNCTION real_expr node args[0]real_expr parent node parentfor name in node parent _astroid_fields child getattr node parent name if isinstance child list tuple for idx compound_child in enumerate child if compound_child is node child[idx] real_exprelif child is node setattr node parent name real_expr yield real_expr else for child in node get_children for result in _extract_expressions child yield result
def network_list provider client _get_client return client extra_action action 'network_list' provider provider names 'names'
def create_security_group name None description None profile None conn _auth profile return conn create_security_group name description
def expand_path path paths []path os path expanduser path path os path expandvars path if os path isdir path for dir dirs files in os walk path for file in files paths append os path join dir file else paths extend glob path return paths
def _EscapeCommandLineArgumentForMSBuild s def _Replace match return len match group 1 / 2 * 4 * '\\' + '\\"' s quote_replacer_regex2 sub _Replace s return s
def meijerint_indefinite f x from sympy import hyper meijergresults []for a in sorted _find_splitting_points f x {S 0 } key default_sort_key res _meijerint_indefinite_1 f subs x x + a x if not res continueres res subs x x - a if _has res hyper meijerg results append res else return resif f has HyperbolicFunction _debug 'Tryrewritinghyperbolicsintermsofexp ' rv meijerint_indefinite _rewrite_hyperbolics_as_exp f x if rv if not type rv is list return collect factor_terms rv rv atoms exp results extend rv if results return next ordered results
def has_hidden layer return hasattr layer 'initial_hidden_state'
def get_fs_home return _get_root_home 'FREESURFER_HOME' 'freesurfer' _fs_home_problem
def transformVector3sByMatrix tetragrid vector3s if getIsIdentityTetragridOrNone tetragrid returnfor vector3 in vector3s transformVector3Blindly tetragrid vector3
def sleep duration return SleepEvent duration
def log_normalize a axis None a_lse logsumexp a axis a - a_lse[ np newaxis]
def delete_branch pr session refs_url pr['head']['repo']['git_refs_url']branch_url refs_url replace '{/sha}' '/heads/' + pr['head']['ref'] return session delete branch_url
def requireAnomalyModel func def _decorator self *args **kwargs if not self getInferenceType InferenceType TemporalAnomaly raise RuntimeError 'MethodrequiredaTemporalAnomalymodel ' if self _getAnomalyClassifier is None raise RuntimeError 'Modeldoesnotsupportthiscommand ModelmustbeanactiveanomalyDetectormodel ' return func self *args **kwargs return _decorator
def get_real_ip request if KEY_REAL_IP in request return request[KEY_REAL_IP]if request app[KEY_USE_X_FORWARDED_FOR] and HTTP_HEADER_X_FORWARDED_FOR in request headers request[KEY_REAL_IP] ip_address request headers get HTTP_HEADER_X_FORWARDED_FOR split ' ' [0] else peername request transport get_extra_info 'peername' if peername request[KEY_REAL_IP] ip_address peername[0] else request[KEY_REAL_IP] Nonereturn request[KEY_REAL_IP]
def head_container url token container http_conn None headers None service_token None if http_conn parsed conn http_connelse parsed conn http_connection url path '%s/%s' % parsed path quote container method 'HEAD'req_headers {'X-Auth-Token' token}if service_token req_headers['X-Service-Token'] service_tokenif headers req_headers update headers conn request method path '' req_headers resp conn getresponse body resp read http_log '%s%s' % url replace parsed path '' path method {'headers' req_headers} resp body if resp status < 200 or resp status > 300 raise ClientException from_response resp 'ContainerHEADfailed' body resp_headers resp_header_dict resp return resp_headers
def _output_to_dict cmdoutput values_mapper None if isinstance cmdoutput dict if cmdoutput['retcode'] 0 or cmdoutput['stderr'] raise CommandExecutionError 'RabbitMQcommandfailed {0}' format cmdoutput['stderr'] cmdoutput cmdoutput['stdout']ret {}if values_mapper is None values_mapper lambda string string split ' DCTB ' data_rows _strip_listing_to_done cmdoutput splitlines for row in data_rows try key values row split ' DCTB ' 1 except ValueError log debug "Couldnotfindanyvaluesforkey'{0}' Settingto'{0}'toanemptystring " format row ret[row] ''continueret[key] values_mapper values return ret
def _output_to_dict cmdoutput values_mapper None if isinstance cmdoutput dict if cmdoutput['retcode'] 0 or cmdoutput['stderr'] raise CommandExecutionError 'RabbitMQcommandfailed {0}' format cmdoutput['stderr'] cmdoutput cmdoutput['stdout']ret {}if values_mapper is None values_mapper lambda string string split ' DCTB ' data_rows _strip_listing_to_done cmdoutput splitlines for row in data_rows try key values row split ' DCTB ' 1 except ValueError log debug "Couldnotfindanyvaluesforkey'{0}' Settingto'{0}'toanemptystring " format row ret[row] ''continueret[key] values_mapper values return ret
@receiver user_logged_out def log_successful_logout sender request user **kwargs if hasattr request 'user' if settings FEATURES['SQUELCH_PII_IN_LOGS'] AUDIT_LOG info u'Logout-user id {0}' format request user id else AUDIT_LOG info u'Logout-{0}' format request user
def has_handshake_cowpatty target capfile nonstrict True if not program_exists 'cowpatty' return Falsecmd ['cowpatty' '-r' capfile '-s' target ssid '-c']if nonstrict cmd append '-2' proc Popen cmd stdout PIPE stderr DN proc wait response proc communicate [0]if response find 'incompletefour-wayhandshakeexchange' -1 return Falseelif response find 'Unsupportedorunrecognizedpcapfile ' -1 return Falseelif response find 'Unabletoopencapturefile Success' -1 return Falsereturn True
def buildCyclicNetwork recurrent Network RecurrentNetwork if recurrent else FeedForwardNetwork N Network 'cyc' a LinearLayer 1 name 'a' b LinearLayer 2 name 'b' c LinearLayer 3 name 'c' d LinearLayer 4 name 'd' N addInputModule a N addModule b N addModule d N addOutputModule c N addConnection FullConnection a b N addConnection FullConnection b c N addConnection FullConnection c d if recurrent N addRecurrentConnection FullConnection d a else N addConnection FullConnection d a N sortModules return N
def get_default_volume_size dataset_backend_configuration default_volume_size GiB 1 if dataset_backend_configuration get 'auth_plugin' 'rackspace' default_volume_size RACKSPACE_MINIMUM_VOLUME_SIZEreturn int default_volume_size to_Byte value
def flatnonzero a return a ravel nonzero [0]
def create_ipsec_site_connection name ipsecpolicy ikepolicy vpnservice peer_cidrs peer_address peer_id psk admin_state_up True profile None **kwargs conn _auth profile return conn create_ipsec_site_connection name ipsecpolicy ikepolicy vpnservice peer_cidrs peer_address peer_id psk admin_state_up **kwargs
def testStandingsBeforeMatches deleteMatches deletePlayers registerPlayer 'MelpomeneMurray' registerPlayer 'RandySchwartz' standings playerStandings if len standings < 2 raise ValueError 'PlayersshouldappearinplayerStandingsevenbeforetheyhaveplayedanymatches ' elif len standings > 2 raise ValueError 'Onlyregisteredplayersshouldappearinstandings ' if len standings[0] 4 raise ValueError 'EachplayerStandingsrowshouldhavefourcolumns ' [ id1 name1 wins1 matches1 id2 name2 wins2 matches2 ] standingsif matches1 0 or matches2 0 or wins1 0 or wins2 0 raise ValueError 'Newlyregisteredplayersshouldhavenomatchesorwins ' if set [name1 name2] set ['MelpomeneMurray' 'RandySchwartz'] raise ValueError "Registeredplayers'namesshouldappearinstandings eveniftheyhavenomatchesplayed " print '6 Newlyregisteredplayersappearinthestandingswithnomatches '
def testStandingsBeforeMatches deleteMatches deletePlayers registerPlayer 'MelpomeneMurray' registerPlayer 'RandySchwartz' standings playerStandings if len standings < 2 raise ValueError 'PlayersshouldappearinplayerStandingsevenbeforetheyhaveplayedanymatches ' elif len standings > 2 raise ValueError 'Onlyregisteredplayersshouldappearinstandings ' if len standings[0] 4 raise ValueError 'EachplayerStandingsrowshouldhavefourcolumns ' [ id1 name1 wins1 matches1 id2 name2 wins2 matches2 ] standingsif matches1 0 or matches2 0 or wins1 0 or wins2 0 raise ValueError 'Newlyregisteredplayersshouldhavenomatchesorwins ' if set [name1 name2] set ['MelpomeneMurray' 'RandySchwartz'] raise ValueError "Registeredplayers'namesshouldappearinstandings eveniftheyhavenomatchesplayed " print '6 Newlyregisteredplayersappearinthestandingswithnomatches '
def testStandingsBeforeMatches deleteMatches deletePlayers registerPlayer 'MelpomeneMurray' registerPlayer 'RandySchwartz' standings playerStandings if len standings < 2 raise ValueError 'PlayersshouldappearinplayerStandingsevenbeforetheyhaveplayedanymatches ' elif len standings > 2 raise ValueError 'Onlyregisteredplayersshouldappearinstandings ' if len standings[0] 4 raise ValueError 'EachplayerStandingsrowshouldhavefourcolumns ' [ id1 name1 wins1 matches1 id2 name2 wins2 matches2 ] standingsif matches1 0 or matches2 0 or wins1 0 or wins2 0 raise ValueError 'Newlyregisteredplayersshouldhavenomatchesorwins ' if set [name1 name2] set ['MelpomeneMurray' 'RandySchwartz'] raise ValueError "Registeredplayers'namesshouldappearinstandings eveniftheyhavenomatchesplayed " print '6 Newlyregisteredplayersappearinthestandingswithnomatches '
def get_text_feedback context return context browser execute_script '\nreturn$ " {text_input_class}" [0] value \n' format text_input_class TEXT_INPUT_CLASS
def get_text_feedback context return context browser execute_script '\nreturn$ " {text_input_class}" [0] value \n' format text_input_class TEXT_INPUT_CLASS
def get_expire name policies _get_account_policy name if 'hardExpireDateGMT' in policies return policies['hardExpireDateGMT']return 'Valuenotset'
def github registry xml_parent data ghtrig XML SubElement xml_parent 'com cloudbees jenkins GitHubPushTrigger' XML SubElement ghtrig 'spec' text ''
def _tgrep_node_label_use_action _s _l tokens assert len tokens 1 assert tokens[0] startswith u' ' return tokens[0][1 ]
def _tgrep_node_label_use_action _s _l tokens assert len tokens 1 assert tokens[0] startswith u' ' return tokens[0][1 ]
def pci_device_get_all_by_instance_uuid context instance_uuid return IMPL pci_device_get_all_by_instance_uuid context instance_uuid
def GetVmodlName typ try return vmodlNames[typ]except KeyError return typ __name__
def _ExtractProxy proxy_yaml_key proxy_config_data proxy Nonetry if proxy_yaml_key in proxy_config_data proxy_data proxy_config_data get proxy_yaml_key original_proxy_keys list proxy_data keys proxy ProxyConfig Proxy proxy_data['host'] proxy_data['port'] username proxy_data get 'username' password proxy_data get 'password' except KeyError raise googleads errors GoogleAdsValueError 'Youryamlfileismissingsomeoftherequiredproxyvalues Requiredvaluesare %s actualvaluesare%s' % _PROXY_KEYS original_proxy_keys return proxy
def CopyStreamPart source destination content_size bytes_copied 0bytes_left content_sizewhile bytes_left > 0 bytes source read min bytes_left COPY_BLOCK_SIZE bytes_read len bytes if bytes_read 0 breakdestination write bytes bytes_copied + bytes_readbytes_left - bytes_readreturn bytes_copied
def _git_str_subprocess gitpath if not os path isdir os path join gitpath ' git' return Nonetry cid subprocess check_output ['git' 'describe' '--tags' '--dirty' '--always'] cwd gitpath decode 'UTF-8' strip date subprocess check_output ['git' 'show' '-s' '--format %ci' 'HEAD'] cwd gitpath decode 'UTF-8' strip return '{} {} ' format cid date except subprocess CalledProcessError OSError return None
def strftime date date_format def strip_zeros x return x lstrip u'0' or u'0' c89_directives u'aAbBcdfHIjmMpSUwWxXyYzZ%'format_options u'%[-]? 'candidates re findall format_options date_format template re sub format_options u'%s' date_format lang_code enc locale getlocale locale LC_TIME formatted_candidates []for candidate in candidates if candidate[ -1 ] in c89_directives if len candidate 3 candidate u'%{}' format candidate[ -1 ] conversion strip_zeroselse conversion Noneif isinstance date SafeDatetime formatted date strftime candidate safe False else formatted date strftime candidate if not six PY3 and enc is not None formatted formatted decode enc if conversion formatted conversion formatted else formatted candidateformatted_candidates append formatted return template % tuple formatted_candidates
def strftime date date_format def strip_zeros x return x lstrip u'0' or u'0' c89_directives u'aAbBcdfHIjmMpSUwWxXyYzZ%'format_options u'%[-]? 'candidates re findall format_options date_format template re sub format_options u'%s' date_format lang_code enc locale getlocale locale LC_TIME formatted_candidates []for candidate in candidates if candidate[ -1 ] in c89_directives if len candidate 3 candidate u'%{}' format candidate[ -1 ] conversion strip_zeroselse conversion Noneif isinstance date SafeDatetime formatted date strftime candidate safe False else formatted date strftime candidate if not six PY3 and enc is not None formatted formatted decode enc if conversion formatted conversion formatted else formatted candidateformatted_candidates append formatted return template % tuple formatted_candidates
def strftime date date_format def strip_zeros x return x lstrip u'0' or u'0' c89_directives u'aAbBcdfHIjmMpSUwWxXyYzZ%'format_options u'%[-]? 'candidates re findall format_options date_format template re sub format_options u'%s' date_format lang_code enc locale getlocale locale LC_TIME formatted_candidates []for candidate in candidates if candidate[ -1 ] in c89_directives if len candidate 3 candidate u'%{}' format candidate[ -1 ] conversion strip_zeroselse conversion Noneif isinstance date SafeDatetime formatted date strftime candidate safe False else formatted date strftime candidate if not six PY3 and enc is not None formatted formatted decode enc if conversion formatted conversion formatted else formatted candidateformatted_candidates append formatted return template % tuple formatted_candidates
def _BooleanAttribute value if value in ['true' '1'] return Trueelif value in ['false' '0'] return Falseelse return None
def range_boundaries range_string m ABSOLUTE_RE match range_string min_col min_row sep max_col max_row m groups min_col column_index_from_string min_col min_row int min_row if max_col is None or max_row is None max_col min_colmax_row min_rowelse max_col column_index_from_string max_col max_row int max_row return min_col min_row max_col max_row
def make_pkg_resources global_conf egg resource_name '' if pkg_resources is None raise NotImplementedError 'Thisfunctionrequirespkg_resources ' return PkgResourcesParser egg resource_name
def make_pkg_resources global_conf egg resource_name '' if pkg_resources is None raise NotImplementedError 'Thisfunctionrequirespkg_resources ' return PkgResourcesParser egg resource_name
def make_pkg_resources global_conf egg resource_name '' if pkg_resources is None raise NotImplementedError 'Thisfunctionrequirespkg_resources ' return PkgResourcesParser egg resource_name
def test_truncate_purified_field_xss s 'safe<script>alert "omg" </script>'t PurifiedTranslation localized_string s actual env from_string '{{s truncate 100 }}' render {'s' t} eq_ actual 'safe&lt script&gt alert "omg" &lt /script&gt ' actual env from_string '{{s truncate 5 }}' render {'s' t} eq_ actual 'safe '
def test_truncate_purified_field_xss s 'safe<script>alert "omg" </script>'t PurifiedTranslation localized_string s actual env from_string '{{s truncate 100 }}' render {'s' t} eq_ actual 'safe&lt script&gt alert "omg" &lt /script&gt ' actual env from_string '{{s truncate 5 }}' render {'s' t} eq_ actual 'safe '
def test_truncate_purified_field_xss s 'safe<script>alert "omg" </script>'t PurifiedTranslation localized_string s actual env from_string '{{s truncate 100 }}' render {'s' t} eq_ actual 'safe&lt script&gt alert "omg" &lt /script&gt ' actual env from_string '{{s truncate 5 }}' render {'s' t} eq_ actual 'safe '
def _item_to_dataset iterator resource return Dataset from_api_repr resource iterator client
def CutCommonSpacePrefix text text_lines text splitlines while text_lines and not text_lines[ -1 ] text_lines text_lines[ -1 ]if text_lines if text_lines[0] and text_lines[0][0] isspace text_first_line []else text_first_line [text_lines pop 0 ]common_prefix os path commonprefix [line for line in text_lines if line] space_prefix_len len common_prefix - len common_prefix lstrip if space_prefix_len for index in xrange len text_lines if text_lines[index] text_lines[index] text_lines[index][space_prefix_len ]return '\n' join text_first_line + text_lines return ''
def _build_tag_param_list params tags keys sorted tags keys i 1for key in keys value tags[key]params['Tags member {0} Key' format i ] keyif value is not None params['Tags member {0} Value' format i ] valuei + 1
def add_days_worked start days step get_step start weeks plus divmod days 5 end start + weeks * 7 + plus * step if weekday end > 5 end + 2 * step end + len [x for x in get_national_holidays start end + step if weekday x < 5 ] * step if weekday end > 5 end + 2 * step return end
def _urlunquote byte_string remap None preserve None if byte_string is None return byte_stringbyte_string unquote_to_bytes byte_string if preserve replacements [u'\x1a' u'\x1c' u'\x1d' u'\x1e' u'\x1f']preserve_unmap {}for char in remap replacement replacements pop 0 preserve_unmap[replacement] charbyte_string byte_string replace char encode u'ascii' replacement encode u'ascii' if remap for char in remap byte_string byte_string replace char encode u'ascii' u'%%%02x' % ord char encode u'ascii' output byte_string decode u'utf-8' u'iriutf8' if preserve for replacement original in preserve_unmap items output output replace replacement original return output
def _schedule_probes probes return __salt__['probes schedule_probes'] _ordered_dict_to_dict probes commit False
def _schedule_probes probes return __salt__['probes schedule_probes'] _ordered_dict_to_dict probes commit False
def save_reg data reg_dir _reg_dir regfile os path join reg_dir 'register' try if not os path exists os makedirs reg_dir except OSError as exc if exc errno errno EEXIST passelse raisetry with salt utils fopen regfile 'a' as fh_ msgpack dump data fh_ fh_ close except log error 'Couldnotwritetomsgpackfile{0}' format __opts__['outdir'] raise
def parse_denoiser_mapping denoiser_map result {}for line in denoiser_map line line strip split ' DCTB ' denoised_id line[0] rstrip ' ' original_ids [denoised_id] + line[1 ] if denoised_id in result raise ValueError 'Duplicatedidentifiersindenoisermappingfile areyousureyoumergedthecorrectfiles?' else result[denoised_id] original_idsreturn result
def test_multicolumn_table_html_fill_values col1 [1 2 3]col2 [ 1 0 1 0 2 0 2 0 3 0 3 0 ]col3 [ 'a' 'a' 'a' 'b' 'b' 'b' 'c' 'c' 'c' ]buffer_output StringIO t Table [col1 col2 col3] names 'C1' 'C2' 'C3' ascii write t buffer_output fill_values 'a' 'z' format 'html' col1 [1 2 3]col2 [ 1 0 1 0 2 0 2 0 3 0 3 0 ]col3 [ 'z' 'z' 'z' 'b' 'b' 'b' 'c' 'c' 'c' ]buffer_expected StringIO t_expected Table [col1 col2 col3] names 'C1' 'C2' 'C3' ascii write t_expected buffer_expected format 'html' assert buffer_output getvalue buffer_expected getvalue
def sig_cmp u v order if u[1] > v[1] return -1 if u[1] v[1] if order u[0] < order v[0] return -1 return 1
def sig_cmp u v order if u[1] > v[1] return -1 if u[1] v[1] if order u[0] < order v[0] return -1 return 1
def sig_cmp u v order if u[1] > v[1] return -1 if u[1] v[1] if order u[0] < order v[0] return -1 return 1
def getYIntersectionInsideYSegment segmentFirstY segmentSecondY beginComplex endComplex x yIntersection euclidean getYIntersectionIfExists beginComplex endComplex x if yIntersection None return Noneif yIntersection < min segmentFirstY segmentSecondY return Noneif yIntersection < max segmentFirstY segmentSecondY return yIntersectionreturn None
def _check_asset location asset_name content_location StaticContent compute_location location course_key asset_name try contentstore find content_location except NotFoundError return Falseelse return True
def _check_asset location asset_name content_location StaticContent compute_location location course_key asset_name try contentstore find content_location except NotFoundError return Falseelse return True
def findCheckerFactory authType for factory in findCheckerFactories if factory authType authType return factoryraise InvalidAuthType authType
def findCheckerFactory authType for factory in findCheckerFactories if factory authType authType return factoryraise InvalidAuthType authType
def findCheckerFactory authType for factory in findCheckerFactories if factory authType authType return factoryraise InvalidAuthType authType
def temporary_mount filesystem options None mountpoint temporary_directory try mounted_fs mount filesystem mountpoint options options except mountpoint remove raisereturn TemporaryMountedFilesystem fs mounted_fs
@register tag name 'wishlists_containing_product' def do_basket_form parse token tokens token split_contents if len tokens 5 raise template TemplateSyntaxError '%rtagusesthefollowingsyntax {%%wishlists_containing_productwishlistsproductasctx_var%%}' % tokens[0] wishlists_var product_var name_var tokens[1] tokens[2] tokens[4] return ProductWishlistsNode wishlists_var product_var name_var
def regions return get_regions 'sns' connection_cls SNSConnection
def job_file_for job return os path join job expt_dir 'jobs' '%08d pb' % job id
def delete_quantum_ports ports root_helper for port in ports if ip_lib device_exists port device ip_lib IPDevice port root_helper device link delete LOG info _ 'Delete%s' port
@receiver models signals post_save sender CreditCourse @receiver models signals post_delete sender CreditCourse def invalidate_credit_courses_cache sender **kwargs cache delete CreditCourse CREDIT_COURSES_CACHE_KEY
def service_update context service_id values return IMPL service_update context service_id values
def assert_no_dunder_name code_obj expr for name in code_obj co_names if '__' in name or name in _UNSAFE_ATTRIBUTES raise NameError 'Accesstoforbiddenname%r %r ' % name expr
@pytest fixturedef tabbed_browser_stubs stubs win_registry win_registry add_window 1 stubs [stubs TabbedBrowserStub stubs TabbedBrowserStub ]objreg register 'tabbed-browser' stubs[0] scope 'window' window 0 objreg register 'tabbed-browser' stubs[1] scope 'window' window 1 yield stubs objreg delete 'tabbed-browser' scope 'window' window 0 objreg delete 'tabbed-browser' scope 'window' window 1
@pytest fixturedef tabbed_browser_stubs stubs win_registry win_registry add_window 1 stubs [stubs TabbedBrowserStub stubs TabbedBrowserStub ]objreg register 'tabbed-browser' stubs[0] scope 'window' window 0 objreg register 'tabbed-browser' stubs[1] scope 'window' window 1 yield stubs objreg delete 'tabbed-browser' scope 'window' window 0 objreg delete 'tabbed-browser' scope 'window' window 1
def chunk_index_list indices chunks []chunk ''for index in indices if len chunk < 3072 if not chunk chunk indexelse chunk + ' ' + index else chunks append chunk split ' ' chunk indexchunks append chunk split ' ' return chunks
def chunk_index_list indices chunks []chunk ''for index in indices if len chunk < 3072 if not chunk chunk indexelse chunk + ' ' + index else chunks append chunk split ' ' chunk indexchunks append chunk split ' ' return chunks
def chunk_index_list indices chunks []chunk ''for index in indices if len chunk < 3072 if not chunk chunk indexelse chunk + ' ' + index else chunks append chunk split ' ' chunk indexchunks append chunk split ' ' return chunks
def erasable msg if color_enabled return _CLEAR_LINE + msg + _CURSUR_UP return msg
def erasable msg if color_enabled return _CLEAR_LINE + msg + _CURSUR_UP return msg
def erasable msg if color_enabled return _CLEAR_LINE + msg + _CURSUR_UP return msg
def erasable msg if color_enabled return _CLEAR_LINE + msg + _CURSUR_UP return msg
def mime_to_document_iters input_file boundary read_chunk_size 4096 doc_files iter_multipart_mime_documents input_file boundary read_chunk_size for i doc_file in enumerate doc_files headers parse_mime_headers doc_file yield headers doc_file
def mime_to_document_iters input_file boundary read_chunk_size 4096 doc_files iter_multipart_mime_documents input_file boundary read_chunk_size for i doc_file in enumerate doc_files headers parse_mime_headers doc_file yield headers doc_file
def mime_to_document_iters input_file boundary read_chunk_size 4096 doc_files iter_multipart_mime_documents input_file boundary read_chunk_size for i doc_file in enumerate doc_files headers parse_mime_headers doc_file yield headers doc_file
def extract_bits data shift length bitmask 1 << length - 1 << shift return data & bitmask >> shift
def attach_dummy_node node name object _marker enode EmptyNode enode object object_attach_local_node node enode name
def attach_dummy_node node name object _marker enode EmptyNode enode object object_attach_local_node node enode name
def setup_platform hass config add_devices_callback discovery_info None import pywemo discovery as discoveryif discovery_info is not None location discovery_info[2]mac discovery_info[3]device discovery device_from_description location mac if device add_devices_callback [WemoBinarySensor device ]
def setup_platform hass config add_devices_callback discovery_info None import pywemo discovery as discoveryif discovery_info is not None location discovery_info[2]mac discovery_info[3]device discovery device_from_description location mac if device add_devices_callback [WemoBinarySensor device ]
def _strip_additional_query_parameters schema req additional_properties schema get 'addtionalProperties' True pattern_regexes []patterns schema get 'patternProperties' None if patterns for regex in patterns pattern_regexes append re compile regex if additional_properties for param in set req GET keys if param not in schema['properties'] keys if not list regex for regex in pattern_regexes if regex match param del req GET[param]
def test_scientific_notation loaded load 'a {a 1e3 b 1 E4 c -1e-3 d 2 3e+4 e 2e4 f 23 53e1 g 32284 2e+9 h 2 333993939e-3}' assert isinstance loaded['a']['a'] float assert isinstance loaded['a']['b'] float assert isinstance loaded['a']['c'] float assert isinstance loaded['a']['d'] float assert isinstance loaded['a']['e'] float assert isinstance loaded['a']['f'] float assert isinstance loaded['a']['g'] float assert isinstance loaded['a']['h'] float
def test_scientific_notation loaded load 'a {a 1e3 b 1 E4 c -1e-3 d 2 3e+4 e 2e4 f 23 53e1 g 32284 2e+9 h 2 333993939e-3}' assert isinstance loaded['a']['a'] float assert isinstance loaded['a']['b'] float assert isinstance loaded['a']['c'] float assert isinstance loaded['a']['d'] float assert isinstance loaded['a']['e'] float assert isinstance loaded['a']['f'] float assert isinstance loaded['a']['g'] float assert isinstance loaded['a']['h'] float
def test_scientific_notation loaded load 'a {a 1e3 b 1 E4 c -1e-3 d 2 3e+4 e 2e4 f 23 53e1 g 32284 2e+9 h 2 333993939e-3}' assert isinstance loaded['a']['a'] float assert isinstance loaded['a']['b'] float assert isinstance loaded['a']['c'] float assert isinstance loaded['a']['d'] float assert isinstance loaded['a']['e'] float assert isinstance loaded['a']['f'] float assert isinstance loaded['a']['g'] float assert isinstance loaded['a']['h'] float
def twiny ax None if ax is None ax gca ax1 ax twiny return ax1
def twiny ax None if ax is None ax gca ax1 ax twiny return ax1
def flipwritable fn mode None if os access fn os W_OK return Noneold_mode os stat fn st_modeos chmod fn stat S_IWRITE old_mode return old_mode
def flipwritable fn mode None if os access fn os W_OK return Noneold_mode os stat fn st_modeos chmod fn stat S_IWRITE old_mode return old_mode
@plugins command 'new' @click argument 'plugin_identifier' callback check_cookiecutter @click option '--template' '-t' type click STRING default 'https //github com/sh4nks/cookiecutter-flaskbb-plugin' help 'Pathtoacookiecuttertemplateortoavalidgitrepo ' def new_plugin plugin_identifier template out_dir os path join current_app root_path 'plugins' plugin_identifier click secho '[+]Creatingnewplugin{}' format plugin_identifier fg 'cyan' cookiecutter template output_dir out_dir click secho '[+]Done Createdin{}' format out_dir fg 'green' bold True
def get_constr_expr lh_op rh_op if rh_op is None return lh_opelse return sum_expr [lh_op neg_expr rh_op ]
def GetAvailabilityZones region ec2 _Connect region return [z name for z in ec2 get_all_zones ]
def slugify_uniquely_for_queryset value queryset slugfield 'slug' suffix 0potential base django_slugify unidecode value if len potential 0 potential 'null'while True if suffix potential '-' join [base str suffix ] if not queryset filter **{slugfield potential} exists return potentialsuffix + 1
def slugify_uniquely_for_queryset value queryset slugfield 'slug' suffix 0potential base django_slugify unidecode value if len potential 0 potential 'null'while True if suffix potential '-' join [base str suffix ] if not queryset filter **{slugfield potential} exists return potentialsuffix + 1
def slugify_uniquely_for_queryset value queryset slugfield 'slug' suffix 0potential base django_slugify unidecode value if len potential 0 potential 'null'while True if suffix potential '-' join [base str suffix ] if not queryset filter **{slugfield potential} exists return potentialsuffix + 1
@_ensure_existsdef start name if _sd_version > 219 ret _machinectl 'start{0}' format name else cmd 'systemctlstartsystemd-nspawn@{0}' format name ret __salt__['cmd run_all'] cmd python_shell False if ret['retcode'] 0 __context__['retcode'] salt defaults exitcodes EX_UNAVAILABLEreturn Falsereturn True
def is_rel s if len s 0 return Trueelif all isinstance el tuple for el in s and len max s len min s return Trueelse raise ValueError u'Set%rcontainssequencesofdifferentlengths' % s
def is_rel s if len s 0 return Trueelif all isinstance el tuple for el in s and len max s len min s return Trueelse raise ValueError u'Set%rcontainssequencesofdifferentlengths' % s
def _get_frequency_grid frequency assume_regular_frequency False frequency np asarray frequency if frequency ndim 1 raise ValueError 'frequencygridmustbe1dimensional' elif len frequency 1 return frequency[0] frequency[0] 1 elif not assume_regular_frequency or _is_regular frequency raise ValueError 'frequencymustbearegulargrid' return frequency[0] frequency[1] - frequency[0] len frequency
def _get_frequency_grid frequency assume_regular_frequency False frequency np asarray frequency if frequency ndim 1 raise ValueError 'frequencygridmustbe1dimensional' elif len frequency 1 return frequency[0] frequency[0] 1 elif not assume_regular_frequency or _is_regular frequency raise ValueError 'frequencymustbearegulargrid' return frequency[0] frequency[1] - frequency[0] len frequency
def kill timeout 15 ret {'killed' None 'retcode' 1}comment []pid __grains__ get 'pid' if not pid comment append 'Unabletofind"pid"ingrains' ret['retcode'] salt defaults exitcodes EX_SOFTWAREelif 'ps kill_pid' not in __salt__ comment append 'Missingcommand ps kill_pid' ret['retcode'] salt defaults exitcodes EX_SOFTWAREelse ret['retcode'] int not __salt__['ps kill_pid'] pid if ret['retcode'] comment append 'ps kill_pidfailed' else for _ in range timeout time sleep 1 signaled __salt__['ps kill_pid'] pid if not signaled ret['killed'] pidbreakelse comment append 'Timedoutwaitingforminiontoexit' ret['retcode'] salt defaults exitcodes EX_TEMPFAILif comment ret['comment'] commentreturn ret
def start_file fname id_ None if isinstance fname string_types if op splitext fname [1] lower ' gz' logger debug 'Writingusinggzip' fid GzipFile fname 'wb' compresslevel 2 else logger debug 'WritingusingnormalI/O' fid open fname 'wb' else logger debug 'Writingusing%sI/O' % type fname fid fnamefid seek 0 write_id fid FIFF FIFF_FILE_ID id_ write_int fid FIFF FIFF_DIR_POINTER -1 write_int fid FIFF FIFF_FREE_LIST -1 return fid
def start_file fname id_ None if isinstance fname string_types if op splitext fname [1] lower ' gz' logger debug 'Writingusinggzip' fid GzipFile fname 'wb' compresslevel 2 else logger debug 'WritingusingnormalI/O' fid open fname 'wb' else logger debug 'Writingusing%sI/O' % type fname fid fnamefid seek 0 write_id fid FIFF FIFF_FILE_ID id_ write_int fid FIFF FIFF_DIR_POINTER -1 write_int fid FIFF FIFF_FREE_LIST -1 return fid
@signals contributor_removed connectdef remove_contributor_from_subscriptions node user if user _id not in node admin_contributor_ids node_subscriptions get_all_node_subscriptions user node for subscription in node_subscriptions subscription remove_user_from_subscription user
def volume_update_db context volume_id host now timeutils utcnow values {'host' host 'scheduled_at' now}return db volume_update context volume_id values
def untar_backup_files source logging info "Untarringbackupfile'{0}' " format source try tar tarfile open source 'r gz' tar extractall path '/' tar close except tarfile TarError as tar_error logging exception tar_error raise backup_exceptions BRException "Exceptionwhileuntarringbackupfile'{0}' " format source logging info "Doneuntarring'{0}' " format source
def app from nltk grammar import CFGgrammar CFG fromstring "\n#Grammaticalproductions \nS->NPVP\nNP->DetNPP DetN\nVP->VNPPP VNP V\nPP->PNP\n#Lexicalproductions \nNP->'I'\nDet->'the' 'a'\nN->'man' 'park' 'dog' 'telescope'\nV->'ate' 'saw'\nP->'in' 'under' 'with'\n" sent 'thedogsawamaninthepark' split RecursiveDescentApp grammar sent mainloop
def possibly_scored usage_key return usage_key block_type in _block_types_possibly_scored
def include_enabled_extensions settings from django db models loading import load_appfrom django db import DatabaseErrorfrom reviewboard extensions base import get_extension_managertry manager get_extension_manager except DatabaseError returnfor extension in manager get_enabled_extensions load_app extension info app_name
def include_enabled_extensions settings from django db models loading import load_appfrom django db import DatabaseErrorfrom reviewboard extensions base import get_extension_managertry manager get_extension_manager except DatabaseError returnfor extension in manager get_enabled_extensions load_app extension info app_name
def include_enabled_extensions settings from django db models loading import load_appfrom django db import DatabaseErrorfrom reviewboard extensions base import get_extension_managertry manager get_extension_manager except DatabaseError returnfor extension in manager get_enabled_extensions load_app extension info app_name
def image_snapshot_delete call None kwargs None if call 'function' raise SaltCloudSystemExit 'Theimage_snapshot_deletefunctionmustbecalledwith-for--function ' if kwargs is None kwargs {}image_id kwargs get 'image_id' None image_name kwargs get 'image_name' None snapshot_id kwargs get 'snapshot_id' None if snapshot_id is None raise SaltCloudSystemExit "Theimage_snapshot_deletefunctionrequiresa'snapshot_id'tobeprovided " if image_id if image_name log warning "Boththe'image_id'and'image_name'argumentswereprovided 'image_id'willtakeprecedence " elif image_name image_id get_image_id kwargs {'name' image_name} else raise SaltCloudSystemExit "Theimage_snapshot_deletefunctionrequireseitheran'image_id'ora'image_name'tobeprovided " server user password _get_xml_rpc auth ' ' join [user password] response server one image snapshotdelete auth int image_id int snapshot_id data {'action' 'image snapshotdelete' 'deleted' response[0] 'snapshot_id' response[1] 'error_code' response[2]}return data
def subclass_exception name parent module return type bytes_if_py2 name parent {u'__module__' module}
def _pecl command defaults False cmdline 'pecl{0}' format command if salt utils is_true defaults cmdline "yes''" + ' ' + cmdline ret __salt__['cmd run_all'] cmdline python_shell True if ret['retcode'] 0 return ret['stdout']else log error 'Problemrunningpecl Isphp-pearinstalled?' return ''
def _pecl command defaults False cmdline 'pecl{0}' format command if salt utils is_true defaults cmdline "yes''" + ' ' + cmdline ret __salt__['cmd run_all'] cmdline python_shell True if ret['retcode'] 0 return ret['stdout']else log error 'Problemrunningpecl Isphp-pearinstalled?' return ''
def get_saml_provider name region None key None keyid None profile None conn _get_conn region region key key keyid keyid profile profile try provider conn get_saml_provider name return provider['get_saml_provider_response']['get_saml_provider_result']['saml_metadata_document']except boto exception BotoServerError as e aws __utils__['boto get_error'] e log debug aws msg 'FailedtogetSAMLproviderdocument 'log error msg return False
def cert_get_domains cert domains []try ext cert extensions get_extension_for_oid x509 OID_SUBJECT_ALTERNATIVE_NAME entries ext value get_values_for_type x509 DNSName for entry in entries domains append entry except Exception as e app logger warning 'FailedtogetSubjectAltName {0}' format e return domains
def cert_get_domains cert domains []try ext cert extensions get_extension_for_oid x509 OID_SUBJECT_ALTERNATIVE_NAME entries ext value get_values_for_type x509 DNSName for entry in entries domains append entry except Exception as e app logger warning 'FailedtogetSubjectAltName {0}' format e return domains
def cycle_colors chunk palette DEFAULT_PALETTE colors []g itertools cycle palette for i in range len chunk colors append next g return colors
def _limit_discount_amount_by_min_price line order_source shop_product line product get_shop_instance order_source shop if shop_product minimum_price min_total shop_product minimum_price value * line quantity base_price line base_unit_price value * line quantity if base_price - line discount_amount value < min_total line discount_amount order_source create_price base_price - min_total
def _limit_discount_amount_by_min_price line order_source shop_product line product get_shop_instance order_source shop if shop_product minimum_price min_total shop_product minimum_price value * line quantity base_price line base_unit_price value * line quantity if base_price - line discount_amount value < min_total line discount_amount order_source create_price base_price - min_total
def skip_under_travis fn None if _travisTesting skip msg pytest skip 'CannotbetestedunderTravis-CI' if fn is not None def _inner skip msg _inner __name__ fn __name__return _innerelse skip msg else return fn
def skip_under_travis fn None if _travisTesting skip msg pytest skip 'CannotbetestedunderTravis-CI' if fn is not None def _inner skip msg _inner __name__ fn __name__return _innerelse skip msg else return fn
def set_builddefaults_facts facts if 'builddefaults' in facts builddefaults facts['builddefaults']common facts['common']if 'http_proxy' not in builddefaults and 'http_proxy' in common builddefaults['http_proxy'] common['http_proxy']if 'https_proxy' not in builddefaults and 'https_proxy' in common builddefaults['https_proxy'] common['https_proxy']if 'no_proxy' not in builddefaults and 'no_proxy' in common builddefaults['no_proxy'] common['no_proxy']if 'git_http_proxy' not in builddefaults and 'http_proxy' in builddefaults builddefaults['git_http_proxy'] builddefaults['http_proxy']if 'git_https_proxy' not in builddefaults and 'https_proxy' in builddefaults builddefaults['git_https_proxy'] builddefaults['https_proxy']if 'git_no_proxy' not in builddefaults and 'no_proxy' in builddefaults builddefaults['git_no_proxy'] builddefaults['no_proxy']if 'config' in builddefaults if 'admission_plugin_config' not in facts['master'] facts['master']['admission_plugin_config'] dict facts['master']['admission_plugin_config'] update builddefaults['config'] delete_empty_keys facts['master']['admission_plugin_config']['BuildDefaults']['configuration']['env'] return facts
def get_model_name model return model _meta model_name
def validateOpfJsonValue value opfJsonSchemaFilename jsonSchemaPath os path join os path dirname __file__ 'jsonschema' opfJsonSchemaFilename jsonhelpers validate value schemaPath jsonSchemaPath return
def validateOpfJsonValue value opfJsonSchemaFilename jsonSchemaPath os path join os path dirname __file__ 'jsonschema' opfJsonSchemaFilename jsonhelpers validate value schemaPath jsonSchemaPath return
def validateOpfJsonValue value opfJsonSchemaFilename jsonSchemaPath os path join os path dirname __file__ 'jsonschema' opfJsonSchemaFilename jsonhelpers validate value schemaPath jsonSchemaPath return
def GetPrettyBytes bytes_num significant_digits 0 byte_prefixes ['' 'K' 'M' 'G' 'T' 'P' 'E']for i in range 0 7 exp i * 10 if bytes_num < 1 << exp + 10 if i 0 formatted_bytes str bytes_num else formatted_bytes '% *f' % significant_digits bytes_num * 1 0 / 1 << exp if formatted_bytes '1' plural 's'else plural ''return '%s%sByte%s' % formatted_bytes byte_prefixes[i] plural logging error 'Numbertoohightoconvert %d' bytes_num return 'Alot'
@db_api retry_if_session_inactive def _ensure_external_network_default_value_callback resource event trigger context request network is_default request get IS_DEFAULT False if event in events BEFORE_CREATE events BEFORE_UPDATE and is_default obj context session query ext_net_models ExternalNetwork filter_by is_default True first if obj and network['id'] obj network_id raise exceptions DefaultExternalNetworkExists net_id obj network_id obj context session query ext_net_models ExternalNetwork filter_by network_id network['id'] obj update {IS_DEFAULT is_default}
@db_api retry_if_session_inactive def _ensure_external_network_default_value_callback resource event trigger context request network is_default request get IS_DEFAULT False if event in events BEFORE_CREATE events BEFORE_UPDATE and is_default obj context session query ext_net_models ExternalNetwork filter_by is_default True first if obj and network['id'] obj network_id raise exceptions DefaultExternalNetworkExists net_id obj network_id obj context session query ext_net_models ExternalNetwork filter_by network_id network['id'] obj update {IS_DEFAULT is_default}
@db_api retry_if_session_inactive def _ensure_external_network_default_value_callback resource event trigger context request network is_default request get IS_DEFAULT False if event in events BEFORE_CREATE events BEFORE_UPDATE and is_default obj context session query ext_net_models ExternalNetwork filter_by is_default True first if obj and network['id'] obj network_id raise exceptions DefaultExternalNetworkExists net_id obj network_id obj context session query ext_net_models ExternalNetwork filter_by network_id network['id'] obj update {IS_DEFAULT is_default}
def select_backend name try mod __import__ name fromlist public_api except ImportError raiseexcept Exception as e import sysfrom zmq utils sixcerpt import reraiseexc_info sys exc_info reraise ImportError ImportError 'Importing%sfailedwith%s' % name e exc_info[2] ns {}for key in public_api ns[key] getattr mod key return ns
def _tgrep_parens_action _s _l tokens assert len tokens 3 assert tokens[0] u' ' assert tokens[2] u' ' return tokens[1]
def check_is_admin context init credentials context to_policy_values if ADMIN_CTX_POLICY not in _ENFORCER rules return Falsereturn _ENFORCER enforce ADMIN_CTX_POLICY credentials credentials
def interpolate_2d values method 'pad' axis 0 limit None fill_value None dtype None transf lambda x x if axis 0 else lambda x x T ndim values ndimif values ndim 1 if axis 0 raise AssertionError 'cannotinterpolateonandim 1withaxis 0' values values reshape tuple 1 + values shape if fill_value is None mask Noneelse mask mask_missing transf values fill_value method clean_fill_method method if method 'pad' values transf pad_2d transf values limit limit mask mask dtype dtype else values transf backfill_2d transf values limit limit mask mask dtype dtype if ndim 1 values values[0]return values
def _salt_configuration_error filename raise SaltConfigurationError 'Configurationerrorin{0}' format filename
def _lcs_ic synset1 synset2 ic verbose False if synset1 _pos synset2 _pos raise WordNetError u'Computingtheleastcommonsubsumerrequires' + u'%sand%stohavethesamepartofspeech ' % synset1 synset2 ic1 information_content synset1 ic ic2 information_content synset2 ic subsumers synset1 common_hypernyms synset2 if len subsumers 0 subsumer_ic 0else subsumer_ic max information_content s ic for s in subsumers if verbose print u'>LCSSubsumerbycontent ' subsumer_ic return ic1 ic2 subsumer_ic
def _lcs_ic synset1 synset2 ic verbose False if synset1 _pos synset2 _pos raise WordNetError u'Computingtheleastcommonsubsumerrequires' + u'%sand%stohavethesamepartofspeech ' % synset1 synset2 ic1 information_content synset1 ic ic2 information_content synset2 ic subsumers synset1 common_hypernyms synset2 if len subsumers 0 subsumer_ic 0else subsumer_ic max information_content s ic for s in subsumers if verbose print u'>LCSSubsumerbycontent ' subsumer_ic return ic1 ic2 subsumer_ic
def _lcs_ic synset1 synset2 ic verbose False if synset1 _pos synset2 _pos raise WordNetError u'Computingtheleastcommonsubsumerrequires' + u'%sand%stohavethesamepartofspeech ' % synset1 synset2 ic1 information_content synset1 ic ic2 information_content synset2 ic subsumers synset1 common_hypernyms synset2 if len subsumers 0 subsumer_ic 0else subsumer_ic max information_content s ic for s in subsumers if verbose print u'>LCSSubsumerbycontent ' subsumer_ic return ic1 ic2 subsumer_ic
def _cmp_by_igp_cost path1 path2 return None
def post_match view name style first second center bfr threshold if first is not None and second is not None diff first size - second size if diff > 0 first first move first begin first end - diff elif diff < 0 second second move second begin - diff second end return first second style
def post_match view name style first second center bfr threshold if first is not None and second is not None diff first size - second size if diff > 0 first first move first begin first end - diff elif diff < 0 second second move second begin - diff second end return first second style
def post_match view name style first second center bfr threshold if first is not None and second is not None diff first size - second size if diff > 0 first first move first begin first end - diff elif diff < 0 second second move second begin - diff second end return first second style
def post_match view name style first second center bfr threshold if first is not None and second is not None diff first size - second size if diff > 0 first first move first begin first end - diff elif diff < 0 second second move second begin - diff second end return first second style
def CyclicGroup n a list range 1 n a append 0 gen _af_new a G PermutationGroup [gen] G _is_abelian TrueG _is_nilpotent TrueG _is_solvable TrueG _degree nG _is_transitive TrueG _order nreturn G
@gen enginedef ListFilesAndDirs store pattern callback result yield gen Task ListAllCommonPrefixes store '/' prefix PrefixFromPattern pattern callback result
def dot inp matrix if 'int' in inp dtype and inp ndim 2 return matrix[inp flatten ]elif 'int' in inp dtype return matrix[inp]elif 'float' in inp dtype and inp ndim 3 shape0 inp shape[0]shape1 inp shape[1]shape2 inp shape[2]return TT dot inp reshape shape0 * shape1 shape2 matrix else return TT dot inp matrix
def call_split_lines x lineno None col None return ast Call func ast Attribute value x attr 'splitlines' ctx ast Load lineno lineno col_offset col args [] keywords [] starargs None kwargs None lineno lineno col_offset col
def standard_deviation input labels None index None return numpy sqrt variance input labels index
def _environment_sanity_check environment assert issubclass environment undefined Undefined 'undefinedmustbeasubclassofundefinedbecausefiltersdependonit 'assert environment block_start_string environment variable_start_string environment comment_start_string 'block variableandcommentstartstringsmustbedifferent'assert environment newline_sequence in '\r' '\r\n' '\n' 'newline_sequencesettounknownlineendingstring 'return environment
def sorting_dates start advertised_start announcement try start dateutil parser parse advertised_start if start tzinfo is None start start replace tzinfo utc except ValueError AttributeError start startnow datetime now utc return announcement start now
def populate_entry_points entry_points for entry_point in entry_points name entry_point nametry entry_point entry_point load except Exception as e warnings warn AstropyUserWarning u'{type}erroroccurredinentrypoint{name} ' format type type e __name__ name name else if not inspect isclass entry_point warnings warn AstropyUserWarning u'Modelingentrypoint{0}expectedtobeaClass ' format name elif issubclass entry_point Fitter name entry_point __name__globals [name] entry_point__all__ append name else warnings warn AstropyUserWarning u'Modelingentrypoint{0}expectedtoextendastropy modeling Fitter' format name
def Dagum name p a b return rv name DagumDistribution p a b
def test_path_issues source 'fromdatetimeimport'assert jedi Script source completions
def get_memcached_client expiration_time 0 if CONF cache enabled and CONF cache memcache_servers _warn_if_null_backend return CacheClient _get_default_cache_region expiration_time expiration_time
def _DefaultValueConstructorForField field if field label _FieldDescriptor LABEL_REPEATED if field has_default_value and field default_value [] raise ValueError 'Repeatedfielddefaultvaluenotemptylist %s' % field default_value if field cpp_type _FieldDescriptor CPPTYPE_MESSAGE message_type field message_typedef MakeRepeatedMessageDefault message return containers RepeatedCompositeFieldContainer message _listener_for_children field message_type return MakeRepeatedMessageDefaultelse type_checker type_checkers GetTypeChecker field def MakeRepeatedScalarDefault message return containers RepeatedScalarFieldContainer message _listener_for_children type_checker return MakeRepeatedScalarDefaultif field cpp_type _FieldDescriptor CPPTYPE_MESSAGE message_type field message_typedef MakeSubMessageDefault message result message_type _concrete_class result _SetListener message _listener_for_children return resultreturn MakeSubMessageDefaultdef MakeScalarDefault message return field default_valuereturn MakeScalarDefault
def generate_cert domain result __salt__['cmd run'] 'icinga2pkinew-cert--cn{0}--key/etc/icinga2/pki/{0} key--cert/etc/icinga2/pki/{0} crt' format domain return result
def get_pip_requirements fname os path join jasperpath LIB_PATH 'requirements txt' logger logging getLogger __name__ if os access fname os R_OK reqs list pip req parse_requirements fname logger debug "Found%dPIPrequirementsinfile'%s'" len reqs fname return reqselse logger debug "PIPrequirementsfile'%s'notfoundornotreadable" fname
def htons integer return ntohs integer
def get_app_qt4 *args **kwargs from IPython external qt_for_kernel import QtGuiapp QtGui QApplication instance if app is None if not args args [''] app QtGui QApplication *args **kwargs return app
@_helpers positional 3 def validate_token key token user_id action_id '' current_time None if not token return Falsetry decoded base64 urlsafe_b64decode token token_time int decoded split DELIMITER [ -1 ] except TypeError ValueError binascii Error return Falseif current_time is None current_time time time if current_time - token_time > DEFAULT_TIMEOUT_SECS return Falseexpected_token generate_token key user_id action_id action_id when token_time if len token len expected_token return Falsedifferent 0for x y in zip bytearray token bytearray expected_token different x ^ y return not different
def get_backend_api cluster_id backend_config_filename environ get 'FLOCKER_ACCEPTANCE_TEST_VOLUME_BACKEND_CONFIG' if backend_config_filename is None raise SkipTest 'ThistestrequirestheabilitytoconstructanIBlockDeviceAPIinordertoverifyconstruction PleasesetFLOCKER_ACCEPTANCE_TEST_VOLUME_BACKEND_CONFIGtoayamlfilepathwiththedatasetconfiguration ' backend_name environ get 'FLOCKER_ACCEPTANCE_VOLUME_BACKEND' if backend_name is None raise SkipTest 'Setacceptancetestingvolumebackendusingthe' + 'FLOCKER_ACCEPTANCE_VOLUME_BACKENDenvironmentvariable ' if backend_name in 'loopback' 'zfs' raise SkipTest "TheloopbackbackendAPIcan'tbeusedremotely " backend_config_filepath FilePath backend_config_filename full_backend_config yaml safe_load backend_config_filepath getContent backend_config full_backend_config get backend_name if 'backend' in backend_config backend_config pop 'backend' backend backend_loader get backend_name return get_api backend pmap backend_config reactor cluster_id
def test_validate_coord result conesearch _validate_coord ICRS 6 02233 * u degree -72 08144 * u degree np testing assert_allclose result [6 022330000000011 -72 08144 ] result conesearch _validate_coord SkyCoord 6 02233 * u degree -72 08144 * u degree frame u'icrs' np testing assert_allclose result [6 022330000000011 -72 08144 ] result conesearch _validate_coord 0 0 np testing assert_allclose result [0 0] result conesearch _validate_coord -1 -1 np testing assert_allclose result [359 -1 ]
def test_validate_coord result conesearch _validate_coord ICRS 6 02233 * u degree -72 08144 * u degree np testing assert_allclose result [6 022330000000011 -72 08144 ] result conesearch _validate_coord SkyCoord 6 02233 * u degree -72 08144 * u degree frame u'icrs' np testing assert_allclose result [6 022330000000011 -72 08144 ] result conesearch _validate_coord 0 0 np testing assert_allclose result [0 0] result conesearch _validate_coord -1 -1 np testing assert_allclose result [359 -1 ]
def test_validate_coord result conesearch _validate_coord ICRS 6 02233 * u degree -72 08144 * u degree np testing assert_allclose result [6 022330000000011 -72 08144 ] result conesearch _validate_coord SkyCoord 6 02233 * u degree -72 08144 * u degree frame u'icrs' np testing assert_allclose result [6 022330000000011 -72 08144 ] result conesearch _validate_coord 0 0 np testing assert_allclose result [0 0] result conesearch _validate_coord -1 -1 np testing assert_allclose result [359 -1 ]
def has_subdirectories path include exclude show_all try return len listdir path include exclude show_all folders_only True > 1 except IOError OSError return False
def add_blob_owner bucket_name blob_name user_email storage_client storage Client bucket storage_client bucket bucket_name blob bucket blob blob_name blob acl reload blob acl user user_email grant_owner blob acl save print 'Addeduser{}asanowneronblob{}inbucket{} ' format user_email blob_name bucket_name
def load_ndarray_transfer name assert name in ['avicenna' 'harry' 'rita' 'sylvester' 'terry' 'ule'] fname os path join preprocess '${PYLEARN2_DATA_PATH}' 'UTLC' 'filetensor' name + '_transfer ft' transfer load_filetensor fname return transfer
def inf_to_nan func def wrap *a **kw v func *a **kw if not np isfinite v return np nanreturn vreturn wrap
def het_breuschpagan resid exog_het x np asarray exog_het y np asarray resid ** 2 nobs nvars x shaperesols OLS y x fit fval resols fvaluefpval resols f_pvaluelm nobs * resols rsquared return lm stats chi2 sf lm nvars - 1 fval fpval
def het_breuschpagan resid exog_het x np asarray exog_het y np asarray resid ** 2 nobs nvars x shaperesols OLS y x fit fval resols fvaluefpval resols f_pvaluelm nobs * resols rsquared return lm stats chi2 sf lm nvars - 1 fval fpval
@register render_tagdef dashboard_column context token column_index int token split_contents [1] output []for tag in settings DASHBOARD_TAGS[column_index] t Template u'{%%load%s%%}{%%%s%%}' % tuple tag split u' ' output append t render context return u'' join output
def get_all_controllers try result utils run 'lssubsys' ignore_status False controllers_str result stdout strip controller_list []for controller in controllers_str splitlines controller_sub_list controller split ' ' controller_list + controller_sub_listexcept error CmdError controller_list ['cpuacct' 'cpu' 'memory' 'cpuset' 'devices' 'freezer' 'blkio' 'netcls']return controller_list
def get_all_controllers try result utils run 'lssubsys' ignore_status False controllers_str result stdout strip controller_list []for controller in controllers_str splitlines controller_sub_list controller split ' ' controller_list + controller_sub_listexcept error CmdError controller_list ['cpuacct' 'cpu' 'memory' 'cpuset' 'devices' 'freezer' 'blkio' 'netcls']return controller_list
def get_all_controllers try result utils run 'lssubsys' ignore_status False controllers_str result stdout strip controller_list []for controller in controllers_str splitlines controller_sub_list controller split ' ' controller_list + controller_sub_listexcept error CmdError controller_list ['cpuacct' 'cpu' 'memory' 'cpuset' 'devices' 'freezer' 'blkio' 'netcls']return controller_list
def _chkconfig_add name cmd '/sbin/chkconfig--add{0}' format name if __salt__['cmd retcode'] cmd python_shell False 0 log info 'Addedinitscript"{0}"tochkconfig' format name return Trueelse log error 'Unabletoaddinitscript"{0}"tochkconfig' format name return False
@parse_data@set_databasedef get_content_items ids None **kwargs if ids values Item select where Item id in_ ids else values Item select return values
def delete_temp_backups older_than 24 file_list os listdir get_backup_path for this_file in file_list this_file_path os path join get_backup_path this_file if is_file_old this_file_path older_than os remove this_file_path
def sendRobust signal Any sender Anonymous *arguments **named responses []for receiver in liveReceivers getAllReceivers sender signal try response robustApply receiver signal signal sender sender *arguments **named except Exception as err responses append receiver err else responses append receiver response return responses
def sendRobust signal Any sender Anonymous *arguments **named responses []for receiver in liveReceivers getAllReceivers sender signal try response robustApply receiver signal signal sender sender *arguments **named except Exception as err responses append receiver err else responses append receiver response return responses
def sendRobust signal Any sender Anonymous *arguments **named responses []for receiver in liveReceivers getAllReceivers sender signal try response robustApply receiver signal signal sender sender *arguments **named except Exception as err responses append receiver err else responses append receiver response return responses
def background im size background None **kwargs if not background return imif not size[0] or not size[1] return im x y im sizeif x > size[0] and y > size[1] return imim colorspace im replace_alpha background **kwargs new_im Image new 'RGB' size background if new_im mode im mode new_im new_im convert im mode offset size[0] - x // 2 size[1] - y // 2 new_im paste im offset return new_im
def add_default version None overwrite False plugins module_loader all path_only True plugins os path splitext os path basename p [0] p for p in plugins plugins p for p in plugins if p[0] not in NONMODULE_MODULE_NAMES processed set diagnostic_messages []for name filename in info for info in plugins if info[0] not in processed try write_metadata filename DEFAULT_METADATA version overwrite except ParseError as e diagnostic_messages append e args[0] continueprocessed add name if diagnostic_messages pprint diagnostic_messages return 0
def add_default version None overwrite False plugins module_loader all path_only True plugins os path splitext os path basename p [0] p for p in plugins plugins p for p in plugins if p[0] not in NONMODULE_MODULE_NAMES processed set diagnostic_messages []for name filename in info for info in plugins if info[0] not in processed try write_metadata filename DEFAULT_METADATA version overwrite except ParseError as e diagnostic_messages append e args[0] continueprocessed add name if diagnostic_messages pprint diagnostic_messages return 0
def add_default version None overwrite False plugins module_loader all path_only True plugins os path splitext os path basename p [0] p for p in plugins plugins p for p in plugins if p[0] not in NONMODULE_MODULE_NAMES processed set diagnostic_messages []for name filename in info for info in plugins if info[0] not in processed try write_metadata filename DEFAULT_METADATA version overwrite except ParseError as e diagnostic_messages append e args[0] continueprocessed add name if diagnostic_messages pprint diagnostic_messages return 0
def make_auth_headers if not os path exists ' appveyor token' raise RuntimeError 'Pleasecreateafilenamed` appveyor token`inthecurrentdirectory Youcangetthetokenfromhttps //ci appveyor com/api-token' with open ' appveyor token' as f token f read strip headers {'Authorization' 'Bearer{}' format token }return headers
@pytest fixturedef super_moderator_user default_groups user User username 'test_super_mod' email 'test_super@example org' password 'test' primary_group default_groups[1] activated True user save return user
def get_file_original_name name re_match re findall ' *_sep_ * ' name if re_match return re_match[0]else return 'Notvalid'
def ulp_abs_check expected got ulp_tol abs_tol ulp_error abs to_ulps expected - to_ulps got abs_error abs expected - got if abs_error < abs_tol or ulp_error < ulp_tol return Noneelse fmt 'error { 3g} { d}ulps permittederror { 3g}or{ d}ulps'return fmt format abs_error ulp_error abs_tol ulp_tol
def test_formatted_decimal_field class TestModelForm ModelForm class Meta model Productfields ['width']values ['0E-9' '0E-30' '1E-9' '123E-10' '-123E-10' '1 12345666666666E20']for value in values product Product width Decimal value form TestModelForm instance product rendered_form force_text form rendered_value re search 'value " *? "' rendered_form group 1 rendered_step re search 'step " *? "' rendered_form group 1 assert rendered_value and 'E' not in rendered_value assert rendered_step and 'E' not in rendered_step large_value '1 23E-10000'product Product width Decimal large_value with pytest raises ValueError form TestModelForm instance product
def test_formatted_decimal_field class TestModelForm ModelForm class Meta model Productfields ['width']values ['0E-9' '0E-30' '1E-9' '123E-10' '-123E-10' '1 12345666666666E20']for value in values product Product width Decimal value form TestModelForm instance product rendered_form force_text form rendered_value re search 'value " *? "' rendered_form group 1 rendered_step re search 'step " *? "' rendered_form group 1 assert rendered_value and 'E' not in rendered_value assert rendered_step and 'E' not in rendered_step large_value '1 23E-10000'product Product width Decimal large_value with pytest raises ValueError form TestModelForm instance product
def test_formatted_decimal_field class TestModelForm ModelForm class Meta model Productfields ['width']values ['0E-9' '0E-30' '1E-9' '123E-10' '-123E-10' '1 12345666666666E20']for value in values product Product width Decimal value form TestModelForm instance product rendered_form force_text form rendered_value re search 'value " *? "' rendered_form group 1 rendered_step re search 'step " *? "' rendered_form group 1 assert rendered_value and 'E' not in rendered_value assert rendered_step and 'E' not in rendered_step large_value '1 23E-10000'product Product width Decimal large_value with pytest raises ValueError form TestModelForm instance product
def test_formatted_decimal_field class TestModelForm ModelForm class Meta model Productfields ['width']values ['0E-9' '0E-30' '1E-9' '123E-10' '-123E-10' '1 12345666666666E20']for value in values product Product width Decimal value form TestModelForm instance product rendered_form force_text form rendered_value re search 'value " *? "' rendered_form group 1 rendered_step re search 'step " *? "' rendered_form group 1 assert rendered_value and 'E' not in rendered_value assert rendered_step and 'E' not in rendered_step large_value '1 23E-10000'product Product width Decimal large_value with pytest raises ValueError form TestModelForm instance product
def user_is_system_admin user_db return user_has_role user_db user_db role SystemRole SYSTEM_ADMIN
def option_from_wire otype wire current olen cls get_option_class otype return cls from_wire otype wire current olen
@fixturedef patch_network_functions monkeypatch import inbox actions backendsfor backend in inbox actions backends module_registry values for method_name in backend __all__ monkeypatch setattr backend __name__ + ' ' + method_name lambda *args **kwargs None
@fixturedef patch_network_functions monkeypatch import inbox actions backendsfor backend in inbox actions backends module_registry values for method_name in backend __all__ monkeypatch setattr backend __name__ + ' ' + method_name lambda *args **kwargs None
@fixturedef patch_network_functions monkeypatch import inbox actions backendsfor backend in inbox actions backends module_registry values for method_name in backend __all__ monkeypatch setattr backend __name__ + ' ' + method_name lambda *args **kwargs None
@core_helperdef flash_success message allow_html False flash message category 'alert-success' allow_html allow_html
def IQR xs cdf Cdf xs return cdf Value 0 25 cdf Value 0 75
def bucket_copy self CopySource Key ExtraArgs None Callback None SourceClient None Config None return self meta client copy CopySource CopySource Bucket self name Key Key ExtraArgs ExtraArgs Callback Callback SourceClient SourceClient Config Config
def mro_lookup cls attr stop set monkey_patched [] for node in cls mro if node in stop try value node __dict__[attr]module_origin value __module__except AttributeError KeyError passelse if module_origin not in monkey_patched return nodereturnif attr in node __dict__ return node
def mro_lookup cls attr stop set monkey_patched [] for node in cls mro if node in stop try value node __dict__[attr]module_origin value __module__except AttributeError KeyError passelse if module_origin not in monkey_patched return nodereturnif attr in node __dict__ return node
def create_shadow_trigger trigger_type_db trigger_type_ref trigger_type_db get_reference refif trigger_type_db parameters_schema LOG debug 'SkipshadowtriggerforTriggerTypewithparameters%s ' trigger_type_ref return Nonetrigger {'name' trigger_type_db name 'pack' trigger_type_db pack 'type' trigger_type_ref 'parameters' {}}return create_or_update_trigger_db trigger
def detrend x order 1 axis 0 if x ndim 2 and int axis 1 x x Telif x ndim > 2 raise NotImplementedError 'x ndim>2isnotimplementeduntilitisneeded' nobs x shape[0]if order 0 resid x - x mean axis 0 else trends np vander np arange float nobs N order + 1 beta np linalg pinv trends dot x resid x - np dot trends beta if x ndim 2 and int axis 1 resid resid Treturn resid
def article_url content return '{content settings[SITEURL]}/{content url}' format content content encode 'utf-8'
def _get_test_cluster reactor control_node environ get 'FLOCKER_ACCEPTANCE_CONTROL_NODE' if control_node is None raise SkipTest 'SetacceptancetestingcontrolnodeIPaddressusingthe' + 'FLOCKER_ACCEPTANCE_CONTROL_NODEenvironmentvariable ' agent_nodes_env_var environ get 'FLOCKER_ACCEPTANCE_NUM_AGENT_NODES' if agent_nodes_env_var is None raise SkipTest 'SetthenumberofconfiguredacceptancetestingnodesusingtheFLOCKER_ACCEPTANCE_NUM_AGENT_NODESenvironmentvariable ' num_agent_nodes int agent_nodes_env_var certificates_path FilePath environ['FLOCKER_ACCEPTANCE_API_CERTIFICATES_PATH'] hostname_to_public_address_env_var environ get 'FLOCKER_ACCEPTANCE_HOSTNAME_TO_PUBLIC_ADDRESS' '{}' hostname_to_public_address json loads hostname_to_public_address_env_var return connected_cluster reactor control_node certificates_path num_agent_nodes hostname_to_public_address
def elide_filename filename length elidestr ' 'if length < len elidestr raise ValueError 'lengthmustbegreaterorequalto3' if len filename < length return filenamelength - len elidestr left length // 2 right length - left if right 0 return filename[ left] + elidestr else return filename[ left] + elidestr + filename[ - right ]
@app template_filter 'external_url' def external_url url url_pattern '^ https? // *$'scheme re match url_pattern url if not scheme url_root request url_root rstrip '/' return '{}{}' format url_root url else return url
def set_verbose verbose global _verbose_verbose verbose
def merge *dicts **kwargs if len dicts 1 and not isinstance dicts[0] dict dicts dicts[0]factory _get_factory merge kwargs rv factory for d in dicts rv update d return rv
def escapedData data inAttribute if isinstance data unicode data data encode 'utf-8' data data replace '&' '&amp ' replace '<' '&lt ' replace '>' '&gt ' if inAttribute data data replace '"' '&quot ' return data
def guess_language text try from guess_language import guessLanguagereturn Language guessLanguage text except ImportError log error u'Cannotdetectthelanguageofthegiventextbody missingdependency guess-language' log error u'PleaseinstallitfromPyPI bydoingeg pipinstallguess-language' return UNDETERMINED
def _parse_selections dpkgselection ret {}if isinstance dpkgselection six string_types dpkgselection dpkgselection split '\n' for line in dpkgselection if line _pkg _state line split if _state in ret ret[_state] append _pkg else ret[_state] [_pkg]return ret
def pretty_depth_cv depth import cvdepth pretty_depth depth image cv CreateImageHeader depth shape[1] depth shape[0] cv IPL_DEPTH_8U 1 cv SetData image depth tostring depth dtype itemsize * depth shape[1] return image
def pretty_depth_cv depth import cvdepth pretty_depth depth image cv CreateImageHeader depth shape[1] depth shape[0] cv IPL_DEPTH_8U 1 cv SetData image depth tostring depth dtype itemsize * depth shape[1] return image
def test_find_number_6 s 'query1e5notfound'r find_number s assert s[r[0] r[1]] '1e5'
def getOverhangAngle elementNode return getCascadeFloatWithoutSelf 45 0 elementNode 'overhangAngle'
def get_occupied_streams realm subs_filter Subscription objects filter active True user_profile__realm realm user_profile__is_active True values 'recipient_id' stream_ids Recipient objects filter type Recipient STREAM id__in subs_filter values 'type_id' return Stream objects filter id__in stream_ids realm realm deactivated False
def evennia_version version 'Unknown'try import evenniaversion evennia __version__except ImportError passtry rev check_output 'gitrev-parse--shortHEAD' shell True cwd EVENNIA_ROOT stderr STDOUT strip version '%s rev%s ' % version rev except IOError CalledProcessError passreturn version
def legacy_mapping block_device_mapping legacy_block_device_mapping []for bdm in block_device_mapping try legacy_block_device BlockDeviceDict bdm legacy except exception InvalidBDMForLegacy continuelegacy_block_device_mapping append legacy_block_device for i dev in enumerate dev for dev in legacy_block_device_mapping if dev['virtual_name'] and is_ephemeral dev['virtual_name'] dev['virtual_name'] dev['virtual_name'][ -1 ] + str i return legacy_block_device_mapping
def debug brule file None if not file from sys import stdoutfile stdoutdef write brl expr result file write 'Rule %s\n' % get_function_name brl file write 'In %s\nOut %s\n\n' % expr result return onaction brule write
def debug brule file None if not file from sys import stdoutfile stdoutdef write brl expr result file write 'Rule %s\n' % get_function_name brl file write 'In %s\nOut %s\n\n' % expr result return onaction brule write
def run *args **kwargs puppet _Puppet buildargs for arg in range len args if args[arg] in ['agent' 'apply'] puppet subcmd args[arg]else buildargs + args[arg] puppet arguments buildargs puppet kwargs update salt utils clean_kwargs **kwargs ret __salt__['cmd run_all'] repr puppet python_shell puppet useshell if ret['retcode'] in [0 2] ret['retcode'] 0else ret['retcode'] 1return ret
def test_missing_keyword_argument_value try can_compile u' fn[x]x x ' except HyTypeError as e assert e message u'Keywordargument xneedsavalue ' else assert False
def absent profile 'pagerduty' subdomain None api_key None **kwargs r __salt__['pagerduty_util resource_absent'] 'schedules' ['name' 'id'] profile subdomain api_key **kwargs return r
@pytest mark parametrize 'parallel' [True False] def test_conversion parallel read_basic text '\nABCDE\n1a345\n2 1910-5 3e4\n42-12 4six\n'table read_basic text parallel parallel assert_equal table['A'] dtype kind 'f' assert table['B'] dtype kind in 'S' 'U' assert_equal table['C'] dtype kind 'i' assert_equal table['D'] dtype kind 'f' assert table['E'] dtype kind in 'S' 'U'
def run_server application port sock eventlet listen '0 0 0 0' port eventlet wsgi server sock application
def run_server application port sock eventlet listen '0 0 0 0' port eventlet wsgi server sock application
def set_range_metadata builder load lower_bound upper_bound range_operands [Constant int load type lower_bound Constant int load type upper_bound ]md builder module add_metadata range_operands load set_metadata 'range' md
def set_range_metadata builder load lower_bound upper_bound range_operands [Constant int load type lower_bound Constant int load type upper_bound ]md builder module add_metadata range_operands load set_metadata 'range' md
def split_ranges mask ranges [ 0 len mask ]for pos val in enumerate mask if not val r ranges pop if pos > r[0] yield r[0] pos if pos + 1 < len mask ranges append pos + 1 len mask if ranges yield ranges[ -1 ]
def split_ranges mask ranges [ 0 len mask ]for pos val in enumerate mask if not val r ranges pop if pos > r[0] yield r[0] pos if pos + 1 < len mask ranges append pos + 1 len mask if ranges yield ranges[ -1 ]
def _delete_rpm_probes probes return __salt__['probes delete_probes'] _ordered_dict_to_dict probes commit False
def call_lights *args **kwargs res dict lights _get_lights for dev_id in 'id' in kwargs and _get_devices kwargs or sorted lights keys if lights get str dev_id res[dev_id] lights[str dev_id ]return res or False
def get_items arg items []for count bookmark in enumerate get_results arg items append alp Item title bookmark title subtitle bookmark path arg bookmark path icon 'CloneRepoIcon png' return items
def combined_levels dimensions default_only False groups []for dim in dimensions if default_only levels dim hierarchy levelselse levels dim levelsgroup [ str dim str level for level in levels]groups append group return tuple itertools product *groups
def python_console namespace None if namespace is None import inspectframe inspect currentframe caller frame f_backif not caller logging error "can'tfindcallerwhostartthisconsole " caller framenamespace dict caller f_globals namespace update caller f_locals return get_python_console namespace namespace interact
def set_input_value page css value input_element page q css css results[0]input_element click input_element send_keys Keys CONTROL + 'a' input_element send_keys value return input_element
def _do_prim_curr rr coils pc np empty len rr * 3 len coils for ci c in enumerate coils pc[ ci] np sum c['w'] * _bem_inf_fields rr c['rmag'] c['cosmag'] 2 ravel return pc
def _do_prim_curr rr coils pc np empty len rr * 3 len coils for ci c in enumerate coils pc[ ci] np sum c['w'] * _bem_inf_fields rr c['rmag'] c['cosmag'] 2 ravel return pc
def normalize_headers response_headers strict True category {}for idx in range len response_headers key val response_headers[idx]head get_header key strict if not head newhead '-' join [x capitalize for x in key replace '_' '-' split '-' ] response_headers[idx] newhead val category[newhead] 4continueresponse_headers[idx] str head val category[str head ] head sort_orderdef key_func item value item[0]return category[value] value response_headers sort key key_func
def read_bitpacked_deprecated file_obj byte_count count width debug_logging raw_bytes array array ARRAY_BYTE_STR file_obj read byte_count tolist mask _mask_for_bits width index 0res []word 0bits_in_word 0while len res < count and index < len raw_bytes if debug_logging logger debug u'index %d' index logger debug u'bitsinword %d' bits_in_word logger debug u'word %s' bin word if bits_in_word > width offset bits_in_word - width value word & mask << offset >> offset if debug_logging logger debug u'offset %d' offset logger debug u'value %d %s ' value bin value res append value bits_in_word - widthelse word word << 8 raw_bytes[index] index + 1bits_in_word + 8return res
@memoizeddef smart_fill_file var_name file_name extension guess_extension var_name file_name _ file_content file_name get_file_from_template extension return NamedStringIO file_content name file_name
def ckan_before_request app_globals app_globals _check_uptodate identify_user
def write_Fasta_from_name_seq_pairs name_seqs fh if fh is None raise ValueError 'Needopenfilehandletowriteto ' for name seq in name_seqs fh write '%s\n' % BiologicalSequence seq id name to_fasta
def testUsingPsychoPyMonitorConfig io launchHubServer psychopy_monitor_name 'testMonitor' display io devices displayprint 'DisplayPsychopyMonitorName ' display getPsychopyMonitorName print 'DisplayDefaultEyeDistance ' display getDefaultEyeDistance print 'DisplayPhysicalDimensions ' display getPhysicalDimensions io quit
def testUsingPsychoPyMonitorConfig io launchHubServer psychopy_monitor_name 'testMonitor' display io devices displayprint 'DisplayPsychopyMonitorName ' display getPsychopyMonitorName print 'DisplayDefaultEyeDistance ' display getDefaultEyeDistance print 'DisplayPhysicalDimensions ' display getPhysicalDimensions io quit
def harvest_lettuces only_the_apps None avoid_apps None path 'features' apps get_apps if isinstance only_the_apps list tuple and any only_the_apps def _filter_only_specified module return module __name__ in only_the_apps apps filter _filter_only_specified apps else apps filter _filter_bultins apps apps filter _filter_configured_apps apps apps filter _filter_configured_avoids apps if isinstance avoid_apps list tuple and any avoid_apps def _filter_avoid module return module __name__ not in avoid_apps apps filter _filter_avoid apps joinpath lambda app join dirname app __file__ path app return map joinpath apps
def restore_file_ownership path os path join os path expanduser '~/' ' w3af' if not os path exists path return Falsetry uid int os getenv 'SUDO_UID' gid int os getenv 'SUDO_GID' except ValueError return Falsetry _chown path uid gid except return Falsereturn True
def make_interp_full_matr x y t k assert x size y size assert t size x size + k + 1 n x sizeA np zeros n n dtype np float_ for j in range n xval x[j]if xval t[k] left kelse left np searchsorted t xval - 1 bb _bspl evaluate_all_bspl t k xval left A[j left - k left + 1 ] bbc sl solve A y return c
def make_interp_full_matr x y t k assert x size y size assert t size x size + k + 1 n x sizeA np zeros n n dtype np float_ for j in range n xval x[j]if xval t[k] left kelse left np searchsorted t xval - 1 bb _bspl evaluate_all_bspl t k xval left A[j left - k left + 1 ] bbc sl solve A y return c
def add_html_component page menu_index boilerplate None page wait_for_component_menu click_css page 'button>span large-html-icon' menu_index require_notification False page wait_for_element_visibility ' new-component-html' 'HTMLcomponentmenuisvisible' component_css 'button[data-category html]'if boilerplate component_css + '[data-boilerplate {}]' format boilerplate else component_css + ' not [data-boilerplate] 'page wait_for_element_visibility component_css 'HTMLcomponent{}isvisible' format boilerplate click_css page component_css 0
@hook on_startdef load_key bot global dev_keydev_key bot config get 'api_keys' {} get 'google_dev_key' None
def get_project_groups_roles request project groups_roles collections defaultdict list project_role_assignments role_assignments_list request project project for role_assignment in project_role_assignments if not hasattr role_assignment 'group' continuegroup_id role_assignment group['id']role_id role_assignment role['id']if 'project' in role_assignment scope and role_assignment scope['project']['id'] project groups_roles[group_id] append role_id return groups_roles
def cleanup_tmpdir tmpdir None keep_so False tmpdir tmpdir or _caller_dir_pycache try filelist os listdir tmpdir except OSError returnif keep_so suffix ' c'else suffix _get_so_suffixes [0] lower for fn in filelist if fn lower startswith '_cffi_' and fn lower endswith suffix or fn lower endswith ' c' try os unlink os path join tmpdir fn except OSError passclean_dir [os path join tmpdir 'build' ]for dir in clean_dir try for fn in os listdir dir fn os path join dir fn if os path isdir fn clean_dir append fn else os unlink fn except OSError pass
def loadExperimentDescriptionScriptFromDir experimentDir descriptionScriptPath os path join experimentDir 'description py' module _loadDescriptionFile descriptionScriptPath return module
@register simple_tag takes_context True def zinnia_loop_template context default_template matching context_object get_context_first_matching_object context ['category' 'tag' 'author' 'pattern' 'year' 'month' 'week' 'day'] context_positions get_context_loop_positions context templates loop_template_list context_positions context_object matching default_template ENTRY_LOOP_TEMPLATES return select_template templates
def load_passphrase_from_file vf_path os path expanduser kVFPassphraseFile assert os access vf_path os F_OK and os access vf_path os R_OK '%smustexistandbereadable' % vf_path with open vf_path as f user_data f read return user_data strip '\n'
def total_seconds td return td microseconds + td seconds + td days * 24 * 3600 * 10 ** 6 / 10 ** 6
def add_to_date date years 0 months 0 days 0 hours 0 as_string False as_datetime False from dateutil relativedelta import relativedeltaif date None date now_datetime if hours as_datetime Trueif isinstance date basestring as_string Trueif u'' in date as_datetime Truedate parser parse date date date + relativedelta years years months months days days hours hours if as_string if as_datetime return date strftime DATETIME_FORMAT else return date strftime DATE_FORMAT else return date
def send_claim_registered_email claimer unclaimed_user node throttle 24 * 3600 unclaimed_record unclaimed_user get_unclaimed_record node _primary_key timestamp unclaimed_record get 'last_sent' if not throttle_period_expired timestamp throttle raise HTTPError http BAD_REQUEST data dict message_long 'Useraccountcanonlybeclaimedwithanexistinguseronceevery24hours' verification_key generate_verification_key verification_type 'claim' unclaimed_record['token'] verification_key['token']unclaimed_record['expires'] verification_key['expires']unclaimed_record['claimer_email'] claimer usernameunclaimed_user save referrer User load unclaimed_record['referrer_id'] claim_url web_url_for 'claim_user_registered' uid unclaimed_user _primary_key pid node _primary_key token unclaimed_record['token'] _external True mails send_mail referrer username mails FORWARD_INVITE_REGISTERED user unclaimed_user referrer referrer node node claim_url claim_url fullname unclaimed_record['name'] unclaimed_record['last_sent'] get_timestamp unclaimed_user save mails send_mail claimer username mails PENDING_VERIFICATION_REGISTERED fullname claimer fullname referrer referrer node node
def send_claim_registered_email claimer unclaimed_user node throttle 24 * 3600 unclaimed_record unclaimed_user get_unclaimed_record node _primary_key timestamp unclaimed_record get 'last_sent' if not throttle_period_expired timestamp throttle raise HTTPError http BAD_REQUEST data dict message_long 'Useraccountcanonlybeclaimedwithanexistinguseronceevery24hours' verification_key generate_verification_key verification_type 'claim' unclaimed_record['token'] verification_key['token']unclaimed_record['expires'] verification_key['expires']unclaimed_record['claimer_email'] claimer usernameunclaimed_user save referrer User load unclaimed_record['referrer_id'] claim_url web_url_for 'claim_user_registered' uid unclaimed_user _primary_key pid node _primary_key token unclaimed_record['token'] _external True mails send_mail referrer username mails FORWARD_INVITE_REGISTERED user unclaimed_user referrer referrer node node claim_url claim_url fullname unclaimed_record['name'] unclaimed_record['last_sent'] get_timestamp unclaimed_user save mails send_mail claimer username mails PENDING_VERIFICATION_REGISTERED fullname claimer fullname referrer referrer node node
def assoc d key value factory dict d2 factory d2[key] valuereturn merge d d2 factory factory
def parse_extra_info info if not info return infofinfos info split ' ' data_for_files {}for finfo in finfos items finfo split '{' if len items 1 fname fake_sff_nameinfo items[0]else fname items[0]info items[1]info info replace '}' '' data {}for item in info split ' ' key value item strip split ' ' key key strip value value strip data[key] valuedata_for_files[fname] datareturn data_for_files
def _get_vnics host_reference return host_reference config network vnic
def decorator caller func None if func is None @functools wraps caller def _decorator f *args **opts @functools wraps f def _caller *args **opts return caller f *args **opts return _caller_decorator func callerreturn _decoratorelse @functools wraps func def _decorated *args **opts return caller func *args **opts _decorated func funcreturn _decorated
def decorator caller func None if func is None @functools wraps caller def _decorator f *args **opts @functools wraps f def _caller *args **opts return caller f *args **opts return _caller_decorator func callerreturn _decoratorelse @functools wraps func def _decorated *args **opts return caller func *args **opts _decorated func funcreturn _decorated
def decorator caller func None if func is None @functools wraps caller def _decorator f *args **opts @functools wraps f def _caller *args **opts return caller f *args **opts return _caller_decorator func callerreturn _decoratorelse @functools wraps func def _decorated *args **opts return caller func *args **opts _decorated func funcreturn _decorated
def create_dock title parent stretch True dock QtWidgets QDockWidget parent dock setWindowTitle title dock setObjectName title titlebar DockTitleBarWidget dock title stretch stretch dock setTitleBarWidget titlebar dock setAutoFillBackground True if hasattr parent u'dockwidgets' parent dockwidgets append dock return dock
def buggy_mkl_svd function @wraps function def dec *args **kwargs try return function *args **kwargs except np linalg LinAlgError as exp if 'SVDdidnotconverge' in str exp from nose plugins skip import SkipTestmsg 'IntelMKLSVDconvergenceerrordetected skippingtest'warn msg raise SkipTest msg raisereturn dec
def loadMimeTypes mimetype_locations ['/etc/mime types'] import mimetypescontentTypes mimetypes types_mapcontentTypes update {' conf' 'text/plain' ' diff' 'text/plain' ' exe' 'application/x-executable' ' flac' 'audio/x-flac' ' java' 'text/plain' ' ogg' 'application/ogg' ' oz' 'text/x-oz' ' swf' 'application/x-shockwave-flash' ' tgz' 'application/x-gtar' ' wml' 'text/vnd wap wml' ' xul' 'application/vnd mozilla xul+xml' ' py' 'text/plain' ' patch' 'text/plain'} for location in mimetype_locations if os path exists location more mimetypes read_mime_types location if more is not None contentTypes update more return contentTypes
def loadMimeTypes mimetype_locations ['/etc/mime types'] import mimetypescontentTypes mimetypes types_mapcontentTypes update {' conf' 'text/plain' ' diff' 'text/plain' ' exe' 'application/x-executable' ' flac' 'audio/x-flac' ' java' 'text/plain' ' ogg' 'application/ogg' ' oz' 'text/x-oz' ' swf' 'application/x-shockwave-flash' ' tgz' 'application/x-gtar' ' wml' 'text/vnd wap wml' ' xul' 'application/vnd mozilla xul+xml' ' py' 'text/plain' ' patch' 'text/plain'} for location in mimetype_locations if os path exists location more mimetypes read_mime_types location if more is not None contentTypes update more return contentTypes
def write_flv_header stream metadata stream write 'FLV\x01' stream write '\x05' stream write '\x00\x00\x00 DCTB ' stream write '\x00\x00\x00\x00' stream write '\x12' stream write struct_pack u' L' len metadata [1 ] stream write '\x00\x00\x00\x00\x00\x00\x00' stream write metadata stream write '\x00\x00\x01s'
def heatmap data vmin None vmax None cmap None center None robust False annot None fmt ' 2g' annot_kws None linewidths 0 linecolor 'white' cbar True cbar_kws None cbar_ax None square False ax None xticklabels True yticklabels True mask None **kwargs plotter _HeatMapper data vmin vmax cmap center robust annot fmt annot_kws cbar cbar_kws xticklabels yticklabels mask kwargs['linewidths'] linewidthskwargs['edgecolor'] linecolorif ax is None ax plt gca if square ax set_aspect 'equal' plotter plot ax cbar_ax kwargs return ax
def getPathsByLists vertexLists vector3Lists getVector3ListsRecursively vertexLists paths []addToPathsRecursively paths vector3Lists return paths
def approx_fprime_cs x f epsilon None args kwargs {} n len x epsilon _get_epsilon x 1 epsilon n increments np identity n * 1j * epsilon partials [ f x + ih *args **kwargs imag / epsilon[i] for i ih in enumerate increments ]return np array partials T
def approx_fprime_cs x f epsilon None args kwargs {} n len x epsilon _get_epsilon x 1 epsilon n increments np identity n * 1j * epsilon partials [ f x + ih *args **kwargs imag / epsilon[i] for i ih in enumerate increments ]return np array partials T
def approx_fprime_cs x f epsilon None args kwargs {} n len x epsilon _get_epsilon x 1 epsilon n increments np identity n * 1j * epsilon partials [ f x + ih *args **kwargs imag / epsilon[i] for i ih in enumerate increments ]return np array partials T
def all_sequential_open_distrib request course_id data {}course_key SlashSeparatedCourseKey from_deprecated_string course_id if has_instructor_access_for_class request user course_key try data dashboard_data get_d3_sequential_open_distrib course_key except Exception as ex log error 'Generatingmetricsfailedwithexception %s' ex data {'error' 'error'}else data {'error' "AccessDenied Userdoesnothaveaccesstothiscourse'sdata"}return HttpResponse json dumps data content_type 'application/json'
def _expand_config config defaults defaults update config return defaults
def for_type typ func oldfunc _type_pprinters get typ None if func is not None _type_pprinters[typ] funcreturn oldfunc
def _get_timestamp data position dummy0 dummy1 dummy2 end position + 8 inc timestamp _UNPACK_TIMESTAMP data[position end] return Timestamp timestamp inc end
def get_colors palette funcs palettes import_required 'bokeh palettes' _BOKEH_MISSING_MSG tz import_required 'toolz' _TOOLZ_MISSING_MSG unique_funcs list sorted tz unique funcs n_funcs len unique_funcs palette_lookup palettes all_palettes[palette]keys list sorted palette_lookup keys index keys[min bisect_left keys n_funcs len keys - 1 ]palette palette_lookup[index]palette list tz unique palette if len palette > n_funcs random Random 42 shuffle palette color_lookup dict zip unique_funcs cycle palette return [color_lookup[n] for n in funcs]
def set_chassis_name name host None admin_username None admin_password None return __execute_cmd 'setsysinfo-cchassisname{0}' format name host host admin_username admin_username admin_password admin_password
def getTimeSinceLastUpdate IOType global last_update_timescurrent_time time last_time last_update_times get IOType if not last_time time_since_update 1else time_since_update current_time - last_time last_update_times[IOType] current_timereturn time_since_update
def serialize_item collection item if item name is None or item name '' raise exceptions RuntimeError 'nameunsetforitem ' if collection collection_type in ['mgmtclass'] filename '/var/lib/cobbler/collections/%ses/%s' % collection collection_type item name else filename '/var/lib/cobbler/collections/%ss/%s' % collection collection_type item name _dict item to_dict if capi CobblerAPI settings serializer_pretty_json sort_keys Trueindent 4else sort_keys Falseindent Nonefilename + ' json'_dict item to_dict fd open filename 'w+' data simplejson dumps _dict encoding 'utf-8' sort_keys sort_keys indent indent fd write data fd close
def nopackages pkg_list pkg_list [pkg for pkg in pkg_list if is_installed pkg ]if pkg_list uninstall pkg_list
def testString tests [ '%s<%s>' % name func __doc__ for name func in testDict iteritems ]return 'validtests %s' % ' ' join tests
def test_doc obj compiled CompiledObject _evaluator '' __getnewargs__ assert obj doc ''
def test_doc obj compiled CompiledObject _evaluator '' __getnewargs__ assert obj doc ''
def ssl_wrap_socket sock keyfile None certfile None cert_reqs None ca_certs None server_hostname None ssl_version None ciphers None ssl_context None context ssl_contextif context is None context create_urllib3_context ssl_version cert_reqs ciphers ciphers if ca_certs try context load_verify_locations ca_certs except IOError as e raise SSLError e except OSError as e if e errno errno ENOENT raise SSLError e raiseif certfile context load_cert_chain certfile keyfile if HAS_SNI return context wrap_socket sock server_hostname server_hostname return context wrap_socket sock
def ssl_wrap_socket sock keyfile None certfile None cert_reqs None ca_certs None server_hostname None ssl_version None ciphers None ssl_context None context ssl_contextif context is None context create_urllib3_context ssl_version cert_reqs ciphers ciphers if ca_certs try context load_verify_locations ca_certs except IOError as e raise SSLError e except OSError as e if e errno errno ENOENT raise SSLError e raiseif certfile context load_cert_chain certfile keyfile if HAS_SNI return context wrap_socket sock server_hostname server_hostname return context wrap_socket sock
def configure_logging config format config get 'log_format' '% name s% levelname s% asctime s% message s' level logging _levelNames[config get 'log_level' 'DEBUG' ]destination config get 'log_destination' 'stdout' log info "Loggingat'%s'levelto'%s'" % level destination root logging getLogger root setLevel level if level < logging DEBUG logging getLogger 'paste httpserver ThreadPool' setLevel logging WARN for h in root handlers[ ] root removeHandler h if destination 'stdout' handler logging StreamHandler sys stdout else handler logging FileHandler destination formatter logging Formatter format handler setFormatter formatter root addHandler handler
def configure_logging config format config get 'log_format' '% name s% levelname s% asctime s% message s' level logging _levelNames[config get 'log_level' 'DEBUG' ]destination config get 'log_destination' 'stdout' log info "Loggingat'%s'levelto'%s'" % level destination root logging getLogger root setLevel level if level < logging DEBUG logging getLogger 'paste httpserver ThreadPool' setLevel logging WARN for h in root handlers[ ] root removeHandler h if destination 'stdout' handler logging StreamHandler sys stdout else handler logging FileHandler destination formatter logging Formatter format handler setFormatter formatter root addHandler handler
@docfillerdef maximum_filter input size None footprint None output None mode 'reflect' cval 0 0 origin 0 return _min_or_max_filter input size footprint None output mode cval origin 0
def update_patch_log patchmodule frappe get_doc {u'doctype' u'PatchLog' u'patch' patchmodule} insert
def update_patch_log patchmodule frappe get_doc {u'doctype' u'PatchLog' u'patch' patchmodule} insert
def sle actual predicted return np power np log np array actual + 1 - np log np array predicted + 1 2
def build_query_rep query divider u'-' return divider join [el[0] for el in query]
def find_module modlist mod_addrs addr pos bisect_right mod_addrs addr - 1 if pos -1 return Nonemod modlist[mod_addrs[pos]]if mod obj_vm address_compare addr mod DllBase -1 and mod obj_vm address_compare addr mod DllBase + mod SizeOfImage -1 return modelse return None
@register assignment_tagdef assignment_two_params one two return 'assignment_two_params-Expectedresult %s %s' % one two
def get_model app_label model_name seed_cache True if seed_cache get_apps try model_dict _app_models[app_label]except KeyError return Nonetry return model_dict[model_name lower ]except KeyError return None
def could_edit request user getattr request 'user' AnonymousUser return getattr user 'is_superuser' False or getattr user 'is_staff' False
def replace text *pairs while pairs text join split text pairs[0] pairs[1] pairs pairs[2 ]return text
def _add_keys_to_request request_field_pb key_pbs for key_pb in key_pbs request_field_pb add CopyFrom key_pb
def escape text text text replace '\\' '\\\\' text text replace '"""' '""\\"' text text replace '\n' '\\n\\\n' return text
def format_datetime dt format None if is_naive dt localtime make_aware dt get_current_timezone logging warning 'oscar core utils format_datetimereceivednativedatetime' else localtime dt astimezone get_current_timezone return date_filter localtime format
def test_cnn_fit_sample_with_wrong_object knn 'rnd'cnn CondensedNearestNeighbour random_state RND_SEED n_neighbors knn assert_raises ValueError cnn fit_sample X Y
def print_environ environ os environ keys environ keys keys sort printprint '<H3>ShellEnvironment </H3>'print '<DL>'for key in keys print '<DT>' escape key '<DD>' escape environ[key] print '</DL>'print
def find_boundaries label_img connectivity 1 mode 'thick' background 0 if label_img dtype 'bool' label_img label_img astype np uint8 ndim label_img ndimselem ndi generate_binary_structure ndim connectivity if mode 'subpixel' boundaries dilation label_img selem erosion label_img selem if mode 'inner' foreground_image label_img background boundaries & foreground_imageelif mode 'outer' max_label np iinfo label_img dtype maxbackground_image label_img background selem ndi generate_binary_structure ndim ndim inverted_background np array label_img copy True inverted_background[background_image] max_labeladjacent_objects dilation label_img selem erosion inverted_background selem & ~ background_image boundaries & background_image adjacent_objects return boundarieselse boundaries _find_boundaries_subpixel label_img return boundaries
def find_boundaries label_img connectivity 1 mode 'thick' background 0 if label_img dtype 'bool' label_img label_img astype np uint8 ndim label_img ndimselem ndi generate_binary_structure ndim connectivity if mode 'subpixel' boundaries dilation label_img selem erosion label_img selem if mode 'inner' foreground_image label_img background boundaries & foreground_imageelif mode 'outer' max_label np iinfo label_img dtype maxbackground_image label_img background selem ndi generate_binary_structure ndim ndim inverted_background np array label_img copy True inverted_background[background_image] max_labeladjacent_objects dilation label_img selem erosion inverted_background selem & ~ background_image boundaries & background_image adjacent_objects return boundarieselse boundaries _find_boundaries_subpixel label_img return boundaries
def setup hass config from zeroconf import Zeroconf ServiceInfozeroconf Zeroconf zeroconf_name '{} {}' format hass config location_name ZEROCONF_TYPE requires_api_password hass config api api_password is not None params {'version' __version__ 'base_url' hass config api base_url 'requires_api_password' requires_api_password}host_ip util get_local_ip try host_ip_pton socket inet_pton socket AF_INET host_ip except socket error host_ip_pton socket inet_pton socket AF_INET6 host_ip info ServiceInfo ZEROCONF_TYPE zeroconf_name host_ip_pton hass http server_port 0 0 params zeroconf register_service info def stop_zeroconf event 'StopZeroconf 'zeroconf unregister_service info zeroconf close hass bus listen_once EVENT_HOMEASSISTANT_STOP stop_zeroconf return True
def setup hass config from zeroconf import Zeroconf ServiceInfozeroconf Zeroconf zeroconf_name '{} {}' format hass config location_name ZEROCONF_TYPE requires_api_password hass config api api_password is not None params {'version' __version__ 'base_url' hass config api base_url 'requires_api_password' requires_api_password}host_ip util get_local_ip try host_ip_pton socket inet_pton socket AF_INET host_ip except socket error host_ip_pton socket inet_pton socket AF_INET6 host_ip info ServiceInfo ZEROCONF_TYPE zeroconf_name host_ip_pton hass http server_port 0 0 params zeroconf register_service info def stop_zeroconf event 'StopZeroconf 'zeroconf unregister_service info zeroconf close hass bus listen_once EVENT_HOMEASSISTANT_STOP stop_zeroconf return True
def make_tarball base_name base_dir compress 'gzip' verbose 0 dry_run 0 compress_ext {'gzip' ' gz' 'bzip2' ' bz2' 'compress' ' Z'}compress_flags {'gzip' ['-f9'] 'compress' ['-f'] 'bzip2' ['-f9']}if compress is not None and compress not in compress_ext keys raise ValueError "badvaluefor'compress' mustbeNone 'gzip' or'compress'"archive_name base_name + ' tar' mkpath os path dirname archive_name dry_run dry_run cmd ['tar' '-cf' archive_name base_dir]spawn cmd dry_run dry_run if compress spawn [compress] + compress_flags[compress] + [archive_name] dry_run dry_run return archive_name + compress_ext[compress] else return archive_name
def SslWrapOnlyOnce org_sslwrap sock *args **kwargs if not isinstance sock ssl SSLSocket ctx Master get_fd_context sock fileno try sock org_sslwrap sock *args **kwargs ctx encryption _explain_encryption sock except socket error IOError as e ctx error '%s' % e raisereturn sock
def SslWrapOnlyOnce org_sslwrap sock *args **kwargs if not isinstance sock ssl SSLSocket ctx Master get_fd_context sock fileno try sock org_sslwrap sock *args **kwargs ctx encryption _explain_encryption sock except socket error IOError as e ctx error '%s' % e raisereturn sock
def get_image_list releaseid return _caa_request releaseid
def cram text maxlen if len text > maxlen pre max 0 maxlen - 3 // 2 post max 0 maxlen - 3 - pre return text[ pre] + ' ' + text[ len text - post ] return text
def test_incorrect_case_file_index data req InstallRequirement from_line 'dinner' None finder PackageFinder [] [data find_links3] session PipSession link finder find_requirement req False assert link url endswith 'Dinner-2 0 tar gz'
def _prefix_commands command which prefixes list env command_prefixes cwd env cwd if which 'remote' else env lcwd redirect '>/dev/null' if not win32 else '' if cwd prefixes insert 0 'cd%s%s' % cwd redirect glue '&&'prefix glue join prefixes + glue if prefixes else '' return prefix + command
def test_none bridge qtbot assert QApplication instance focusWidget is None for name method in inspect getmembers bridge inspect ismethod if name startswith 'rl_' method
def checksum_question question timestamp challenge u'' join settings SECRET_KEY question timestamp sha hashlib sha1 challenge encode u'utf-8' return sha hexdigest
def openHubFile filepath filename mode global _hubFileshubFile openFile os path join filepath filename mode _hubFiles append hubFile return hubFile
def openHubFile filepath filename mode global _hubFileshubFile openFile os path join filepath filename mode _hubFiles append hubFile return hubFile
def openHubFile filepath filename mode global _hubFileshubFile openFile os path join filepath filename mode _hubFiles append hubFile return hubFile
@jingo register filterdef no_links string if not string return stringif hasattr string '__html__' string string __html__ allowed_tags bleach ALLOWED_TAGS[ ]allowed_tags remove 'a' no_links bleach clean string tags allowed_tags strip True return jinja2 Markup clean_nl no_links strip
def _filter_names names sorted_names _sort_names names code names z_util interfaces IDisplay checklist 'WhichnameswouldyouliketoactivateHTTPSfor?' tags sorted_names cli_flag '--domains' force_interactive True return code [str s for s in names]
def FriendlyExceptionDlg message class w3af_message_dialog gtk MessageDialog def dialog_response_cb self widget response_id '\nhttp //faq pygtk org/index py?req show&file faq10 017 htp\n'self destroy def dialog_run self '\nhttp //faq pygtk org/index py?req show&file faq10 017 htp\n'if not self modal self set_modal True self connect 'response' self dialog_response_cb self show dlg w3af_message_dialog None gtk DIALOG_MODAL gtk MESSAGE_WARNING gtk BUTTONS_OK message dlg set_icon_from_file W3AF_ICON dlg set_title 'Error' dlg dialog_run return
def FriendlyExceptionDlg message class w3af_message_dialog gtk MessageDialog def dialog_response_cb self widget response_id '\nhttp //faq pygtk org/index py?req show&file faq10 017 htp\n'self destroy def dialog_run self '\nhttp //faq pygtk org/index py?req show&file faq10 017 htp\n'if not self modal self set_modal True self connect 'response' self dialog_response_cb self show dlg w3af_message_dialog None gtk DIALOG_MODAL gtk MESSAGE_WARNING gtk BUTTONS_OK message dlg set_icon_from_file W3AF_ICON dlg set_title 'Error' dlg dialog_run return
def upgrade migrate_engine meta MetaData meta bind migrate_engineworkers Table 'workers' meta autoload True race_preventer Column 'race_preventer' Integer nullable False default 0 server_default text '0' race_preventer create workers populate_default True
def curve4_bezier p1 p2 p3 p4 x1 y1 p1 x2 y2 p2 x3 y3 p3 x4 y4 p4points []_curve4_recursive_bezier points x1 y1 x2 y2 x3 y3 x4 y4 dx dy points[0][0] - x1 points[0][1] - y1 if dx * dx + dy * dy > 1e-10 points insert 0 x1 y1 dx dy points[ -1 ][0] - x4 points[ -1 ][1] - y4 if dx * dx + dy * dy > 1e-10 points append x4 y4 return np array points reshape len points 2
def mmodule saltenv fun *args **kwargs mminion _MMinion saltenv return mminion functions[fun] *args **kwargs
def remove_package package dst path os path join package where package name run_scripts path scripts ['prerm'] shutil rmtree os path join path 'data/' shutil rmtree os path join path 'docs/' run_scripts os path join dst package name scripts ['postrm'] shutil rmtree os path join path 'scripts/' shutil rmtree path update_installed_list 'r' package
def scan ret []devices bluetooth discover_devices lookup_names True for device in devices ret append {device[0] device[1]} return ret
def scan ret []devices bluetooth discover_devices lookup_names True for device in devices ret append {device[0] device[1]} return ret
def get_raising_file_and_line tb None if not tb tb sys exc_info [2] filename lineno _context _line traceback extract_tb tb [ -1 ]return filename lineno
def get_raising_file_and_line tb None if not tb tb sys exc_info [2] filename lineno _context _line traceback extract_tb tb [ -1 ]return filename lineno
def write_hostnames save_path hostnames_ips file_path dir_find save_path hostnames_ip_file os path join file_path 'openstack_hostnames_ips yml' with open hostnames_ip_file 'wb' as f f write json dumps hostnames_ips indent 4 sort_keys True
def decode_table encoded offset result {}tablesize struct unpack_from '>I' encoded offset [0]offset + 4limit offset + tablesize while offset < limit key offset decode_short_string encoded offset value offset decode_value encoded offset result[key] valuereturn result offset
def decode_table encoded offset result {}tablesize struct unpack_from '>I' encoded offset [0]offset + 4limit offset + tablesize while offset < limit key offset decode_short_string encoded offset value offset decode_value encoded offset result[key] valuereturn result offset
def decode_table encoded offset result {}tablesize struct unpack_from '>I' encoded offset [0]offset + 4limit offset + tablesize while offset < limit key offset decode_short_string encoded offset value offset decode_value encoded offset result[key] valuereturn result offset
def prepare_private_key if request method 'GET' returnif request args get 'view_only' '' returnif request referrer referrer_parsed urlparse urlparse request referrer scheme referrer_parsed schemekey urlparse parse_qs urlparse urlparse request referrer query get 'view_only' if key key key[0]else scheme Nonekey Noneif key and not session is_authenticated new_url add_key_to_url request url scheme key return redirect new_url code http TEMPORARY_REDIRECT
def prepare_private_key if request method 'GET' returnif request args get 'view_only' '' returnif request referrer referrer_parsed urlparse urlparse request referrer scheme referrer_parsed schemekey urlparse parse_qs urlparse urlparse request referrer query get 'view_only' if key key key[0]else scheme Nonekey Noneif key and not session is_authenticated new_url add_key_to_url request url scheme key return redirect new_url code http TEMPORARY_REDIRECT
def execute_on_completion application config callback def inner environ start_response try result application environ start_response except callback environ raisereturn generate_close_and_callback result callback environ return inner
def ValidateCombinedSourceReferencesString source_refs if len source_refs > SOURCE_REFERENCES_MAX_SIZE raise validation ValidationError 'Totalsourcereference s sizeexceedsthelimit %d>%d' % len source_refs SOURCE_REFERENCES_MAX_SIZE for ref in source_refs splitlines ValidateSourceReference ref strip
def ValidateCombinedSourceReferencesString source_refs if len source_refs > SOURCE_REFERENCES_MAX_SIZE raise validation ValidationError 'Totalsourcereference s sizeexceedsthelimit %d>%d' % len source_refs SOURCE_REFERENCES_MAX_SIZE for ref in source_refs splitlines ValidateSourceReference ref strip
def run_network filename n eta random seed 12345678 np random seed 12345678 training_data validation_data test_data mnist_loader load_data_wrapper net network2 Network [784 n 10] cost network2 CrossEntropyCost print 'Trainthenetworkusingthedefaultstartingweights ' default_vc default_va default_tc default_ta net SGD training_data 30 10 eta lmbda 5 0 evaluation_data validation_data monitor_evaluation_accuracy True print 'Trainthenetworkusingthelargestartingweights 'net large_weight_initializer large_vc large_va large_tc large_ta net SGD training_data 30 10 eta lmbda 5 0 evaluation_data validation_data monitor_evaluation_accuracy True f open filename 'w' json dump {'default_weight_initialization' [default_vc default_va default_tc default_ta] 'large_weight_initialization' [large_vc large_va large_tc large_ta]} f f close
def KAMA ds count timeperiod - 2 ** 31 return call_talib_with_ds ds count talib KAMA timeperiod
def remove_ignorable_whitespace node if node tail and node tail strip '' node tail Nonefor child in node if node text and node text strip '' node text Noneremove_ignorable_whitespace child
def remove_ignorable_whitespace node if node tail and node tail strip '' node tail Nonefor child in node if node text and node text strip '' node text Noneremove_ignorable_whitespace child
def do_hypervisor_stats cs args stats cs hypervisor_stats statistics utils print_dict stats to_dict
def read_element_float stream size if size 4 return unpack '>f' _read stream 4 [0]elif size 8 return unpack '>d' _read stream 8 [0]else raise SizeError size
def reset global _COURSES_COURSES []global _ENROLLMENTS_ENROLLMENTS []global _VERIFIED_MODE_EXPIRED_VERIFIED_MODE_EXPIRED []
def reset global _COURSES_COURSES []global _ENROLLMENTS_ENROLLMENTS []global _VERIFIED_MODE_EXPIRED_VERIFIED_MODE_EXPIRED []
def adjust_log image gain 1 inv False _assert_non_negative image dtype image dtype typescale float dtype_limits image True [1] - dtype_limits image True [0] if inv out 2 ** image / scale - 1 * scale * gain return dtype out out np log2 1 + image / scale * scale * gain return dtype out
def hstack tables join_type u'outer' uniq_col_name u'{col_name}_{table_name}' table_names None metadata_conflicts u'warn' tables _get_list_of_tables tables if len tables 1 return tables[0]col_name_map OrderedDict out _hstack tables join_type uniq_col_name table_names col_name_map _merge_col_meta out tables col_name_map metadata_conflicts metadata_conflicts _merge_table_meta out tables metadata_conflicts metadata_conflicts return out
def _test_jd2jcal import randomn 1000year [random randint -4699 2200 for i in range n ]month [random randint 1 12 for i in range n ]day [random randint 1 28 for i in range n ]jd [jcal2jd y m d [1] for y m d in zip year month day ]x [jd2gcal MJD_0 i for i in jd]for i in range n assert x[i][0] year[i] assert x[i][1] month[i] assert x[i][2] day[i] assert x[i][3] < 1e-15
def assert_deb_headers test_case expected_headers package_path output check_output ['dpkg' '--info' package_path path] actual_headers parse_colon_dict output assert_dict_contains test_case expected_headers actual_headers 'MissingdpkgHeaders '
def test_slicing_on_instance_with_parameterless_model p2 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 p1 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 mapping Mapping 0 1 0 1 offx Shift -2 name u'x_translation' offy Shift -1 name u'y_translation' aff AffineTransformation2D matrix [[1 2] [3 4]] name u'rotation' model mapping p1 & p2 offx & offy aff assert model param_names u'c0_0_1' u'c1_0_1' u'c0_1_1' u'c0_0_2' u'c1_0_2' u'c0_1_2' u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert model 1 2 23 0 53 0 m model[3 ]assert m param_names u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert m 1 2 1 0 1 0
def test_slicing_on_instance_with_parameterless_model p2 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 p1 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 mapping Mapping 0 1 0 1 offx Shift -2 name u'x_translation' offy Shift -1 name u'y_translation' aff AffineTransformation2D matrix [[1 2] [3 4]] name u'rotation' model mapping p1 & p2 offx & offy aff assert model param_names u'c0_0_1' u'c1_0_1' u'c0_1_1' u'c0_0_2' u'c1_0_2' u'c0_1_2' u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert model 1 2 23 0 53 0 m model[3 ]assert m param_names u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert m 1 2 1 0 1 0
def test_slicing_on_instance_with_parameterless_model p2 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 p1 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 mapping Mapping 0 1 0 1 offx Shift -2 name u'x_translation' offy Shift -1 name u'y_translation' aff AffineTransformation2D matrix [[1 2] [3 4]] name u'rotation' model mapping p1 & p2 offx & offy aff assert model param_names u'c0_0_1' u'c1_0_1' u'c0_1_1' u'c0_0_2' u'c1_0_2' u'c0_1_2' u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert model 1 2 23 0 53 0 m model[3 ]assert m param_names u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert m 1 2 1 0 1 0
def test_slicing_on_instance_with_parameterless_model p2 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 p1 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 mapping Mapping 0 1 0 1 offx Shift -2 name u'x_translation' offy Shift -1 name u'y_translation' aff AffineTransformation2D matrix [[1 2] [3 4]] name u'rotation' model mapping p1 & p2 offx & offy aff assert model param_names u'c0_0_1' u'c1_0_1' u'c0_1_1' u'c0_0_2' u'c1_0_2' u'c0_1_2' u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert model 1 2 23 0 53 0 m model[3 ]assert m param_names u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert m 1 2 1 0 1 0
def test_slicing_on_instance_with_parameterless_model p2 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 p1 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 mapping Mapping 0 1 0 1 offx Shift -2 name u'x_translation' offy Shift -1 name u'y_translation' aff AffineTransformation2D matrix [[1 2] [3 4]] name u'rotation' model mapping p1 & p2 offx & offy aff assert model param_names u'c0_0_1' u'c1_0_1' u'c0_1_1' u'c0_0_2' u'c1_0_2' u'c0_1_2' u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert model 1 2 23 0 53 0 m model[3 ]assert m param_names u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert m 1 2 1 0 1 0
def test_slicing_on_instance_with_parameterless_model p2 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 p1 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 mapping Mapping 0 1 0 1 offx Shift -2 name u'x_translation' offy Shift -1 name u'y_translation' aff AffineTransformation2D matrix [[1 2] [3 4]] name u'rotation' model mapping p1 & p2 offx & offy aff assert model param_names u'c0_0_1' u'c1_0_1' u'c0_1_1' u'c0_0_2' u'c1_0_2' u'c0_1_2' u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert model 1 2 23 0 53 0 m model[3 ]assert m param_names u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert m 1 2 1 0 1 0
def test_slicing_on_instance_with_parameterless_model p2 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 p1 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 mapping Mapping 0 1 0 1 offx Shift -2 name u'x_translation' offy Shift -1 name u'y_translation' aff AffineTransformation2D matrix [[1 2] [3 4]] name u'rotation' model mapping p1 & p2 offx & offy aff assert model param_names u'c0_0_1' u'c1_0_1' u'c0_1_1' u'c0_0_2' u'c1_0_2' u'c0_1_2' u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert model 1 2 23 0 53 0 m model[3 ]assert m param_names u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert m 1 2 1 0 1 0
def test_slicing_on_instance_with_parameterless_model p2 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 p1 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 mapping Mapping 0 1 0 1 offx Shift -2 name u'x_translation' offy Shift -1 name u'y_translation' aff AffineTransformation2D matrix [[1 2] [3 4]] name u'rotation' model mapping p1 & p2 offx & offy aff assert model param_names u'c0_0_1' u'c1_0_1' u'c0_1_1' u'c0_0_2' u'c1_0_2' u'c0_1_2' u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert model 1 2 23 0 53 0 m model[3 ]assert m param_names u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert m 1 2 1 0 1 0
def test_slicing_on_instance_with_parameterless_model p2 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 p1 Polynomial2D 1 c0_0 1 c1_0 2 c0_1 3 mapping Mapping 0 1 0 1 offx Shift -2 name u'x_translation' offy Shift -1 name u'y_translation' aff AffineTransformation2D matrix [[1 2] [3 4]] name u'rotation' model mapping p1 & p2 offx & offy aff assert model param_names u'c0_0_1' u'c1_0_1' u'c0_1_1' u'c0_0_2' u'c1_0_2' u'c0_1_2' u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert model 1 2 23 0 53 0 m model[3 ]assert m param_names u'offset_3' u'offset_4' u'matrix_5' u'translation_5' assert m 1 2 1 0 1 0
def os_is_running pid if isinstance pid six string_types pid int pid if HAS_PSUTIL return psutil pid_exists pid else try os kill pid 0 return Trueexcept OSError return False
def os_is_running pid if isinstance pid six string_types pid int pid if HAS_PSUTIL return psutil pid_exists pid else try os kill pid 0 return Trueexcept OSError return False
def apply_palette256 image return image convert 'RGB' convert 'P' palette Image ADAPTIVE colors 256 dither Image NONE
def apply_palette256 image return image convert 'RGB' convert 'P' palette Image ADAPTIVE colors 256 dither Image NONE
def append filename text use_sudo False partial False escape True shell False func use_sudo and sudo or run if isinstance text basestring text [text]for line in text regex '^' + _escape_for_regex line + '' if partial else '$' if exists filename use_sudo use_sudo and line and contains filename regex use_sudo use_sudo escape False shell shell continueline line replace "'" "'\\\\''" if escape else line func "echo'%s'>>%s" % line _expand_path filename
def test_ipv4_detect assert _is_ipv4_like '1 1 1 1' is True assert _is_ipv4_like '1 1 1 256' is True assert _is_ipv4_like '-1 1 1 1' is True assert _is_ipv4_like '1 1 1 hello' is False assert _is_ipv4_like 'hello' is False assert _is_ipv4_like '-1 1 1' is False assert _is_ipv4_like '-1 1 1 ' is False
def _primitive_root_prime_iter p p as_int p v [ p - 1 // i for i in factorint p - 1 keys ]a 2while a < p for pw in v if pow a pw p 1 breakelse yield a a + 1
def _attribute_matcher kwargs def match node if 'terminal' in kwargs kwa_copy kwargs copy pattern kwa_copy pop 'terminal' if pattern is not None and not hasattr node 'is_terminal' or node is_terminal pattern return Falseelse kwa_copy kwargsfor key pattern in kwa_copy items if not hasattr node key return Falsetarget getattr node key if isinstance pattern basestring return isinstance target basestring and re match pattern + '$' target if isinstance pattern bool return pattern bool target if isinstance pattern int return pattern target if pattern is None return target is None raise TypeError 'invalidquerytype %s' % type pattern return Truereturn match
def get_hqe_percentage_complete **filter_data query models HostQueueEntry query_objects filter_data complete_count query filter complete True count total_count query count if total_count 0 return 1return float complete_count / total_count
def get_hqe_percentage_complete **filter_data query models HostQueueEntry query_objects filter_data complete_count query filter complete True count total_count query count if total_count 0 return 1return float complete_count / total_count
def get_hqe_percentage_complete **filter_data query models HostQueueEntry query_objects filter_data complete_count query filter complete True count total_count query count if total_count 0 return 1return float complete_count / total_count
def fsplit pred objs t []f []for obj in objs if pred obj t append obj else f append obj return t f
def GetCookieFromResponse response user_cookie_header_list [h for h in response headers get_list 'Set-Cookie' if h startswith 'user ' ]if not user_cookie_header_list return Nonereturn re match 'user "? [^" ]* ' user_cookie_header_list[ -1 ] group 1
def _update_datacenter_cache_from_objects session dcs while dcs for dco in dcs objects dc_ref dco objds_refs []prop_dict vm_util propset_dict dco propSet name prop_dict get 'name' vmFolder prop_dict get 'vmFolder' datastore_refs prop_dict get 'datastore' if datastore_refs datastore_refs datastore_refs ManagedObjectReferencefor ds in datastore_refs ds_refs append ds value else LOG debug "Datacenter%sdoesn'thaveanydatastoreassociatedwithit ignoringit" name for ds_ref in ds_refs _DS_DC_MAPPING[ds_ref] DcInfo ref dc_ref name name vmFolder vmFolder dcs session _call_method vutil 'continue_retrieval' dcs
def revoke_privilege database privilege username **client_args client _client **client_args client revoke_privilege privilege database username return True
def testImport module_name try return testImport cache[module_name]except KeyError try __import__ module_name except ImportError result Falseelse result TruetestImport cache[module_name] resultreturn result
def get_image_model from django apps import appsmodel_string get_image_model_string try return apps get_model model_string except ValueError raise ImproperlyConfigured u"WAGTAILIMAGES_IMAGE_MODELmustbeoftheform'app_label model_name'" except LookupError raise ImproperlyConfigured u"WAGTAILIMAGES_IMAGE_MODELreferstomodel'%s'thathasnotbeeninstalled" % model_string
def ensure_test_file_dir_set galaxy_test_file_dir os environ get 'GALAXY_TEST_FILE_DIR' GALAXY_TEST_FILE_DIR os environ['GALAXY_TEST_FILE_DIR'] galaxy_test_file_dirfirst_test_file_dir galaxy_test_file_dir split ' ' [0]return first_test_file_dir
def downgrade migrate_engine dialect migrate_engine url get_dialect if dialect is not postgresql dialect meta MetaData bind migrate_engine for table column in TABLE_COLUMNS t Table table meta autoload True getattr t c column alter type String 43
def downgrade migrate_engine dialect migrate_engine url get_dialect if dialect is not postgresql dialect meta MetaData bind migrate_engine for table column in TABLE_COLUMNS t Table table meta autoload True getattr t c column alter type String 43
def group_albums session def group item return item albumartist or item artist item album task Nonewhile True task yield task if task skip continuetasks []sorted_items sorted task items key group for _ items in itertools groupby sorted_items group items list items task ImportTask task toppath [i path for i in items] items tasks + task handle_created session tasks append SentinelImportTask task toppath task paths task pipeline multiple tasks
def group_albums session def group item return item albumartist or item artist item album task Nonewhile True task yield task if task skip continuetasks []sorted_items sorted task items key group for _ items in itertools groupby sorted_items group items list items task ImportTask task toppath [i path for i in items] items tasks + task handle_created session tasks append SentinelImportTask task toppath task paths task pipeline multiple tasks
def group_albums session def group item return item albumartist or item artist item album task Nonewhile True task yield task if task skip continuetasks []sorted_items sorted task items key group for _ items in itertools groupby sorted_items group items list items task ImportTask task toppath [i path for i in items] items tasks + task handle_created session tasks append SentinelImportTask task toppath task paths task pipeline multiple tasks
def group_albums session def group item return item albumartist or item artist item album task Nonewhile True task yield task if task skip continuetasks []sorted_items sorted task items key group for _ items in itertools groupby sorted_items group items list items task ImportTask task toppath [i path for i in items] items tasks + task handle_created session tasks append SentinelImportTask task toppath task paths task pipeline multiple tasks
def unpack_patches hg_unbundle10_obj remaining while remaining > 12 start end blocklen struct unpack '>lll' readexactly hg_unbundle10_obj 12 remaining - 12if blocklen > remaining raise Exception 'unexpectedendofpatchstream' block readexactly hg_unbundle10_obj blocklen remaining - blocklen yield {'start' start 'end' end 'blocklen' blocklen 'block' block encode 'string_escape' } if remaining > 0 log error 'Unexpectedendofpatchstream %sremaining' remaining raise Exception 'unexpectedendofpatchstream'
def start nick host port 6667 username None password None channels None use_ssl False use_sasl False char ' ' allow_hosts False allow_nicks False disable_query True client IRCClient nick host port username password channels or [] use_ssl use_sasl char allow_hosts allow_nicks disable_query client io_loop start
@require_contextdef group_type_get_all context inactive False filters None marker None limit None sort_keys None sort_dirs None offset None list_result False session get_session with session begin filters filters or {} filters['context'] contextquery _generate_paginate_query context session marker limit sort_keys sort_dirs filters offset models GroupTypes if query is None if list_result return []return {}rows query all if list_result result [_dict_with_group_specs_if_authorized context row for row in rows]return resultresult {row['name'] _dict_with_group_specs_if_authorized context row for row in rows}return result
def _ensure_timestamp_field dataset_expr deltas checkpoints measure dataset_expr dshape measureif TS_FIELD_NAME not in measure names dataset_expr bz transform dataset_expr **{TS_FIELD_NAME dataset_expr[AD_FIELD_NAME]} deltas _ad_as_ts deltas checkpoints _ad_as_ts checkpoints else _check_datetime_field TS_FIELD_NAME measure return dataset_expr deltas checkpoints
def _ensure_timestamp_field dataset_expr deltas checkpoints measure dataset_expr dshape measureif TS_FIELD_NAME not in measure names dataset_expr bz transform dataset_expr **{TS_FIELD_NAME dataset_expr[AD_FIELD_NAME]} deltas _ad_as_ts deltas checkpoints _ad_as_ts checkpoints else _check_datetime_field TS_FIELD_NAME measure return dataset_expr deltas checkpoints
def clear_managers for manager in proxies values manager close proxies clear
def getPassword prompt 'Password ' confirm 0 forceTTY 0 confirmPrompt 'Confirmpassword ' mismatchMessage "Passwordsdon'tmatch " isaTTY hasattr sys stdin 'isatty' and sys stdin isatty old Nonetry if not isaTTY if forceTTY try old sys stdin sys stdout sys stdin sys stdout open '/dev/tty' 'r+' except raise RuntimeError 'CannotobtainaTTY' else password sys stdin readline if password[ -1 ] '\n' password password[ -1 ]return passwordwhile 1 try1 _getpass prompt if not confirm return try1try2 _getpass confirmPrompt if try1 try2 return try1else sys stderr write mismatchMessage + '\n' finally if old sys stdin close sys stdin sys stdout old
@contextmanagerdef temp_pipeline_engine calendar sids random_seed symbols None equity_info make_simple_equity_info sids sids start_date calendar[0] end_date calendar[ -1 ] symbols symbols loader make_seeded_random_loader random_seed calendar sids def get_loader column return loaderwith tmp_asset_finder equities equity_info as finder yield SimplePipelineEngine get_loader calendar finder
@contextmanagerdef temp_pipeline_engine calendar sids random_seed symbols None equity_info make_simple_equity_info sids sids start_date calendar[0] end_date calendar[ -1 ] symbols symbols loader make_seeded_random_loader random_seed calendar sids def get_loader column return loaderwith tmp_asset_finder equities equity_info as finder yield SimplePipelineEngine get_loader calendar finder
@csrf_exemptdef gitlab_build request if request method 'POST' try data json loads request body url data['project']['http_url']search_url re sub '^https? // *? ? \\ git $' '\\1' url branches [data['ref'] replace 'refs/heads/' '' ]except ValueError TypeError KeyError log error 'InvalidGitLabwebhookpayload' exc_info True return HttpResponse 'Invalidrequest' status 400 log info 'GitLabwebhooksearch url %sbranches %s' search_url branches projects get_project_from_url search_url if projects return _build_url search_url projects branches else log error 'Projectmatchnotfound url %s' search_url return HttpResponseNotFound 'Projectmatchnotfound' else return HttpResponse 'Methodnotallowed POSTisrequired' status 405
def get_a_line src number global FILE_CACHEif src not in FILE_CACHE FILE_CACHE[src] []for line in open src 'r' FILE_CACHE[src] append line try return FILE_CACHE[src][ number - 1 ]except return ''
def parse_cachecontrol header directives {}for directive in header split ' ' key sep val directive strip partition ' ' if key directives[key lower ] val if sep else None return directives
def passwd_check hashed_passphrase passphrase try algorithm salt pw_digest hashed_passphrase split ' ' 2 except ValueError TypeError return Falsetry h hashlib new algorithm except ValueError return Falseif len pw_digest 0 return Falseh update cast_bytes passphrase 'utf-8' + cast_bytes salt 'ascii' return h hexdigest pw_digest
def passwd_check hashed_passphrase passphrase try algorithm salt pw_digest hashed_passphrase split ' ' 2 except ValueError TypeError return Falsetry h hashlib new algorithm except ValueError return Falseif len pw_digest 0 return Falseh update cast_bytes passphrase 'utf-8' + cast_bytes salt 'ascii' return h hexdigest pw_digest
def get_option option return _ldap_function_call None _ldap get_option option
def _mantissa dval bb _double_as_bytes dval mantissa bb[1] & 15 << 48 mantissa + bb[2] << 40 mantissa + bb[3] << 32 mantissa + bb[4]return mantissa
def action_allowed request app action allowed any match_rules group rules app action for group in getattr request 'groups' return allowed
def get_configured_hdfs_client config hdfs custom config clientconf_usinf_snakebite ['snakebite_with_hadoopcli_fallback' 'snakebite']if six PY3 and custom in conf_usinf_snakebite warnings warn 'snakebiteclientnotcompatiblewithpython3atthemomentfallingbackonhadoopcli' stacklevel 2 return 'hadoopcli'return custom
def get_configured_hdfs_client config hdfs custom config clientconf_usinf_snakebite ['snakebite_with_hadoopcli_fallback' 'snakebite']if six PY3 and custom in conf_usinf_snakebite warnings warn 'snakebiteclientnotcompatiblewithpython3atthemomentfallingbackonhadoopcli' stacklevel 2 return 'hadoopcli'return custom
def set_proxy_facts facts if 'common' in facts common facts['common']if 'http_proxy' in common or 'https_proxy' in common if 'no_proxy' in common and isinstance common['no_proxy'] string_types common['no_proxy'] common['no_proxy'] split ' ' elif 'no_proxy' not in common common['no_proxy'] []if 'generate_no_proxy_hosts' in common and safe_get_bool common['generate_no_proxy_hosts'] if 'no_proxy_internal_hostnames' in common common['no_proxy'] extend common['no_proxy_internal_hostnames'] split ' ' common['no_proxy'] append ' ' + common['dns_domain'] common['no_proxy'] append common['hostname'] common['no_proxy'] sort_unique common['no_proxy'] facts['common'] commonreturn facts
def send_alerts products Product objects filter productalert__status ProductAlert ACTIVE distinct logger info 'Found%dproductswithactivealerts' products count for product in products send_product_alerts product
def new key nonce None if nonce is None nonce get_random_bytes 8 return Salsa20Cipher key nonce
def total_seconds delta if sys version_info[ 2] 2 6 return delta total_seconds day_in_seconds delta days * 24 * 3600 0 micro_in_seconds delta microseconds / 10 0 ** 6 return day_in_seconds + delta seconds + micro_in_seconds
def GetVersionNamespace version ns nsMap[version]if not ns ns serviceNsMap[version]versionId versionIdMap[version]if not versionId namespace nselse namespace '%s/%s' % ns versionId return namespace
def safe_bed_file infile fix_pat re compile '^ track browser ' fd fname tempfile mkstemp in_handle open infile out_handle open fname 'w' for line in in_handle if fix_pat match line line '#' + line out_handle write line in_handle close out_handle close return fname
def to_asyncio_future tornado_future tornado_future convert_yielded tornado_future af asyncio Future tornado concurrent chain_future tornado_future af return af
def _find_exe_version cmd executable cmd split [0]if find_executable executable is None return Noneout Popen cmd shell True stdout PIPE stdouttry out_string out read finally out close result RE_VERSION search out_string if result is None return Nonereturn LooseVersion result group 1 decode
def _find_exe_version cmd executable cmd split [0]if find_executable executable is None return Noneout Popen cmd shell True stdout PIPE stdouttry out_string out read finally out close result RE_VERSION search out_string if result is None return Nonereturn LooseVersion result group 1 decode
def _find_exe_version cmd executable cmd split [0]if find_executable executable is None return Noneout Popen cmd shell True stdout PIPE stdouttry out_string out read finally out close result RE_VERSION search out_string if result is None return Nonereturn LooseVersion result group 1 decode
def unify_button_widths *buttons wid 0for btn in buttons wid max wid len btn['text'] for btn in buttons btn['width'] wid
def unify_button_widths *buttons wid 0for btn in buttons wid max wid len btn['text'] for btn in buttons btn['width'] wid
def time_since time delta time - time_utcnow return format_timedelta delta add_direction True
def test_disposable x DisposableTest with x passAreEqual x Called True Assert hasattr x '__enter__' Assert hasattr x '__exit__' x DisposableTest x __enter__ try passfinally AreEqual x __exit__ None None None None AreEqual x Called True Assert '__enter__' in dir x Assert '__exit__' in dir x Assert '__enter__' in dir DisposableTest Assert '__exit__' in dir DisposableTest
def test_disposable x DisposableTest with x passAreEqual x Called True Assert hasattr x '__enter__' Assert hasattr x '__exit__' x DisposableTest x __enter__ try passfinally AreEqual x __exit__ None None None None AreEqual x Called True Assert '__enter__' in dir x Assert '__exit__' in dir x Assert '__enter__' in dir DisposableTest Assert '__exit__' in dir DisposableTest
def single_line text return re sub '+' '' normalize_newlines text replace '\n' '' strip
def get_file_title files_path_list filename fname os path basename filename same_name_files get_same_name_files files_path_list fname if len same_name_files > 1 compare_path shortest_path same_name_files if compare_path filename same_name_files remove path_components filename compare_path shortest_path same_name_files diff_path differentiate_prefix path_components filename path_components compare_path diff_path_length len diff_path path_component path_components diff_path if diff_path_length > 20 and len path_component > 2 if path_component[0] '/' and path_component[0] '' path_component [path_component[0] ' ' path_component[ -1 ]]else path_component [path_component[2] ' ' path_component[ -1 ]]diff_path os path join *path_component fname fname + '-' + diff_path return fname
def get_file_title files_path_list filename fname os path basename filename same_name_files get_same_name_files files_path_list fname if len same_name_files > 1 compare_path shortest_path same_name_files if compare_path filename same_name_files remove path_components filename compare_path shortest_path same_name_files diff_path differentiate_prefix path_components filename path_components compare_path diff_path_length len diff_path path_component path_components diff_path if diff_path_length > 20 and len path_component > 2 if path_component[0] '/' and path_component[0] '' path_component [path_component[0] ' ' path_component[ -1 ]]else path_component [path_component[2] ' ' path_component[ -1 ]]diff_path os path join *path_component fname fname + '-' + diff_path return fname
def merge_geometries geometries_str sep '$' geometries geometries_str split sep if len geometries 1 return geometries_strelse pool OGRGeometry geometries[0] for geom in geometries pool pool union OGRGeometry geom return pool wkt
def wrap_exceptions fun @functools wraps fun def wrapper self *args **kwargs try return fun self *args **kwargs except EnvironmentError as err if err errno in errno ENOENT errno ESRCH raise NoSuchProcess self pid self _name if err errno in errno EPERM errno EACCES raise AccessDenied self pid self _name raisereturn wrapper
def render_purchase_form_html cart **kwargs return render_to_string 'shoppingcart/cybersource_form html' {'action' get_purchase_endpoint 'params' get_signed_purchase_params cart }
@routes route '/health' def health content Markup markdown markdown 'Theserverishealthy ' return content
@routes route '/health' def health content Markup markdown markdown 'Theserverishealthy ' return content
@routes route '/health' def health content Markup markdown markdown 'Theserverishealthy ' return content
def get_scores video if isinstance video Episode return episode_scoreselif isinstance video Movie return movie_scoresraise ValueError 'videomustbeaninstanceofEpisodeorMovie'
def _get_engine data_dict connection_url data_dict['connection_url']engine _engines get connection_url if not engine extras {'url' connection_url}engine sqlalchemy engine_from_config config 'ckan datastore sqlalchemy ' **extras _engines[connection_url] enginereturn engine
def resolve_cert_reqs candidate if candidate is None return CERT_NONEif isinstance candidate str res getattr ssl candidate None if res is None res getattr ssl 'CERT_' + candidate return resreturn candidate
def extractField field event keyFlattener KeyFlattener [[literalText fieldName formatSpec conversion]] aFormatter parse '{' + field + '}' key keyFlattener flatKey fieldName formatSpec conversion if 'log_flattened' not in event flattenEvent event return event['log_flattened'][key]
def to_arr this return [this get str e for e in xrange len this ]
def to_arr this return [this get str e for e in xrange len this ]
def last_focused_window try return get 'last-focused-main-window' except KeyError return window_by_index -1
def create_variable name shape initializer tf contrib layers xavier_initializer_conv2d variable tf Variable initializer shape shape name name return variable
def get_model_field_name field field slugify field field field replace '-' '_' field field replace ' ' '_' if field in 'id' field + '_'if field upper in PG_RESERVED_KEYWORDS field + '_'if field[ -1 ] '_' field + 'field'try int field float field field '_%s' % field except ValueError passreturn field
def _evaluate_for_annotation evaluator annotation index None if annotation is not None definitions evaluator eval_element _fix_forward_reference evaluator annotation if index is not None definitions list itertools chain from_iterable definition py__getitem__ index for definition in definitions if definition type 'tuple' and len list definition py__iter__ > index return list itertools chain from_iterable evaluator execute d for d in definitions else return []
def allclose a b rtol 1e-05 atol 1e-08 equal_nan False return all isclose a b rtol atol equal_nan
def allclose a b rtol 1e-05 atol 1e-08 equal_nan False return all isclose a b rtol atol equal_nan
def appendTextElements e contentsList se def uconcat text newText se if type newText type text if type text is str text text decode se else newText newText decode se return text + newText e text ''lastElement Nonefor content in contentsList if not isinstance content Text newElement content toElement se if newElement is None continuelastElement newElementlastElement tail ''e append lastElement elif lastElement is None e text uconcat e text content text se else lastElement tail uconcat lastElement tail content text se
def parse_events logger line match log_event_pattern match line if match groups match groupdict groups update {'alert_type' alert_types get groups['alert_type'] '' 'timestamp' calendar timegm datetime strptime groups['timestamp'] '%Y-%m-%d%H %M %S' timetuple 'msg_text' line} return groupselse return None
@keras_testdef test_temporal_classification X_train y_train X_test y_test get_test_data nb_train 500 nb_test 500 input_shape 3 5 classification True nb_class 2 y_train to_categorical y_train y_test to_categorical y_test model Sequential model add GRU y_train shape[ -1 ] input_shape X_train shape[1] X_train shape[2] activation 'softmax' model compile loss 'categorical_crossentropy' optimizer 'adagrad' metrics ['accuracy'] history model fit X_train y_train nb_epoch 20 batch_size 32 validation_data X_test y_test verbose 0 assert history history['val_acc'][ -1 ] > 0 8
@keras_testdef test_temporal_classification X_train y_train X_test y_test get_test_data nb_train 500 nb_test 500 input_shape 3 5 classification True nb_class 2 y_train to_categorical y_train y_test to_categorical y_test model Sequential model add GRU y_train shape[ -1 ] input_shape X_train shape[1] X_train shape[2] activation 'softmax' model compile loss 'categorical_crossentropy' optimizer 'adagrad' metrics ['accuracy'] history model fit X_train y_train nb_epoch 20 batch_size 32 validation_data X_test y_test verbose 0 assert history history['val_acc'][ -1 ] > 0 8
def YamlDumper aff4object aff4object Flush result {}for attribute values in aff4object synced_attributes items result[attribute predicate] []for value in values value value ToRDFValue result[attribute predicate] append [value __class__ __name__ value SerializeToString str value age ] return yaml dump dict aff4_class aff4object __class__ __name__ _urn aff4object urn SerializeToString attributes result age_policy aff4object age_policy
def _exception_data type value traceback sys exc_info while traceback tb_next traceback traceback tb_nextcode traceback tb_frame f_codereturn type __name__ value code co_filename traceback tb_lineno code co_name
def _get_system_username import getpassusername Nonetry username getpass getuser except KeyError passexcept ImportError if win32 import win32apiimport win32securityimport win32profileusername win32api GetUserName return username
def map_base_attributes cube mapper_class naming locale None base [attr for attr in cube all_attributes if attr is_base]mapper mapper_class cube naming locale mapped {attr ref mapper[attr] for attr in base}return mapper fact_name mapped
def app environ start_response data 'Hello World \n'status '200OK'response_headers [ 'Content-type' 'text/plain' 'Transfer-Encoding' 'chunked' ]sys stdout write 'requestreceived' sys stdout flush start_response status response_headers return TestIter
def put url data api_key None try opener request build_request_with_data url data api_key 'PUT' put_request opener open request return json loads put_request read except urllib2 HTTPError as e return dict status 'error' message str e read 1024
def patch_builtins if sys version_info[ 2] < 3 3 patch_module 'builtins'
def get_thunk_type_set it_types []i_types []j 0getter_code 'if 0 {}'for I_typenum I_type in I_TYPES piece '\nelseif I_typenum % I_typenum s {\nif T_typenum -1 {return% j s }'getter_code + piece % dict I_typenum I_typenum j j i_types append j I_typenum None I_type None j + 1for T_typenum T_type in T_TYPES piece '\nelseif T_typenum % T_typenum s {return% j s }'getter_code + piece % dict T_typenum T_typenum j j it_types append j I_typenum T_typenum I_type T_type j + 1getter_code + '\n}'return i_types it_types GET_THUNK_CASE_TEMPLATE % dict content getter_code
def get_thunk_type_set it_types []i_types []j 0getter_code 'if 0 {}'for I_typenum I_type in I_TYPES piece '\nelseif I_typenum % I_typenum s {\nif T_typenum -1 {return% j s }'getter_code + piece % dict I_typenum I_typenum j j i_types append j I_typenum None I_type None j + 1for T_typenum T_type in T_TYPES piece '\nelseif T_typenum % T_typenum s {return% j s }'getter_code + piece % dict T_typenum T_typenum j j it_types append j I_typenum T_typenum I_type T_type j + 1getter_code + '\n}'return i_types it_types GET_THUNK_CASE_TEMPLATE % dict content getter_code
def guess_plateau x y if len x len y return 0diffs []indexes range len y for i in indexes if i + 1 not in indexes continuediffs append y[ i + 1 ] - y[i] diffs np array diffs ymax y[ -1 ]for i in indexes if y[i] > ymax - diffs std and y[i] < ymax + diffs std ymax y[i]breakreturn ymax
def max_seeds_from_threads threads seeds 0for background in threads log debug u'Comingupnext %s' % background tracker background join seeds max seeds background tracker_seeds log debug u'Currenthightestnumberofseedsfound %s' % seeds return seeds
def max_seeds_from_threads threads seeds 0for background in threads log debug u'Comingupnext %s' % background tracker background join seeds max seeds background tracker_seeds log debug u'Currenthightestnumberofseedsfound %s' % seeds return seeds
def _ParseOutputSpec output_spec pattern re compile ' O 0 1 2 l s c \\d+ ' m pattern match output_spec if m is None raise ValueError 'Failedtoparseoutputspec ' + output_spec out_dims int m group 2 out_func m group 3 if out_func 'c' and out_dims 1 raise ValueError 'CTCcanonlybeusedwitha1-Dsequence ' num_classes int m group 4 return out_dims out_func num_classes
def rest_api_context_factory ca_certificate control_credential return _ControlServiceContextFactory ca_certificate control_credential 'user-'
def rest_api_context_factory ca_certificate control_credential return _ControlServiceContextFactory ca_certificate control_credential 'user-'
def _get_k_p_a font left right chars left + right args [None 1 cf kCFTypeDictionaryKeyCallBacks cf kCFTypeDictionaryValueCallBacks]attributes cf CFDictionaryCreateMutable *args cf CFDictionaryAddValue attributes kCTFontAttributeName font string cf CFAttributedStringCreate None CFSTR chars attributes typesetter ct CTTypesetterCreateWithAttributedString string cf CFRelease string cf CFRelease attributes range CFRange 0 1 line ct CTTypesetterCreateLine typesetter range offset ct CTLineGetOffsetForStringIndex line 1 None cf CFRelease line cf CFRelease typesetter return offset
def setup_platform hass config add_devices discovery_info None from concord232 import client as concord232_clienthost config get CONF_HOST port config get CONF_PORT exclude config get CONF_EXCLUDE_ZONES zone_types config get CONF_ZONE_TYPES sensors []try _LOGGER debug 'InitializingClient' client concord232_client Client 'http //{} {}' format host port client zones client list_zones client last_zone_update datetime datetime now except requests exceptions ConnectionError as ex _LOGGER error 'UnabletoconnecttoConcord232 %s' str ex return Falsefor zone in client zones _LOGGER info 'LoadingZonefound %s' zone['name'] if zone['number'] not in exclude sensors append Concord232ZoneSensor hass client zone zone_types get zone['number'] get_opening_type zone add_devices sensors return True
def get_all_key_combinations data flattened_schema schema_prefixes set [key[ -1 ] for key in flattened_schema] combinations set [ ] for key in sorted data keys key flattened_order_key key_prefix key[ -1 2]if key_prefix not in schema_prefixes continueif tuple tuple key[ -3 ] not in combinations continuecombinations add tuple key[ -1 ] return combinations
def get_all_key_combinations data flattened_schema schema_prefixes set [key[ -1 ] for key in flattened_schema] combinations set [ ] for key in sorted data keys key flattened_order_key key_prefix key[ -1 2]if key_prefix not in schema_prefixes continueif tuple tuple key[ -3 ] not in combinations continuecombinations add tuple key[ -1 ] return combinations
def book_info td title td find 'div' 'thumbheader' a textby_author td find 'div' 'AuthorName' textauthors [x strip for x in re sub '^By' '' by_author split ' ' ]isbn_link td find 'div' 'thumbheader' a get 'href' isbn re match '/product/ * \\ do' isbn_link groups [0]date td find 'span' 'directorydate' text strip return {'title' title 'authors' authors 'isbn' isbn 'date' date}
def book_info td title td find 'div' 'thumbheader' a textby_author td find 'div' 'AuthorName' textauthors [x strip for x in re sub '^By' '' by_author split ' ' ]isbn_link td find 'div' 'thumbheader' a get 'href' isbn re match '/product/ * \\ do' isbn_link groups [0]date td find 'span' 'directorydate' text strip return {'title' title 'authors' authors 'isbn' isbn 'date' date}
def fix_s3_host request signature_version region_name default_endpoint_url 's3 amazonaws com' **kwargs if signature_version is not botocore UNSIGNED and 's3v4' in signature_version returnelif not _allowed_region region_name returntry switch_to_virtual_host_style request signature_version default_endpoint_url except InvalidDNSNameError as e bucket_name e kwargs['bucket_name']logger debug 'NotchangingURI bucketisnotDNScompatible %s' bucket_name
def trigger_build project version None record True force False basic False from readthedocs projects tasks import update_docsfrom readthedocs builds models import Buildif project skip return Noneif not version version project versions get slug LATEST kwargs dict pk project pk version_pk version pk record record force force basic basic build Noneif record build Build objects create project project version version type 'html' state 'triggered' success True kwargs['build_pk'] build pkoptions {}if project build_queue options['queue'] project build_queuetime_limit DOCKER_LIMITS['time']try if project container_time_limit time_limit int project container_time_limit except ValueError passoptions['soft_time_limit'] time_limitoptions['time_limit'] int time_limit * 1 2 update_docs apply_async kwargs kwargs **options return build
def trigger_build project version None record True force False basic False from readthedocs projects tasks import update_docsfrom readthedocs builds models import Buildif project skip return Noneif not version version project versions get slug LATEST kwargs dict pk project pk version_pk version pk record record force force basic basic build Noneif record build Build objects create project project version version type 'html' state 'triggered' success True kwargs['build_pk'] build pkoptions {}if project build_queue options['queue'] project build_queuetime_limit DOCKER_LIMITS['time']try if project container_time_limit time_limit int project container_time_limit except ValueError passoptions['soft_time_limit'] time_limitoptions['time_limit'] int time_limit * 1 2 update_docs apply_async kwargs kwargs **options return build
def trigger_build project version None record True force False basic False from readthedocs projects tasks import update_docsfrom readthedocs builds models import Buildif project skip return Noneif not version version project versions get slug LATEST kwargs dict pk project pk version_pk version pk record record force force basic basic build Noneif record build Build objects create project project version version type 'html' state 'triggered' success True kwargs['build_pk'] build pkoptions {}if project build_queue options['queue'] project build_queuetime_limit DOCKER_LIMITS['time']try if project container_time_limit time_limit int project container_time_limit except ValueError passoptions['soft_time_limit'] time_limitoptions['time_limit'] int time_limit * 1 2 update_docs apply_async kwargs kwargs **options return build
def _bem_specify_els bem els mults sol np zeros len els bem['solution'] shape[1] scalp bem['surfs'][0]rrs np concatenate [apply_trans bem['head_mri_t']['trans'] el['rmag'] for el in els] axis 0 ws np concatenate [el['w'] for el in els] tri_weights tri_idx _project_onto_surface rrs scalp tri_weights * wsweights np einsum 'ij jik->jk' tri_weights bem['solution'][scalp['tris'][tri_idx]] edges np concatenate [[0] np cumsum [len el['w'] for el in els] ] for ii start stop in enumerate zip edges[ -1 ] edges[1 ] sol[ii] weights[start stop] sum 0 sol * multsreturn sol
def _bem_specify_els bem els mults sol np zeros len els bem['solution'] shape[1] scalp bem['surfs'][0]rrs np concatenate [apply_trans bem['head_mri_t']['trans'] el['rmag'] for el in els] axis 0 ws np concatenate [el['w'] for el in els] tri_weights tri_idx _project_onto_surface rrs scalp tri_weights * wsweights np einsum 'ij jik->jk' tri_weights bem['solution'][scalp['tris'][tri_idx]] edges np concatenate [[0] np cumsum [len el['w'] for el in els] ] for ii start stop in enumerate zip edges[ -1 ] edges[1 ] sol[ii] weights[start stop] sum 0 sol * multsreturn sol
def check_envelope result func cargs offset -1 env ptr_byref cargs offset return env
def action name text confirmation None def wrap f f _action name text confirmation return freturn wrap
def action name text confirmation None def wrap f f _action name text confirmation return freturn wrap
def action name text confirmation None def wrap f f _action name text confirmation return freturn wrap
@register simple_tag takes_context True def translate_url context language try request context[u'request']except KeyError return u''view resolve request path current_language translation get_language translation activate language try url reverse view func args view args kwargs view kwargs except NoReverseMatch try url_name view url_name if not view namespace else u'%s %s' % view namespace view url_name url reverse url_name args view args kwargs view kwargs except NoReverseMatch url_name u'admin ' + view url_name url reverse url_name args view args kwargs view kwargs translation activate current_language if context[u'request'] META[u'QUERY_STRING'] url + u'?' + context[u'request'] META[u'QUERY_STRING'] return url
def negaterow row K return [ - element for element in row]
def media_prev_track hass hass services call DOMAIN SERVICE_MEDIA_PREVIOUS_TRACK
def user_has_perm_note note profile request None if note author and note author id profile id return Trueif request and note note_type comm REVIEWER_COMMENT return acl check_reviewer request if request and note note_type comm DEVELOPER_COMMENT and acl check_reviewer request return Trueuser_is_author profile addons filter pk note thread _addon_id if user_is_author exists and note read_permission_developer or note note_type comm REVIEWER_PUBLIC_COMMENT return Truereturn check_acls_comm_obj note profile
def add_gateway_router router ext_network profile None conn _auth profile return conn add_gateway_router router ext_network
def get_requests_resp_json resp if callable resp json return resp json return resp json
def levelize_path path parts tuple filter None path split '/' return [ '/' + '/' join parts[ n] for n in range len parts 0 -1 ]
def _check_rc rc errno None if rc -1 if errno is None from zmq backend import zmq_errnoerrno zmq_errno from zmq import EAGAIN ETERMif errno EINTR raise InterruptedSystemCall errno elif errno EAGAIN raise Again errno elif errno ETERM raise ContextTerminated errno else raise ZMQError errno
def _check_rc rc errno None if rc -1 if errno is None from zmq backend import zmq_errnoerrno zmq_errno from zmq import EAGAIN ETERMif errno EINTR raise InterruptedSystemCall errno elif errno EAGAIN raise Again errno elif errno ETERM raise ContextTerminated errno else raise ZMQError errno
def remove_headers *headers def _decorator func '\nDecoratesthegivenfunction \n'@wraps func def _inner *args **kwargs '\nAlterstheresponse \n'response func *args **kwargs remove_headers_from_response response *headers return responsereturn _innerreturn _decorator
def http_response_to_document_iters response read_chunk_size 4096 chunked is_chunked dict response getheaders if response status 200 if chunked return iter [ 0 None None response getheaders response ] content_length int response getheader 'Content-Length' return iter [ 0 content_length - 1 content_length response getheaders response ] content_type params_list parse_content_type response getheader 'Content-Type' if content_type 'multipart/byteranges' start end length parse_content_range response getheader 'Content-Range' return iter [ start end length response getheaders response ] else params dict params_list return multipart_byteranges_to_document_iters response params['boundary'] read_chunk_size
def name dev return info dev get 'N' None
def setup_platform hass config add_entities discovery_info None if config[CONF_TYPE] TEMPERATURE minimum_value maximum_value update_and_define_min_max config KNX_TEMP_MIN KNX_TEMP_MAX add_entities [KNXSensorFloatClass hass KNXConfig config TEMP_CELSIUS minimum_value maximum_value ] elif config[CONF_TYPE] SPEED_MS minimum_value maximum_value update_and_define_min_max config KNX_SPEED_MS_MIN KNX_SPEED_MS_MAX add_entities [KNXSensorFloatClass hass KNXConfig config SPEED_METERPERSECOND minimum_value maximum_value ] elif config[CONF_TYPE] ILLUMINANCE minimum_value maximum_value update_and_define_min_max config KNX_LUX_MIN KNX_LUX_MAX add_entities [KNXSensorFloatClass hass KNXConfig config ILLUMINANCE_LUX minimum_value maximum_value ]
def has_discussion_privileges user course_id roles get_role_ids course_id for role in roles if user id in roles[role] return Truereturn False
def box on None ax gca on _string_to_bool on if on is None on not ax get_frame_on ax set_frame_on on
def _get_service_command_generator run utils run global _service_command_generatortry return _service_command_generatorexcept NameError command_generator _command_generators[get_name_of_init run ]_service_command_generator _ServiceCommandGenerator command_generator return _service_command_generator
def runnable_payloads shell_obj result []for payload_name in get_payload_list payload get_payload_instance payload_name shell_obj if payload can_run result append payload_name return result
def _get_xunit_func obj name meth getattr obj name None if fixtures getfixturemarker meth is None return meth
def _get_xunit_func obj name meth getattr obj name None if fixtures getfixturemarker meth is None return meth
@release command def ghrelease version get_version 1 tag 'v' + version with open os path join BASE 'changelog md' as f cl_md f read subprocess check_call ['github-release' 'release' '-u' GITHUB_USER '-r' GITHUB_REPO '--tag' tag '--name' '{}{}' format GITHUB_REPO version '--description' cl_md] tarball os path join BASE 'dist' 'beets-{} tar gz' format version subprocess check_call ['github-release' 'upload' '-u' GITHUB_USER '-r' GITHUB_REPO '--tag' tag '--name' os path basename tarball '--file' tarball]
@release command def ghrelease version get_version 1 tag 'v' + version with open os path join BASE 'changelog md' as f cl_md f read subprocess check_call ['github-release' 'release' '-u' GITHUB_USER '-r' GITHUB_REPO '--tag' tag '--name' '{}{}' format GITHUB_REPO version '--description' cl_md] tarball os path join BASE 'dist' 'beets-{} tar gz' format version subprocess check_call ['github-release' 'upload' '-u' GITHUB_USER '-r' GITHUB_REPO '--tag' tag '--name' os path basename tarball '--file' tarball]
def catch_ioerror meth @wraps meth def wrapper self *args **kwargs try return meth self *args **kwargs except IOError as errno strerror if errno ENOSPC msg 'Nospaceleftondevice'raise ScanMustStopByKnownReasonExc msg return wrapper
def register_local_role name role_fn set_implicit_options role_fn _roles[name] role_fn
def mean_quadratic_weighted_kappa kappas weights None kappas np array kappas dtype float if weights is None weights np ones np shape kappas else weights weights / np mean weights kappas np array [min x 0 999 for x in kappas] kappas np array [max x -0 999 for x in kappas] z 0 5 * np log 1 + kappas / 1 - kappas * weights z np mean z return np exp 2 * z - 1 / np exp 2 * z + 1
def mean_quadratic_weighted_kappa kappas weights None kappas np array kappas dtype float if weights is None weights np ones np shape kappas else weights weights / np mean weights kappas np array [min x 0 999 for x in kappas] kappas np array [max x -0 999 for x in kappas] z 0 5 * np log 1 + kappas / 1 - kappas * weights z np mean z return np exp 2 * z - 1 / np exp 2 * z + 1
def mean_quadratic_weighted_kappa kappas weights None kappas np array kappas dtype float if weights is None weights np ones np shape kappas else weights weights / np mean weights kappas np array [min x 0 999 for x in kappas] kappas np array [max x -0 999 for x in kappas] z 0 5 * np log 1 + kappas / 1 - kappas * weights z np mean z return np exp 2 * z - 1 / np exp 2 * z + 1
def parse_freshdesk_event event_string data event_string replace '{' '' replace '}' '' replace ' ' ' ' split ' ' if len data 2 return dataelse property _ from_state _ to_state datareturn [property property_name property int from_state property_name property int to_state ]
def prepare_token_revocation_request url token token_type_hint u'access_token' callback None body u'' **kwargs if not is_secure_transport url raise InsecureTransportError params [ u'token' token ]if token_type_hint params append u'token_type_hint' token_type_hint for k in kwargs if kwargs[k] params append unicode_type k kwargs[k] headers {u'Content-Type' u'application/x-www-form-urlencoded'}if callback params append u'callback' callback return add_params_to_uri url params headers body else return url headers add_params_to_qs body params
def load path debug False if os environ get 'APPENGINE_RUNTIME' 'python27' warnings warn _PYTHON27_DEPRECATION DeprecationWarning stacklevel 2 return _load_internal_django path debug else return _load_user_django path debug
def process_message data if u'printusers' in data[u'text'] for user in sc api_call u'users list' [u'members'] print user[u'name'] user[u'id']
def inlineCallbacks f @wraps f def unwindGenerator *args **kwargs try gen f *args **kwargs except _DefGen_Return raise TypeError 'inlineCallbacksrequires%rtoproduceagenerator insteadcaughtreturnValuebeingusedinanon-generator' % f if not isinstance gen types GeneratorType raise TypeError 'inlineCallbacksrequires%rtoproduceagenerator insteadgot%r' % f gen return _inlineCallbacks None gen Deferred return unwindGenerator
def get_current_device return current_context device
def updateTwistedVersionInformation baseDirectory now for project in findTwistedProjects baseDirectory project updateVersion getNextVersion project getVersion now now
def generate_patterns django_name gwt_name pattern_list defaults patterns django_name '^rpc/' 'views handle_rpc' '^rpc_doc' 'views rpc_documentation' debug_pattern_list defaults patterns '' '^ ?P<forward_addr>autotest * ' 'autotest frontend afe views gwt_forward' '^client/ ?P<path> * $' 'django views static serve' {'document_root' os path join os path dirname __file__ ' ' 'frontend' 'client' 'www' } '^$' 'django views generic simple redirect_to' {'url' 'client/autotest % name s/% name s html' % dict name gwt_name } return pattern_list debug_pattern_list
def deserialize_field field value try deserialized json loads value if deserialized is None return deserializedtry field from_json deserialized return deserializedexcept ValueError TypeError return valueexcept ValueError TypeError return value
def test_tokenize_0 s '123klsdgh56 7?98 2---\\%-1e3'true_tokens ['' 123 'klsdgh' 56 7 '?' 98 2 '---\\%' -1000 0 ]tokens tokenize_by_number s assert token_lists_match tokens true_tokens
def test_tokenize_0 s '123klsdgh56 7?98 2---\\%-1e3'true_tokens ['' 123 'klsdgh' 56 7 '?' 98 2 '---\\%' -1000 0 ]tokens tokenize_by_number s assert token_lists_match tokens true_tokens
def listbucket path_prefix marker None prefix None max_keys None delimiter None retry_params None _account_id None if prefix common validate_bucket_path path_prefix bucket path_prefixelse bucket prefix common _process_path_prefix path_prefix if marker and marker startswith bucket marker marker[ len bucket + 1 ]api storage_api _get_storage_api retry_params retry_params account_id _account_id options {}if marker options['marker'] markerif max_keys options['max-keys'] max_keysif prefix options['prefix'] prefixif delimiter options['delimiter'] delimiterreturn _Bucket api bucket options
def listbucket path_prefix marker None prefix None max_keys None delimiter None retry_params None _account_id None if prefix common validate_bucket_path path_prefix bucket path_prefixelse bucket prefix common _process_path_prefix path_prefix if marker and marker startswith bucket marker marker[ len bucket + 1 ]api storage_api _get_storage_api retry_params retry_params account_id _account_id options {}if marker options['marker'] markerif max_keys options['max-keys'] max_keysif prefix options['prefix'] prefixif delimiter options['delimiter'] delimiterreturn _Bucket api bucket options
def can_change_page request from cms utils import page_permissionsuser request usercurrent_page request current_pageif current_page return page_permissions user_can_change_page user current_page site Site objects get_current request return page_permissions user_can_change_all_pages user site
def can_change_page request from cms utils import page_permissionsuser request usercurrent_page request current_pageif current_page return page_permissions user_can_change_page user current_page site Site objects get_current request return page_permissions user_can_change_all_pages user site
def get_user_location if c user and c user pref_use_global_defaults return ''return get_request_location request c
def _group_tasks_by_name_and_status task_dict group_status {}for task in task_dict if task task_family not in group_status group_status[task task_family] []group_status[task task_family] append task return group_status
def _group_tasks_by_name_and_status task_dict group_status {}for task in task_dict if task task_family not in group_status group_status[task task_family] []group_status[task task_family] append task return group_status
def dimension_div a b if a datashape var or b datashape var return datashape varif isinstance a Fixed a int a if isinstance b Fixed b int b return int ceil a / b
@cache request env path_info time_expire 5 cache_model cache disk def cache_controller_on_disk t time ctime return dict time t link A 'clicktoreload' _href URL r request
def upload_ssh_key host username password ssh_key None ssh_key_file None protocol None port None certificate_verify False if protocol is None protocol 'https'if port is None port 443url '{0} //{1} {2}/host/ssh_root_authorized_keys' format protocol host port ret {}result Nonetry if ssh_key result salt utils http query url status True text True method 'PUT' username username password password data ssh_key verify_ssl certificate_verify elif ssh_key_file result salt utils http query url status True text True method 'PUT' username username password password data_file ssh_key_file data_render False verify_ssl certificate_verify if result get 'status' 200 ret['status'] Trueelse ret['status'] Falseret['Error'] result['error']except Exception as msg ret['status'] Falseret['Error'] msgreturn ret
def exponential_decay_noise xp shape dtype hook opt std numpy sqrt hook eta / numpy power 1 + opt t 0 55 return xp random normal 0 std shape astype dtype
def updating name jail None chroot None root None filedate None filename None opts ''if filedate opts + 'd{0}' format filedate if filename opts + 'f{0}' format filename cmd _pkg jail chroot root cmd append 'updating' if opts cmd append '-' + opts cmd append name return __salt__['cmd run'] cmd output_loglevel 'trace' python_shell False
def migrate_node node for key file_id in node files_current iteritems if key not in node files_versions node files_versions[key] [file_id]elif file_id not in node files_versions[key] node files_versions[key] append file_id node save
def migrate_node node for key file_id in node files_current iteritems if key not in node files_versions node files_versions[key] [file_id]elif file_id not in node files_versions[key] node files_versions[key] append file_id node save
def create_and_copy test fixture pool fixture test service service_for_pool test pool volume service get MY_VOLUME pool2 fixture test service2 service_for_pool test pool2 volume2 Volume node_id service node_id name MY_VOLUME service service2 d pool create volume def created_filesystem filesystem path filesystem get_path path child 'file' setContent 'somebytes' path child 'directory' makedirs copying copy volume volume2 copying addCallback lambda ignored CopyVolumes from_volume volume to_volume volume2 return copyingd addCallback created_filesystem return d
def create_and_copy test fixture pool fixture test service service_for_pool test pool volume service get MY_VOLUME pool2 fixture test service2 service_for_pool test pool2 volume2 Volume node_id service node_id name MY_VOLUME service service2 d pool create volume def created_filesystem filesystem path filesystem get_path path child 'file' setContent 'somebytes' path child 'directory' makedirs copying copy volume volume2 copying addCallback lambda ignored CopyVolumes from_volume volume to_volume volume2 return copyingd addCallback created_filesystem return d
def _serialize_discussion_entities request context discussion_entities requested_fields discussion_entity_type results []usernames []include_profile_image _include_profile_image requested_fields for entity in discussion_entities if discussion_entity_type DiscussionEntity thread serialized_entity ThreadSerializer entity context context dataelif discussion_entity_type DiscussionEntity comment serialized_entity CommentSerializer entity context context dataresults append serialized_entity if include_profile_image if serialized_entity['author'] and serialized_entity['author'] not in usernames usernames append serialized_entity['author'] if 'endorsed' in serialized_entity and serialized_entity['endorsed'] and 'endorsed_by' in serialized_entity and serialized_entity['endorsed_by'] and serialized_entity['endorsed_by'] not in usernames usernames append serialized_entity['endorsed_by'] results _add_additional_response_fields request results usernames discussion_entity_type include_profile_image return results
def unzip_archive archive assert os path exists archive 'Filenotfound-%s' % archive tmpdir os path join tempfile gettempdir os path basename archive assert tmpdir archive if os path exists tmpdir passelif tarfile is_tarfile archive print 'Extractingtarfile 'with tarfile open archive as tf tf extractall path tmpdir elif zipfile is_zipfile archive print 'Extractingzipfile 'with zipfile ZipFile archive as zf zf extractall path tmpdir else raise ValueError 'Unknownfiletypefor%s' % os path basename archive return tmpdir
def unzip_archive archive assert os path exists archive 'Filenotfound-%s' % archive tmpdir os path join tempfile gettempdir os path basename archive assert tmpdir archive if os path exists tmpdir passelif tarfile is_tarfile archive print 'Extractingtarfile 'with tarfile open archive as tf tf extractall path tmpdir elif zipfile is_zipfile archive print 'Extractingzipfile 'with zipfile ZipFile archive as zf zf extractall path tmpdir else raise ValueError 'Unknownfiletypefor%s' % os path basename archive return tmpdir
def sort_and_count a if len a 1 return a 0 b x sort_and_count a[ len a / 2 ] c y sort_and_count a[ len a / 2 ] d z merge_and_count_inv b c return d x + y + z
def sort_and_count a if len a 1 return a 0 b x sort_and_count a[ len a / 2 ] c y sort_and_count a[ len a / 2 ] d z merge_and_count_inv b c return d x + y + z
def get_interface dev bInterfaceNumber bmRequestType util build_request_type util CTRL_IN util CTRL_TYPE_STANDARD util CTRL_RECIPIENT_INTERFACE return dev ctrl_transfer bmRequestType bmRequestType bRequest 10 wIndex bInterfaceNumber data_or_wLength 1 [0]
def libvlc_media_list_player_set_media_player p_mlp p_mi f _Cfunctions get 'libvlc_media_list_player_set_media_player' None or _Cfunction 'libvlc_media_list_player_set_media_player' 1 1 None None MediaListPlayer MediaPlayer return f p_mlp p_mi
def libvlc_media_list_player_set_media_player p_mlp p_mi f _Cfunctions get 'libvlc_media_list_player_set_media_player' None or _Cfunction 'libvlc_media_list_player_set_media_player' 1 1 None None MediaListPlayer MediaPlayer return f p_mlp p_mi
def load_all_models s3db load_all_models return 'ok'
def do_pdp_descriptor conf cert pdp md PDPDescriptor pdp protocol_support_enumeration samlp NAMESPACEendps conf getattr 'endpoints' 'pdp' if endps for endpoint instlist in do_endpoints endps ENDPOINTS['pdp'] items setattr pdp endpoint instlist _do_nameid_format pdp conf 'pdp' if cert pdp key_descriptor do_key_descriptor cert return pdp
def find_wheels projects search_dirs wheels []for project in projects for dirname in search_dirs files glob glob os path join dirname project + '-* whl' if files wheels append os path abspath files[0] breakelse logger fatal 'Cannotfindawheelfor%s' % project return wheels
def github_pull_request_merge registry xml_parent data osb XML SubElement xml_parent 'org jenkinsci plugins ghprb GhprbPullRequestMerge' mapping [ 'only-admins-merge' 'onlyAdminsMerge' 'false' 'disallow-own-code' 'disallowOwnCode' 'false' 'merge-comment' 'mergeComment' '' 'fail-on-non-merge' 'failOnNonMerge' 'false' 'delete-on-merge' 'deleteOnMerge' 'false' ]helpers convert_mapping_to_xml osb data mapping fail_required True
def github_pull_request_merge registry xml_parent data osb XML SubElement xml_parent 'org jenkinsci plugins ghprb GhprbPullRequestMerge' mapping [ 'only-admins-merge' 'onlyAdminsMerge' 'false' 'disallow-own-code' 'disallowOwnCode' 'false' 'merge-comment' 'mergeComment' '' 'fail-on-non-merge' 'failOnNonMerge' 'false' 'delete-on-merge' 'deleteOnMerge' 'false' ]helpers convert_mapping_to_xml osb data mapping fail_required True
def github_pull_request_merge registry xml_parent data osb XML SubElement xml_parent 'org jenkinsci plugins ghprb GhprbPullRequestMerge' mapping [ 'only-admins-merge' 'onlyAdminsMerge' 'false' 'disallow-own-code' 'disallowOwnCode' 'false' 'merge-comment' 'mergeComment' '' 'fail-on-non-merge' 'failOnNonMerge' 'false' 'delete-on-merge' 'deleteOnMerge' 'false' ]helpers convert_mapping_to_xml osb data mapping fail_required True
def github_pull_request_merge registry xml_parent data osb XML SubElement xml_parent 'org jenkinsci plugins ghprb GhprbPullRequestMerge' mapping [ 'only-admins-merge' 'onlyAdminsMerge' 'false' 'disallow-own-code' 'disallowOwnCode' 'false' 'merge-comment' 'mergeComment' '' 'fail-on-non-merge' 'failOnNonMerge' 'false' 'delete-on-merge' 'deleteOnMerge' 'false' ]helpers convert_mapping_to_xml osb data mapping fail_required True
def github_pull_request_merge registry xml_parent data osb XML SubElement xml_parent 'org jenkinsci plugins ghprb GhprbPullRequestMerge' mapping [ 'only-admins-merge' 'onlyAdminsMerge' 'false' 'disallow-own-code' 'disallowOwnCode' 'false' 'merge-comment' 'mergeComment' '' 'fail-on-non-merge' 'failOnNonMerge' 'false' 'delete-on-merge' 'deleteOnMerge' 'false' ]helpers convert_mapping_to_xml osb data mapping fail_required True
def denoise_tv_chambolle im weight 0 1 eps 0 0002 n_iter_max 200 multichannel False im_type im dtypeif not im_type kind 'f' im img_as_float im if multichannel out np zeros_like im for c in range im shape[ -1 ] out[ c] _denoise_tv_chambolle_nd im[ c] weight eps n_iter_max else out _denoise_tv_chambolle_nd im weight eps n_iter_max return out
def migRing populations k selection replacement None migarray None nbr_demes len populations if migarray is None migarray range 1 nbr_demes + [0] immigrants [[] for i in xrange nbr_demes ]emigrants [[] for i in xrange nbr_demes ]for from_deme in xrange nbr_demes emigrants[from_deme] extend selection populations[from_deme] k if replacement is None immigrants[from_deme] emigrants[from_deme]else immigrants[from_deme] extend replacement populations[from_deme] k for from_deme to_deme in enumerate migarray for i immigrant in enumerate immigrants[to_deme] indx populations[to_deme] index immigrant populations[to_deme][indx] emigrants[from_deme][i]
def empty shape dtype float order 'C' return cupy ndarray shape dtype dtype order order
@command 'historyclear' def clear_history g userhist['history'] songs []history save g message 'Historycleared'g content logo
def python_branch return _sys_version [2]
def apply_all_middleware request **attrs request apply_view_middleware apply_request_middleware request for key value in attrs items setattr request key value return request
def ErrCheckBool result func args if not result raise WinError return args
def pr_image_represent image_name format None size table current s3db pr_image_libraryquery table original_name image_name if format query query & table format format if size query query & table width size[0] & table height size[1] image current db query select table new_name limitby 0 1 first if image return image new_nameelse return image_name
def find_test_dir start_dir None if not start_dir start_dir ' 'target os path abspath start_dir while True if os path isdir os path join target 'Bio' and os path isdir os path join target 'Tests' return os path abspath os path join target 'Tests' new tmp os path split target if target new breaktarget newraise ValueError 'NotwithinBiopythonsourcetree %r' % os path abspath start_dir
def find_test_dir start_dir None if not start_dir start_dir ' 'target os path abspath start_dir while True if os path isdir os path join target 'Bio' and os path isdir os path join target 'Tests' return os path abspath os path join target 'Tests' new tmp os path split target if target new breaktarget newraise ValueError 'NotwithinBiopythonsourcetree %r' % os path abspath start_dir
def hypervolume pointset ref hv _HyperVolume ref return hv compute pointset
def backup_dir dir ext ' bak' n 1extension extwhile os path exists dir + extension n + 1extension ext + str n return dir + extension
def jupyter_info import nbconvertreturn dict nbconvert_version nbconvert __version__
def _get_report_choice key import doctestreturn {DOCTEST_REPORT_CHOICE_UDIFF doctest REPORT_UDIFF DOCTEST_REPORT_CHOICE_CDIFF doctest REPORT_CDIFF DOCTEST_REPORT_CHOICE_NDIFF doctest REPORT_NDIFF DOCTEST_REPORT_CHOICE_ONLY_FIRST_FAILURE doctest REPORT_ONLY_FIRST_FAILURE DOCTEST_REPORT_CHOICE_NONE 0}[key]
def restore fid url build_url RESOURCE id fid route 'restore' return request 'post' url
def do_baremetal_node_list cs _args _emit_deprecation_warning 'baremetal-node-list' nodes cs baremetal list _print_baremetal_nodes_list nodes
def checkEnvironment if platform system 'Darwin' fatal 'ThisscriptshouldberunonaMacOSX10 4system' if platform release < '8 ' fatal 'ThisscriptshouldberunonaMacOSX10 4system' if not os path exists SDKPATH fatal 'PleaseinstallthelatestversionofXcodeandthe%sSDK' % os path basename SDKPATH[ -4 ]
def quote_sheetname sheetname if not sheetname isalnum and not sheetname startswith "'" sheetname sheetname replace "'" "''" sheetname "'%s'" % sheetname return sheetname
def quote_sheetname sheetname if not sheetname isalnum and not sheetname startswith "'" sheetname sheetname replace "'" "''" sheetname "'%s'" % sheetname return sheetname
def get_patches_from_dir path patches []for root subdirs files in core walk path for name in [f for f in files if f endswith u' patch' ] patches append core decode os path join root name return patches
def colormaps pass
def resource_filename fn return os path join os path dirname sys testing_document_filename 'resources' os path splitext os path basename sys testing_document_filename [0] fn
def resource_filename fn return os path join os path dirname sys testing_document_filename 'resources' os path splitext os path basename sys testing_document_filename [0] fn
def test_constructed_module ModuleType type sys TopModule ModuleType 'TopModule' sys modules['TopModule'] TopModuleSubModule ModuleType 'SubModule' SubModule Object object TopModule SubModule SubModuletry import TopModule SubModuleAssertUnreachable except ImportError passdel sys modules['TopModule']
def test_constructed_module ModuleType type sys TopModule ModuleType 'TopModule' sys modules['TopModule'] TopModuleSubModule ModuleType 'SubModule' SubModule Object object TopModule SubModule SubModuletry import TopModule SubModuleAssertUnreachable except ImportError passdel sys modules['TopModule']
def atleast_1d *arys res []for a in arys if not isinstance a cupy ndarray raise TypeError 'Onlycupyarrayscanbeatleast_1d' if a ndim 0 a a reshape 1 res append a if len res 1 res res[0]return res
@pytest mark multithreaddef test_num_reads_threads import threadingdef count_reads rparser for _ in rparser passn_threads 4threads []rparser ReadParser utils get_test_data '100-reads fq gz' for _ in range n_threads thr threading Thread target count_reads args [rparser] threads append thr thr start for thr in threads thr join assert rparser num_reads 100
@pytest mark multithreaddef test_num_reads_threads import threadingdef count_reads rparser for _ in rparser passn_threads 4threads []rparser ReadParser utils get_test_data '100-reads fq gz' for _ in range n_threads thr threading Thread target count_reads args [rparser] threads append thr thr start for thr in threads thr join assert rparser num_reads 100
def split_url_or_path url_or_path if ' //' in url_or_path return url_or_path rstrip '/' rsplit '/' 1 return osp split url_or_path rstrip osp sep
def reduce_abs_inequalities exprs gen return And *[reduce_abs_inequality expr rel gen for expr rel in exprs]
def timeout reactor deferred timeout_sec reason None def _timeout deferred cancel delayed_timeout reactor callLater timeout_sec _timeout if reason is not None def maybe_replace_reason passthrough if delayed_timeout active return passthroughreturn Failure reason deferred addErrback maybe_replace_reason def abort_timeout passthrough if delayed_timeout active delayed_timeout cancel return passthroughdeferred addBoth abort_timeout return deferred
def fix makefile do_apply fixed Falselines open makefile readlines for old new in CHANGES i findline lines new if i > 0 continuei findline lines old if i < 0 print 'fixapplepython23 Pythoninstallationnotfixed appearsbroken 'print 'fixapplepython23 missingline ' oldreturn 2lines[i] newfixed Trueif fixed if do_apply print 'fixapplepython23 FixtoApple-installedPython2 3applied'os rename makefile makefile + '~' open makefile 'w' writelines lines return 0else print 'fixapplepython23 FixtoApple-installedPython2 3shouldbeapplied'return 1else print 'fixapplepython23 Nofixneeded appearstohavebeenappliedbefore'return 0
def parse_host_connect_string hcs if '@' in hcs p re compile ' ?P<username>[^@ ]* ? ?P<password> * ? \\\\ @ ?P<hostname>[^ ]* ? ?P<port>[0-9]* ' else p re compile ' ?P<username> ?P<password> ?P<hostname>[^ ]* ? ?P<port>[0-9]* ' m p search hcs d m groupdict d['password'] d['password'] replace '\\@' '@' return d
def parse_host_connect_string hcs if '@' in hcs p re compile ' ?P<username>[^@ ]* ? ?P<password> * ? \\\\ @ ?P<hostname>[^ ]* ? ?P<port>[0-9]* ' else p re compile ' ?P<username> ?P<password> ?P<hostname>[^ ]* ? ?P<port>[0-9]* ' m p search hcs d m groupdict d['password'] d['password'] replace '\\@' '@' return d
def get_presence jid from_jid None get_show False if not jid raise InvalidJidError request xmpp_service_pb PresenceRequest response xmpp_service_pb PresenceResponse request set_jid _to_str jid if from_jid request set_from_jid _to_str from_jid try apiproxy_stub_map MakeSyncCall 'xmpp' 'GetPresence' request response except apiproxy_errors ApplicationError as e if e application_error xmpp_service_pb XmppServiceError INVALID_JID raise InvalidJidError else raise Error if get_show show Noneif response has_presence presence response presence if presence xmpp_service_pb PresenceResponse NORMAL show PRESENCE_SHOW_NONEelif presence xmpp_service_pb PresenceResponse AWAY show PRESENCE_SHOW_AWAYelif presence xmpp_service_pb PresenceResponse DO_NOT_DISTURB show PRESENCE_SHOW_DNDelif presence xmpp_service_pb PresenceResponse CHAT show PRESENCE_SHOW_CHATelif presence xmpp_service_pb PresenceResponse EXTENDED_AWAY show PRESENCE_SHOW_XAreturn bool response is_available show else return bool response is_available
def generate_strings total_strings string_length 20 statements []for _ in range 0 total_strings text '' join random choice string ascii_letters + string digits + '' for _ in range string_length statements append text return statements
def make_libvirtError error_class msg error_code None error_domain None error_message None error_level None str1 None str2 None str3 None int1 None int2 None exc error_class msg exc err error_code error_domain error_message error_level str1 str2 str3 int1 int2 return exc
def make_libvirtError error_class msg error_code None error_domain None error_message None error_level None str1 None str2 None str3 None int1 None int2 None exc error_class msg exc err error_code error_domain error_message error_level str1 str2 str3 int1 int2 return exc
def make_libvirtError error_class msg error_code None error_domain None error_message None error_level None str1 None str2 None str3 None int1 None int2 None exc error_class msg exc err error_code error_domain error_message error_level str1 str2 str3 int1 int2 return exc
def make_libvirtError error_class msg error_code None error_domain None error_message None error_level None str1 None str2 None str3 None int1 None int2 None exc error_class msg exc err error_code error_domain error_message error_level str1 str2 str3 int1 int2 return exc
def make_libvirtError error_class msg error_code None error_domain None error_message None error_level None str1 None str2 None str3 None int1 None int2 None exc error_class msg exc err error_code error_domain error_message error_level str1 str2 str3 int1 int2 return exc
def get_provider_user_states user states []found_user_auths list models DjangoStorage user get_social_auth_for_user user for enabled_provider in provider Registry enabled association Nonefor auth in found_user_auths if enabled_provider match_social_auth auth association authbreakif enabled_provider accepts_logins or association states append ProviderUserState enabled_provider user association return states
def CreateConfig **kwds return datastore_rpc Configuration **kwds
def escape inp def conv obj 'Convertobj 'if isinstance obj list rv as_unicode '[' + ' ' join conv o for o in obj + ']' elif isinstance obj dict rv as_unicode '{' + ' ' join [ '%s %s' % conv key conv value for key value in obj iteritems ] + '}' else rv as_unicode '"%s"' % as_unicode obj replace '"' '\\"' return rvreturn conv inp
def is_prerelease vers normalized version _suggest_normalized_version vers if normalized is None return Trueparsed version _normalized_key normalized return any [any [ y in set ['a' 'b' 'c' 'rc' 'dev'] for y in x] for x in parsed]
def is_prerelease vers normalized version _suggest_normalized_version vers if normalized is None return Trueparsed version _normalized_key normalized return any [any [ y in set ['a' 'b' 'c' 'rc' 'dev'] for y in x] for x in parsed]
def add_constant data prepend True has_constant 'skip' if _is_using_pandas data None or _is_recarray data from statsmodels tsa tsatools import add_trendreturn add_trend data trend 'c' prepend prepend has_constant has_constant x np asanyarray data if x ndim 1 x x[ None]elif x ndim > 2 raise ValueError 'Onlyimplementd2-dimensionalarrays' is_nonzero_const np ptp x axis 0 0 is_nonzero_const & np all x 0 0 axis 0 if is_nonzero_const any if has_constant 'skip' return xelif has_constant 'raise' raise ValueError 'dataalreadycontainsaconstant' x [np ones x shape[0] x]x x if prepend else x[ -1 ] return np column_stack x
def _get_safelint_counts filename report_contents _get_report_contents filename rule_count_regex re compile '^ ?P<rule_id>[a-z-]+ \\s+ ?P<count>\\d+ violations' re MULTILINE total_count_regex re compile '^ ?P<count>\\d+ violationstotal' re MULTILINE violations {'rules' {}}for violation_match in rule_count_regex finditer report_contents try violations['rules'][violation_match group 'rule_id' ] int violation_match group 'count' except ValueError violations['rules'][violation_match group 'rule_id' ] Nonetry violations['total'] int total_count_regex search report_contents group 'count' except AttributeError ValueError violations['total'] Nonereturn violations
def print_bad print BAD_OUTPUTprint >>sys stdout BAD_OUTPUTprint >>sys stderr BAD_OUTPUTsys stdout write BAD_OUTPUT sys stdout writelines [BAD_OUTPUT] sys stderr write BAD_OUTPUT sys stderr writelines [BAD_OUTPUT]
def _get_featurestate name featurestate _featurestate_cache get name None if featurestate is None featurestate FeatureState name _world _featurestate_cache[name] featurestatereturn featurestate
def select_url_from_video_api html c json loads html video_dic {}for i in c['mp4'] video_dic[i['code']] i['http']quality_preference_list ['sdvd' 'hd' 'dvd' 'sd']url [video_dic[quality] for quality in quality_preference_list if quality in video_dic ][0]html get_html url c json loads html return [i['urls'][0] for i in c['playlist']]
def select_url_from_video_api html c json loads html video_dic {}for i in c['mp4'] video_dic[i['code']] i['http']quality_preference_list ['sdvd' 'hd' 'dvd' 'sd']url [video_dic[quality] for quality in quality_preference_list if quality in video_dic ][0]html get_html url c json loads html return [i['urls'][0] for i in c['playlist']]
def find_files_with_cmd dirname ' ' try proc subprocess Popen ['hg' 'locate' '-I' os path abspath dirname ] stdin subprocess PIPE stderr subprocess PIPE stdout subprocess PIPE cwd dirname stdout stderr proc communicate except return []return stdout splitlines
def find_files_with_cmd dirname ' ' try proc subprocess Popen ['hg' 'locate' '-I' os path abspath dirname ] stdin subprocess PIPE stderr subprocess PIPE stdout subprocess PIPE cwd dirname stdout stderr proc communicate except return []return stdout splitlines
def main if len sys argv > 1 writeOutput '' join sys argv[1 ] else settings startMainLoopFromConstructor getNewRepository
def _date_now now time time date_arr np array [np floor now 1000000 0 * now - np floor now ] dtype 'int32' return date_arr
def resolve_environment service_dict environment None env {}for env_file in service_dict get u'env_file' [] env update env_vars_from_file env_file env update parse_environment service_dict get u'environment' return dict resolve_env_var k v environment for k v in six iteritems env
def _dict_to_name_value data if isinstance data dict sorted_data sorted data items key lambda s s[0] result []for name value in sorted_data if isinstance value dict result append {name _dict_to_name_value value } else result append {name value} else result datareturn result
def buf_lookup_strided_code proto defin name nd args ' ' join [ 'i%d s%d' % i i for i in range nd ] offset '+' join [ 'i%d*s%d' % i i for i in range nd ] proto putln '#define%s type buf %s type char* buf+%s ' % name args offset
def push_monitor model name transfer_experience False save_records False assert hasattr model 'monitor' old_monitor model monitorsetattr model name old_monitor del model monitorif transfer_experience monitor Monitor get_monitor model assert monitor is not old_monitor monitor _num_batches_seen old_monitor _num_batches_seenmonitor _examples_seen old_monitor _examples_seenmonitor _epochs_seen old_monitor _epochs_seenif save_records monitor on_channel_conflict 'copy_history'monitor channels copy copy old_monitor channels for key value in list monitor channels items value prereqs Nonereturn model
def push_monitor model name transfer_experience False save_records False assert hasattr model 'monitor' old_monitor model monitorsetattr model name old_monitor del model monitorif transfer_experience monitor Monitor get_monitor model assert monitor is not old_monitor monitor _num_batches_seen old_monitor _num_batches_seenmonitor _examples_seen old_monitor _examples_seenmonitor _epochs_seen old_monitor _epochs_seenif save_records monitor on_channel_conflict 'copy_history'monitor channels copy copy old_monitor channels for key value in list monitor channels items value prereqs Nonereturn model
def push_monitor model name transfer_experience False save_records False assert hasattr model 'monitor' old_monitor model monitorsetattr model name old_monitor del model monitorif transfer_experience monitor Monitor get_monitor model assert monitor is not old_monitor monitor _num_batches_seen old_monitor _num_batches_seenmonitor _examples_seen old_monitor _examples_seenmonitor _epochs_seen old_monitor _epochs_seenif save_records monitor on_channel_conflict 'copy_history'monitor channels copy copy old_monitor channels for key value in list monitor channels items value prereqs Nonereturn model
def push_monitor model name transfer_experience False save_records False assert hasattr model 'monitor' old_monitor model monitorsetattr model name old_monitor del model monitorif transfer_experience monitor Monitor get_monitor model assert monitor is not old_monitor monitor _num_batches_seen old_monitor _num_batches_seenmonitor _examples_seen old_monitor _examples_seenmonitor _epochs_seen old_monitor _epochs_seenif save_records monitor on_channel_conflict 'copy_history'monitor channels copy copy old_monitor channels for key value in list monitor channels items value prereqs Nonereturn model
def all_languages languages [ lang[0] _ lang[1] for lang in settings ALL_LANGUAGES]return sorted languages key lambda lang lang[1]
def reset app interactive True from django db import connection transactionfrom django conf import settingsapp_name app __name__ split ' ' [ -2 ]disable_termcolors _check_for_validation_errors app sql_list get_sql_reset app if interactive confirm raw_input '\nYouhaverequestedadatabasereset \nThiswillIRREVERSIBLYDESTROYanydatafor\nthe"%s"applicationinthedatabase"%s" \nAreyousureyouwanttodothis?\n\nType\'yes\'tocontinue or\'no\'tocancel ' % app_name settings DATABASE_NAME else confirm 'yes'if confirm 'yes' try cursor connection cursor for sql in sql_list cursor execute sql except Exception as e sys stderr write style ERROR "Error %scouldn'tbereset Possiblereasons \n*Thedatabaseisn'trunningorisn'tconfiguredcorrectly \n*Atleastoneofthedatabasetablesdoesn'texist \n*TheSQLwasinvalid \nHint Lookattheoutputof'django-admin pysqlreset%s' That'stheSQLthiscommandwasn'tabletorun \nThefullerror " % app_name app_name + style ERROR_OUTPUT str e + '\n' transaction rollback_unless_managed sys exit 1 transaction commit_unless_managed else print 'Resetcancelled '
def create_blockdevicedeployer test_case hostname u'192 0 2 1' node_uuid uuid4 eventually_consistent False api loopbackblockdeviceapi_for_test test_case if eventually_consistent api EventuallyConsistentBlockDeviceAPI api async_api _SyncToThreadedAsyncAPIAdapter _sync api _reactor NonReactor _threadpool NonThreadPool return BlockDeviceDeployer hostname hostname node_uuid node_uuid block_device_api api _async_block_device_api async_api mountroot mountroot_for_test test_case
def get_module_file_attribute package try loader pkgutil find_loader package attr loader get_filename package except AttributeError ImportError __file__statement '\nimport%sasp\nprint p __file__ \n'attr exec_statement __file__statement % package if not attr strip raise ImportErrorreturn attr
def createMemoryWorker def perform if not worker _pending return Falseif worker _pending[0] is NoMoreWork return Falseworker _pending pop 0 return Trueworker MemoryWorker return worker perform
def createMemoryWorker def perform if not worker _pending return Falseif worker _pending[0] is NoMoreWork return Falseworker _pending pop 0 return Trueworker MemoryWorker return worker perform
def _lanscan_getnode return _find_mac 'lanscan' '-ai' ['lan0'] lambda i 0
def applications apps _call_system_profiler 'SPApplicationsDataType' appdict {}for a in apps details dict a details pop '_name' if 'lastModified' in details details['lastModified'] details['lastModified'] strftime '%Y-%m-%d%H %M %S' if 'info' in details try details['info'] '{0} {1}' format details['info'][0] details['info'][1] strftime '%Y-%m-%d%H %M %S' except IndexError AttributeError passif a['_name'] not in appdict appdict[a['_name']] []appdict[a['_name']] append details return appdict
def create_issue **kwargs owner kwargs pop 'owner' None if owner is None owner UserFactory create project kwargs pop 'project' None if project is None project ProjectFactory create owner owner defaults {'project' project 'owner' owner 'status' IssueStatusFactory create project project 'milestone' MilestoneFactory create project project 'priority' PriorityFactory create project project 'severity' SeverityFactory create project project 'type' IssueTypeFactory create project project }defaults update kwargs return IssueFactory create **defaults
def cgsnapshot_get_all_by_group context group_id filters None return IMPL cgsnapshot_get_all_by_group context group_id filters
def getScreens if importCtypesFailed return Falsecount CGDisplayCount cocoa CGGetActiveDisplayList 0 None ctypes byref count displays CGDirectDisplayID * count value cocoa CGGetActiveDisplayList count value displays ctypes byref count return [id for id in displays]
def parse_template token_dict template_html descriptors parser SlybotTemplatePageParser token_dict parser feed template_html return parser to_template descriptors
def parse_template token_dict template_html descriptors parser SlybotTemplatePageParser token_dict parser feed template_html return parser to_template descriptors
def Opt re result Alt re Empty result str 'Opt %s ' % re return result
def set_startup_disk path if path not in list_startup_disks msg 'Invalidvaluepassedforpath \nMustbeavalidstartupdiskasfoundinsystem list_startup_disks \nPassed {0}' format path raise SaltInvocationError msg cmd 'systemsetup-setstartupdisk{0}' format path salt utils mac_utils execute_return_result cmd return salt utils mac_utils confirm_updated path get_startup_disk
def set_startup_disk path if path not in list_startup_disks msg 'Invalidvaluepassedforpath \nMustbeavalidstartupdiskasfoundinsystem list_startup_disks \nPassed {0}' format path raise SaltInvocationError msg cmd 'systemsetup-setstartupdisk{0}' format path salt utils mac_utils execute_return_result cmd return salt utils mac_utils confirm_updated path get_startup_disk
def checkTorrentFinished logger info 'Checkingifanytorrentshavefinishedseedingandcanberemoved' with postprocessor_lock myDB db DBConnection results myDB select 'SELECT*fromsnatchedWHEREStatus "Seed_Processed"' for album in results hash album['FolderName']albumid album['AlbumID']torrent_removed Falseif headphones CONFIG TORRENT_DOWNLOADER 1 torrent_removed transmission removeTorrent hash True else torrent_removed utorrent removeTorrent hash True if torrent_removed myDB action 'DELETEfromsnatchedWHEREstatus "Seed_Processed"andAlbumID ?' [albumid] logger info 'Checkingfinishedtorrentscompleted'
def monomial_key order None gens None if order is None order lexif isinstance order Symbol order str order if isinstance order str try order _monomial_key[order]except KeyError raise ValueError "supportedmonomialorderingsare'lex' 'grlex'and'grevlex' got%r" % order if hasattr order '__call__' if gens is not None def _order expr return order expr as_poly *gens degree_list return _orderreturn orderelse raise ValueError 'monomialorderingspecificationmustbeastringoracallable got%s' % order
def ensure_valid_usage_key view_func @wraps view_func def inner request *args **kwargs usage_key kwargs get 'usage_key_string' if usage_key is not None try UsageKey from_string usage_key except InvalidKeyError raise Http404response view_func request *args **kwargs return responsereturn inner
def ensure_valid_usage_key view_func @wraps view_func def inner request *args **kwargs usage_key kwargs get 'usage_key_string' if usage_key is not None try UsageKey from_string usage_key except InvalidKeyError raise Http404response view_func request *args **kwargs return responsereturn inner
def strip_console_codes output if '\x1b' not in output return outputold_word ''return_str ''index 0output '\x1b[m%s' % output console_codes '%G \\[m \\[[\\d ]+[HJnrm]'while index < len output tmp_index 0tmp_word ''while len re findall '\x1b' tmp_word < 2 and index + tmp_index < len output tmp_word + output[ index + tmp_index ]tmp_index + 1tmp_word re sub '\x1b' '' tmp_word index + len tmp_word + 1 if tmp_word old_word continuetry special_code re findall console_codes tmp_word [0]except IndexError if index + tmp_index < len output raise ValueError '%sisnotincludedintheknownconsolecodeslist%s' % tmp_word console_codes continueif special_code tmp_word continueold_word tmp_wordreturn_str + tmp_word[len special_code ]return return_str
def _get_record gcdns zone record_type record_name record_id '%s %s' % record_type record_name try return gcdns get_record zone id record_id except RecordDoesNotExistError return None
def evaluate_deltas e accepted_functions Add if isinstance e accepted_functions return e func *[evaluate_deltas arg for arg in e args] elif isinstance e Mul deltas []indices {}for i in e args for s in i free_symbols if s in indices indices[s] + 1else indices[s] 0if isinstance i KroneckerDelta deltas append i for d in deltas if indices[d killable_index] e e subs d killable_index d preferred_index if len deltas > 1 return evaluate_deltas e elif indices[d preferred_index] and d indices_contain_equal_information e e subs d preferred_index d killable_index if len deltas > 1 return evaluate_deltas e else passreturn eelse return e
def evaluate_deltas e accepted_functions Add if isinstance e accepted_functions return e func *[evaluate_deltas arg for arg in e args] elif isinstance e Mul deltas []indices {}for i in e args for s in i free_symbols if s in indices indices[s] + 1else indices[s] 0if isinstance i KroneckerDelta deltas append i for d in deltas if indices[d killable_index] e e subs d killable_index d preferred_index if len deltas > 1 return evaluate_deltas e elif indices[d preferred_index] and d indices_contain_equal_information e e subs d preferred_index d killable_index if len deltas > 1 return evaluate_deltas e else passreturn eelse return e
def evaluate_deltas e accepted_functions Add if isinstance e accepted_functions return e func *[evaluate_deltas arg for arg in e args] elif isinstance e Mul deltas []indices {}for i in e args for s in i free_symbols if s in indices indices[s] + 1else indices[s] 0if isinstance i KroneckerDelta deltas append i for d in deltas if indices[d killable_index] e e subs d killable_index d preferred_index if len deltas > 1 return evaluate_deltas e elif indices[d preferred_index] and d indices_contain_equal_information e e subs d preferred_index d killable_index if len deltas > 1 return evaluate_deltas e else passreturn eelse return e
def evaluate_deltas e accepted_functions Add if isinstance e accepted_functions return e func *[evaluate_deltas arg for arg in e args] elif isinstance e Mul deltas []indices {}for i in e args for s in i free_symbols if s in indices indices[s] + 1else indices[s] 0if isinstance i KroneckerDelta deltas append i for d in deltas if indices[d killable_index] e e subs d killable_index d preferred_index if len deltas > 1 return evaluate_deltas e elif indices[d preferred_index] and d indices_contain_equal_information e e subs d preferred_index d killable_index if len deltas > 1 return evaluate_deltas e else passreturn eelse return e
def _paint_path fill stroke if stroke if fill return Op fill_strokeelse return Op strokeelif fill return Op fillelse return Op endpath
def _paint_path fill stroke if stroke if fill return Op fill_strokeelse return Op strokeelif fill return Op fillelse return Op endpath
def pmap_field key_type value_type optional False invariant _valid initial _UNDEFINED factory None input_factory factoryclass TheMap CheckedPMap __key_type__ key_type__value_type__ value_typeTheMap __name__ key_type __name__ capitalize + value_type __name__ capitalize + 'PMap' if optional def mapping_factory argument if argument is None return Noneelse return TheMap argument else mapping_factory TheMapif input_factory factory lambda x mapping_factory input_factory x else factory mapping_factoryif initial is _UNDEFINED initial TheMap else initial factory initial return field mandatory True initial initial type optional_type TheMap if optional else TheMap factory factory invariant invariant
def CreateServer root_path login_url port template_dir None serve_address '' allow_skipped_files False static_caching True python_path_list sys path sdk_dir SDK_ROOT default_partition None frontend_port None interactive_console True secret_hash 'xxx' absolute_root_path os path realpath root_path FakeFile SetAllowedPaths absolute_root_path [sdk_dir] FakeFile SetAllowSkippedFiles allow_skipped_files handler_class CreateRequestHandler absolute_root_path login_url static_caching default_partition interactive_console secret_hash secret_hash if absolute_root_path not in python_path_list python_path_list insert 0 absolute_root_path server HTTPServerWithScheduler serve_address port handler_class queue_stub apiproxy_stub_map apiproxy GetStub 'taskqueue' if queue_stub and hasattr queue_stub 'StartBackgroundExecution' queue_stub StartBackgroundExecution channel_stub apiproxy_stub_map apiproxy GetStub 'channel' if channel_stub channel_stub _add_event server AddEventchannel_stub _update_event server UpdateEventserver frontend_hostport '%s %d' % serve_address or 'localhost' frontend_port or port return server
def getIndex form pos 'noun' def trySubstitutions trySubstitutions form substitutions lookup 1 dictionary dictionaryFor pos if lookup and dictionary has_key form return dictionary[form]elif substitutions old new substitutions[0]substitute string replace form old new and substitute form if substitute and dictionary has_key substitute return dictionary[substitute]return trySubstitutions trySubstitutions form substitutions[1 ] lookup 0 or substitute and trySubstitutions trySubstitutions substitute substitutions[1 ] return trySubstitutions returnMatch form GET_INDEX_SUBSTITUTIONS
def getIndex form pos 'noun' def trySubstitutions trySubstitutions form substitutions lookup 1 dictionary dictionaryFor pos if lookup and dictionary has_key form return dictionary[form]elif substitutions old new substitutions[0]substitute string replace form old new and substitute form if substitute and dictionary has_key substitute return dictionary[substitute]return trySubstitutions trySubstitutions form substitutions[1 ] lookup 0 or substitute and trySubstitutions trySubstitutions substitute substitutions[1 ] return trySubstitutions returnMatch form GET_INDEX_SUBSTITUTIONS
def numpy_cupy_array_almost_equal_nulp nulp 1 name 'xp' type_check True accept_error False def check_func x y array assert_array_almost_equal_nulp x y nulp return _make_decorator check_func name type_check accept_error
def leaky_relu x slope 0 2 return LeakyReLU slope x
@depends HAS_PYVMOMI def update_host_password host username password new_password protocol None port None service_instance salt utils vmware get_service_instance host host username username password password protocol protocol port port account_manager salt utils vmware get_inventory service_instance accountManageruser_account vim host LocalAccountManager AccountSpecification user_account id usernameuser_account password new_passwordtry account_manager UpdateUser user_account except vmodl fault SystemError as err raise CommandExecutionError err msg except vim fault UserNotFound raise CommandExecutionError "'vsphere update_host_password'failedforhost{0} Userwasnotfound " format host except vim fault AlreadyExists passreturn True
def verify_signed_data token data if data startswith MAC_MARKER try data data[len MAC_MARKER ]mac_data json loads base64 b64decode data mac compute_mac token mac_data['serialized_data'] if mac mac_data['mac'] raise InvalidMacError 'invalidMAC expect %s actual %s' % mac_data['mac'] mac return json loads mac_data['serialized_data'] except raise InvalidMacError 'invalidMAC dataappearedtobecorrupted' else return data
def api_doc section uses_site False **kwargs def add_metadata api_function doc api_function _api_doc getattr api_function '_api_doc' {} if 'extends' in kwargs kwargs['extends'] kwargs['extends'] _api_docdoc update kwargs doc['uses_site'] uses_sitedoc['section'] sectiondoc['lineno'] api_function func_code co_firstlinenofile_path abspath api_function func_code co_filename root_dir g paths['root']if file_path startswith root_dir doc['relfilepath'] relpath file_path root_dir doc['source_root_url'] 'https //github com/reddit/reddit/blob/master/r2/r2/'else for plugin in g plugins plugin_root plugin pathif plugin source_root_url and file_path startswith plugin_root doc['relfilepath'] relpath file_path plugin_root doc['source_root_url'] plugin source_root_urlreturn api_functionreturn add_metadata
def _get_offset_param params offset params pop 'offset' 0 return utils validate_integer offset 'offset' 0 constants DB_MAX_INT
def get_ipython_package_dir ipdir os path dirname IPython __file__ return py3compat cast_unicode ipdir fs_encoding
def get_ipython_package_dir ipdir os path dirname IPython __file__ return py3compat cast_unicode ipdir fs_encoding
def make_case_flags info flags info flags & CASE_FLAGS if info flags & ASCII flags & ~ FULLCASE return flags
def run_job name force False ret {'comment' [] 'result' True}if not name ret['comment'] 'Jobnameisrequired 'ret['result'] Falseschedule list_ show_all True return_yaml False if name in schedule data schedule[name]if 'enabled' in data and not data['enabled'] and not force ret['comment'] 'Job{0}isdisabled ' format name else out __salt__['event fire'] {'name' name 'func' 'run_job'} 'manage_schedule' if out ret['comment'] 'SchedulingJob{0}onminion ' format name else ret['comment'] 'Failedtorunjob{0}onminion ' format name ret['result'] Falseelse ret['comment'] 'Job{0}doesnotexist ' format name ret['result'] Falsereturn ret
def get_texpath def _get_texpath try texpath get_setting sublime platform {} get 'texpath' except AttributeError exc_info sys exc_infotry reload sys modules[get_texpath __module__] texpath get_setting sublime platform {} get 'texpath' except reraise *exc_info return expand_vars texpath if texpath is not None else None return run_on_main_thread _get_texpath default_value None
def libvlc_audio_equalizer_get_amp_at_index p_equalizer u_band f _Cfunctions get 'libvlc_audio_equalizer_get_amp_at_index' None or _Cfunction 'libvlc_audio_equalizer_get_amp_at_index' 1 1 None ctypes c_float ctypes c_void_p ctypes c_uint return f p_equalizer u_band
def _one_step mu u if np abs mu max > 1 0 return 1 0_compose_linear_fitting_data mu u u['uu'] u['sing'] u['vv'] linalg svd u['M'] u['resi'][ ] u['y'][ ]for p in range u['nfit'] - 1 dot np dot u['uu'][p] u['y'] for k in range u['nterms'] - 1 u['resi'][k] u['resi'][k] - u['uu'][ p k ] * dot return np dot u['resi'] u['resi']
def get_service hass config discovery_info None api_key config get CONF_API_KEY sender config get CONF_SENDER recipient config get CONF_RECIPIENT return SendgridNotificationService api_key sender recipient
def get_mock_hadoop_cmd_args cmd_log os path join get_mock_dir 'cmd log' if not os path exists cmd_log return []with open cmd_log as f return [shlex_split cmd for cmd in f]
def _train_iis xs classes features f_sharp alphas e_empirical max_newton_iterations newton_converge p_yx _calc_p_class_given_x xs classes features alphas N len xs newalphas alphas[ ]for i in range len alphas delta _iis_solve_delta N features[i] f_sharp e_empirical[i] p_yx max_newton_iterations newton_converge newalphas[i] + deltareturn newalphas
def _train_iis xs classes features f_sharp alphas e_empirical max_newton_iterations newton_converge p_yx _calc_p_class_given_x xs classes features alphas N len xs newalphas alphas[ ]for i in range len alphas delta _iis_solve_delta N features[i] f_sharp e_empirical[i] p_yx max_newton_iterations newton_converge newalphas[i] + deltareturn newalphas
def newline_with_copy_margin event b event current_buffercursor_start_pos b document cursor_position_colb newline copy_margin True b cursor_up count 1 cursor_end_pos b document cursor_position_colif cursor_start_pos cursor_end_pos pos_diff cursor_start_pos - cursor_end_pos b cursor_right count pos_diff
def newline_with_copy_margin event b event current_buffercursor_start_pos b document cursor_position_colb newline copy_margin True b cursor_up count 1 cursor_end_pos b document cursor_position_colif cursor_start_pos cursor_end_pos pos_diff cursor_start_pos - cursor_end_pos b cursor_right count pos_diff
def create_default_options allowed_methods allowed ' ' join allowed_methods def on_options req resp **kwargs resp status HTTP_204resp set_header 'Allow' allowed resp set_header 'Content-Length' '0' return on_options
def parse_short_time_label label days hours minutes seconds '0' '0' '0' '0' if '-' in label days label label split '-' 1 time_comp label split ' ' if len time_comp 3 hours minutes seconds time_compelif len time_comp 2 minutes seconds time_compelse raise ValueError "Invalidtimeformat weexpected'[[dd-]hh ]mm ss'or'mm ss ss' %s" % label try time_sum int float seconds time_sum + int minutes * 60 time_sum + int hours * 3600 time_sum + int days * 86400 return time_sumexcept ValueError raise ValueError 'Non-numericvalueintimeentry %s' % label
def _model_to_fit_params model fitparam_indices list range len model param_names if any model fixed values or any model tied values params list model parameters param_metrics model _param_metricsfor idx name in list enumerate model param_names [ -1 ] if model fixed[name] or model tied[name] slice_ param_metrics[name][u'slice']del params[slice_]del fitparam_indices[idx]return np array params fitparam_indices else return model parameters fitparam_indices
def _model_to_fit_params model fitparam_indices list range len model param_names if any model fixed values or any model tied values params list model parameters param_metrics model _param_metricsfor idx name in list enumerate model param_names [ -1 ] if model fixed[name] or model tied[name] slice_ param_metrics[name][u'slice']del params[slice_]del fitparam_indices[idx]return np array params fitparam_indices else return model parameters fitparam_indices
def _model_to_fit_params model fitparam_indices list range len model param_names if any model fixed values or any model tied values params list model parameters param_metrics model _param_metricsfor idx name in list enumerate model param_names [ -1 ] if model fixed[name] or model tied[name] slice_ param_metrics[name][u'slice']del params[slice_]del fitparam_indices[idx]return np array params fitparam_indices else return model parameters fitparam_indices
def _model_to_fit_params model fitparam_indices list range len model param_names if any model fixed values or any model tied values params list model parameters param_metrics model _param_metricsfor idx name in list enumerate model param_names [ -1 ] if model fixed[name] or model tied[name] slice_ param_metrics[name][u'slice']del params[slice_]del fitparam_indices[idx]return np array params fitparam_indices else return model parameters fitparam_indices
def _model_to_fit_params model fitparam_indices list range len model param_names if any model fixed values or any model tied values params list model parameters param_metrics model _param_metricsfor idx name in list enumerate model param_names [ -1 ] if model fixed[name] or model tied[name] slice_ param_metrics[name][u'slice']del params[slice_]del fitparam_indices[idx]return np array params fitparam_indices else return model parameters fitparam_indices
def _model_to_fit_params model fitparam_indices list range len model param_names if any model fixed values or any model tied values params list model parameters param_metrics model _param_metricsfor idx name in list enumerate model param_names [ -1 ] if model fixed[name] or model tied[name] slice_ param_metrics[name][u'slice']del params[slice_]del fitparam_indices[idx]return np array params fitparam_indices else return model parameters fitparam_indices
def _generate_graphs with gzip open ATLAS_FILE 'rb' as f line f readline while line and line startswith 'GRAPH' graph_index int line[6 ] rstrip line f readline num_nodes int line[6 ] rstrip edgelist []line f readline while line and not line startswith 'GRAPH' edgelist append line rstrip line f readline G nx Graph G name 'G{}' format graph_index G add_nodes_from range num_nodes G add_edges_from tuple map int e split for e in edgelist yield G
def check_normal_basic shape_as_symbolic dim_as_symbolic False rng CURAND_RandomStreams 234 if shape_as_symbolic shape constant 10 10 elif dim_as_symbolic shape 10 constant 10 else shape 10 10 u0 rng normal shape u1 rng normal shape f0 theano function [] u0 mode mode_with_gpu f1 theano function [] u1 mode mode_with_gpu v0list [f0 for i in range 3 ]v1list [f1 for i in range 3 ]assert numpy all v0list[0] v0list[1] assert numpy all v1list[0] v1list[1] assert numpy all v0list[0] v1list[0] for v in v0list assert v shape 10 10 assert v min < v max assert -0 5 < v mean < 0 5
def check_normal_basic shape_as_symbolic dim_as_symbolic False rng CURAND_RandomStreams 234 if shape_as_symbolic shape constant 10 10 elif dim_as_symbolic shape 10 constant 10 else shape 10 10 u0 rng normal shape u1 rng normal shape f0 theano function [] u0 mode mode_with_gpu f1 theano function [] u1 mode mode_with_gpu v0list [f0 for i in range 3 ]v1list [f1 for i in range 3 ]assert numpy all v0list[0] v0list[1] assert numpy all v1list[0] v1list[1] assert numpy all v0list[0] v1list[0] for v in v0list assert v shape 10 10 assert v min < v max assert -0 5 < v mean < 0 5
def simpleMultivariateNormalPdf z detFactorSigma dim len z return exp -0 5 * dot z z / power 2 0 * pi dim / 2 0 * detFactorSigma
def get_repo backend mat return backend open_repository url_prefix mat
@require_chanmsg@require_privilege OP u'Youarenotachanneloperator ' @commands u'topic' def topic bot trigger if bot privileges[trigger sender][bot nick] < HALFOP return bot reply u"I'mnotachanneloperator " if not trigger group 2 returnchannel trigger sender lower narg 1mask Nonemask bot db get_channel_value channel u'topic_mask' mask mask or default_mask trigger mask mask replace u'%s' u'{}' narg len re findall u'{}' mask top trigger group 2 args []if top args top split u'~' narg if len args narg message u'Notenougharguments Yougave{} itrequires{} ' format len args narg return bot say message topic mask format *args bot write u'TOPIC' channel + u' ' + topic
def run_changed drain False min_size 500 limit 1000 sleep_time 10 use_safe_get False verbose False if use_safe_get CloudSearchUploader use_safe_get Trueamqp handle_items 'cloudsearch_changes' _run_changed min_size min_size limit limit drain drain sleep_time sleep_time verbose verbose
def run_changed drain False min_size 500 limit 1000 sleep_time 10 use_safe_get False verbose False if use_safe_get CloudSearchUploader use_safe_get Trueamqp handle_items 'cloudsearch_changes' _run_changed min_size min_size limit limit drain drain sleep_time sleep_time verbose verbose
def run_changed drain False min_size 500 limit 1000 sleep_time 10 use_safe_get False verbose False if use_safe_get CloudSearchUploader use_safe_get Trueamqp handle_items 'cloudsearch_changes' _run_changed min_size min_size limit limit drain drain sleep_time sleep_time verbose verbose
def run_changed drain False min_size 500 limit 1000 sleep_time 10 use_safe_get False verbose False if use_safe_get CloudSearchUploader use_safe_get Trueamqp handle_items 'cloudsearch_changes' _run_changed min_size min_size limit limit drain drain sleep_time sleep_time verbose verbose
def build config live_server False dump_json False dirty False if not dirty log info u'Cleaningsitedirectory' utils clean_directory config[u'site_dir'] else log warning u"A'dirty'buildisbeingperformed thiswilllikelyleadtoinaccuratenavigationandotherlinkswithinyoursite Thisoptionisdesignedforsitedevelopmentpurposesonly " if not live_server log info u'Buildingdocumentationtodirectory %s' config[u'site_dir'] if dirty and site_directory_contains_stale_files config[u'site_dir'] log info u'Thedirectorycontainsstalefiles Use--cleantoremovethem ' if dump_json build_pages config dump_json True dirty dirty returnfor theme_dir in reversed config[u'theme_dir'] log debug u'Copyingstaticassetsfromtheme %s' theme_dir utils copy_media_files theme_dir config[u'site_dir'] exclude [u'* py' u'* pyc' u'* html'] dirty dirty log debug u'Copyingstaticassetsfromthedocsdir ' utils copy_media_files config[u'docs_dir'] config[u'site_dir'] dirty dirty log debug u'Buildingmarkdownpages ' build_pages config dirty dirty
def read_png filename x Reader filename try alpha x asDirect [3]['alpha']if alpha y x asRGBA8 [2]n 4else y x asRGB8 [2]n 3y np array [yy for yy in y] np uint8 finally x file close y shape y shape[0] y shape[1] // n n return y
def read_png filename x Reader filename try alpha x asDirect [3]['alpha']if alpha y x asRGBA8 [2]n 4else y x asRGB8 [2]n 3y np array [yy for yy in y] np uint8 finally x file close y shape y shape[0] y shape[1] // n n return y
def get_cached_clients if OAuth state_key not in current_app extensions raise RuntimeError '%risnotinitialized ' % current_app state current_app extensions[OAuth state_key]return state cached_clients
def get_cached_clients if OAuth state_key not in current_app extensions raise RuntimeError '%risnotinitialized ' % current_app state current_app extensions[OAuth state_key]return state cached_clients
def validate_bucket_name name if '_' in name or len name < 3 or len name > 63 or not name[ -1 ] isalnum return Falseelif ' -' in name or '- ' in name or ' ' in name or not name[0] isalnum return Falseelif re match '^ [0-9] [1-9][0-9] 1[0-9]{2} 2[0-4][0-9] 25[0-5] \\ {3} [0-9] [1-9][0-9] 1[0-9]{2} 2[0-4][0-9] 25[0-5] $' name return Falseelse return True
def occurrence request event_id template_name 'schedule/occurrence html' *args **kwargs event occurrence get_occurrence event_id *args **kwargs back_url request META get 'HTTP_REFERER' None return render_to_response template_name {'event' event 'occurrence' occurrence 'back_url' back_url} context_instance RequestContext request
def require_version namespace version global _versionsrepo GIRepository namespaces repo get_loaded_namespaces if namespace in namespaces loaded_version repo get_version namespace if loaded_version version raise ValueError 'Namespace%sisalreadyloadedwithversion%s' % namespace loaded_version if namespace in _versions and _versions[namespace] version raise ValueError 'Namespace%salreadyrequiresversion%s' % namespace _versions[namespace] available_versions repo enumerate_versions namespace if not available_versions raise ValueError 'Namespace%snotavailable' % namespace if version not in available_versions raise ValueError 'Namespace%snotavailableforversion%s' % namespace version _versions[namespace] version
def sample_final_epsilon final_epsilons np array [0 1 0 01 0 5] probabilities np array [0 4 0 3 0 3] return np random choice final_epsilons 1 p list probabilities [0]
def _get_mask X value_to_mask if value_to_mask 'NaN' or np isnan value_to_mask return np isnan X else return X value_to_mask
def addYGroove derivation negatives x if derivation topBevel < 0 0 returnbottom derivation height - derivation topBevel top derivation heightgroove [complex x bottom complex x - derivation topBevel top complex x + derivation topBevel top ]triangle_mesh addSymmetricYPath negatives groove 1 0001 * derivation topRight imag
def start_event_loop_qt4 app None if app is None app get_app_qt4 [''] if not is_event_loop_running_qt4 app app _in_event_loop Trueapp exec_ app _in_event_loop Falseelse app _in_event_loop True
def edns_from_text text return _from_text text _edns_by_text
def validate_gs_bucket_name bucket_name if len bucket_name > MAX_BUCKET_LEN raise BackupValidationException 'Bucketnamelengthshouldnotbelongerthan%d' % MAX_BUCKET_LEN if len bucket_name < MIN_BUCKET_LEN raise BackupValidationException 'Bucketnamelengthshouldbelongerthan%d' % MIN_BUCKET_LEN if bucket_name lower startswith 'goog' raise BackupValidationException 'Bucketnameshouldnotstartwitha"goog"prefix' bucket_elements bucket_name split ' ' for bucket_element in bucket_elements if len bucket_element > MAX_BUCKET_SEGMENT_LEN raise BackupValidationException 'Segmentlengthofbucketnameshouldnotbelongerthan%d' % MAX_BUCKET_SEGMENT_LEN if not re match BUCKET_PATTERN bucket_name raise BackupValidationException 'Invalidbucketname"%s"' % bucket_name
def S_IFMT mode return mode & 61440
def S_IFMT mode return mode & 61440
@publicdef refine_root f s t eps None steps None fast False check_sqf False try F Poly f except GeneratorsNeeded raise PolynomialError "can'trefinearootof%s notapolynomial" % f return F refine_root s t eps eps steps steps fast fast check_sqf check_sqf
def context_dict revision ready_for_l10n False revision_approved False diff ''l10n revision document revisions filter is_ready_for_localization True approved revision document revisions filter is_approved True if ready_for_l10n and l10n count > 1 old_rev l10n order_by '-created' [1]diff get_diff_for revision document old_rev revision elif revision_approved and approved count > 1 old_rev approved order_by '-created' [1]diff get_diff_for revision document old_rev revision elif revision document current_revision is not None old_rev revision document current_revisiondiff get_diff_for revision document old_rev revision return {'document_title' revision document title 'creator' revision creator 'host' Site objects get_current domain 'diff' diff 'summary' clean revision summary ALLOWED_TAGS ALLOWED_ATTRIBUTES 'fulltext' clean revision content ALLOWED_TAGS ALLOWED_ATTRIBUTES }
def color_dict_to_objects d colorspace 'hsv' result {}for k v in d items result[k] Color k v colorspace return result
def color_dict_to_objects d colorspace 'hsv' result {}for k v in d items result[k] Color k v colorspace return result
def show DEBUG_MSG 'show ' 3 None for figwin in Gcf get_all_fig_managers figwin frame Show if show _needmain and not matplotlib is_interactive wxapp wx GetApp if wxapp is not None imlr getattr wxapp 'IsMainLoopRunning' lambda False if not imlr wxapp MainLoop show _needmain False
def show DEBUG_MSG 'show ' 3 None for figwin in Gcf get_all_fig_managers figwin frame Show if show _needmain and not matplotlib is_interactive wxapp wx GetApp if wxapp is not None imlr getattr wxapp 'IsMainLoopRunning' lambda False if not imlr wxapp MainLoop show _needmain False
def show DEBUG_MSG 'show ' 3 None for figwin in Gcf get_all_fig_managers figwin frame Show if show _needmain and not matplotlib is_interactive wxapp wx GetApp if wxapp is not None imlr getattr wxapp 'IsMainLoopRunning' lambda False if not imlr wxapp MainLoop show _needmain False
def serialize_args fn def wrapper obj *args **kwargs args [ utils strtime arg if isinstance arg datetime datetime else arg for arg in args]for k v in kwargs items if k 'exc_val' and v kwargs[k] six text_type v elif k 'exc_tb' and v and not isinstance v six string_types kwargs[k] '' join traceback format_tb v elif isinstance v datetime datetime kwargs[k] utils strtime v if hasattr fn '__call__' return fn obj *args **kwargs return fn __get__ None obj *args **kwargs wrapper remotable getattr fn 'remotable' False wrapper original_fn fnreturn functools wraps fn wrapper if hasattr fn '__call__' else classmethod wrapper
def serialize_args fn def wrapper obj *args **kwargs args [ utils strtime arg if isinstance arg datetime datetime else arg for arg in args]for k v in kwargs items if k 'exc_val' and v kwargs[k] six text_type v elif k 'exc_tb' and v and not isinstance v six string_types kwargs[k] '' join traceback format_tb v elif isinstance v datetime datetime kwargs[k] utils strtime v if hasattr fn '__call__' return fn obj *args **kwargs return fn __get__ None obj *args **kwargs wrapper remotable getattr fn 'remotable' False wrapper original_fn fnreturn functools wraps fn wrapper if hasattr fn '__call__' else classmethod wrapper
def create_rule_instance rule return SecurityRule rule['protocol'] rule['source_address_prefix'] rule['destination_address_prefix'] rule['access'] rule['direction'] id rule get 'id' None description rule get 'description' None source_port_range rule get 'source_port_range' None destination_port_range rule get 'destination_port_range' None priority rule get 'priority' None provisioning_state rule get 'provisioning_state' None name rule get 'name' None etag rule get 'etag' None
def setup_platform hass config add_devices discovery_info None from limitlessled bridge import Bridgeconfig rewrite_legacy config lights []for bridge_conf in config get CONF_BRIDGES bridge Bridge bridge_conf get CONF_HOST port bridge_conf get CONF_PORT DEFAULT_PORT version bridge_conf get CONF_VERSION DEFAULT_VERSION for group_conf in bridge_conf get CONF_GROUPS group bridge add_group group_conf get CONF_NUMBER group_conf get CONF_NAME group_conf get CONF_TYPE DEFAULT_LED_TYPE lights append LimitlessLEDGroup factory group add_devices lights
def jail_path_configured try jc JailsConfiguration objects latest 'id' except JailsConfiguration DoesNotExist jc Nonereturn jc and jc jc_path and os path exists jc jc_path
@not_implemented_for 'directed' def generalized_degree G nodes None if nodes in G return next _triangles_and_degree_iter G nodes [3]return {v gd for v d t gd in _triangles_and_degree_iter G nodes }
@webob dec wsgify@util check_accept 'application/json' def list_usages req context req environ['placement context']uuid util wsgi_path_item req environ 'uuid' try resource_provider objects ResourceProvider get_by_uuid context uuid except exception NotFound as exc raise webob exc HTTPNotFound _ 'Noresourceproviderwithuuid% uuid sfound % error s' % {'uuid' uuid 'error' exc} json_formatter util json_error_formatter usage objects UsageList get_all_by_resource_provider_uuid context uuid response req responseresponse body encodeutils to_utf8 jsonutils dumps _serialize_usages resource_provider usage req response content_type 'application/json'return req response
def get_cloud_init_mime cloud_init if isinstance cloud_init six string_types cloud_init json loads cloud_init _cloud_init email mime multipart MIMEMultipart if 'boothooks' in cloud_init for script_name script in six iteritems cloud_init['boothooks'] _script email mime text MIMEText script 'cloud-boothook' _cloud_init attach _script if 'scripts' in cloud_init for script_name script in six iteritems cloud_init['scripts'] _script email mime text MIMEText script 'x-shellscript' _cloud_init attach _script if 'cloud-config' in cloud_init cloud_config cloud_init['cloud-config']_cloud_config email mime text MIMEText _safe_dump cloud_config 'cloud-config' _cloud_init attach _cloud_config return _cloud_init as_string
def get_converter map name args if not name in map converters raise LookupError 'theconverter%rdoesnotexist' % name if args args kwargs parse_converter_args args else args kwargs {}return map converters[name] map *args **kwargs
def init_test testdir class options tag ''verbose Nonecont Falseharness 'autoserv'hostname Noneuser Nonelog Truereturn client_setup_job init_test options testdir
def get_subplotspec_list axes_list grid_spec None subplotspec_list []for ax in axes_list axes_or_locator ax get_axes_locator if axes_or_locator is None axes_or_locator axif hasattr axes_or_locator 'get_subplotspec' subplotspec axes_or_locator get_subplotspec subplotspec subplotspec get_topmost_subplotspec gs subplotspec get_gridspec if grid_spec is not None if gs grid_spec subplotspec Noneelif gs locally_modified_subplot_params subplotspec Noneelse subplotspec Nonesubplotspec_list append subplotspec return subplotspec_list
def stop port global client_cancelclient_cancel Truetry if transparent_torification s socket socket s connect '127 0 0 1' port s sendall 'GET/{0 s}/shutdownHTTP/1 1\r\n\r\n' format shutdown_slug else urlopen 'http //127 0 0 1 {0 d}/{1 s}/shutdown' format port shutdown_slug read except pass
def CreateCustomizerFeed client feed_name ad_customizer_feed_service client GetService 'AdCustomizerFeedService' customizer_feed {'feedName' feed_name 'feedAttributes' [{'type' 'STRING' 'name' 'Name'} {'type' 'STRING' 'name' 'Price'} {'type' 'DATE_TIME' 'name' 'Date'}]}feed_service_operation {'operator' 'ADD' 'operand' customizer_feed}response ad_customizer_feed_service mutate [feed_service_operation] if response and 'value' in response feed response['value'][0]feed_data {'feedId' feed['feedId'] 'nameId' feed['feedAttributes'][0]['id'] 'priceId' feed['feedAttributes'][1]['id'] 'dateId' feed['feedAttributes'][2]['id']}print "Feedwithname'%s'andID%swasaddedwith \n DCTB NameattributeID%sandpriceattributeID%sanddateattributeID%s" % feed['feedName'] feed['feedId'] feed_data['nameId'] feed_data['priceId'] feed_data['dateId'] return feedelse raise errors GoogleAdsError 'Nofeedswereadded'
def order_at_oo a d t if a is_zero return ooreturn d degree t - a degree t
def suspend instance_id profile None conn _auth profile return conn suspend instance_id
@db_api retry_if_session_inactive @db_api context_manager writerdef set_all_quota_usage_dirty context resource dirty True query db_utils model_query context quota_models QuotaUsage query query filter_by resource resource return query update {'dirty' dirty}
def demo_high_accuracy_rules postag num_sents 3000 min_acc 0 96 min_score 10
def build_paged_url request base request build_absolute_uri request path items [ k v for k in request GET if k 'page' for v in request GET getlist k if v]qsa urlencode items return u'%s?%s' % base qsa
def ensure_Image global Imageif Image is None raise RuntimeError "YouaretryingtousePIL-dependentfunctionalitybutdon'thavePILinstalled "
def _instance_overrides_method base instance method_name bound_method getattr instance method_name unbound_method getattr base method_name return six get_unbound_function unbound_method six get_method_function bound_method
def read_rle file_obj header bit_width debug_logging count header >> 1 zero_data '\x00\x00\x00\x00'width bit_width + 7 // 8 data file_obj read width data data + zero_data[len data ] value struct unpack '<i' data [0]if debug_logging logger debug u'ReadRLEgroupwithvalue%sofbyte-width%sandcount%s' value width count for _ in range count yield value
def normprob z direction 'two-sided' mean 0 std 1 if direction 'two-sided' if z > 0 return 2 * 1 0 - norm cdf z mean std else return 2 * norm cdf z mean std elif direction 'high' return 1 - norm cdf z mean std elif direction 'low' return norm cdf z mean std else raise ValueError 'Unknowndirection '
def debugerror if _hasTemplating out str djangoerror else out '<p>You\'vesetweb pytousethefancierdebugerrorerrormessages \nbutthesemessagesrequireyouinstalltheCheetahtemplate\nsystem Formoreinformation see\n<ahref "http //webpy org/">theweb pywebsite</a> </p>\n\n<p>Inthemeantime here\'saplainolderrormessage </p>\n\n<pre>%s</pre>\n\n<p> Ifitsayssomethingabout\'Compiler\' thenit\'sprobably\nbecauseyou\'retryingtousetemplatesandyouhaven\'t\ninstalledCheetah Seeabove </p>\n' % htmlquote traceback format_exc context status '500InternalServerError'context headers [ 'Content-Type' 'text/html' ]context output out
def _export_spreadsheet client spreadsheet_key worksheet_id headers cleaned_headers _get_cleaned_headers headers spreadsheet_lines [headers]rows_feed client GetListFeed spreadsheet_key worksheet_id visibility 'public' projection 'values' while True found_data Falsefor row in rows_feed entry line []for header_idx header cleaned_header in enumerate zip headers cleaned_headers try cell_data row custom[cleaned_header] textexcept KeyError raise GoogleSpreadsheetError "Couldnotmapheader'%s'toGoogleSpreadsheet'sinternalrepresentationoftheheader WesuggestchangingthenameoftheheaderinyourGoogleSpreadsheettobealphanumericifpossible asthiswilllikelysolvetheissue Notethatthenameisn't*required*tobealphanumeric butitmayfixissueswithconvertingtoGoogleSpreadsheet'sinternalformatinsomecases " % header if not found_data and header_idx 0 and cell_data lstrip startswith '#' line append cell_data breakelse line append cell_data found_data Truespreadsheet_lines append line next_link rows_feed GetNextLink if next_link rows_feed client Get next_link href converter SpreadsheetsListFeedFromString else breakreturn spreadsheet_lines
def llite_fs directory for fs in os listdir directory fs_name _ fs_id fs partition '-' yield fs_name
def split_txt txt epub_split_size_kb 0 if epub_split_size_kb > 0 if isinstance txt unicode txt txt encode 'utf-8' length_byte len txt chunk_size long length_byte / int length_byte / epub_split_size_kb * 1024 + 2 if len filter lambda x len x > chunk_size txt split '\n\n' txt '\n\n' join [split_string_separator line chunk_size for line in txt split '\n\n' ] if isbytestring txt txt txt decode 'utf-8' return txt
def _expand_authorized_keys_path path user home converted_path ''had_escape Falsefor char in path if had_escape had_escape Falseif char '%' converted_path + '%'elif char 'u' converted_path + userelif char 'h' converted_path + homeelse error 'AuthorizedKeysFilepath unknowntokencharacter"%{0}"' format char raise CommandExecutionError error continueelif char '%' had_escape Trueelse converted_path + charif had_escape error "AuthorizedKeysFilepath Lastcharactercan'tbeescapecharacter"raise CommandExecutionError error return converted_path
def rpc name dest None format 'xml' args None **kwargs ret {'name' name 'changes' {} 'result' True 'comment' ''}if args is not None ret['changes'] __salt__['junos rpc'] name dest format *args **kwargs else ret['changes'] __salt__['junos rpc'] name dest format **kwargs return ret
def make_2d a a np atleast_2d a T Tn a shape[0]newshape np product a shape[1 ] astype int a a reshape n newshape order 'F' return a
def make_2d a a np atleast_2d a T Tn a shape[0]newshape np product a shape[1 ] astype int a a reshape n newshape order 'F' return a
def cleanup_tempdir the_dir if os path exists the_dir shutil rmtree the_dir
def libvlc_media_player_get_title p_mi f _Cfunctions get 'libvlc_media_player_get_title' None or _Cfunction 'libvlc_media_player_get_title' 1 None ctypes c_int MediaPlayer return f p_mi
def follow_redirects link sites None def follow url return sites None or urlparse urlparse url hostname in sites class RedirectHandler urllib2 HTTPRedirectHandler def __init__ self self last_url Nonedef redirect_request self req fp code msg hdrs newurl self last_url newurlif not follow newurl return Noner urllib2 HTTPRedirectHandler redirect_request self req fp code msg hdrs newurl r get_method lambda 'HEAD' return rif not follow link return linkredirect_handler RedirectHandler opener urllib2 build_opener redirect_handler req urllib2 Request link req get_method lambda 'HEAD' try with contextlib closing opener open req timeout 1 as site return site urlexcept return redirect_handler last_url if redirect_handler last_url else link
@click command u'doctor' @click option u'--site' help u'sitename' def doctor site None from frappe utils doctor import doctor as _doctorreturn _doctor site site
def find_highest_files directory unversioned regex files find_matching_files directory regex get_numbers re compile ' \\d+ \\d+ \\d+ ' def max2 a b 'Returnsthelargerofthetwovalues'av get_numbers search os path basename a groups bv get_numbers search os path basename b groups ret cmp av[0] bv[0] or cmp av[1] bv[1] or cmp av[2] bv[2] if ret < 0 return breturn aif len files > 0 return reduce max2 files last_chance os path join directory unversioned if os path exists last_chance return last_chancereturn None
def _generate_min_degree gamma average_degree max_degree tolerance max_iters min_deg_top max_degreemin_deg_bot 1min_deg_mid min_deg_top - min_deg_bot / 2 + min_deg_bot itrs 0mid_avg_deg 0while abs mid_avg_deg - average_degree > tolerance if itrs > max_iters raise nx ExceededMaxIterations 'Couldnotmatchaverage_degree' mid_avg_deg 0for x in range int min_deg_mid max_degree + 1 mid_avg_deg + x ** - gamma + 1 / zeta gamma min_deg_mid tolerance if mid_avg_deg > average_degree min_deg_top min_deg_midmin_deg_mid min_deg_top - min_deg_bot / 2 + min_deg_bot else min_deg_bot min_deg_midmin_deg_mid min_deg_top - min_deg_bot / 2 + min_deg_bot itrs + 1return round min_deg_mid
def _generate_min_degree gamma average_degree max_degree tolerance max_iters min_deg_top max_degreemin_deg_bot 1min_deg_mid min_deg_top - min_deg_bot / 2 + min_deg_bot itrs 0mid_avg_deg 0while abs mid_avg_deg - average_degree > tolerance if itrs > max_iters raise nx ExceededMaxIterations 'Couldnotmatchaverage_degree' mid_avg_deg 0for x in range int min_deg_mid max_degree + 1 mid_avg_deg + x ** - gamma + 1 / zeta gamma min_deg_mid tolerance if mid_avg_deg > average_degree min_deg_top min_deg_midmin_deg_mid min_deg_top - min_deg_bot / 2 + min_deg_bot else min_deg_bot min_deg_midmin_deg_mid min_deg_top - min_deg_bot / 2 + min_deg_bot itrs + 1return round min_deg_mid
def _get_host_utilization context host ram_mb disk_gb instances instance_get_all_by_host context host vms len instances free_ram_mb ram_mb - FLAGS reserved_host_memory_mb free_disk_gb disk_gb - FLAGS reserved_host_disk_mb * 1024 work 0for instance in instances free_ram_mb - instance memory_mbfree_disk_gb - instance root_gbfree_disk_gb - instance ephemeral_gbif instance vm_state in [vm_states BUILDING vm_states REBUILDING vm_states MIGRATING vm_states RESIZING] work + 1return dict free_ram_mb free_ram_mb free_disk_gb free_disk_gb current_workload work running_vms vms
def delete config unused_plugins cert_manager delete config
def _retrieve_course course_key course modulestore get_course course_key depth 0 if course is None raise CourseNotFoundErrorreturn course
def p4BranchesInGit branchesAreInRemotes True branches {}cmdline 'gitrev-parse--symbolic'if branchesAreInRemotes cmdline + '--remotes'else cmdline + '--branches'for line in read_pipe_lines cmdline line line strip if not line startswith 'p4/' continueif line 'p4/HEAD' continuebranch line[len 'p4/' ]branches[branch] parseRevision line return branches
@anonymous_csrf@mobile_template 'questions/{mobile/}marketplace_refund html' def marketplace_refund request template error_message Noneif request method 'GET' form MarketplaceRefundForm request user else form MarketplaceRefundForm request user request POST if form is_valid try form submit_ticket return HttpResponseRedirect reverse 'questions marketplace_aaq_success' except ZendeskError error_message ZENDESK_ERROR_MESSAGEreturn render request template {'form' form 'error_message' error_message}
def ensure_tenant_exists keystone tenant_name tenant_description check_mode try tenant get_tenant keystone tenant_name except KeyError passelse if tenant description tenant_description return False tenant id elif check_mode return True tenant id else tenant update description tenant_description return True tenant id if check_mode return True None ks_tenant keystone tenants create tenant_name tenant_name description tenant_description enabled True return True ks_tenant id
def _get_block_count_url_string xblock_type_set block_url ''if len xblock_type_set > 0 block_url + '&all_blocks true&block_counts 'for index block_type in enumerate xblock_type_set block_url + block_typeif index < len xblock_type_set - 1 block_url + ' 'return block_url
def _get_block_count_url_string xblock_type_set block_url ''if len xblock_type_set > 0 block_url + '&all_blocks true&block_counts 'for index block_type in enumerate xblock_type_set block_url + block_typeif index < len xblock_type_set - 1 block_url + ' 'return block_url
def getOrderedNestedRings nestedRings insides []orderedNestedRings []for loopIndex in xrange len nestedRings nestedRing nestedRings[loopIndex]otherLoops []for beforeIndex in xrange loopIndex otherLoops append nestedRings[beforeIndex] boundary for afterIndex in xrange loopIndex + 1 len nestedRings otherLoops append nestedRings[afterIndex] boundary if isPathEntirelyInsideLoops otherLoops nestedRing boundary insides append nestedRing else orderedNestedRings append nestedRing for outside in orderedNestedRings outside getFromInsideSurroundings insides return orderedNestedRings
def getOrderedNestedRings nestedRings insides []orderedNestedRings []for loopIndex in xrange len nestedRings nestedRing nestedRings[loopIndex]otherLoops []for beforeIndex in xrange loopIndex otherLoops append nestedRings[beforeIndex] boundary for afterIndex in xrange loopIndex + 1 len nestedRings otherLoops append nestedRings[afterIndex] boundary if isPathEntirelyInsideLoops otherLoops nestedRing boundary insides append nestedRing else orderedNestedRings append nestedRing for outside in orderedNestedRings outside getFromInsideSurroundings insides return orderedNestedRings
def assert_crypto_availability f @functools wraps f def wrapper *args **kwds if AES is None raise CryptoUnavailableError return f *args **kwds return wrapper
def make_providers_strings providers if not providers return Noneif len providers 1 providers_string providers[0]elif len providers 2 providers_string _ '{first_provider}and{second_provider}' format first_provider providers[0] second_provider providers[1] else providers_string _ '{first_providers} and{last_provider}' format first_providers u' ' join providers[ -1 ] last_provider providers[ -1 ] return providers_string
def make_providers_strings providers if not providers return Noneif len providers 1 providers_string providers[0]elif len providers 2 providers_string _ '{first_provider}and{second_provider}' format first_provider providers[0] second_provider providers[1] else providers_string _ '{first_providers} and{last_provider}' format first_providers u' ' join providers[ -1 ] last_provider providers[ -1 ] return providers_string
def cont_inputs f return typefilter inputvars f continuous_types
def humanReadableMask mask s []for k v in _FLAG_TO_HUMAN if k & mask s append v return s
def integer_powers exprs terms {}for term in exprs for j in terms a cancel term / j if a is_Rational terms[j] append term a breakelse terms[term] [ term S 1 ]newterms {}for term in terms common_denom reduce ilcm [i as_numer_denom [1] for _ i in terms[term]] newterm term / common_denom newmults [ i j * common_denom for i j in terms[term]]newterms[newterm] newmultsreturn sorted iter newterms items key lambda item item[0] sort_key
def main argv env None return GenericArgparseImplementation env main argv
def check_cycle reg assignments return check_cycle_ reg assignments []
def _inspect_environment app_version os environ['CURRENT_VERSION_ID'] rsplit ' ' 1 [0]conf_version int os environ get 'CURRENT_CONFIGURATION_VERSION' '0' development os environ get 'SERVER_SOFTWARE' '' startswith 'Development/' return app_version conf_version development
@register u'quoted-insert' def quoted_insert event event cli quoted_insert True
def cpu_affinity_by_task pid vcpu_pid cmd "cat/proc/%s/task/%s/status grepCpus_allowed awk'{print$2}'" % pid vcpu_pid output system_output cmd ignore_status False return output
def cpu_affinity_by_task pid vcpu_pid cmd "cat/proc/%s/task/%s/status grepCpus_allowed awk'{print$2}'" % pid vcpu_pid output system_output cmd ignore_status False return output
def cpu_affinity_by_task pid vcpu_pid cmd "cat/proc/%s/task/%s/status grepCpus_allowed awk'{print$2}'" % pid vcpu_pid output system_output cmd ignore_status False return output
def _prepare_create_request instance parent_name 'projects/' + instance _client project message messages_v2_pb2 CreateInstanceRequest parent parent_name instance_id instance instance_id instance data_v2_pb2 Instance display_name instance display_name cluster message clusters[instance instance_id]cluster name instance name + '/clusters/' + instance instance_id cluster location parent_name + '/locations/' + instance _cluster_location_id cluster serve_nodes instance _cluster_serve_nodesreturn message
def fgraph_updated_vars fgraph expanded_inputs updated_vars {}potential_values list fgraph outputs if len expanded_inputs len fgraph inputs raise ValueError 'expanded_inputsmustmatchlen fgraph inputs ' for e_input ivar in reversed list zip expanded_inputs fgraph inputs if e_input update is not None updated_vars[ivar] potential_values pop return updated_vars
@endpoint u'/interface-data/tag-browser' def tag_browser ctx rd db library_id get_library_data ctx rd [ 2]etag u'%s %s %s' % db last_modified rd username library_id etag hashlib sha1 etag encode u'utf-8' hexdigest def generate db library_id get_library_data ctx rd [ 2]return json ctx rd tag_browser categories_as_json ctx rd db return rd etagged_dynamic_response etag generate
def get_essid_from_cap bssid capfile if not program_exists 'tshark' return ''cmd ['tshark' '-r' capfile '-R' 'wlan fc type_subtype 0x05&&wlan sa %s' % bssid '-n']proc Popen cmd stdout PIPE stderr DN proc wait for line in proc communicate [0] split '\n' if line find 'SSID ' -1 essid line[ line find 'SSID ' + 5 ]print GR + '[+]' + W + 'guessedessid %s' % G + essid + W return essidprint R + '[ ]' + O + 'unabletoguessessid ' + W return ''
def get_entity_kind key_path if isinstance key_path entity_pb EntityProto key_path key_path key return key_path path element_list [ -1 ] type
def getScalarMetricWithTimeOfDayAnomalyParams metricData minVal None maxVal None minResolution None tmImplementation 'cpp' if minResolution is None minResolution 0 001if minVal is None or maxVal is None compMinVal compMaxVal _rangeGen metricData if minVal is None minVal compMinValif maxVal is None maxVal compMaxValif minVal maxVal maxVal minVal + 1 if tmImplementation is 'cpp' paramFileRelativePath os path join 'anomaly_params_random_encoder' 'best_single_metric_anomaly_params_cpp json' elif tmImplementation is 'tm_cpp' paramFileRelativePath os path join 'anomaly_params_random_encoder' 'best_single_metric_anomaly_params_tm_cpp json' else raise ValueError 'InvalidstringfortmImplementation Trycpportm_cpp' with resource_stream __name__ paramFileRelativePath as infile paramSet json load infile _fixupRandomEncoderParams paramSet minVal maxVal minResolution return paramSet
def getScalarMetricWithTimeOfDayAnomalyParams metricData minVal None maxVal None minResolution None tmImplementation 'cpp' if minResolution is None minResolution 0 001if minVal is None or maxVal is None compMinVal compMaxVal _rangeGen metricData if minVal is None minVal compMinValif maxVal is None maxVal compMaxValif minVal maxVal maxVal minVal + 1 if tmImplementation is 'cpp' paramFileRelativePath os path join 'anomaly_params_random_encoder' 'best_single_metric_anomaly_params_cpp json' elif tmImplementation is 'tm_cpp' paramFileRelativePath os path join 'anomaly_params_random_encoder' 'best_single_metric_anomaly_params_tm_cpp json' else raise ValueError 'InvalidstringfortmImplementation Trycpportm_cpp' with resource_stream __name__ paramFileRelativePath as infile paramSet json load infile _fixupRandomEncoderParams paramSet minVal maxVal minResolution return paramSet
def coarsen reduction x axes trim_excess False for i in range x ndim if i not in axes axes[i] 1if trim_excess ind tuple slice 0 - d % axes[i] if d % axes[i] else slice None None for i d in enumerate x shape x x[ind]newshape tuple concat [ x shape[i] / axes[i] axes[i] for i in range x ndim ] return reduction x reshape newshape axis tuple range 1 x ndim * 2 2
def coarsen reduction x axes trim_excess False for i in range x ndim if i not in axes axes[i] 1if trim_excess ind tuple slice 0 - d % axes[i] if d % axes[i] else slice None None for i d in enumerate x shape x x[ind]newshape tuple concat [ x shape[i] / axes[i] axes[i] for i in range x ndim ] return reduction x reshape newshape axis tuple range 1 x ndim * 2 2
def coarsen reduction x axes trim_excess False for i in range x ndim if i not in axes axes[i] 1if trim_excess ind tuple slice 0 - d % axes[i] if d % axes[i] else slice None None for i d in enumerate x shape x x[ind]newshape tuple concat [ x shape[i] / axes[i] axes[i] for i in range x ndim ] return reduction x reshape newshape axis tuple range 1 x ndim * 2 2
@docfillerdef uniform_filter1d input size axis -1 output None mode 'reflect' cval 0 0 origin 0 input numpy asarray input if numpy iscomplexobj input raise TypeError 'Complextypenotsupported' axis _ni_support _check_axis axis input ndim if size < 1 raise RuntimeError 'incorrectfiltersize' output return_value _ni_support _get_output output input if size // 2 + origin < 0 or size // 2 + origin > size raise ValueError 'invalidorigin' mode _ni_support _extend_mode_to_code mode _nd_image uniform_filter1d input size axis output mode cval origin return return_value
def parse_rx_excludes options fatal excluded_patterns []for flag in options option parameter flagif option '--exclude-rx' try excluded_patterns append re compile parameter except re error as ex fatal 'invalid--exclude-rxpattern %s %s' % parameter ex elif option '--exclude-rx-from' try f open resolve_parent parameter except IOError as e raise fatal "couldn'tread%s" % parameter for pattern in f readlines spattern pattern rstrip '\n' if not spattern continuetry excluded_patterns append re compile spattern except re error as ex fatal 'invalid--exclude-rxpattern %s %s' % spattern ex return excluded_patterns
def beaker registry xml_parent data beaker XML SubElement xml_parent 'org jenkinsci plugins beakerbuilder BeakerBuilder' jobSource XML SubElement beaker 'jobSource' if 'content' in data and 'path' in data raise JenkinsJobsException "Usejustoneof'content'or'path'" elif 'content' in data jobSourceClass 'org jenkinsci plugins beakerbuilder StringJobSource'jobSource set 'class' jobSourceClass XML SubElement jobSource 'jobContent' text data['content']elif 'path' in data jobSourceClass 'org jenkinsci plugins beakerbuilder FileJobSource'jobSource set 'class' jobSourceClass XML SubElement jobSource 'jobPath' text data['path']else raise JenkinsJobsException "Useoneof'content'or'path'" XML SubElement beaker 'downloadFiles' text str data get 'download-logs' False lower
def beaker registry xml_parent data beaker XML SubElement xml_parent 'org jenkinsci plugins beakerbuilder BeakerBuilder' jobSource XML SubElement beaker 'jobSource' if 'content' in data and 'path' in data raise JenkinsJobsException "Usejustoneof'content'or'path'" elif 'content' in data jobSourceClass 'org jenkinsci plugins beakerbuilder StringJobSource'jobSource set 'class' jobSourceClass XML SubElement jobSource 'jobContent' text data['content']elif 'path' in data jobSourceClass 'org jenkinsci plugins beakerbuilder FileJobSource'jobSource set 'class' jobSourceClass XML SubElement jobSource 'jobPath' text data['path']else raise JenkinsJobsException "Useoneof'content'or'path'" XML SubElement beaker 'downloadFiles' text str data get 'download-logs' False lower
def console_auth_token_destroy_all_by_instance context instance_uuid return IMPL console_auth_token_destroy_all_by_instance context instance_uuid
def bandpass_pre_cache lows 80 100 120 highs 1200 3000 8000 bands 2000 8000 rate 44100 for low in lows for high in highs _butter 6 low high rate rate for band in bands _butter 6 band rate rate
def _add_tag request question_id tag_name request POST get 'tag-name' '' strip if tag_name question get_object_or_404 Question pk question_id try canonical_name add_existing_tag tag_name question tags except Tag DoesNotExist if request user has_perm 'taggit add_tag' question tags add tag_name canonical_name tag_nameelse raisetag_added send sender Question question_id question id tag_name canonical_name return question canonical_name return None None
def rm_job name response salt utils http query '{0}/scheduler/job/{1}' format _base_url name method 'DELETE' return True
def CredibleInterval pmf percentage 90 cdf pmf MakeCdf prob 1 - percentage / 100 0 / 2 interval cdf Value prob cdf Value 1 - prob return interval
def get_exploration_rights exploration_id strict True model exp_models ExplorationRightsModel get exploration_id strict strict if model is None return Nonereturn _get_activity_rights_from_model model feconf ACTIVITY_TYPE_EXPLORATION
def variance input labels None index None count sum sum_c_sq _stats input labels index centered True return sum_c_sq / np asanyarray count astype float
def get_updated_changeset_revisions_from_tool_shed app tool_shed_url name owner changeset_revision tool_shed_url common_util get_tool_shed_url_from_tool_shed_registry app tool_shed_url if tool_shed_url is None or name is None or owner is None or changeset_revision is None message 'UnabletogetupdatedchangesetrevisionsfromtheToolShedbecauseoneormoreofthefollowing'message + 'requiredparametersisNone tool_shed_url %s name %s owner %s changeset_revision %s' % str tool_shed_url str name str owner str changeset_revision raise Exception message params dict name name owner owner changeset_revision changeset_revision pathspec ['repository' 'updated_changeset_revisions']text util url_get tool_shed_url password_mgr app tool_shed_registry url_auth tool_shed_url pathspec pathspec params params return text
def _filter_host field value match_level None if match_level is None if '#' in value match_level 'pool'elif '@' in value match_level 'backend'else match_level 'host'conn_str CONF database connectionif conn_str startswith 'mysql' and conn_str[5] in ['+' ' '] cmp_value func binary value like_op 'LIKEBINARY'else cmp_value valuelike_op 'LIKE'conditions [ field cmp_value ]if match_level 'pool' conditions append field op like_op value + '#%' if match_level 'host' conditions append field op like_op value + '@%' return or_ *conditions
def require_content content_type def decorator f @functools wraps f def decorated_function req if req content_type content_type raise webob exc HTTPUnsupportedMediaType _ 'Themediatype% bad_type sisnotsupported use% good_type s' % {'bad_type' req content_type or 'None' 'good_type' content_type} json_formatter json_error_formatter else return f req return decorated_functionreturn decorator
@taskdef celery_error **kw log info 'abouttoraiseanexceptionfromcelery' raise RuntimeError 'thisisanexceptionfromcelery'
def posix_time_to_http posix_time if posix_time return email_utils formatdate posix_time usegmt True
def posix_time_to_http posix_time if posix_time return email_utils formatdate posix_time usegmt True
def first_hour_average timeseries last_hour_threshold time - FULL_DURATION - 3600 series pandas Series [x[1] for x in timeseries if x[0] < last_hour_threshold ] mean series mean stdDev series std t tail_avg timeseries return abs t - mean > 3 * stdDev
def test_unicode_escapes s '"a\\xac\\u1234\\u20ac\\U00008000"'assert len s 29 entry tokenize s [0]assert len entry 5 assert [ord x for x in entry] [97 172 4660 8364 32768]
def _bootstrap_deb root arch flavor repo_url None static_qemu None pkgs None exclude_pkgs None if repo_url is None repo_url 'http //ftp debian org/debian/'deb_args ['debootstrap' '--foreign' '--arch' _cmd_quote arch '--include'] + pkgs + ['--exclude'] + exclude_pkgs + [_cmd_quote flavor _cmd_quote root _cmd_quote repo_url ] __salt__['cmd run'] deb_args python_shell False __salt__['cmd run'] 'cp{qemu}{root}/usr/bin/' format qemu _cmd_quote static_qemu root _cmd_quote root env {'DEBIAN_FRONTEND' 'noninteractive' 'DEBCONF_NONINTERACTIVE_SEEN' 'true' 'LC_ALL' 'C' 'LANGUAGE' 'C' 'LANG' 'C' 'PATH' '/sbin /bin /usr/bin'}__salt__['cmd run'] 'chroot{root}/debootstrap/debootstrap--second-stage' format root _cmd_quote root env env __salt__['cmd run'] 'chroot{root}dpkg--configure-a' format root _cmd_quote root env env
def genFull pset min_ max_ type_ None def condition height depth 'Expressiongenerationstopswhenthedepthisequaltoheight 'return depth height return generate pset min_ max_ condition type_
def genFull pset min_ max_ type_ None def condition height depth 'Expressiongenerationstopswhenthedepthisequaltoheight 'return depth height return generate pset min_ max_ condition type_
def get_first_defined data keys default_value None for key in keys try return data[key]except KeyError passreturn default_value
def trim_silence audio threshold energy librosa feature rmse audio frames np nonzero energy > threshold indices librosa core frames_to_samples frames [1]return audio[indices[0] indices[ -1 ]] if indices size else audio[0 0]
def trim_silence audio threshold energy librosa feature rmse audio frames np nonzero energy > threshold indices librosa core frames_to_samples frames [1]return audio[indices[0] indices[ -1 ]] if indices size else audio[0 0]
def has_studio_write_access user course_key return bool STUDIO_EDIT_CONTENT & get_user_permissions user course_key
def _TestQueryFollowed tester user_cookie request_dict validator tester validator user_id device_id tester GetIdsFromCookie user_cookie def _MakeViewpointDict followed 'Createaviewpointdictfromthefollowedobjectplusits\nreferencedviewpointobject \n'viewpoint validator GetModelObject Viewpoint followed viewpoint_id follower validator GetModelObject Follower DBKey followed user_id followed viewpoint_id metadata_dict viewpoint MakeMetadataDict follower if follower CanViewContent and 'cover_photo' in metadata_dict photo_dict metadata_dict['cover_photo']obj_store ObjectStore GetInstance ObjectStore PHOTO _AddPhotoUrls obj_store photo_dict return metadata_dictactual_dict tester SendRequest 'query_followed' user_cookie request_dict followed validator QueryModelObjects Followed user_id limit request_dict get 'limit' None start_key request_dict get 'start_key' None expected_dict {'viewpoints' [_MakeViewpointDict f for f in followed]}if len followed > 0 expected_dict['last_key'] followed[ -1 ] sort_keytester _CompareResponseDicts 'query_followed' user_id request_dict expected_dict actual_dict return actual_dict
@log_calldef metadef_resource_type_create context values global DATAresource_type_values copy deepcopy values resource_type_name resource_type_values['name']allowed_attrubites ['name' 'protected']for resource_type in DATA['metadef_resource_types'] if resource_type['name'] resource_type_name raise exception Duplicate incorrect_keys set resource_type_values keys - set allowed_attrubites if incorrect_keys raise exception Invalid 'Thekeys%sarenotvalid' % str incorrect_keys resource_type _format_resource_type resource_type_values DATA['metadef_resource_types'] append resource_type return resource_type
def _ensure_regexp source n markers ' +~"\' [% ? *^ &- /\\'k 0while True k + 1if n - k < 0 return Truechar source[ n - k ]if char in markers return Trueif char '' and char '\n' breakreturn False
def _ensure_regexp source n markers ' +~"\' [% ? *^ &- /\\'k 0while True k + 1if n - k < 0 return Truechar source[ n - k ]if char in markers return Trueif char '' and char '\n' breakreturn False
def _ensure_regexp source n markers ' +~"\' [% ? *^ &- /\\'k 0while True k + 1if n - k < 0 return Truechar source[ n - k ]if char in markers return Trueif char '' and char '\n' breakreturn False
def colon_separated_user_group arg try parts arg split ' ' 1 if len parts 1 uid name_to_uid parts[0] gid -1 else uid name_to_uid parts[0] gid name_to_gid parts[1] return uid gid except raise ValueError 'Invaliduser groupdefinition%s' % arg
def GetValidHostsForCert cert if 'subjectAltName' in cert return [x[1] for x in cert['subjectAltName'] if x[0] lower 'dns' ]else return [x[0][1] for x in cert['subject'] if x[0][0] lower 'commonname' ]
def split line if not line strip raise exceptions MpdNoCommand u'Nocommandgiven' match WORD_RE match line if not match raise exceptions MpdUnknownError u'Invalidwordcharacter' whitespace command remainder match groups if whitespace raise exceptions MpdUnknownError u'Letterexpected' result [command]while remainder match PARAM_RE match remainder if not match msg _determine_error_message remainder raise exceptions MpdArgError msg command command unquoted quoted remainder match groups result append unquoted or UNESCAPE_RE sub u'\\g<1>' quoted return result
def seq3 seq custom_map None undef_code 'Xaa' if custom_map is None custom_map {'*' 'Ter'}threecode dict list IUPACData protein_letters_1to3_extended items + list custom_map items return '' join threecode get aa undef_code for aa in seq
def equalize image mask None if image mode 'P' image image convert 'RGB' h image histogram mask lut []for b in range 0 len h 256 histo [_f for _f in h[b b + 256 ] if _f]if len histo < 1 lut extend list range 256 else step functools reduce operator add histo - histo[ -1 ] // 255 if not step lut extend list range 256 else n step // 2 for i in range 256 lut append n // step n n + h[ i + b ] return _lut image lut
@taskdef apiserver ctx port 8000 wait True autoreload True host '127 0 0 1' pty True env os environ copy cmd 'DJANGO_SETTINGS_MODULE api base settings{}manage pyrunserver{} {}--nothreading' format sys executable host port if not autoreload cmd + '--noreload'if settings SECURE_MODE cmd cmd replace 'runserver' 'runsslserver' cmd + '--certificate{}--key{}' format settings OSF_SERVER_CERT settings OSF_SERVER_KEY if wait return ctx run cmd echo True pty pty from subprocess import Popenreturn Popen cmd shell True env env
def sm_backend_conf_create context values return IMPL sm_backend_conf_create context values
def _get_filter_query args query Q for arg in args if hasattr Document arg and args[arg] append Q **{str 'document__' + arg + '__id' long args[arg] } append append Q **{str 'file__' + arg + '__id' long args[arg] } append append Q **{str 'weblink__' + arg + '__id' long args[arg] } query query & append return query
def number_aware_alphabetical_cmp str1 str2 def flatten_tokens tokens l []for token in tokens if isinstance token str for char in token l append char else assert isinstance token float l append token return lseq1 flatten_tokens tokenize_by_number str1 seq2 flatten_tokens tokenize_by_number str2 l min len seq1 len seq2 i 0while i < l if isinstance seq1[i] float and isinstance seq2[i] string_types return -1 elif isinstance seq1[i] string_types and isinstance seq2[i] float return 1elif seq1[i] < seq2[i] return -1 elif seq1[i] > seq2[i] return 1i + 1if len seq1 < len seq2 return -1 elif len seq1 > len seq2 return 1return 0
def setup_proximity_component hass name config ignored_zones config get CONF_IGNORED_ZONES proximity_devices config get CONF_DEVICES tolerance config get CONF_TOLERANCE proximity_zone nameunit_of_measurement config get CONF_UNIT_OF_MEASUREMENT hass config units length_unit zone_id 'zone {}' format proximity_zone proximity Proximity hass proximity_zone DEFAULT_DIST_TO_ZONE DEFAULT_DIR_OF_TRAVEL DEFAULT_NEAREST ignored_zones proximity_devices tolerance zone_id unit_of_measurement proximity entity_id '{} {}' format DOMAIN proximity_zone proximity update_ha_state track_state_change hass proximity_devices proximity check_proximity_state_change return True
def blend_image img bgcolor u'#ffffff' canvas QImage img size QImage Format_RGB32 canvas fill QColor bgcolor overlay_image img canvas return canvas
def blend_image img bgcolor u'#ffffff' canvas QImage img size QImage Format_RGB32 canvas fill QColor bgcolor overlay_image img canvas return canvas
def load_parameters file_ with closing _load_parameters_npzfile file_ as npz_file return {name replace SERIALIZATION_BRICK_DELIMITER BRICK_DELIMITER value for name value in npz_file items }
def openshift_build_verify registry xml_parent data osb XML SubElement xml_parent 'com openshift jenkins plugins pipeline OpenShiftBuildVerifier' mapping [ 'api-url' 'apiURL' 'https //openshift default svc cluster local' 'bld-cfg' 'bldCfg' 'frontend' 'namespace' 'namespace' 'test' 'auth-token' 'authToken' '' 'verbose' 'verbose' False ]convert_mapping_to_xml osb data mapping fail_required True
def openshift_build_verify registry xml_parent data osb XML SubElement xml_parent 'com openshift jenkins plugins pipeline OpenShiftBuildVerifier' mapping [ 'api-url' 'apiURL' 'https //openshift default svc cluster local' 'bld-cfg' 'bldCfg' 'frontend' 'namespace' 'namespace' 'test' 'auth-token' 'authToken' '' 'verbose' 'verbose' False ]convert_mapping_to_xml osb data mapping fail_required True
def openshift_build_verify registry xml_parent data osb XML SubElement xml_parent 'com openshift jenkins plugins pipeline OpenShiftBuildVerifier' mapping [ 'api-url' 'apiURL' 'https //openshift default svc cluster local' 'bld-cfg' 'bldCfg' 'frontend' 'namespace' 'namespace' 'test' 'auth-token' 'authToken' '' 'verbose' 'verbose' False ]convert_mapping_to_xml osb data mapping fail_required True
def openshift_build_verify registry xml_parent data osb XML SubElement xml_parent 'com openshift jenkins plugins pipeline OpenShiftBuildVerifier' mapping [ 'api-url' 'apiURL' 'https //openshift default svc cluster local' 'bld-cfg' 'bldCfg' 'frontend' 'namespace' 'namespace' 'test' 'auth-token' 'authToken' '' 'verbose' 'verbose' False ]convert_mapping_to_xml osb data mapping fail_required True
def triangle_code X centroids X_sqr T sqr X sum axis 1 dimshuffle 0 'x' c_sqr T sqr centroids sum axis 1 dimshuffle 'x' 0 c_sqr name 'c_sqr'Xc T dot X centroids T Xc name 'Xc'sq_dists c_sqr + X_sqr - 2 0 * Xc sq_dists_safe T clip sq_dists 0 0 1e+30 Z T sqrt sq_dists_safe Z name 'Z'mu Z mean axis 1 mu name 'mu'mu mu dimshuffle 0 'x' mu name 'mu_broadcasted'rval T clip mu - Z 0 0 1e+30 rval name 'triangle_code'return rval
def get_release_group_image_list releasegroupid return _caa_request releasegroupid entitytype 'release-group'
def iterServices xrd_tree xrd getYadisXRD xrd_tree return prioSort xrd findall service_tag
@contextmanagerdef current_user user old_user get_current_user set_current_user user yield set_current_user old_user
def get_unicode_modules modules []try import codecsmodules append 'codecs' except ImportError logger error "Cannotdetectmodules'codecs' " return modules
def get_unicode_modules modules []try import codecsmodules append 'codecs' except ImportError logger error "Cannotdetectmodules'codecs' " return modules
def get_unicode_modules modules []try import codecsmodules append 'codecs' except ImportError logger error "Cannotdetectmodules'codecs' " return modules
def choose_plugin prepared question opts [ plugin_ep description_with_name + '[Misconfigured]' if plugin_ep misconfigured else '' for plugin_ep in prepared]while True disp z_util interfaces IDisplay code index disp menu question opts help_label 'MoreInfo' force_interactive True if code display_util OK plugin_ep prepared[index]if plugin_ep misconfigured z_util interfaces IDisplay notification 'Theselectedpluginencounteredanerrorwhileparsingyourserverconfigurationandcannotbeused Theerrorwas \n\n{0}' format plugin_ep prepare pause False else return plugin_epelif code display_util HELP if prepared[index] misconfigured msg 'ReportedError %s' % prepared[index] prepare else msg prepared[index] init more_info z_util interfaces IDisplay notification msg force_interactive True else return None
def checksum data start 0 skip_word None if len data % 2 0 arr array array 'H' data[ -1 ] else arr array array 'H' data if skip_word is not None for i in range 0 len arr if i skip_word continuestart + arr[i]else for i in range 0 len arr start + arr[i]if len data % 2 0 start + struct unpack 'H' data[ -1 ] + '\x00' [0]start start >> 16 + start & 65535 start + start >> 16 return ntohs ~ start & 65535
def wait_on_interfaces interfaces timeout 10 result defaultdict list timeout time time + timeout while len result < len interfaces and time time < timeout rin [i for i in interfaces values ]win [i for i in interfaces values if i unsent_requests] rout wout xout select select rin win [] 1 for interface in wout interface send_requests for interface in rout responses interface get_responses if responses result[interface server] extend responses return result
def __VersionIsSupported desiredVersion serviceVersionDescription root serviceVersionDescriptionif root tag 'namespaces' if root get 'version' '1 0' raise RuntimeError 'vimServiceVersions xmlhasversion%s whichisnotunderstood' % root get 'version' desiredVersionId versionIdMap[desiredVersion]supportedVersion Nonefor namespace in root findall 'namespace' versionId namespace findtext 'version' if versionId desiredVersionId return Trueelse for versionId in namespace findall 'priorVersions/version' if versionId text desiredVersionId return Trueelse wsdlNS 'http //schemas xmlsoap org/wsdl/'importElement serviceVersionDescription find ' //{%s}import' % wsdlNS supportedVersion versionMap[importElement get 'namespace' [4 ]]if IsChildVersion supportedVersion desiredVersion return Truereturn False
def __VersionIsSupported desiredVersion serviceVersionDescription root serviceVersionDescriptionif root tag 'namespaces' if root get 'version' '1 0' raise RuntimeError 'vimServiceVersions xmlhasversion%s whichisnotunderstood' % root get 'version' desiredVersionId versionIdMap[desiredVersion]supportedVersion Nonefor namespace in root findall 'namespace' versionId namespace findtext 'version' if versionId desiredVersionId return Trueelse for versionId in namespace findall 'priorVersions/version' if versionId text desiredVersionId return Trueelse wsdlNS 'http //schemas xmlsoap org/wsdl/'importElement serviceVersionDescription find ' //{%s}import' % wsdlNS supportedVersion versionMap[importElement get 'namespace' [4 ]]if IsChildVersion supportedVersion desiredVersion return Truereturn False
def get keys **kwargs return get_async keys **kwargs get_result
def median input labels None index None return _select input labels index find_median True [0]
def _CheckGeoPoint geo_point if not isinstance geo_point GeoPoint raise TypeError 'geo_pointmustbeaGeoPoint got%s' % geo_point __class__ __name__ return geo_point
def is_valid_definition params language return not exclude_bracket params get 'enabled' BH_ENABLED params get 'language_filter' BH_LANG_FILTER params get 'language_list' BH_LANG_LIST language and params get 'open' None is not None and params get 'close' None is not None
def main zk_connection_locations appscale_info get_zk_locations_string zookeeper zk ZKTransaction host zk_connection_locations start_gc False db_info appscale_info get_db_info table db_info[' table']master appscale_info get_db_master_ip datastore_path '{0} 8888' format master ds_groomer DatastoreGroomer zookeeper table datastore_path logging debug 'Tryingtogetgroomerlock ' if ds_groomer get_groomer_lock logging info 'Gotthegroomerlock ' try ds_groomer run_groomer except Exception as exception logging exception 'Encounteredexception{}whilerunningthegroomer ' format str exception try ds_groomer zoo_keeper release_lock_with_path zk DS_GROOM_LOCK_PATH except zk ZKTransactionException as zk_exception logging error 'Unabletoreleasezklock{0} ' format str zk_exception except zk ZKInternalException as zk_exception logging error 'Unabletoreleasezklock{0} ' format str zk_exception finally zookeeper close else logging info 'Didnotgetthegroomerlock '
def main zk_connection_locations appscale_info get_zk_locations_string zookeeper zk ZKTransaction host zk_connection_locations start_gc False db_info appscale_info get_db_info table db_info[' table']master appscale_info get_db_master_ip datastore_path '{0} 8888' format master ds_groomer DatastoreGroomer zookeeper table datastore_path logging debug 'Tryingtogetgroomerlock ' if ds_groomer get_groomer_lock logging info 'Gotthegroomerlock ' try ds_groomer run_groomer except Exception as exception logging exception 'Encounteredexception{}whilerunningthegroomer ' format str exception try ds_groomer zoo_keeper release_lock_with_path zk DS_GROOM_LOCK_PATH except zk ZKTransactionException as zk_exception logging error 'Unabletoreleasezklock{0} ' format str zk_exception except zk ZKInternalException as zk_exception logging error 'Unabletoreleasezklock{0} ' format str zk_exception finally zookeeper close else logging info 'Didnotgetthegroomerlock '
def get_course_enrollment_details course_id include_expired False cache_key u'enrollment course details {course_id} {include_expired}' format course_id course_id include_expired include_expired cached_enrollment_data Nonetry cached_enrollment_data cache get cache_key except Exception log exception u'Erroroccurredwhileretrievingcourseenrollmentdetailsfromthecache' if cached_enrollment_data log info u'Getenrollmentdataforcourse%s cached ' course_id return cached_enrollment_datacourse_enrollment_details _data_api get_course_enrollment_info course_id include_expired try cache_time_out getattr settings 'ENROLLMENT_COURSE_DETAILS_CACHE_TIMEOUT' 60 cache set cache_key course_enrollment_details cache_time_out except Exception log exception u'Erroroccurredwhilecachingcourseenrollmentdetailsforcourse%s' course_id raise errors CourseEnrollmentError u'Anunexpectederroroccurredwhileretrievingcourseenrollmentdetails ' log info u'Getenrollmentdataforcourse%s' course_id return course_enrollment_details
def make_server host port app server_class WSGIServer handler_class WSGIRequestHandler server server_class host port handler_class server set_app app return server
def whathdr filename f open filename 'rb' h f read 512 for tf in tests res tf h f if res return resreturn None
def dup_sign_variations f K prev k K zero 0 for coeff in f if K is_negative coeff * prev k + 1if coeff prev coeffreturn k
@contextmanagerdef set_trap app trap Trap prev_tls _state _tls_state set_default_app trap class NonTLS object current_app trap_state _tls NonTLS yield _state _tls prev_tls
@contextmanagerdef set_trap app trap Trap prev_tls _state _tls_state set_default_app trap class NonTLS object current_app trap_state _tls NonTLS yield _state _tls prev_tls
def assert_before lst item1 item2 assert_less lst index item1 lst index item2 '{0 r}appearsbefore{1 r}' format item1 item2
def assert_before lst item1 item2 assert_less lst index item1 lst index item2 '{0 r}appearsbefore{1 r}' format item1 item2
def generate_module src if not src return UNKNOWN_MODULE filename ext splitext urlsplit src path if ext not in ' js' ' jsx' ' coffee' return UNKNOWN_MODULEif filename endswith ' min' filename filename[ -4 ]tokens filename split '/' for idx token in enumerate tokens if VERSION_RE match token return '/' join tokens[ idx + 1 ] return CLEAN_MODULE_RE sub '' filename or UNKNOWN_MODULE
def generate_module src if not src return UNKNOWN_MODULE filename ext splitext urlsplit src path if ext not in ' js' ' jsx' ' coffee' return UNKNOWN_MODULEif filename endswith ' min' filename filename[ -4 ]tokens filename split '/' for idx token in enumerate tokens if VERSION_RE match token return '/' join tokens[ idx + 1 ] return CLEAN_MODULE_RE sub '' filename or UNKNOWN_MODULE
def generate_module src if not src return UNKNOWN_MODULE filename ext splitext urlsplit src path if ext not in ' js' ' jsx' ' coffee' return UNKNOWN_MODULEif filename endswith ' min' filename filename[ -4 ]tokens filename split '/' for idx token in enumerate tokens if VERSION_RE match token return '/' join tokens[ idx + 1 ] return CLEAN_MODULE_RE sub '' filename or UNKNOWN_MODULE
@click command u'version' def get_version frappe init u'' for m in sorted frappe get_all_apps module frappe get_module m if hasattr module u'__version__' print u'{0}{1}' format m module __version__
def reg name ret {'name' name 'changes' {} 'comment' '' 'result' True}now time time if 'status' not in __reg__ __reg__['status'] {}__reg__['status']['val'] {}for event in __events__ if fnmatch fnmatch event['tag'] 'salt/beacon/*/status/*' idata {'recv_time' now}for key in event['data']['data'] if key in 'id' 'recv_time' continueidata[key] event['data'][key]__reg__['status']['val'][event['data']['data']['id']] idataret['changes'][event['data']['data']['id']] Truereturn ret
def test_regression_futuretimes_4302 from utils exceptions import AstropyWarningfrom builtin_frames import utilsif hasattr utils u'__warningregistry__' utils __warningregistry__ clear with catch_warnings as found_warnings future_time Time u'2511-5-1' c CIRS 1 * u deg 2 * u deg obstime future_time c transform_to ITRS obstime future_time if not isinstance iers IERS_Auto iers_table iers IERS_Auto saw_iers_warnings Falsefor w in found_warnings if issubclass w category AstropyWarning if u' some timesareoutsideofrangecoveredbyIERStable' in str w message saw_iers_warnings Truebreakassert saw_iers_warnings u'NeversawIERSwarning'
def strip_esc_sequence txt if isinstance txt six string_types return txt replace '\x1b' '?' else return txt
def strip_esc_sequence txt if isinstance txt six string_types return txt replace '\x1b' '?' else return txt
def strip_esc_sequence txt if isinstance txt six string_types return txt replace '\x1b' '?' else return txt
@for_app 'whois' at_least 1 def match command return True
def write_summary allcodes print '\n\n----------------------------------------------------------------------------\nCheckedallbinaryfilesforCPUextensioncodes Foundthefollowingcodes 'for code in allcodes print codeprint '\n----------------------------------------------------------------------------\n'
def setup_platform hass config add_devices_callback discovery_info None _configured_partitions discovery_info['partitions']for part_num in _configured_partitions _device_config_data PARTITION_SCHEMA _configured_partitions[part_num] _device EnvisalinkSensor _device_config_data[CONF_PARTITIONNAME] part_num EVL_CONTROLLER alarm_state['partition'][part_num] EVL_CONTROLLER add_devices_callback [_device]
def _str2records filename rel recs []contents nltk data load u'corpora/chat80/%s' % filename format u'text' for line in contents splitlines if line startswith rel line re sub rel + u'\\ ' u'' line line re sub u'\\ \\ $' u'' line record line split u' ' recs append record return recs
def strip_between a b string p '%s *?%s' % a b p re compile p re DOTALL re I return re sub p '' string
def strip_between a b string p '%s *?%s' % a b p re compile p re DOTALL re I return re sub p '' string
def thread value def add_attribute function function thread valuereturn functionreturn add_attribute
def required_length nmin nmax class RequiredLength Action def __call__ self parser args values option_string None if not nmin < len values < nmax raise ArgumentError self u'requiresbetween%sand%sarguments' % nmin nmax setattr args self dest values return RequiredLength
def _get_table_item doc name return doc xpath '//th[contains text "%s" ]/following-sibling td' % name [0]
def absent name region None key None keyid None profile None ret {'name' name 'result' True 'comment' '' 'changes' {}}exists __salt__['boto_asg launch_configuration_exists'] name region region key key keyid keyid profile profile if exists if __opts__['test'] ret['comment'] 'Launchconfigurationsettobedeleted 'ret['result'] Nonereturn retdeleted __salt__['boto_asg delete_launch_configuration'] name region region key key keyid keyid profile profile if deleted ret['changes']['old'] nameret['changes']['new'] Noneret['comment'] 'Deletedlaunchconfiguration 'else ret['result'] Falseret['comment'] 'Failedtodeletelaunchconfiguration 'else ret['comment'] 'Launchconfigurationdoesnotexist 'return ret
@cli command name u'gh-deploy' @click option u'-c' u'--clean/--dirty' is_flag True default True help clean_help @click option u'-f' u'--config-file' type click File u'rb' help config_help @click option u'-m' u'--message' help commit_message_help @click option u'-b' u'--remote-branch' help remote_branch_help @click option u'-r' u'--remote-name' help remote_name_help @click option u'--force' is_flag True help force_help @common_optionsdef gh_deploy_command config_file clean message remote_branch remote_name force try cfg config load_config config_file config_file remote_branch remote_branch remote_name remote_name build build cfg dirty not clean gh_deploy gh_deploy cfg message message force force except exceptions ConfigurationError as e raise SystemExit u'\n' + str e
@cli command name u'gh-deploy' @click option u'-c' u'--clean/--dirty' is_flag True default True help clean_help @click option u'-f' u'--config-file' type click File u'rb' help config_help @click option u'-m' u'--message' help commit_message_help @click option u'-b' u'--remote-branch' help remote_branch_help @click option u'-r' u'--remote-name' help remote_name_help @click option u'--force' is_flag True help force_help @common_optionsdef gh_deploy_command config_file clean message remote_branch remote_name force try cfg config load_config config_file config_file remote_branch remote_branch remote_name remote_name build build cfg dirty not clean gh_deploy gh_deploy cfg message message force force except exceptions ConfigurationError as e raise SystemExit u'\n' + str e
def FindPreviousMatchingAngleBracket clean_lines linenum init_prefix line init_prefixnesting_stack ['>']while True match Search '^ * [<> \\[\\]] [^<> \\[\\]]*$' line if match operator match group 2 line match group 1 if nesting_stack[ -1 ] '>' if operator in '>' ' ' ']' nesting_stack append operator elif operator '<' nesting_stack pop if not nesting_stack return Trueelif operator ' ' return Trueelse return Falseelif operator in '>' ' ' ']' nesting_stack append operator elif operator in ' ' '[' nesting_stack pop else linenum - 1if linenum < 0 breakline clean_lines elided[linenum]return False
def FindPreviousMatchingAngleBracket clean_lines linenum init_prefix line init_prefixnesting_stack ['>']while True match Search '^ * [<> \\[\\]] [^<> \\[\\]]*$' line if match operator match group 2 line match group 1 if nesting_stack[ -1 ] '>' if operator in '>' ' ' ']' nesting_stack append operator elif operator '<' nesting_stack pop if not nesting_stack return Trueelif operator ' ' return Trueelse return Falseelif operator in '>' ' ' ']' nesting_stack append operator elif operator in ' ' '[' nesting_stack pop else linenum - 1if linenum < 0 breakline clean_lines elided[linenum]return False
def get_devmm name name os path expanduser name if is_chrdev name or is_blkdev name stat_structure os stat name return os major stat_structure st_rdev os minor stat_structure st_rdev else return 0 0
def isProcedureDoneOrFileIsEmpty gcodeText procedure if gcodeText '' return Truereturn isProcedureDone gcodeText procedure
def isProcedureDoneOrFileIsEmpty gcodeText procedure if gcodeText '' return Truereturn isProcedureDone gcodeText procedure
def remove_hosts_file known_hosts_file '%s/ ssh/known_hosts' % os getenv 'HOME' if os path isfile known_hosts_file logging debug 'Deletingknownhostsfile%s' known_hosts_file os remove known_hosts_file
def _find_vector_rotation a b R np eye 3 v np cross a b if np allclose v 0 0 return Rs np dot v v c np dot a b vx _skew_symmetric_cross v R + vx + np dot vx vx * 1 - c / s return R
def _find_vector_rotation a b R np eye 3 v np cross a b if np allclose v 0 0 return Rs np dot v v c np dot a b vx _skew_symmetric_cross v R + vx + np dot vx vx * 1 - c / s return R
def _find_vector_rotation a b R np eye 3 v np cross a b if np allclose v 0 0 return Rs np dot v v c np dot a b vx _skew_symmetric_cross v R + vx + np dot vx vx * 1 - c / s return R
def get_rising_items omit_sr_ids count 4 all_rising rising get_all_rising candidate_sr_ids {sr_id for link score sr_id in all_rising} difference omit_sr_ids link_fullnames [link for link score sr_id in all_rising if sr_id in candidate_sr_ids ]link_fullnames_to_show random_sample link_fullnames count rising_links Link _by_fullname link_fullnames_to_show return_dict False data True rising_items [ExploreItem TYPE_RISING 'ris' Subreddit _byID l sr_id l for l in rising_links]return rising_items
def get_credit_providers providers_list None return CreditProvider get_credit_providers providers_list providers_list
def _setup_logging stream None hdlr logging StreamHandler stream defaultFmt '% name s % levelname s % message s'infoFmt '% name s % message s'fmtr _PerLevelFormatter fmt defaultFmt fmtFromLevel {logging INFO infoFmt} hdlr setFormatter fmtr logging root addHandler hdlr log setLevel logging INFO
def solidity_library_symbol library_name length min len library_name 36 library_piece library_name[ length]hold_piece '_' * 36 - length return '__{library}{hold}__' format library library_piece hold hold_piece
def solidity_library_symbol library_name length min len library_name 36 library_piece library_name[ length]hold_piece '_' * 36 - length return '__{library}{hold}__' format library library_piece hold hold_piece
@task@timeddef i18n_ltr sh 'i18n_tooltransifexltr' print 'Nowgeneratinglangugagefiles 'sh 'i18n_toolgenerate--ltr' print 'Committingtranslations 'sh 'gitclean-fdXconf/locale' sh 'gitaddconf/locale' sh 'gitcommit--amend'
def get_runnertype_by_id runnertype_id try runnertype RunnerType get_by_id runnertype_id except ValueError ValidationError as e LOG warning 'Databaselookupforrunnertypewithid "%s"resultedinexception %s' runnertype_id e raise StackStormDBObjectNotFoundError 'Unabletofindrunnertypewithid "%s"' % runnertype_id return runnertype
def loadIcon stock_item_id stock_item getattr gtk stock_item_id local_icon os path join GUI_DATA_PATH 'icons' '16' '%s png' % stock_item if os path exists local_icon im gtk Image im set_from_file local_icon im show return im get_pixbuf else icon_theme gtk IconTheme try icon icon_theme load_icon stock_item 16 except icon loadImage 'missing-image png' get_pixbuf return icon
def loadIcon stock_item_id stock_item getattr gtk stock_item_id local_icon os path join GUI_DATA_PATH 'icons' '16' '%s png' % stock_item if os path exists local_icon im gtk Image im set_from_file local_icon im show return im get_pixbuf else icon_theme gtk IconTheme try icon icon_theme load_icon stock_item 16 except icon loadImage 'missing-image png' get_pixbuf return icon
def simple_traceback limit stack_trace traceback extract_stack limit limit [ -2 ]return '\n' join '-' join os path basename filename function_name str line_number for filename line_number function_name text in stack_trace
def _add_constant name container None c getattr constants name _UNDEFINED if c _UNDEFINED returnglobals [name] c__all__ append name if container is not None container add c return c
def _add_constant name container None c getattr constants name _UNDEFINED if c _UNDEFINED returnglobals [name] c__all__ append name if container is not None container add c return c
def getElementsByLocalName childNodes localName elementsByLocalName getChildElementsByLocalName childNodes localName for childNode in childNodes if childNode getNodeType 1 elementsByLocalName + childNode getElementsByLocalName localName return elementsByLocalName
def getElementsByLocalName childNodes localName elementsByLocalName getChildElementsByLocalName childNodes localName for childNode in childNodes if childNode getNodeType 1 elementsByLocalName + childNode getElementsByLocalName localName return elementsByLocalName
def getFirstWord splitLine if len splitLine > 0 return splitLine[0]return ''
def getIPx domain try return socket gethostbyname_ex domain [2]except Exception return False
def getIPx domain try return socket gethostbyname_ex domain [2]except Exception return False
def tile x reps return Tile reps x
def validate_input trans error_map param_values page_param_map first param_values['name1']second param_values['name2']if first second error_map['name1'] 'Thevaluenamesshouldbedifferent '
def _start **kwargs import osfrom psychopy import visualopenWindows visual window openWindowsif len openWindows 0 print 'ThePsychoPyWindowmustbecreatedpriortostartingiohub Exiting 'sys exit 1 '\nDisplay \nname display\nreporting_unit_type pix\ndevice_number 0\nphysical_dimensions \nwidth 500\nheight 281\nunit_type mm\ndefault_eye_distance \nsurface_center 550\nunit_type mm\npsychopy_monitor_name default\noverride_using_psycho_settings False\n'if not kwargs has_key 'experiment_code' kwargs['experiment_code'] 'default_exp'if kwargs has_key 'iohub_config_name' print 'Startingiohubwithiohub_config_nameprovided 'return launchHubServer **kwargs iohub_config_name ' /iohub_config yaml'if os path isfile os path normpath os path abspath iohub_config_name kwargs['iohub_config_name'] os path normpath os path abspath iohub_config_name print 'Startingiohubwithiohub_config_name ' kwargs['iohub_config_name']return launchHubServer **kwargs print 'Startingiohubwithdefaultsettings 'return launchHubServer **kwargs
def write global _mbnamesif _mbnames is_enabled is not True returnif _mbnames get_incremental is not True _mbnames write
def ValidateAttributes tag attributes goodattributes all_good Truefor attr in attributes keys if not attr in goodattributes output Error 'Unknown%sattribute %s' % tag attr all_good Falsereturn all_good
def ValidateAttributes tag attributes goodattributes all_good Truefor attr in attributes keys if not attr in goodattributes output Error 'Unknown%sattribute %s' % tag attr all_good Falsereturn all_good
def ValidateAttributes tag attributes goodattributes all_good Truefor attr in attributes keys if not attr in goodattributes output Error 'Unknown%sattribute %s' % tag attr all_good Falsereturn all_good
def ValidateAttributes tag attributes goodattributes all_good Truefor attr in attributes keys if not attr in goodattributes output Error 'Unknown%sattribute %s' % tag attr all_good Falsereturn all_good
def ValidateAttributes tag attributes goodattributes all_good Truefor attr in attributes keys if not attr in goodattributes output Error 'Unknown%sattribute %s' % tag attr all_good Falsereturn all_good
def ValidateAttributes tag attributes goodattributes all_good Truefor attr in attributes keys if not attr in goodattributes output Error 'Unknown%sattribute %s' % tag attr all_good Falsereturn all_good
def ValidateAttributes tag attributes goodattributes all_good Truefor attr in attributes keys if not attr in goodattributes output Error 'Unknown%sattribute %s' % tag attr all_good Falsereturn all_good
def _add_inline_definition item statement global _current_statementbackup _current_statement type_ options _expand_one_key_dictionary item _current_statement UnnamedStatement type type_ _parse_statement options statement add_child _current_statement _current_statement backup
def _cancelledToTimedOutError value timeout if isinstance value failure Failure value trap CancelledError raise TimeoutError timeout 'Deferred' return value
def _cancelledToTimedOutError value timeout if isinstance value failure Failure value trap CancelledError raise TimeoutError timeout 'Deferred' return value
def describe_role name region None key None keyid None profile None conn _get_conn region region key key keyid keyid profile profile try info conn get_role name if not info return Falserole info get_role_response get_role_result rolerole['assume_role_policy_document'] json loads _unquote role assume_role_policy_document for policy_key policy in role['assume_role_policy_document'] items if policy_key 'Statement' for val in policy if 'Sid' in val and not val['Sid'] del val['Sid']return roleexcept boto exception BotoServerError as e log debug e msg 'Failedtoget{0}information 'log error msg format name return False
def get_email_addresses survey startdate enddate token settings SURVEYGIZMO_API_TOKENsecret settings SURVEYGIZMO_API_TOKEN_SECRETemails []page 1more_pages Truesurvey_id SURVEYS[survey]['email_collection_survey_id']if token is None or secret is None return emailswhile more_pages response requests get 'https //restapi surveygizmo com/v2/survey/{survey}/surveyresponse?filter[field][0] datesubmitted&filter[operator][0] > &filter[value][0] {start}+0 0 0filter[field][1] datesubmitted&filter[operator][1] <&filter[value][1] {end}+0 0 0&filter[field][2] status&filter[operator][2] &filter[value][2] Complete&resultsperpage 500&page {page}&api_token {token}&api_token_secret {secret}' format survey survey_id start startdate end enddate page page token token secret secret timeout 300 results json loads response content total_pages results['total_pages']more_pages page < total_pages emails emails + [r['[question 13 ]'] for r in results['data']] page + 1return emails
def get_email_addresses survey startdate enddate token settings SURVEYGIZMO_API_TOKENsecret settings SURVEYGIZMO_API_TOKEN_SECRETemails []page 1more_pages Truesurvey_id SURVEYS[survey]['email_collection_survey_id']if token is None or secret is None return emailswhile more_pages response requests get 'https //restapi surveygizmo com/v2/survey/{survey}/surveyresponse?filter[field][0] datesubmitted&filter[operator][0] > &filter[value][0] {start}+0 0 0filter[field][1] datesubmitted&filter[operator][1] <&filter[value][1] {end}+0 0 0&filter[field][2] status&filter[operator][2] &filter[value][2] Complete&resultsperpage 500&page {page}&api_token {token}&api_token_secret {secret}' format survey survey_id start startdate end enddate page page token token secret secret timeout 300 results json loads response content total_pages results['total_pages']more_pages page < total_pages emails emails + [r['[question 13 ]'] for r in results['data']] page + 1return emails
def remove_template s n_open n_close 0 0 starts ends [] [] in_template Falseprev_c Nonefor i c in enumerate iter s if not in_template if c '{' and c prev_c starts append i - 1 in_template Truen_open 1if in_template if c '{' n_open + 1elif c '}' n_close + 1if n_open n_close ends append i in_template False n_open n_close 0 0 prev_c cs '' join [s[ end + 1 start] for start end in zip starts + [None] [ -1 ] + ends ] return s
def ReadFemPreg dct_file '2002FemPreg dct' dat_file '2002FemPreg dat gz' dct thinkstats2 ReadStataDct dct_file df dct ReadFixedWidth dat_file compression 'gzip' CleanFemPreg df return df
def region_code_for_country_code country_code regions COUNTRY_CODE_TO_REGION_CODE get country_code None if regions is None return UNKNOWN_REGIONelse return regions[0]
def region_code_for_country_code country_code regions COUNTRY_CODE_TO_REGION_CODE get country_code None if regions is None return UNKNOWN_REGIONelse return regions[0]
def migrate_log logs logs_count logs count count 0for log in logs count + 1node log params get 'node' or log params get 'project' params_node Node load node if params_node is_registration log params['node'] get_registered_from params_node log params['registration'] params_node _idelse log params['registration'] RegistrationApproval load log params['registration_approval_id'] _get_registration _idlog save logger info '{}/{}Finishedmigratinglog{} registrationaction{} params[node] {}andparams[registration] {}' format count logs_count log _id log action log params['node'] log params['registration']
def _expected_cols expected_attrs if not expected_attrs return expected_attrssimple_cols [attr for attr in expected_attrs if attr in _INSTANCE_OPTIONAL_JOINED_FIELDS ]complex_cols [ 'extra %s' % field for field in _INSTANCE_EXTRA_FIELDS if field in expected_attrs ]if complex_cols simple_cols append 'extra' simple_cols [x for x in simple_cols if x not in _INSTANCE_EXTRA_FIELDS ]expected_cols simple_cols + complex_cols return sorted list set expected_cols key expected_cols index
def _RemainingDataSize input_buffer current_position input_buffer tell input_buffer seek 0 2 remaining_data_size input_buffer tell - current_position input_buffer seek current_position return remaining_data_size
def _RemainingDataSize input_buffer current_position input_buffer tell input_buffer seek 0 2 remaining_data_size input_buffer tell - current_position input_buffer seek current_position return remaining_data_size
def lookup path parent None user None exists None url build_url RESOURCE route 'lookup' params make_params path path parent parent user user exists exists return request 'get' url params params
def updatedb dt meta None res frappe db sql u'selectissinglefromtabDocTypewherename %s' dt if not res raise Exception u'Wrongdoctype"%s"inupdatedb' % dt if not res[0][0] tab DbTable dt u'tab' meta tab validate frappe db commit tab sync frappe db begin
def test_detect_nan nan_detected [False]def detect_nan i node fn for output in fn outputs if numpy isnan output[0] any print '***NaNdetected***' theano printing debugprint node print 'Inputs %s' % [input[0] for input in fn inputs] print 'Outputs %s' % [output[0] for output in fn outputs] nan_detected[0] Truebreakx theano tensor dscalar 'x' f theano function [x] [ theano tensor log x * x ] mode theano compile MonitorMode post_func detect_nan f 0 assert nan_detected[0]
def _parse_sv8_int fileobj limit 9 num 0for i in xrange limit c fileobj read 1 if len c 1 raise EOFErrorc bytearray c num num << 7 c[0] & 127 if not c[0] & 128 return num i + 1 if limit > 0 raise ValueErrorreturn 0 0
def test_find_number_2 s 'aghwirougiuhfajlsopka"-987?'r find_number s assert s[r[0] r[1]] '-987'
@pytest mark django_dbdef test_toggle_quality_check rf admin qc_filter dict false_positive False unit__state TRANSLATED unit__store__translation_project__project__disabled False qc QualityCheck objects filter **qc_filter first unit qc unitdata 'mute 'request create_api_request rf method 'post' user admin data data encode_as_json False response toggle_qualitycheck request unit id qc id assert response status_code 200 assert QualityCheck objects get id qc id false_positive is True request create_api_request rf method 'post' user admin response toggle_qualitycheck request unit id qc id assert response status_code 200 assert QualityCheck objects get id qc id false_positive is False
def constant_app status headers body def application unused_environ start_response start_response status headers return [body]return application
def constant_app status headers body def application unused_environ start_response start_response status headers return [body]return application
def load_certificates directory ' ' certs {}if not os path isdir directory raise IOError 'Invalidcertificatedirectory {0}' format directory glob_string os path join directory '* key' cert_files glob glob glob_string for cert_file in cert_files public_key _ load_certificate cert_file if public_key certs[public_key] Truereturn certs
def blob_to_file filename_hint_propertyname None directory_hint '' directory []def transform_function value bulkload_state if not directory parent_dir os path dirname bulkload_state filename directory append os path join parent_dir directory_hint if directory[0] and not os path exists directory[0] os makedirs directory[0] filename_hint 'blob_'suffix ''filename ''if filename_hint_propertyname filename_hint bulkload_state current_entity[filename_hint_propertyname]filename os path join directory[0] filename_hint if os path exists filename filename '' filename_hint suffix os path splitext filename_hint if not filename filename tempfile mktemp suffix filename_hint directory[0] f open filename 'wb' f write value f close return filenamereturn transform_function
def test_reset ip reset nvars_user_ns len ip user_ns nvars_hidden len ip user_ns_hidden ip user_ns['x'] 1ip user_ns['y'] 1ip reset nt assert_equal len ip user_ns nvars_user_ns nt assert_equal len ip user_ns_hidden nvars_hidden
def test_reset ip reset nvars_user_ns len ip user_ns nvars_hidden len ip user_ns_hidden ip user_ns['x'] 1ip user_ns['y'] 1ip reset nt assert_equal len ip user_ns nvars_user_ns nt assert_equal len ip user_ns_hidden nvars_hidden
def write_dau_pack16 fid kind data data_size 2data np array data dtype '>i2' T_write fid data kind data_size FIFF FIFFT_DAU_PACK16 '>i2'
def validate_read_preference_mode dummy name try return read_pref_mode_from_name name except ValueError raise ValueError '%sisnotavalidreadpreference' % name
def _object_reducer o names 'id' 'name' 'path' 'httpMethod' 'statusCode' 'Created' 'Deleted' 'Updated' 'Flushed' 'Associated' 'Disassociated' result {}if isinstance o dict for k v in six iteritems o if isinstance v dict reduced v if k 'variables' else _object_reducer v names if reduced or _name_matches k names result[k] reducedelif isinstance v list newlist []for val in v reduced _object_reducer val names if reduced or _name_matches k names newlist append reduced if newlist result[k] newlistelif _name_matches k names result[k] vreturn result
def _object_reducer o names 'id' 'name' 'path' 'httpMethod' 'statusCode' 'Created' 'Deleted' 'Updated' 'Flushed' 'Associated' 'Disassociated' result {}if isinstance o dict for k v in six iteritems o if isinstance v dict reduced v if k 'variables' else _object_reducer v names if reduced or _name_matches k names result[k] reducedelif isinstance v list newlist []for val in v reduced _object_reducer val names if reduced or _name_matches k names newlist append reduced if newlist result[k] newlistelif _name_matches k names result[k] vreturn result
def _object_reducer o names 'id' 'name' 'path' 'httpMethod' 'statusCode' 'Created' 'Deleted' 'Updated' 'Flushed' 'Associated' 'Disassociated' result {}if isinstance o dict for k v in six iteritems o if isinstance v dict reduced v if k 'variables' else _object_reducer v names if reduced or _name_matches k names result[k] reducedelif isinstance v list newlist []for val in v reduced _object_reducer val names if reduced or _name_matches k names newlist append reduced if newlist result[k] newlistelif _name_matches k names result[k] vreturn result
def read_file filename with codecs open filename 'r' 'utf8' as f return f read
def skip_if_no_package *args **kwargs from nose import SkipTestpackage_check exc_failed_import SkipTest exc_failed_check SkipTest *args **kwargs
def _filter_schemas schemas schema_tables exclude_table_columns return [_filter_schema s schema_tables exclude_table_columns for s in schemas]
def stop_app_instance app_name port if not misc is_app_name_valid app_name logging error 'Unabletokillappprocess%sonport%dbecauseofinvalidnameforapplication' % app_name int port return Falselogging info 'Stoppingapplication%s' % app_name watch 'app___' + app_name + '-' + str port if not monit_interface stop watch is_group False logging error 'Unabletostopapplicationserverforapp{0}onport{1}' format app_name port return Falsemonit_config_file '{}/appscale-{} cfg' format MONIT_CONFIG_DIR watch try os remove monit_config_file except OSError as os_error logging error 'Errordeleting{0}' format monit_config_file return True
def DocToHelp doc doc doc strip whitespace_only_line re compile '^[ DCTB ]+$' re M doc whitespace_only_line sub '' doc doc CutCommonSpacePrefix doc doc re sub ' ?< \\S \n ? \\S ' '' doc re M return doc
def DocToHelp doc doc doc strip whitespace_only_line re compile '^[ DCTB ]+$' re M doc whitespace_only_line sub '' doc doc CutCommonSpacePrefix doc doc re sub ' ?< \\S \n ? \\S ' '' doc re M return doc
def group_for_key key if not isinstance key entity_pb Reference key entity_pb Reference key first_element key path element 0 key_copy entity_pb Reference key_copy CopyFrom key key_copy path clear_element element key_copy path add_element element MergeFrom first_element return key_copy
def schema_serializer_class serializer_class **kwargs def decorator func func schema_serializer_class serializer_classfunc kwargs kwargsreturn funcreturn decorator
def schema_serializer_class serializer_class **kwargs def decorator func func schema_serializer_class serializer_classfunc kwargs kwargsreturn funcreturn decorator
def schema_serializer_class serializer_class **kwargs def decorator func func schema_serializer_class serializer_classfunc kwargs kwargsreturn funcreturn decorator
def exhaust _iter i Nonefor i in _iter passreturn i
def exhaust _iter i Nonefor i in _iter passreturn i
def test_NORBSmall skip_if_no_data data NORBSmall 'train' assert data X shape 24300 18432 assert data X dtype 'float32' assert data y shape 24300 assert data y_labels 5 assert data get_topological_view shape 24300 96 96 2 data NORBSmall 'test' assert data X shape 24300 18432 assert data X dtype 'float32' assert data y shape 24300 assert data y_labels 5 assert data get_topological_view shape 24300 96 96 2
def timer_helper num_handlers 1 sleep_time 5 event_handlers [] aTimer None error_margin 0 25 global COUNTglobal TIMER_HELPER_FINISHEDCOUNT 0TIMER_HELPER_FINISHED Falsetry if aTimer None aTimer System Timers Timer for i in xrange num_handlers aTimer Elapsed + System Timers ElapsedEventHandler onTimedEvent for handler in event_handlers aTimer Elapsed + System Timers ElapsedEventHandler handler aTimer Interval 100aTimer Enabled Truesleep sleep_time / 10 0 aTimer Enabled Falsesleep sleep_time / 10 0 Assert COUNT > int sleep_time - sleep_time * error_margin * num_handlers + len event_handlers str COUNT Assert COUNT < int sleep_time + sleep_time * error_margin * num_handlers + len event_handlers str COUNT finally TIMER_HELPER_FINISHED True
def timer_helper num_handlers 1 sleep_time 5 event_handlers [] aTimer None error_margin 0 25 global COUNTglobal TIMER_HELPER_FINISHEDCOUNT 0TIMER_HELPER_FINISHED Falsetry if aTimer None aTimer System Timers Timer for i in xrange num_handlers aTimer Elapsed + System Timers ElapsedEventHandler onTimedEvent for handler in event_handlers aTimer Elapsed + System Timers ElapsedEventHandler handler aTimer Interval 100aTimer Enabled Truesleep sleep_time / 10 0 aTimer Enabled Falsesleep sleep_time / 10 0 Assert COUNT > int sleep_time - sleep_time * error_margin * num_handlers + len event_handlers str COUNT Assert COUNT < int sleep_time + sleep_time * error_margin * num_handlers + len event_handlers str COUNT finally TIMER_HELPER_FINISHED True
def test_pretty_class class C passclass D object passassert pretty C str C assert pretty D str D
@receiver m2m_changed sender LocalSite users through def _on_local_site_users_changed sender instance model action pk_set **kwargs if action u'post_add' if isinstance instance User users [instance]local_sites LocalSite objects filter id__in pk_set else users User objects filter id__in pk_set local_sites [instance]for user in users for local_site in local_sites local_site_user_added send sender LocalSite user user local_site local_site
def cityblock u v u _validate_vector u v _validate_vector v return abs u - v sum
def formList tables ['irs_ireport' 'rms_req' 'cr_shelter' 'pr_person' 'pr_image']xml TAG forms for tablename in tables xml append TAG form get_name tablename _url 'http //' + request env http_host + URL f 'create' args tablename response headers['Content-Type'] 'text/xml'response view 'xforms xml'return xml
def formList tables ['irs_ireport' 'rms_req' 'cr_shelter' 'pr_person' 'pr_image']xml TAG forms for tablename in tables xml append TAG form get_name tablename _url 'http //' + request env http_host + URL f 'create' args tablename response headers['Content-Type'] 'text/xml'response view 'xforms xml'return xml
def read_fmt bib_name bib_file cache_name formatted_cache_name _cache_name bib_name bib_file try meta_data formatted_entries cache read_global formatted_cache_name except raise cache CacheMiss modified_time os path getmtime bib_file if modified_time > meta_data['cache_time'] raise cache CacheMiss if meta_data['version'] _VERSION or any meta_data[s] get_setting 'cite_' + s for s in ['panel_format' 'autocomplete_format'] print 'Formattingstringhaschanged updatingcache ' current_time bib_entries cache read_global cache_name formatted_entries _create_formatted_entries formatted_cache_name bib_entries current_time return formatted_entries
def dropfile cachedir user None dfn os path join cachedir ' dfn' mask os umask 191 try log info 'RotatingAESkey' if os path isfile dfn log info 'AESkeyrotationalreadyrequested' returnif os path isfile dfn and not os access dfn os W_OK os chmod dfn stat S_IRUSR stat S_IWUSR with salt utils fopen dfn 'wb+' as fp_ fp_ write '' os chmod dfn stat S_IRUSR if user try import pwduid pwd getpwnam user pw_uidos chown dfn uid -1 except KeyError ImportError OSError IOError passfinally os umask mask
def dropfile cachedir user None dfn os path join cachedir ' dfn' mask os umask 191 try log info 'RotatingAESkey' if os path isfile dfn log info 'AESkeyrotationalreadyrequested' returnif os path isfile dfn and not os access dfn os W_OK os chmod dfn stat S_IRUSR stat S_IWUSR with salt utils fopen dfn 'wb+' as fp_ fp_ write '' os chmod dfn stat S_IRUSR if user try import pwduid pwd getpwnam user pw_uidos chown dfn uid -1 except KeyError ImportError OSError IOError passfinally os umask mask
def dropfile cachedir user None dfn os path join cachedir ' dfn' mask os umask 191 try log info 'RotatingAESkey' if os path isfile dfn log info 'AESkeyrotationalreadyrequested' returnif os path isfile dfn and not os access dfn os W_OK os chmod dfn stat S_IRUSR stat S_IWUSR with salt utils fopen dfn 'wb+' as fp_ fp_ write '' os chmod dfn stat S_IRUSR if user try import pwduid pwd getpwnam user pw_uidos chown dfn uid -1 except KeyError ImportError OSError IOError passfinally os umask mask
def _sphere_pot_or_field rr mri_rr mri_Q coils sphere bem_rr n_jobs coil_type fun _eeg_spherepot_coil if coil_type 'eeg' else _sphere_field parallel p_fun _ parallel_func fun n_jobs B np concatenate parallel p_fun r coils sphere for r in np array_split rr n_jobs return B
def SampleSum dists n pmf Pmf RandomSum dists for i in range n return pmf
def get_matplotlib_backend_module_names import_statement '\nimportos sys\n\n#Preservestdout \nsys_stdout sys stdout\n\ntry \n#Redirectoutputprintedbythisimportationto"/dev/null" preventing\n#suchoutputfrombeingerroneouslyinterpretedasanerror \nwithopen os devnull \'w\' asdev_null \nsys stdout dev_null\n__import__ \'%s\' \n#IfthisisanImportError printthisexception\'smessagewithoutatraceback \n#ImportErrormessagesarehuman-readableandrequirenoadditionalcontext \nexceptImportErrorasexc \nsys stdout sys_stdout\nprint exc \n#Else printthisexceptionprecededbyatraceback traceback print_exc \n#printstostderrratherthanstdoutandmustnotbecalledhere \nexceptException \nsys stdout sys_stdout\nimporttraceback\nprint traceback format_exc \n'backend_names eval_statement 'importmatplotlib print matplotlib rcsetup all_backends ' module_names []if not is_darwin and 'CocoaAgg' in backend_names backend_names remove 'CocoaAgg' for backend_name in backend_names module_name 'matplotlib backends backend_%s' % backend_name lower stdout exec_statement import_statement % module_name if not stdout module_names append module_name logger info 'Matplotlibbackend"%s" added' % backend_name else logger info 'Matplotlibbackend"%s" ignored\n%s' % backend_name stdout return module_names
def get_matplotlib_backend_module_names import_statement '\nimportos sys\n\n#Preservestdout \nsys_stdout sys stdout\n\ntry \n#Redirectoutputprintedbythisimportationto"/dev/null" preventing\n#suchoutputfrombeingerroneouslyinterpretedasanerror \nwithopen os devnull \'w\' asdev_null \nsys stdout dev_null\n__import__ \'%s\' \n#IfthisisanImportError printthisexception\'smessagewithoutatraceback \n#ImportErrormessagesarehuman-readableandrequirenoadditionalcontext \nexceptImportErrorasexc \nsys stdout sys_stdout\nprint exc \n#Else printthisexceptionprecededbyatraceback traceback print_exc \n#printstostderrratherthanstdoutandmustnotbecalledhere \nexceptException \nsys stdout sys_stdout\nimporttraceback\nprint traceback format_exc \n'backend_names eval_statement 'importmatplotlib print matplotlib rcsetup all_backends ' module_names []if not is_darwin and 'CocoaAgg' in backend_names backend_names remove 'CocoaAgg' for backend_name in backend_names module_name 'matplotlib backends backend_%s' % backend_name lower stdout exec_statement import_statement % module_name if not stdout module_names append module_name logger info 'Matplotlibbackend"%s" added' % backend_name else logger info 'Matplotlibbackend"%s" ignored\n%s' % backend_name stdout return module_names
def primitive_delete course num tabs course tabsvalidate_args num tabs[num] get 'type' '' del tabs[num]modulestore update_item course ModuleStoreEnum UserID primitive_command
@login_requireddef course_notifications_handler request course_key_string None action_state_id None if not course_key_string or not action_state_id return HttpResponseBadRequest response_format request GET get 'format' or request POST get 'format' or 'html' course_key CourseKey from_string course_key_string if response_format 'json' or 'application/json' in request META get 'HTTP_ACCEPT' 'application/json' if not has_studio_write_access request user course_key raise PermissionDenied if request method 'GET' return _course_notifications_json_get action_state_id elif request method 'DELETE' return _dismiss_notification request action_state_id elif request method 'PUT' raise NotImplementedError elif request method 'POST' raise NotImplementedError else return HttpResponseBadRequest else return HttpResponseNotFound
def check_c_int context builder n _maxint 2 ** 31 - 1 def impl n if n > _maxint raise OverflowError 'arraysizetoolargetofitinCint' context compile_internal builder impl signature types none types intp n
def create_theme name **extra_kwargs kwargs {'status' STATUS_PUBLIC 'name' name 'slug' slugify name 'bayesian_rating' random uniform 1 5 'average_daily_users' random randint 200 2000 'weekly_downloads' random randint 200 2000 'created' datetime now 'last_updated' datetime now }kwargs update extra_kwargs theme Addon objects create type ADDON_EXTENSION **kwargs generate_version addon theme theme update_version theme status STATUS_PUBLICtheme type ADDON_PERSONAPersona objects create addon theme popularity theme weekly_downloads persona_id 0 theme save return theme
def runtests *test_labels from django_nose import NoseTestSuiteRunnerrunner NoseTestSuiteRunner verbosity 1 interactive True failures runner run_tests test_labels sys exit failures
def hpsModelSynth hfreq hmag hphase stocEnv N H fs yh SM sineModelSynth hfreq hmag hphase N H fs yst STM stochasticModelSynth stocEnv H H * 2 y yh[ min yh size yst size ] + yst[ min yh size yst size ] return y yh yst
def dump_object object for item in object __dict__ iteritems print itemtry key value itemdump_object value except Exception continue
def is_installable_dir path if not os path isdir path return Falsesetup_py os path join path 'setup py' if os path isfile setup_py return Truereturn False
def test_refresh_called_twice refresher callbacks Mock pgexecute Mock special Mock def dummy_bg_refresh *args time sleep 3 refresher _bg_refresh dummy_bg_refreshactual1 refresher refresh pgexecute special callbacks time sleep 1 assert len actual1 1 assert len actual1[0] 4 assert actual1[0][3] 'Auto-completionrefreshstartedinthebackground ' actual2 refresher refresh pgexecute special callbacks time sleep 1 assert len actual2 1 assert len actual2[0] 4 assert actual2[0][3] 'Auto-completionrefreshrestarted '
def get_ignored_traceback tb tb_list []while tb tb_list append tb tb tb tb_nextignored_tracebacks []for tb in reversed tb_list if '__unittest' in tb tb_frame f_globals ignored_tracebacks append tb else breakif ignored_tracebacks return ignored_tracebacks[ -1 ]
def read_global name cache_path _global_cache_path return _read cache_path name
def read_global name cache_path _global_cache_path return _read cache_path name
def generate_random_edx_username allowable_chars string ascii_letters + string digits username ''for _index in range 30 username username + random SystemRandom choice allowable_chars return username
def create_random_person locale None minimum_name_comp_len 0 fake get_faker [u'person' u'internet' u'address'] locale locale while True first_name fake first_name last_name fake last_name name u'%s%s' % first_name last_name if len first_name > minimum_name_comp_len and len last_name > minimum_name_comp_len breakemail get_random_email fake phone fake phone_number prefix u''suffix u''address create_random_address fake fake name name prefix prefix suffix suffix email email phone phone return PersonContact objects create email email phone phone name name first_name first_name last_name last_name prefix prefix suffix suffix default_shipping_address address default_billing_address address gender random choice u'mfuo' language fake language
def consume iterator n deque itertools islice iterator n maxlen 0
def register_arguments func args None if args is None args func args argsif func args vararg func set_local func args vararg func args if func args kwarg func set_local func args kwarg func args for arg in args if isinstance arg Name func set_local arg name arg else register_arguments func arg elts
def register_arguments func args None if args is None args func args argsif func args vararg func set_local func args vararg func args if func args kwarg func set_local func args kwarg func args for arg in args if isinstance arg Name func set_local arg name arg else register_arguments func arg elts
def run_mcr_job job log 'RunningacompiledMatlabjob \n' os chdir job expt_dir if os environ has_key 'MATLAB' mcr_loc os environ['MATLAB']else mcr_loc MCR_LOCATIONcmd ' /run_%s sh%s%s' % job name mcr_loc job_file_for job log "Executingcommand'%s'\n" % cmd sh cmd
def test_output_quiet with AssertNotPrints '2' ip run_cell '1+1 ' store_history True with AssertNotPrints '2' ip run_cell '1+1 #commentwithasemicolon' store_history True with AssertNotPrints '2' ip run_cell '1+1 \n#commented_out_function ' store_history True
def _get_credentials ret {'user' False 'passwd' False}for item in ret for struct in [__opts__ __grains__ __pillar__] for config_key in __valid_configs[item] value salt utils traverse_dict_and_list struct config_key None if value ret[item] valuebreakreturn ret['user'] ret['passwd']
def log_list_volumes function counter itertools count 1 def _count_calls *args **kwargs '\nRungivenfunctionwithcount \n'CALL_LIST_VOLUMES function function __name__ count next counter write return function *args **kwargs return _count_calls
def application_start context view context app set_view view view show view raise_ runtask qtutils RunTask parent view init_update_task view runtask context model fsmonitor current start msg_timer QtCore QTimer msg_timer setSingleShot True msg_timer timeout connect _send_msg msg_timer start 0 result context app exec_ fsmonitor current stop QtCore QThreadPool globalInstance waitForDone return result
def regex_tuple_from_key_alias obj return re compile ordered_permutation_regex '' join [obj key] + obj aliases all _RE_FLAGS obj obj key
def check_lapack_return context builder res with builder if_then cgutils is_not_null builder res likely False pyapi context get_python_api builder pyapi gil_ensure pyapi fatal_error 'LAPACKwrapperreturnedwithanerror'
def intranges_from_list list_ sorted_list sorted list_ ranges []last_write -1 for i in range len sorted_list if i + 1 < len sorted_list if sorted_list[i] sorted_list[ i + 1 ] - 1 continuecurrent_range sorted_list[ last_write + 1 i + 1 ]range_tuple current_range[0] current_range[ -1 ] + 1 ranges append range_tuple last_write ireturn tuple ranges
def ScaleData data old_min old_max new_min new_max def ScalePoint x if x is None return Nonereturn scale * x + translate if old_min old_max scale 1else scale new_max - new_min / float old_max - old_min translate new_min - scale * old_min return map ScalePoint data
def tamper payload **kwargs retVal payloadif payload retVal ''i 0while i < len payload if payload[i] '%' and i < len payload - 2 and payload[ i + 1 i + 2 ] in string hexdigits and payload[ i + 2 i + 3 ] in string hexdigits retVal + '%%25%s' % payload[ i + 1 i + 3 ] i + 3else retVal + '%%25% 2X' % ord payload[i] i + 1return retVal
def tamper payload **kwargs retVal payloadif payload retVal ''i 0while i < len payload if payload[i] '%' and i < len payload - 2 and payload[ i + 1 i + 2 ] in string hexdigits and payload[ i + 2 i + 3 ] in string hexdigits retVal + '%%25%s' % payload[ i + 1 i + 3 ] i + 3else retVal + '%%25% 2X' % ord payload[i] i + 1return retVal
def inverse_hankel_transform F k r nu **hints return InverseHankelTransform F k r nu doit **hints
@Profiler profiledef test_core n for i in range n with engine begin as conn conn execute Customer __table__ insert dict name 'customername%d' % i description 'customerdescription%d' % i
def data_upload_progress req if 'id' in req GET upload_id str req GET['id'] if upload_id in req session upload_obj get_object_or_404 Upload import_id upload_id user req user upload_session upload_obj get_session else upload_session req session[upload_id]import_session upload_session import_sessionprogress import_session tasks[0] get_progress return json_response progress else return json_response {'state' 'NONE'}
def data_upload_progress req if 'id' in req GET upload_id str req GET['id'] if upload_id in req session upload_obj get_object_or_404 Upload import_id upload_id user req user upload_session upload_obj get_session else upload_session req session[upload_id]import_session upload_session import_sessionprogress import_session tasks[0] get_progress return json_response progress else return json_response {'state' 'NONE'}
def data_upload_progress req if 'id' in req GET upload_id str req GET['id'] if upload_id in req session upload_obj get_object_or_404 Upload import_id upload_id user req user upload_session upload_obj get_session else upload_session req session[upload_id]import_session upload_session import_sessionprogress import_session tasks[0] get_progress return json_response progress else return json_response {'state' 'NONE'}
def get_all_exploration_summaries return _get_exploration_summaries_from_models exp_models ExpSummaryModel get_all
def CreateGRRTempFileVFS directory None filename None lifetime 0 mode 'w+b' suffix '' fd CreateGRRTempFile directory directory filename filename lifetime lifetime mode mode suffix suffix pathspec rdf_paths PathSpec path fd name pathtype rdf_paths PathSpec PathType TMPFILE return fd pathspec
def find_in_path name path for dir in path split os pathsep binpath pjoin dir name if os path exists binpath return os path abspath binpath return None
def color columns None palette None bin False **kwargs if palette is not None kwargs['palette'] palettekwargs['columns'] columnskwargs['bin'] binreturn ColorAttr **kwargs
def _get_transformed_points_tps new_points source_points coefficients num_points batch_size to_transform new_points dimshuffle 0 'x' 1 2 stacked_transform T tile to_transform 1 num_points 1 1 r_2 T sum stacked_transform - source_points dimshuffle 'x' 1 0 'x' ** 2 axis 2 log_r_2 T log r_2 distances T switch T isnan log_r_2 r_2 * log_r_2 0 0 upper_array T concatenate [T ones batch_size 1 new_points shape[2] dtype theano config floatX new_points] axis 1 right_mat T concatenate [upper_array distances] axis 1 new_value T batched_dot coefficients right_mat return new_value
def _get_transformed_points_tps new_points source_points coefficients num_points batch_size to_transform new_points dimshuffle 0 'x' 1 2 stacked_transform T tile to_transform 1 num_points 1 1 r_2 T sum stacked_transform - source_points dimshuffle 'x' 1 0 'x' ** 2 axis 2 log_r_2 T log r_2 distances T switch T isnan log_r_2 r_2 * log_r_2 0 0 upper_array T concatenate [T ones batch_size 1 new_points shape[2] dtype theano config floatX new_points] axis 1 right_mat T concatenate [upper_array distances] axis 1 new_value T batched_dot coefficients right_mat return new_value
def _get_transformed_points_tps new_points source_points coefficients num_points batch_size to_transform new_points dimshuffle 0 'x' 1 2 stacked_transform T tile to_transform 1 num_points 1 1 r_2 T sum stacked_transform - source_points dimshuffle 'x' 1 0 'x' ** 2 axis 2 log_r_2 T log r_2 distances T switch T isnan log_r_2 r_2 * log_r_2 0 0 upper_array T concatenate [T ones batch_size 1 new_points shape[2] dtype theano config floatX new_points] axis 1 right_mat T concatenate [upper_array distances] axis 1 new_value T batched_dot coefficients right_mat return new_value
def get_ids lines field bad_ids None debug False result defaultdict list for line in lines if line startswith '>' fields map strip line[1 ] split label fields[0]if not '_' in label continue lib id_ label rsplit '_' 1 if bad_ids and label in bad_ids if debug print 'Excludedbadid %s' % label else result[lib] append fields[field] return result
def serializeTransform transformObj return '' join [ command + ' ' + '' join [scourUnitlessLength number for number in numbers] + ' ' for command numbers in transformObj]
def create_dark_lang_config apps schema_editor DarkLangConfig apps get_model u'dark_lang' u'DarkLangConfig' objects DarkLangConfig objectsif not objects exists objects create enabled True
def base64_encodefile fname encoded_f StringIO StringIO with open fname 'rb' as f base64 encode f encoded_f encoded_f seek 0 return encoded_f read
def getCraftValue preferenceName preferences for preference in preferences if preference name startswith preferenceName return preference valuereturn None
def get_zone_by_name conn module zone_name want_private zone_id want_vpc_id for zone in conn get_zones private_zone module boolean zone config get 'PrivateZone' False if private_zone want_private and zone name zone_name and zone_id None or zone id replace '/hostedzone/' '' zone_id if want_vpc_id zone_details conn get_hosted_zone zone id ['GetHostedZoneResponse']if isinstance zone_details['VPCs'] dict if zone_details['VPCs']['VPC']['VPCId'] want_vpc_id return zoneelif want_vpc_id in [v['VPCId'] for v in zone_details['VPCs']] return zoneelse return zonereturn None
def extract_rfc2822_addresses text if not text return []candidates address_pattern findall ustr text encode 'utf-8' return filter try_coerce_ascii candidates
def widget_settings_dir return os path join data_dir 'widgets'
def widget_settings_dir return os path join data_dir 'widgets'
def widget_settings_dir return os path join data_dir 'widgets'
def respond request code redirect request GET get 'next' request POST get 'next' if redirect return HttpResponseRedirect redirect return type 'Response%d' % code HttpResponse {'status_code' code}
def getMinimumByPaths elementNode return euclidean getMinimumByVector3Paths elementNode xmlObject getTransformedPaths
def remove_config chassis_id None community None contact None location None test False commit True dic {'template_name' 'delete_snmp_config' 'test' test 'commit' commit}if chassis_id dic['chassis_id'] chassis_idif community dic['community'] communityif contact dic['contact'] contactif location dic['location'] locationreturn __salt__['net load_template'] **dic
def test_timeout_set_interval qtbot t usertypes Timer with qtbot waitSignal t timeout timeout 3000 t setInterval 200 t start
def rs_compose_add p1 p2 R p1 ringx R gens[0]prec p1 degree * p2 degree + 1 np1 rs_newton p1 x prec np1e rs_hadamard_exp np1 np2 rs_newton p2 x prec np2e rs_hadamard_exp np2 np3e rs_mul np1e np2e x prec np3 rs_hadamard_exp np3e True np3a np3[ 0 ] - np3 / x q rs_integrate np3a x q rs_exp q x prec q _invert_monoms q q q primitive [1]dp p1 degree * p2 degree - q degree if dp q q * x ** dp return q
@utils arg 'class_name' metavar '<class>' help 'Nameofquotaclasstosetthequotasfor ' @utils arg '--monitors' metavar '<monitors>' type int default None help 'Newvalueforthe"monitors"quota ' @utils arg '--snapshots' metavar '<snapshots>' type int default None help 'Newvalueforthe"snapshots"quota ' @utils arg '--gigabytes' metavar '<gigabytes>' type int default None help 'Newvalueforthe"gigabytes"quota ' @utils service_type 'monitor' def do_quota_class_update cs args _quota_update cs quota_classes args class_name args
def _wait timeout cloud server action for count in shade _utils _iterate_timeout timeout 'Timeoutwaitingforservertocomplete%s' % action try server cloud get_server server id except Exception continueif server status _action_map[action] returnif server status 'ERROR' module fail_json msg 'ServerreachedERRORstatewhileattemptingto%s' % action
def getSynset pos offset return _dictionaryFor pos getSynset offset
def input_dir dirname dirname dirname rstrip '/' if excluded dirname returnfor root dirs files in os walk dirname if options verbose message 'directory' + root options counters['directories'] options counters get 'directories' 0 + 1 dirs sort for subdir in dirs if excluded subdir dirs remove subdir files sort for filename in files input_file os path join root filename
def my_callback ax ch_idx ax plot freqs psds[ch_idx] color 'red' ax set_xlabel 'Frequency Hz 'ax set_ylabel 'Power dB '
def my_callback ax ch_idx ax plot freqs psds[ch_idx] color 'red' ax set_xlabel 'Frequency Hz 'ax set_ylabel 'Power dB '
def defined_at if DEBUG frame inspect currentframe while frame try if frame f_globals['__package__'] __package__ breakexcept KeyError breakframe frame f_backret Frame frame f_lineno frame f_globals get '__package__' frame f_globals get '__name__' frame f_code co_filename del framereturn ret
def _test if sys argv[1 ] if sys argv[2 ] sys stderr write 'usage pythondis py[- file]\n' sys exit 2 fn sys argv[1]if not fn or fn '-' fn Noneelse fn Noneif fn is None f sys stdinelse f open fn source f read if fn is not None f close else fn '<stdin>'code compile source fn 'exec' dis code
def hist_bins bins vals hist zeros len bins j 0for i in vals while bins[j] < i j + 1hist[j] + 1return asarray bins hist
def addpackage sitedir name known_paths if known_paths is None _init_pathinfo reset 1else reset 0fullname os path join sitedir name try f open fullname 'rU' except IOError returntry for line in f if line startswith '#' continueif line startswith 'import' exec linecontinueline line rstrip dir dircase makepath sitedir line if not dircase in known_paths and os path exists dir sys path append dir known_paths add dircase finally f close if reset known_paths Nonereturn known_paths
def addpackage sitedir name known_paths if known_paths is None _init_pathinfo reset 1else reset 0fullname os path join sitedir name try f open fullname 'rU' except IOError returntry for line in f if line startswith '#' continueif line startswith 'import' exec linecontinueline line rstrip dir dircase makepath sitedir line if not dircase in known_paths and os path exists dir sys path append dir known_paths add dircase finally f close if reset known_paths Nonereturn known_paths
def addpackage sitedir name known_paths if known_paths is None _init_pathinfo reset 1else reset 0fullname os path join sitedir name try f open fullname 'rU' except IOError returntry for line in f if line startswith '#' continueif line startswith 'import' exec linecontinueline line rstrip dir dircase makepath sitedir line if not dircase in known_paths and os path exists dir sys path append dir known_paths add dircase finally f close if reset known_paths Nonereturn known_paths
def create_folder_structure depth 2 sibling 2 parent None if depth > 0 and sibling > 0 depth_range list range 1 depth + 1 depth_range reverse for d in depth_range for s in range 1 sibling + 1 name 'folder %s--%s' % str d str s folder Folder name name parent parent folder save create_folder_structure depth d - 1 sibling sibling parent folder
def gateway_in_subnet_exists client subnet_id allocation_id None check_mode False allocation_id_exists Falsegateways []states ['available' 'pending'] gws_retrieved _ gws get_nat_gateways client subnet_id states states check_mode check_mode if not gws_retrieved return gateways allocation_id_exists for gw in gws for address in gw['nat_gateway_addresses'] if allocation_id if address get 'allocation_id' allocation_id allocation_id_exists Truegateways append gw else gateways append gw return gateways allocation_id_exists
def app environ start_response data 'Hello World \n'status '200OK'response_headers [ 'Content-type' 'text/plain' 'Content-Length' str len data ]sys stdout write 'requestreceived pausing10seconds' sys stdout flush time sleep 10 start_response status response_headers return iter [data]
def expandvars p l 512b swi block l return b tostring 0 swi swi 'OS_GSTrans' 'sbi i' p b l
def datetime_to_long dt return timedelta_to_usecs dt - dt min
def ManifestFromXML xmlstr manifest Manifest manifest parse_string xmlstr return manifest
def set_name_by_naming_series doc if not doc naming_series doc naming_series get_default_naming_series doc doctype if not doc naming_series frappe throw frappe _ u'NamingSeriesmandatory' doc name make_autoname doc naming_series + u' #####' u'' doc
def all_valid_collectors now int time time for col in all_collectors if not col dead or now - col lastspawn > 3600 yield col
def all_valid_collectors now int time time for col in all_collectors if not col dead or now - col lastspawn > 3600 yield col
def get_default_compiler is_post compilers post_pages filtered [entry for entry in post_pages if entry[3] is_post ]for entry in filtered extension os path splitext entry[0] [ -1 ]for compiler extensions in compilers items if extension in extensions return compilerreturn u'rest'
def canonicalize_stderr stderr stderr stderr strip split '\n' [ -1 ]substitutions [ "NameError globalname'" "NameError name'" "AttributeError ' \\w+ 'objectattribute' \\w+ 'isread-only" 'AttributeError \\2' 'TypeError object __new__\\ \\ takesnoparameters' 'TypeError object takesnoparameters' 'IndexError listassignmentindexoutofrange' 'IndexError listindexoutofrange' "unqualifiedexecisnotallowedinfunction' \\w+ 'it * " "unqualifiedexecisnotallowedinfunction'\\1'becauseit\\2" ]for pattern subst_with in substitutions stderr re sub pattern subst_with stderr return stderr
def canonicalize_stderr stderr stderr stderr strip split '\n' [ -1 ]substitutions [ "NameError globalname'" "NameError name'" "AttributeError ' \\w+ 'objectattribute' \\w+ 'isread-only" 'AttributeError \\2' 'TypeError object __new__\\ \\ takesnoparameters' 'TypeError object takesnoparameters' 'IndexError listassignmentindexoutofrange' 'IndexError listindexoutofrange' "unqualifiedexecisnotallowedinfunction' \\w+ 'it * " "unqualifiedexecisnotallowedinfunction'\\1'becauseit\\2" ]for pattern subst_with in substitutions stderr re sub pattern subst_with stderr return stderr
def histograms img nbins return _histograms histograms img nbins
def request_lastfm method **kwargs kwargs['method'] methodkwargs setdefault 'api_key' API_KEY kwargs setdefault 'format' 'json' logger debug 'CallingLast FMmethod %s' method logger debug 'Last FMcallparameters %s' kwargs data request request_json ENTRY_POINT timeout TIMEOUT params kwargs lock lastfm_lock if not data logger error 'ErrorcallingLast FMmethod %s' method returnif 'error' in data logger error 'Last FMreturnedanerror %s' data['message'] returnreturn data
def make_ifilesystemsnapshots_tests fixture class IFilesystemSnapshotsTests AsyncTestCase '\nTestsfor class `IFilesystemSnapshots`implementors \n\nThesearefunctionaltestsifrunagainstrealfilesystems \n'def test_interface self '\nThetestedobjectprovides class `IFilesystemSnapshots` \n'fs_snapshots fixture self self assertTrue verifyObject IFilesystemSnapshots fs_snapshots def test_created self '\nSnapshotscreatedwith``create ``arelistedinthatorderin\n``list `` \n'fs_snapshots fixture self d fs_snapshots create 'first' d addCallback lambda _ fs_snapshots create 'another' d addCallback lambda _ fs_snapshots list d addCallback self assertEqual ['first' 'another'] return dreturn IFilesystemSnapshotsTests
def setup hass config from pyzabbix import ZabbixAPI ZabbixAPIExceptionconf config[DOMAIN]if conf[CONF_SSL] schema 'https'else schema 'http'url urljoin '{} //{}' format schema conf[CONF_HOST] conf[CONF_PATH] username conf get CONF_USERNAME None password conf get CONF_PASSWORD None zapi ZabbixAPI url try zapi login username password _LOGGER info 'ConnectedtoZabbixAPIVersion%s' zapi api_version except ZabbixAPIException _LOGGER error 'UnabletologintotheZabbixAPI' return Falsehass data[DOMAIN] zapireturn True
def swirl image center None strength 1 radius 100 rotation 0 output_shape None order 1 mode None cval 0 clip True preserve_range False if mode is None warn 'Thedefaultof`mode`in`skimage transform swirl`willchangeto`reflect`inversion0 15 ' mode 'constant'if center is None center np array image shape [ 2] / 2 warp_args {'center' center 'rotation' rotation 'strength' strength 'radius' radius}return warp image _swirl_mapping map_args warp_args output_shape output_shape order order mode mode cval cval clip clip preserve_range preserve_range
def _get_cached_requirements requirements saltenv req_file senv salt utils url parse requirements if senv saltenv senvif req_file not in __salt__['cp list_master'] saltenv return Falsecached_requirements __salt__['cp is_cached'] requirements saltenv if not cached_requirements cached_requirements __salt__['cp cache_file'] requirements saltenv if __salt__['cp hash_file'] requirements saltenv __salt__['cp hash_file'] cached_requirements saltenv cached_requirements __salt__['cp cache_file'] requirements saltenv return cached_requirements
def _get_oauth2_client_id_and_secret settings_instance secret_json getattr settings_instance 'GOOGLE_OAUTH2_CLIENT_SECRETS_JSON' None if secret_json is not None return _load_client_secrets secret_json else client_id getattr settings_instance 'GOOGLE_OAUTH2_CLIENT_ID' None client_secret getattr settings_instance 'GOOGLE_OAUTH2_CLIENT_SECRET' None if client_id is not None and client_secret is not None return client_id client_secret else raise exceptions ImproperlyConfigured 'MustspecifyeitherGOOGLE_OAUTH2_CLIENT_SECRETS_JSON orbothGOOGLE_OAUTH2_CLIENT_IDandGOOGLE_OAUTH2_CLIENT_SECRETinsettings py'
def withparent meth def wrapped self *args **kwargs res meth self *args **kwargs if getattr self 'parent' None is not None getattr self parent meth __name__ *args **kwargs return reswrapped __name__ meth __name__return wrapped
def get_first_id lines result set for line in lines if line startswith '>' result add line[1 ] split [0] return result
@CELERY_APP taskdef send_ccx_course_published course_key course_key CourseLocator from_string course_key for ccx in CustomCourseForEdX objects filter course_id course_key try ccx_key CCXLocator from_course_locator course_key unicode ccx id except InvalidKeyError log info 'Attempttopublishcoursewithdeprecatedid Course %s CCX %s' course_key ccx id continueresponses SignalHandler course_published send sender ccx course_key ccx_key for rec response in responses log info 'Signalfiredwhencourseispublished Receiver %s Response %s' rec response
def hardlink_if_possible fullname node meta global targets_writtentarget meta hardlink_targettarget_versions targets_written get target if target_versions for target_path target_vfs_path target_meta in target_versions if hardlink_compatible target_path target_vfs_path target_meta node meta try os link target_path fullname return Trueexcept OSError as e if e errno errno EXDEV raiseelse target_versions []targets_written[target] target_versionsfull_vfs_path node fullname target_versions append fullname full_vfs_path meta return False
def eigenhproblem_standard desc dim dtype overwrite lower turbo eigenvalues if iscomplex empty 1 dtype dtype a _complex_symrand dim dtype else a symrand dim astype dtype if overwrite a_c a copy else a_c a w z eigh a overwrite_a overwrite lower lower eigvals eigenvalues assert_dtype_equal z dtype dtype w w astype dtype diag_ diag dot z T conj dot a_c z realassert_array_almost_equal diag_ w DIGITS[dtype]
def iterator_impl iterable_type iterator_type def wrapper cls iternext cls iternext@iternext_impldef iternext_wrapper context builder sig args result value argsiterobj cls context builder value return iternext iterobj context builder result lower_builtin 'iternext' iterator_type iternext_wrapper return clsreturn wrapper
def started if not is_started start 'shorewall'
def bezier_subdivide cp t c00 c01 c02 c03 cpc10 c00 * 1 - t + c01 * t c11 c01 * 1 - t + c02 * t c12 c02 * 1 - t + c03 * t c20 c10 * 1 - t + c11 * t c21 c11 * 1 - t + c12 * t c30 c20 * 1 - t + c21 * t first [c00 c10 c20 c30]second [c30 c21 c12 c03]return first second
def calculate_compile_sources targets is_thrift_target basedirs set sources set def collect_sources target basedirs add target target_base sources update target sources_relative_to_buildroot for target in targets target walk collect_sources predicate is_thrift_target return basedirs sources
def calculate_compile_sources targets is_thrift_target basedirs set sources set def collect_sources target basedirs add target target_base sources update target sources_relative_to_buildroot for target in targets target walk collect_sources predicate is_thrift_target return basedirs sources
def calculate_compile_sources targets is_thrift_target basedirs set sources set def collect_sources target basedirs add target target_base sources update target sources_relative_to_buildroot for target in targets target walk collect_sources predicate is_thrift_target return basedirs sources
def pinv_extended X rcond 1e-15 X np asarray X X X conjugate u s vt np linalg svd X 0 s_orig np copy s m u shape[0]n vt shape[1]cutoff rcond * np maximum reduce s for i in range min n m if s[i] > cutoff s[i] 1 0 / s[i] else s[i] 0 0res np dot np transpose vt np multiply s[ np core newaxis] np transpose u return res s_orig
def pinv_extended X rcond 1e-15 X np asarray X X X conjugate u s vt np linalg svd X 0 s_orig np copy s m u shape[0]n vt shape[1]cutoff rcond * np maximum reduce s for i in range min n m if s[i] > cutoff s[i] 1 0 / s[i] else s[i] 0 0res np dot np transpose vt np multiply s[ np core newaxis] np transpose u return res s_orig
def init_model pass
def adjust_split_point split_point log sp split_pointwhile True parent sp getparent if parent is None or barename parent tag in {u'body' u'html'} or parent text and parent text strip or parent index sp > 0 breaksp parentif sp is not split_point log debug u'Adjustedsplitpointtoancestor' return sp
def adjust_split_point split_point log sp split_pointwhile True parent sp getparent if parent is None or barename parent tag in {u'body' u'html'} or parent text and parent text strip or parent index sp > 0 breaksp parentif sp is not split_point log debug u'Adjustedsplitpointtoancestor' return sp
def contains_point poly point n len poly c Falsei 0j n - 1 while i < n if poly[i][0] > point[0] poly[j][0] > point[0] and point[1] < poly[j][1] - poly[i][1] * point[0] - poly[i][0] / poly[j][0] - poly[i][0] + poly[i][1] c not c j ii + 1return c
def contains_point poly point n len poly c Falsei 0j n - 1 while i < n if poly[i][0] > point[0] poly[j][0] > point[0] and point[1] < poly[j][1] - poly[i][1] * point[0] - poly[i][0] / poly[j][0] - poly[i][0] + poly[i][1] c not c j ii + 1return c
@pytest fixturedef project_dir request rendered_dir 'fake-project-templated'def remove_generated_project if os path isdir rendered_dir utils rmtree rendered_dir request addfinalizer remove_generated_project return rendered_dir
def skipif skip_condition msg None def skip_decorator f import noseif callable skip_condition skip_val lambda skip_condition else skip_val lambda skip_condition def get_msg func msg None 'Skipmessagewithinformationaboutfunctionbeingskipped 'if msg is None out 'Testskippedduetotestcondition'else out '\n' + msg return 'Skippingtest %s%s' % func __name__ out def skipper_func *args **kwargs 'Skipperfornormaltestfunctions 'if skip_val raise nose SkipTest get_msg f msg else return f *args **kwargs def skipper_gen *args **kwargs 'Skipperfortestgenerators 'if skip_val raise nose SkipTest get_msg f msg else for x in f *args **kwargs yield x if nose util isgenerator f skipper skipper_genelse skipper skipper_funcreturn nose tools make_decorator f skipper return skip_decorator
def test_install_folder_using_dot_slash script script scratch_path join 'mock' mkdir pkg_path script scratch_path / 'mock' pkg_path join 'setup py' write mock100_setup_py result script pip 'install' ' /mock' egg_folder script site_packages / 'mock-100 1-py%s egg-info' % pyversion assert egg_folder in result files_created str result
def org_customise_org_resource_fields method s3db current s3dbtable s3db org_resourcetable location_id represent s3db gis_LocationRepresent sep ' ' list_fields ['organisation_id' 'location_id' 'parameter_id' 'value' 'comments']if method in 'datalist' 'profile' table modified_by represent s3_auth_user_represent_nametable modified_on represent lambda dt S3DateTime datetime_represent dt utc True list_fields + ['modified_by' 'modified_on' 'organisation_id$logo']s3db configure 'org_resource' list_fields list_fields
def getExtraFillLoops loops radius greaterThanRadius 1 4 * radius extraFillLoops []centers intercircle getCentersFromPoints intercircle getPointsFromLoops loops greaterThanRadius greaterThanRadius for center in centers inset intercircle getSimplifiedInsetFromClockwiseLoop center radius if intercircle isLargeSameDirection inset center radius if euclidean getIsInFilledRegion loops euclidean getLeftPoint inset inset reverse extraFillLoops append inset return extraFillLoops
def getExtraFillLoops loops radius greaterThanRadius 1 4 * radius extraFillLoops []centers intercircle getCentersFromPoints intercircle getPointsFromLoops loops greaterThanRadius greaterThanRadius for center in centers inset intercircle getSimplifiedInsetFromClockwiseLoop center radius if intercircle isLargeSameDirection inset center radius if euclidean getIsInFilledRegion loops euclidean getLeftPoint inset inset reverse extraFillLoops append inset return extraFillLoops
def is_enabled name current_schedule __salt__['schedule list'] show_all False return_yaml False if name in current_schedule return current_schedule[name]else return {}
def dedent text if not text return ''return textwrap dedent text
def dedent text if not text return ''return textwrap dedent text
def test_expected_operation_order expected instructions CHECK_FETCH instructions FETCH instructions CHECK_EXTRACT instructions EXTRACT instructions UNLINK instructions LINK instructions SYMLINK_CONDA instructions RM_EXTRACTED instructions RM_FETCHED assert expected instructions ACTION_CODES
def file_has_articles nzf has Falsedecodetable nzf decodetablefor articlenum in decodetable sleep 0 01 article decodetable[articlenum]data ArticleCache do load_article article if data has Truereturn has
def file_has_articles nzf has Falsedecodetable nzf decodetablefor articlenum in decodetable sleep 0 01 article decodetable[articlenum]data ArticleCache do load_article article if data has Truereturn has
def get_plug_point_class_instances global pp_class_instancesif pp_class_instances is None pp_class_instances []pp_classes []try slps resources global_env get_stack_lifecycle_plugins pp_classes [cls for name cls in slps]except Exception LOG exception _LE 'failedtogetlifecycleplugpointclasses' for ppc in pp_classes try pp_class_instances append ppc except Exception LOG exception _LE 'failedtoinstantiatestacklifecycleclass%s' ppc try pp_class_instances sorted pp_class_instances key lambda ppci ppci get_ordinal except Exception LOG exception _LE 'failedtosortlifecycleplugpointclasses' return pp_class_instances
def argmin x axis None return ArgMin axis x
def send_arrays socket arrays stop False if arrays arrays [numpy ascontiguousarray array for array in arrays]if stop headers {'stop' True}socket send_json headers else headers [header_data_from_array_1_0 array for array in arrays]socket send_json headers zmq SNDMORE for array in arrays[ -1 ] socket send array zmq SNDMORE socket send arrays[ -1 ]
def delete_service protocol None service_address None cmd '{0}-D{1}' format __detect_os _build_cmd protocol protocol service_address service_address out __salt__['cmd run_all'] cmd python_shell False if out['retcode'] ret out['stderr'] strip else ret Truereturn ret
def get_ec2_driver aws ec2 get_driver Provider EC2 aws['access_key'] aws['secret_access_token'] region aws['region'] return ec2
def hashPassword sid password if not isinstance sid unicode raise TypeError 'Thesessionidentifiermustbeaunicodeobject' if not isinstance password unicode raise TypeError 'Thepasswordmustbeaunicodeobject' input u'%s%s' % sid password return sha1 input encode 'utf-8' hexdigest
def get_vcs_root path previous_path pathwhile get_vcs_info path is None path abspardir path if path previous_path returnelse previous_path pathreturn osp abspath path
def get_permission_for_course_about return configuration_helpers get_value 'COURSE_ABOUT_VISIBILITY_PERMISSION' settings COURSE_ABOUT_VISIBILITY_PERMISSION
def _strip_rst_role type_str match REST_ROLE_PATTERN match type_str if match return match group 1 else return type_str
def get_redis_from_settings settings params defaults REDIS_PARAMS copy params update settings getdict 'REDIS_PARAMS' for source dest in SETTINGS_PARAMS_MAP items val settings get source if val params[dest] valif isinstance params get 'redis_cls' six string_types params['redis_cls'] load_object params['redis_cls'] return get_redis **params
def get_redis_from_settings settings params defaults REDIS_PARAMS copy params update settings getdict 'REDIS_PARAMS' for source dest in SETTINGS_PARAMS_MAP items val settings get source if val params[dest] valif isinstance params get 'redis_cls' six string_types params['redis_cls'] load_object params['redis_cls'] return get_redis **params
@taskdef prepare_apt sudo 'apt-get-qqupdate' sudo 'apt-get-yinstallgitpython3makepython-virtualenvzippython-devpython-mpmathpython3-setuptools' sudo 'easy_install3pip 7 1 2' sudo 'pip3installmpmath' sudo '/usr/bin/pipinstalltwine' sudo 'apt-get-yinstallgraphvizinkscapetexlivetexlive-xetextexlive-fonts-recommendedtexlive-latex-extralibrsvg2-bindocbook2x' sudo 'apt-get-yinstallpython-software-properties' sudo 'add-apt-repository-yppa fkrull/deadsnakes' sudo 'apt-get-yupdate' sudo 'apt-get-yinstallpython3 3'
def condition_yaw heading relative False if relative is_relative 1else is_relative 0msg vehicle message_factory command_long_encode 0 0 mavutil mavlink MAV_CMD_CONDITION_YAW 0 heading 0 1 is_relative 0 0 0 vehicle send_mavlink msg
def condition_yaw heading relative False if relative is_relative 1else is_relative 0msg vehicle message_factory command_long_encode 0 0 mavutil mavlink MAV_CMD_CONDITION_YAW 0 heading 0 1 is_relative 0 0 0 vehicle send_mavlink msg
def attribute attrib_type def make_decorator *args **kwargs return CustomAttributeDecorator attrib_type *args **kwargs return make_decorator
def strpdate string format whence datetime strptime string format return whence date
def get_inheritance obj_name obj_type 'file' dacl Dacl obj_name obj_type inherited win32security INHERITED_ACEfor i in range 0 dacl dacl GetAceCount ace dacl dacl GetAce i if ace[0][1] & inherited inherited return Truereturn False
def unique iterable seen set def observed el observed el in seen seen add el return observedreturn el for el in iterable if not observed el
def set_cover container cover_path report None options None report report or lambda x x if container book_type u'azw3' set_azw3_cover container cover_path report options options else set_epub_cover container cover_path report options options
def hash_file path digest None digest digest or hashlib sha1 with open path u'rb' as fd s fd read 8192 while s digest update s s fd read 8192 return digest hexdigest
def is_computable_float v tmp float v if not logical_or isnan tmp isinf tmp return tmpelse return False
def POST k n return _XXX k n _POST
def POST k n return _XXX k n _POST
def to_text keyring textring {}for keyname in keyring keytext keyname to_text secret base64 encodestring keyring[keyname] textring[keytext] secretreturn textring
def make_assert error def openssl_assert ok '\nIf*ok*isnotTrue retrievetheerrorfromOpenSSLandraiseit \n'if ok is not True exception_from_error_queue error return openssl_assert
def make_assert error def openssl_assert ok '\nIf*ok*isnotTrue retrievetheerrorfromOpenSSLandraiseit \n'if ok is not True exception_from_error_queue error return openssl_assert
def _finish_auth_url params return u'{}?{}' format reverse 'finish_auth' urllib urlencode params
def _finish_auth_url params return u'{}?{}' format reverse 'finish_auth' urllib urlencode params
def equateCylindricalDotZ point returnValue point z returnValue
def toPickle filename data f open filename 'w' cPickle dump data f f close
def _update branch repo_location gitpil _LegacyGitPillar branch repo_location __opts__ return gitpil update
def _update branch repo_location gitpil _LegacyGitPillar branch repo_location __opts__ return gitpil update
@pytest fixture autouse True def urlutils_config_stub config_stub monkeypatch config_stub data {'general' {'auto-search' True} 'searchengines' {'test' 'http //www qutebrowser org/?q {}' 'test-with-dash' 'http //www example org/?q {}' 'DEFAULT' 'http //www example com/?q {}'}}monkeypatch setattr 'qutebrowser utils urlutils config' config_stub return config_stub
def getCumulativeVector3Remove defaultVector3 elementNode prefix if prefix '' defaultVector3 x evaluate getEvaluatedFloat defaultVector3 x elementNode 'x' defaultVector3 y evaluate getEvaluatedFloat defaultVector3 y elementNode 'y' defaultVector3 z evaluate getEvaluatedFloat defaultVector3 z elementNode 'z' euclidean removeElementsFromDictionary elementNode attributes ['x' 'y' 'z'] prefix 'cartesian'defaultVector3 evaluate getVector3ByPrefix defaultVector3 elementNode prefix euclidean removePrefixFromDictionary elementNode attributes prefix return defaultVector3
def getCumulativeVector3Remove defaultVector3 elementNode prefix if prefix '' defaultVector3 x evaluate getEvaluatedFloat defaultVector3 x elementNode 'x' defaultVector3 y evaluate getEvaluatedFloat defaultVector3 y elementNode 'y' defaultVector3 z evaluate getEvaluatedFloat defaultVector3 z elementNode 'z' euclidean removeElementsFromDictionary elementNode attributes ['x' 'y' 'z'] prefix 'cartesian'defaultVector3 evaluate getVector3ByPrefix defaultVector3 elementNode prefix euclidean removePrefixFromDictionary elementNode attributes prefix return defaultVector3
def get_tool_dependency_definition_metadata_from_tool_shed app tool_shed_url name owner tool_shed_url common_util get_tool_shed_url_from_tool_shed_registry app tool_shed_url params dict name name owner owner pathspec ['repository' 'get_tool_dependency_definition_metadata']metadata util url_get tool_shed_url password_mgr app tool_shed_registry url_auth tool_shed_url pathspec pathspec params params return metadata
def get_scaling_policy_arn as_group scaling_policy_name region None key None keyid None profile None conn _get_conn region region key key keyid keyid profile profile policies conn get_all_policies as_group as_group for policy in policies if policy name scaling_policy_name return policy policy_arnlog error 'Couldnotconvert {0}' format as_group return None
def str_get_dummies arr sep ' ' arr arr fillna '' try arr sep + arr + sep except TypeError arr sep + arr astype str + sep tags set for ts in arr str split sep tags update ts tags sorted tags - set [''] dummies np empty len arr len tags dtype np int64 for i t in enumerate tags pat sep + t + sep dummies[ i] lib map_infer arr values lambda x pat in x return dummies tags
def TimeFromTicks ticks return Time *time gmtime ticks [3 6]
def empty_cell empty True def f print aif not empty a 1729return f __closure__[0]
def tokenize text lowercase False deacc False errors 'strict' to_lower False lower False lowercase lowercase or to_lower or lower text to_unicode text errors errors if lowercase text text lower if deacc text deaccent text for match in PAT_ALPHABETIC finditer text yield match group
def get_rect_xmin data return min data[0][0] data[1][0] data[2][0] data[3][0]
def escape string for ch in '{' '}' '[' ']' ' ' ' ' '_' ' ' ' ' '*' '+' '^' string string replace ch '\\' + ch return string
def paged_call function *args **kwargs marker_flag kwargs pop 'marker_flag' 'NextMarker' marker_arg kwargs pop 'marker_arg' 'Marker' while True ret function *args **kwargs marker ret get marker_flag yield ret if not marker breakkwargs[marker_arg] marker
@given "I'vestartedtheintro" def step_impl context go_to_manage_page context start_intro context modal context modal find_css_class_with_wait context MODAL_CLASS wait_time 30 context skip_button modal find_element_by_class_name SKIP_BUTTON_CLASS context next_button modal find_element_by_class_name NEXT_BUTTON_CLASS context back_button modal find_element_by_class_name BACK_BUTTON_CLASS
def get_subclasses c return c __subclasses__ + sum map get_subclasses c __subclasses__ []
def libvlc_vlm_get_media_instance_position p_instance psz_name i_instance f _Cfunctions get 'libvlc_vlm_get_media_instance_position' None or _Cfunction 'libvlc_vlm_get_media_instance_position' 1 1 1 None ctypes c_float Instance ctypes c_char_p ctypes c_int return f p_instance psz_name i_instance
@register simple_tag name 'page_title_breadcrumbs' takes_context True def page_title_breadcrumbs_tag context *crumbs return configuration_helpers page_title_breadcrumbs *crumbs
def add_status_query_managers sender **kwargs if not issubclass sender StatusModel returnfor value display in getattr sender u'STATUS' try sender _meta get_field value raise ImproperlyConfigured u"StatusModel Model'%s'hasafieldnamed'%s'whichconflictswithastatusofthesamename " % sender __name__ value except FieldDoesNotExist passsender add_to_class value QueryManager status value
def test_possible_string_format_functions t QTable [ [1 2] * u m ] t['col0'] info format '% 3f'assert t pformat ['col0' 'm' '-----' '1 000' '2 000'] t['col0'] info format 'hi{ 3f}'assert t pformat ['col0' 'm' '--------' 'hi1 000' 'hi2 000'] t['col0'] info format ' 4f'assert t pformat ['col0' 'm' '------' '1 0000' '2 0000']
def test_possible_string_format_functions t QTable [ [1 2] * u m ] t['col0'] info format '% 3f'assert t pformat ['col0' 'm' '-----' '1 000' '2 000'] t['col0'] info format 'hi{ 3f}'assert t pformat ['col0' 'm' '--------' 'hi1 000' 'hi2 000'] t['col0'] info format ' 4f'assert t pformat ['col0' 'm' '------' '1 0000' '2 0000']
def getAllPhotometers from import minolta prfrom import crsphotometers [pr PR650 pr PR655 minolta LS100]if hasattr crs 'ColorCAL' photometers append crs ColorCAL return photometers
def channel_session func @functools wraps func def inner message *args **kwargs if hasattr message 'channel_session' try return func message *args **kwargs finally if message channel_session modified message channel_session save if not message reply_channel raise ValueError 'Noreply_channelsenttoconsumer @channel_session' + 'canonlybeusedonmessagescontainingit ' session session_for_reply_channel message reply_channel name if not session exists session session_key try session save must_create True except CreateError raise ConsumeLater message channel_session sessiontry return func message *args **kwargs finally if session modified and not session is_empty session save return inner
def channel_session func @functools wraps func def inner message *args **kwargs if hasattr message 'channel_session' try return func message *args **kwargs finally if message channel_session modified message channel_session save if not message reply_channel raise ValueError 'Noreply_channelsenttoconsumer @channel_session' + 'canonlybeusedonmessagescontainingit ' session session_for_reply_channel message reply_channel name if not session exists session session_key try session save must_create True except CreateError raise ConsumeLater message channel_session sessiontry return func message *args **kwargs finally if session modified and not session is_empty session save return inner
def channel_session func @functools wraps func def inner message *args **kwargs if hasattr message 'channel_session' try return func message *args **kwargs finally if message channel_session modified message channel_session save if not message reply_channel raise ValueError 'Noreply_channelsenttoconsumer @channel_session' + 'canonlybeusedonmessagescontainingit ' session session_for_reply_channel message reply_channel name if not session exists session session_key try session save must_create True except CreateError raise ConsumeLater message channel_session sessiontry return func message *args **kwargs finally if session modified and not session is_empty session save return inner
def channel_session func @functools wraps func def inner message *args **kwargs if hasattr message 'channel_session' try return func message *args **kwargs finally if message channel_session modified message channel_session save if not message reply_channel raise ValueError 'Noreply_channelsenttoconsumer @channel_session' + 'canonlybeusedonmessagescontainingit ' session session_for_reply_channel message reply_channel name if not session exists session session_key try session save must_create True except CreateError raise ConsumeLater message channel_session sessiontry return func message *args **kwargs finally if session modified and not session is_empty session save return inner
def channel_session func @functools wraps func def inner message *args **kwargs if hasattr message 'channel_session' try return func message *args **kwargs finally if message channel_session modified message channel_session save if not message reply_channel raise ValueError 'Noreply_channelsenttoconsumer @channel_session' + 'canonlybeusedonmessagescontainingit ' session session_for_reply_channel message reply_channel name if not session exists session session_key try session save must_create True except CreateError raise ConsumeLater message channel_session sessiontry return func message *args **kwargs finally if session modified and not session is_empty session save return inner
def s_random value min_length max_length num_mutations 25 fuzzable True step None name None random primitives random_data value min_length max_length num_mutations fuzzable step name blocks CURRENT push random
def s_random value min_length max_length num_mutations 25 fuzzable True step None name None random primitives random_data value min_length max_length num_mutations fuzzable step name blocks CURRENT push random
def s_random value min_length max_length num_mutations 25 fuzzable True step None name None random primitives random_data value min_length max_length num_mutations fuzzable step name blocks CURRENT push random
def AddPluginsSubparsers classes sorted export_plugins plugin ExportPlugin classes itervalues key lambda cls cls name subparsers flags PARSER add_subparsers title 'Subcommands' for cls in classes if not cls name continuesubparser subparsers add_parser cls name help cls description plugin_obj cls plugin_obj ConfigureArgParser subparser subparser set_defaults func plugin_obj Run
def _install_custom_language lang_file resources config_home u'language' if not core exists lang_file returntry lang core read lang_file strip except returnif lang compat setenv u'LANGUAGE' lang
def test_get_init_4 nt assert_is_none mp get_init TMP_TEST_DIR
def test_get_init_4 nt assert_is_none mp get_init TMP_TEST_DIR
def test_get_init_4 nt assert_is_none mp get_init TMP_TEST_DIR
def time hour None min None sec None micro None offset None obj False now dt datetime utcnow if hour is None hour now hourif min is None min now minuteif sec is None sec now secondif micro is None micro now microsecondif offset is None offset tzutc elif not isinstance offset dt tzinfo offset tzoffset None offset value dt time hour min sec micro offset if obj return valuereturn format_time value
def time hour None min None sec None micro None offset None obj False now dt datetime utcnow if hour is None hour now hourif min is None min now minuteif sec is None sec now secondif micro is None micro now microsecondif offset is None offset tzutc elif not isinstance offset dt tzinfo offset tzoffset None offset value dt time hour min sec micro offset if obj return valuereturn format_time value
def dg_demo grammar DependencyGrammar fromstring u"\n'scratch'->'cats' 'walls'\n'walls'->'the'\n'cats'->'the'\n" print grammar
def get_instances_to_sync context updated_since None project_id None deleted True shuffle False uuids_only False filters {}if updated_since is not None filters['changes-since'] updated_sinceif project_id is not None filters['project_id'] project_idif not deleted filters['deleted'] Falseinstances db instance_get_all_by_filters context filters 'deleted' 'asc' if shuffle random shuffle instances for instance in instances if uuids_only yield instance['uuid'] else yield instance
def get_instances_to_sync context updated_since None project_id None deleted True shuffle False uuids_only False filters {}if updated_since is not None filters['changes-since'] updated_sinceif project_id is not None filters['project_id'] project_idif not deleted filters['deleted'] Falseinstances db instance_get_all_by_filters context filters 'deleted' 'asc' if shuffle random shuffle instances for instance in instances if uuids_only yield instance['uuid'] else yield instance
def _kill_listen_processes namespace force False pids find_listen_pids_namespace namespace pids_to_kill {utils find_fork_top_parent pid for pid in pids}kill_signal signal SIGTERMif force kill_signal signal SIGKILLchildren [utils find_child_pids pid True for pid in pids_to_kill]pids_to_kill update itertools chain from_iterable children for pid in pids_to_kill LOG warning _LW 'Killing % signal d [% pid s]% cmdline s' {'signal' kill_signal 'pid' pid 'cmdline' '' join utils get_cmdline_from_pid pid [ 80]} try utils kill_process pid kill_signal run_as_root True except Exception as ex LOG error _LE 'Anerroroccurredwhilekilling[% pid s] % msg s' {'pid' pid 'msg' ex} return len pids
def get_num_args f if inspect ismethod f return f __func__ __code__ co_argcount - 1 elif inspect isfunction f return f __code__ co_argcountelse raise TypeError u'fshouldbeafunctionoramethod'
def _execute q table context model context['model']session model Sessionreturn session execute q
def get_reader Reader None Inputter None Outputter None **kwargs if Reader is None Reader basic Basicreader core _get_reader Reader Inputter Inputter Outputter Outputter **kwargs return reader
def save_tweets filename tweets if len tweets 0 returntry archive open filename 'w' except IOError as e err 'Cannotsavetweets %s' % str e returnfor k in sorted tweets keys try archive write '%i%s\n' % k tweets[k] encode 'utf-8' except Exception as ex err 'archivingtweet%sfaileddueto%s' % k unicode ex archive close
def save_tweets filename tweets if len tweets 0 returntry archive open filename 'w' except IOError as e err 'Cannotsavetweets %s' % str e returnfor k in sorted tweets keys try archive write '%i%s\n' % k tweets[k] encode 'utf-8' except Exception as ex err 'archivingtweet%sfaileddueto%s' % k unicode ex archive close
def spidercls_for_request spider_loader request default_spidercls None log_none False log_multiple False snames spider_loader find_by_request request if len snames 1 return spider_loader load snames[0] if len snames > 1 and log_multiple logger error 'Morethanonespidercanhandle % request s-% snames s' {'request' request 'snames' ' ' join snames } if len snames 0 and log_none logger error 'Unabletofindspiderthathandles % request s' {'request' request} return default_spidercls
def full_restart name cmd ['service' name '--full-restart']return not __salt__['cmd retcode'] cmd python_shell False
def _filter_plans attr name plans return [plan for plan in plans if plan[attr] name ]
def set_fonts self font_set current deployment_settings get_pdf_export_font if font_set try font_name font_set[0]font_name_bold font_set[1]folder current request folderpdfmetrics registerFont TTFont font_name os path join folder 'static' 'fonts' '%s ttf' % font_name pdfmetrics registerFont TTFont font_name_bold os path join folder 'static' 'fonts' '%s ttf' % font_name_bold except current log error '%sFontnotfound PleaseinstallittoseethecorrectfontsinPDFexports' % font_set[0] self font_name 'Helvetica'self font_name_bold 'Helvetica-Bold'else self font_name font_nameself font_name_bold font_name_boldelse self font_name 'Helvetica'self font_name_bold 'Helvetica-Bold'
def set_fonts self font_set current deployment_settings get_pdf_export_font if font_set try font_name font_set[0]font_name_bold font_set[1]folder current request folderpdfmetrics registerFont TTFont font_name os path join folder 'static' 'fonts' '%s ttf' % font_name pdfmetrics registerFont TTFont font_name_bold os path join folder 'static' 'fonts' '%s ttf' % font_name_bold except current log error '%sFontnotfound PleaseinstallittoseethecorrectfontsinPDFexports' % font_set[0] self font_name 'Helvetica'self font_name_bold 'Helvetica-Bold'else self font_name font_nameself font_name_bold font_name_boldelse self font_name 'Helvetica'self font_name_bold 'Helvetica-Bold'
def set_fonts self font_set current deployment_settings get_pdf_export_font if font_set try font_name font_set[0]font_name_bold font_set[1]folder current request folderpdfmetrics registerFont TTFont font_name os path join folder 'static' 'fonts' '%s ttf' % font_name pdfmetrics registerFont TTFont font_name_bold os path join folder 'static' 'fonts' '%s ttf' % font_name_bold except current log error '%sFontnotfound PleaseinstallittoseethecorrectfontsinPDFexports' % font_set[0] self font_name 'Helvetica'self font_name_bold 'Helvetica-Bold'else self font_name font_nameself font_name_bold font_name_boldelse self font_name 'Helvetica'self font_name_bold 'Helvetica-Bold'
@LocalContextdef alphanumeric raw_bytes *a **kw return encode raw_bytes expr re_alphanumeric *a **kw
@LocalContextdef alphanumeric raw_bytes *a **kw return encode raw_bytes expr re_alphanumeric *a **kw
def create_download_tasks subtitles_by_video languages multi tasks []for video subtitles in subtitles_by_video iteritems if not subtitles continueif not multi task DownloadTask video list subtitles logger debug u'Createdtask%r' % task tasks append task continuefor _ by_language in groupby subtitles lambda s languages index s language task DownloadTask video list by_language logger debug u'Createdtask%r' % task tasks append task return tasks
def _escapify label text ''for c in label if c in _escaped text + '\\' + c elif ord c > 32 and ord c < 127 text + celse text + '\\%03d' % ord c return text
def _escapify label text ''for c in label if c in _escaped text + '\\' + c elif ord c > 32 and ord c < 127 text + celse text + '\\%03d' % ord c return text
def hue_permission_required action app def decorator view_func @wraps view_func def decorated request *args **kwargs if not request user has_hue_permission action app raise PopupException _ 'Permissiondenied % action s/% app s ' % {'action' action 'app' app} return view_func request *args **kwargs return decoratedreturn decorator
def hue_permission_required action app def decorator view_func @wraps view_func def decorated request *args **kwargs if not request user has_hue_permission action app raise PopupException _ 'Permissiondenied % action s/% app s ' % {'action' action 'app' app} return view_func request *args **kwargs return decoratedreturn decorator
def load_ipython_extension ip ip define_magic 'mprun' magic_mprun ip define_magic 'memit' magic_memit
def _checkPrefix ip prefixlen version bits _ipVersionToLen version if prefixlen < 0 or prefixlen > bits return Noneif ip 0 zbits bits + 1 else zbits _count0Bits ip if zbits < bits - prefixlen return 0else return 1
def _checkPrefix ip prefixlen version bits _ipVersionToLen version if prefixlen < 0 or prefixlen > bits return Noneif ip 0 zbits bits + 1 else zbits _count0Bits ip if zbits < bits - prefixlen return 0else return 1
def vbox margin spacing *items return box QtWidgets QVBoxLayout margin spacing *items
def create_attached_volume dataset_id mountpoint maximum_size None metadata pmap return AttachedVolume manifestation Manifestation dataset Dataset dataset_id dataset_id maximum_size maximum_size metadata metadata primary True mountpoint FilePath mountpoint
def complete_hybi00 headers challenge key1 headers['Sec-WebSocket-Key1']key2 headers['Sec-WebSocket-Key2']first int '' join i for i in key1 if i in digits // key1 count '' second int '' join i for i in key2 if i in digits // key2 count '' nonce pack '>II8s' first second challenge return md5 nonce digest
def LineEnding lines endings {CRLF 0 CR 0 LF 0}for line in lines if line endswith CRLF endings[CRLF] + 1elif line endswith CR endings[CR] + 1elif line endswith LF endings[LF] + 1return sorted endings key endings get reverse True or [LF] [0]
def update_connection_pool maxsize 1 get_pool connection_pool_kw update maxsize maxsize
def _writeGifToFile fp images durations loops frames 0previous Nonefor im in images if not previous palette getheader im [1]data getdata im imdes data data[0] data[1 ] header getheaderAnim im appext getAppExt loops graphext getGraphicsControlExt durations[0] fp write header fp write palette fp write appext fp write graphext fp write imdes for d in data fp write d else data getdata im imdes data data[0] data[1 ] graphext getGraphicsControlExt durations[frames] fp write graphext fp write imdes for d in data fp write d previous im copy frames frames + 1 fp write ' ' return frames
def _tile_perimeter_width coord projection perimeter _tile_perimeter coord projection False return perimeter[8][0] - perimeter[0][0]
@verbosedef read_raw_egi input_fname montage None eog None misc None include None exclude None preload False verbose None return RawEGI input_fname montage eog misc include exclude preload verbose
@task queue 'web' time_limit EMAIL_TIME_LIMIT def send_email_task recipient subject template template_html context None msg EmailMultiAlternatives subject get_template template render context settings DEFAULT_FROM_EMAIL [recipient] try msg attach_alternative get_template template_html render context 'text/html' except TemplateDoesNotExist passmsg send log info 'Sentemailtorecipient %s' recipient
@task queue 'web' time_limit EMAIL_TIME_LIMIT def send_email_task recipient subject template template_html context None msg EmailMultiAlternatives subject get_template template render context settings DEFAULT_FROM_EMAIL [recipient] try msg attach_alternative get_template template_html render context 'text/html' except TemplateDoesNotExist passmsg send log info 'Sentemailtorecipient %s' recipient
@task queue 'web' time_limit EMAIL_TIME_LIMIT def send_email_task recipient subject template template_html context None msg EmailMultiAlternatives subject get_template template render context settings DEFAULT_FROM_EMAIL [recipient] try msg attach_alternative get_template template_html render context 'text/html' except TemplateDoesNotExist passmsg send log info 'Sentemailtorecipient %s' recipient
def ipv4_to_int ip return struct unpack ' I' addrconv ipv4 text_to_bin ip [0]
def get_qiime_library_version qiime_dir get_qiime_project_dir qiime_version qiime_library_versionsha_cmd 'git--git-dir%s/ gitrev-parseHEAD' % qiime_dir sha_o sha_e sha_r qiime_system_call sha_cmd git_sha sha_o strip branch_cmd 'git--git-dir%s/ gitrev-parse--abbrev-refHEAD' % qiime_dir branch_o branch_e branch_r qiime_system_call branch_cmd git_branch branch_o strip if is_valid_git_refname git_branch and is_valid_git_sha1 git_sha return '%s %s@%s' % __version__ git_branch git_sha[0 7] else return '%s' % __version__
def create_xml_attributes module xml xml_attrs {}for attr val in xml attrib iteritems if attr not in module fields if attr 'parent_sequential_url' attr 'parent_url'xml_attrs[attr] valmodule xml_attributes xml_attrs
def create_xml_attributes module xml xml_attrs {}for attr val in xml attrib iteritems if attr not in module fields if attr 'parent_sequential_url' attr 'parent_url'xml_attrs[attr] valmodule xml_attributes xml_attrs
def create_xml_attributes module xml xml_attrs {}for attr val in xml attrib iteritems if attr not in module fields if attr 'parent_sequential_url' attr 'parent_url'xml_attrs[attr] valmodule xml_attributes xml_attrs
def create_xml_attributes module xml xml_attrs {}for attr val in xml attrib iteritems if attr not in module fields if attr 'parent_sequential_url' attr 'parent_url'xml_attrs[attr] valmodule xml_attributes xml_attrs
def first seq key lambda x bool x default None apply lambda x x return next apply x for x in seq if key x default if callable default else default
def first seq key lambda x bool x default None apply lambda x x return next apply x for x in seq if key x default if callable default else default
def get_roles_for_user user_db role_names UserRoleAssignment query user user_db name only 'role' scalar 'role' result Role query name__in role_names return result
def illumina_data_to_fastq record_data number_of_bases None seq_index 8qual_index 9pass_filter_index 10try pass_filter int record_data[pass_filter_index] except IndexError pass_filter 2if number_of_bases is None seq record_data[seq_index] replace ' ' 'N' qual record_data[qual_index]else seq record_data[seq_index][ number_of_bases] replace ' ' 'N' qual record_data[qual_index][ number_of_bases]header '%s_%s %s %s %s %s#%s/%s' % record_data[0] record_data[1] record_data[2] record_data[3] record_data[4] record_data[5] record_data[6] record_data[7] return '@%s\n%s\n+\n%s' % header seq qual pass_filter
def getCarving fileName '' carving SVGCarving carving parseSVG fileName archive getFileText fileName return carving
def setup_platform hass config add_devices discovery_info None from miflora import miflora_pollercache config get CONF_CACHE poller miflora_poller MiFloraPoller config get CONF_MAC cache_timeout cache force_update config get CONF_FORCE_UPDATE median config get CONF_MEDIAN poller ble_timeout config get CONF_TIMEOUT poller retries config get CONF_RETRIES devs []for parameter in config[CONF_MONITORED_CONDITIONS] name SENSOR_TYPES[parameter][0]unit SENSOR_TYPES[parameter][1]prefix config get CONF_NAME if len prefix > 0 name '{}{}' format prefix name devs append MiFloraSensor poller parameter name unit force_update median add_devices devs
def load_environment config_path environment env_plugins dir_find config_path 'env d' raise_if_missing False if env_plugins is not False _extra_config user_defined_config environment base_dir env_plugins logger debug 'Loadedenvironmentfrom{}' format config_path return environment
def namedChildren parent nodeName return [n for n in parent childNodes if getattr n 'tagName' '' nodeName ]
def namedChildren parent nodeName return [n for n in parent childNodes if getattr n 'tagName' '' nodeName ]
@pytest mark skipif 'notHAS_YAML' def test_regression_5604 t Table t meta {'foo' 5 * u km 'foo2' u s}t['bar'] [7] * u km out StringIO t write out format 'ascii ecsv' assert ' astropy units Unit' in out getvalue assert ' astropy units Quantity' in out getvalue
def init_list doctype doc frappe get_meta doctype make_boilerplate u'controller_list js' doc make_boilerplate u'controller_list html' doc
def test_compound_custom_inverse poly Polynomial1D 1 c0 1 c1 2 scale Scale 1 shift Shift 1 model1 poly scale model1 inverse polymodel2 shift model1 assert_allclose model2 inverse 1 poly shift inverse 1 with pytest raises NotImplementedError shift + model1 inversewith pytest raises NotImplementedError model1 & poly inverse
def test_compound_custom_inverse poly Polynomial1D 1 c0 1 c1 2 scale Scale 1 shift Shift 1 model1 poly scale model1 inverse polymodel2 shift model1 assert_allclose model2 inverse 1 poly shift inverse 1 with pytest raises NotImplementedError shift + model1 inversewith pytest raises NotImplementedError model1 & poly inverse
def test_compound_custom_inverse poly Polynomial1D 1 c0 1 c1 2 scale Scale 1 shift Shift 1 model1 poly scale model1 inverse polymodel2 shift model1 assert_allclose model2 inverse 1 poly shift inverse 1 with pytest raises NotImplementedError shift + model1 inversewith pytest raises NotImplementedError model1 & poly inverse
def list_cidr_ips_ipv6 cidr ips netaddr IPNetwork cidr return [str ip ipv6 for ip in list ips ]
def tag_string_convert key data errors context if isinstance data[key] basestring tags [tag strip for tag in data[key] split ' ' if tag strip ]else tags data[key]current_index max [int k[1] for k in data keys if len k 3 and k[0] 'tags' ] + [ -1 ] for num tag in zip count current_index + 1 tags data[ 'tags' num 'name' ] tagfor tag in tags tag_length_validator tag context tag_name_validator tag context
def getFilesForName name if not os path exists name if containsAny name '*?[]' files glob glob name list []for file in files list extend getFilesForName file return listname _get_modpkg_path name if not name return []if os path isdir name list []os path walk name _visit_pyfiles list return listelif os path exists name return [name]return []
@register inclusion_tag u'includes/pagination html' takes_context True def pagination_for context current_page page_var u'page' exclude_vars u'' querystring context[u'request'] GET copy exclude_vars [v for v in exclude_vars split u' ' if v] + [page_var] for exclude_var in exclude_vars if exclude_var in querystring del querystring[exclude_var]querystring querystring urlencode return {u'current_page' current_page u'querystring' querystring u'page_var' page_var}
@register inclusion_tag u'includes/pagination html' takes_context True def pagination_for context current_page page_var u'page' exclude_vars u'' querystring context[u'request'] GET copy exclude_vars [v for v in exclude_vars split u' ' if v] + [page_var] for exclude_var in exclude_vars if exclude_var in querystring del querystring[exclude_var]querystring querystring urlencode return {u'current_page' current_page u'querystring' querystring u'page_var' page_var}
def select_app_name name utils generate_app_name while App objects filter id name exists name utils generate_app_name return name
def groups username *args **kwargs return get_group_list username
def groups username *args **kwargs return get_group_list username
def setup_platform hass config add_devices discovery_info None insteonhub hass data['insteon_local']conf_lights config_from_file hass config path INSTEON_LOCAL_LIGHTS_CONF if len conf_lights for device_id in conf_lights setup_light device_id conf_lights[device_id] insteonhub hass add_devices linked insteonhub get_linked for device_id in linked if linked[device_id]['cat_type'] 'dimmer' and device_id not in conf_lights request_configuration device_id insteonhub linked[device_id]['model_name'] + '' + linked[device_id]['sku'] hass add_devices
def _finish_backtrace sequenceA sequenceB ali_seqA ali_seqB row col gap_char if row ali_seqA + sequenceA[ row - 1 -1 ]if col ali_seqB + sequenceB[ col - 1 -1 ]if row > col ali_seqB + gap_char * len ali_seqA - len ali_seqB elif col > row ali_seqA + gap_char * len ali_seqB - len ali_seqA return ali_seqA ali_seqB
def is_local_device my_ips my_port dev_ip dev_port candidate_ips []if not is_valid_ip dev_ip and is_valid_hostname dev_ip try addrinfo socket getaddrinfo dev_ip dev_port for addr in addrinfo family addr[0]dev_ip addr[4][0]if family socket AF_INET6 dev_ip expand_ipv6 dev_ip candidate_ips append dev_ip except socket gaierror return Falseelse if is_valid_ipv6 dev_ip dev_ip expand_ipv6 dev_ip candidate_ips [dev_ip]for dev_ip in candidate_ips if dev_ip in my_ips and my_port is None or dev_port my_port return Truereturn False
def is_local_device my_ips my_port dev_ip dev_port candidate_ips []if not is_valid_ip dev_ip and is_valid_hostname dev_ip try addrinfo socket getaddrinfo dev_ip dev_port for addr in addrinfo family addr[0]dev_ip addr[4][0]if family socket AF_INET6 dev_ip expand_ipv6 dev_ip candidate_ips append dev_ip except socket gaierror return Falseelse if is_valid_ipv6 dev_ip dev_ip expand_ipv6 dev_ip candidate_ips [dev_ip]for dev_ip in candidate_ips if dev_ip in my_ips and my_port is None or dev_port my_port return Truereturn False
def _my_principal_branch expr period full_pb False from sympy import principal_branchres principal_branch expr period if not full_pb res res replace principal_branch lambda x y x return res
def _my_principal_branch expr period full_pb False from sympy import principal_branchres principal_branch expr period if not full_pb res res replace principal_branch lambda x y x return res
def _my_principal_branch expr period full_pb False from sympy import principal_branchres principal_branch expr period if not full_pb res res replace principal_branch lambda x y x return res
@app route '/' def timeline if not g user return redirect url_for 'public_timeline' return render_template 'timeline html' messages query_db '\nselectmessage * user *frommessage user\nwheremessage author_id user user_idand \nuser user_id ?or\nuser user_idin selectwhom_idfromfollower\nwherewho_id ? \norderbymessage pub_datedesclimit?' [session['user_id'] session['user_id'] PER_PAGE]
def splrep x y w None xb None xe None k 3 task 0 s None t None full_output 0 per 0 quiet 1 res _impl splrep x y w xb xe k task s t full_output per quiet return res
def get_tag_from_other_config bridge port_name other_config Nonetry other_config bridge db_get_val 'Port' port_name 'other_config' return int other_config['tag'] except KeyError TypeError ValueError raise exceptions OVSFWTagNotFound port_name port_name other_config other_config
def get_tag_from_other_config bridge port_name other_config Nonetry other_config bridge db_get_val 'Port' port_name 'other_config' return int other_config['tag'] except KeyError TypeError ValueError raise exceptions OVSFWTagNotFound port_name port_name other_config other_config
def compute_hash localfn with open localfn u'rb' as f h hashlib md5 block f read conf compute_hash_block_size while block h update block block f read conf compute_hash_block_size return h hexdigest
def _removeHook hook interface adapter_hooks remove hook
def md return Mde2mdConverter
def replication func func replication Truereturn func
def handle_request sock aphorism recv_until sock '?' answer get_answer aphorism sock sendall answer
def _get_comment_and_context request comment_id try cc_comment Comment id comment_id retrieve _ context _get_thread_and_context request cc_comment['thread_id'] return cc_comment context except CommentClientRequestError raise CommentNotFoundError 'Commentnotfound '
def text string encoding 'utf8' if isinstance string six text_type return stringelif isinstance string six binary_type return string decode encoding else return six text_type string
def quote s if not isinstance s basestring return sres list s for i in range len res c res[i]if c in ' /_#? @& +$ "<>%\\' res[i] '_%02X' % ord c return '' join res
def quote s if not isinstance s basestring return sres list s for i in range len res c res[i]if c in ' /_#? @& +$ "<>%\\' res[i] '_%02X' % ord c return '' join res
def quote s if not isinstance s basestring return sres list s for i in range len res c res[i]if c in ' /_#? @& +$ "<>%\\' res[i] '_%02X' % ord c return '' join res
def quote s if not isinstance s basestring return sres list s for i in range len res c res[i]if c in ' /_#? @& +$ "<>%\\' res[i] '_%02X' % ord c return '' join res
def necklaces n k free False return uniq minlex i directed not free for i in variations list range k n repetition True
def abort status_code None detail '' headers None comment None if status_code 403 for item in p PluginImplementations p IAuthenticator result item abort status_code detail headers comment status_code detail headers comment resultif detail and status_code 503 h flash_error detail detail detail encode 'utf8' return _abort status_code status_code detail detail headers headers comment comment
def hexii s width 16 skip True return hexdump s width skip True
def add_course_author user course global_admin AdminFactory for role in CourseStaffRole CourseInstructorRole auth add_users global_admin role course id user
def add_course_author user course global_admin AdminFactory for role in CourseStaffRole CourseInstructorRole auth add_users global_admin role course id user
def heldout_score clf X_test y_test score np zeros n_estimators dtype np float64 for i y_pred in enumerate clf staged_decision_function X_test score[i] clf loss_ y_test y_pred return score
def heldout_score clf X_test y_test score np zeros n_estimators dtype np float64 for i y_pred in enumerate clf staged_decision_function X_test score[i] clf loss_ y_test y_pred return score
def assert_element_text_is output path text assert_element_text_matches output path re escape text
def assert_element_text_is output path text assert_element_text_matches output path re escape text
def process_chain_both callbacks errbacks input *a **kw d defer Deferred for cb eb in zip callbacks errbacks d addCallbacks cb eb callbackArgs a callbackKeywords kw errbackArgs a errbackKeywords kw if isinstance input failure Failure d errback input else d callback input return d
def basename p return split p [1]
def assert_http_server test host port path '' expected_response 'hi' d query_http_server host port path d addCallback test assertEqual expected_response return d
def assert_http_server test host port path '' expected_response 'hi' d query_http_server host port path d addCallback test assertEqual expected_response return d
def assert_http_server test host port path '' expected_response 'hi' d query_http_server host port path d addCallback test assertEqual expected_response return d
def CountWordErrors ocr_text truth_text return CountErrors ocr_text split truth_text split
def shutdown opts log debug 'fx2proxyshutdown called '
@keep_lazy_textdef normalize_newlines text text force_text text return re_newlines sub '\n' text
@keep_lazy_textdef normalize_newlines text text force_text text return re_newlines sub '\n' text
def loads string encoding None return pop string encoding [0]
def rotate color percent return adjust color 0 percent
@taskdef virtualenv_activate requirements_revision None assert not is_old_env 'Activeenvironmentisold-style directory notsymlink Manualinterventionrequired 'req_rev requirements_revision or latest_requirements_revision assert virtualenv_verify req_rev 'Desiredenvrevision%sinvalid cannotbemadeactive' % req_rev env_dir 'env %s' % req_rev run 'echo"importsys sys setdefaultencoding \'utf-8\' ">%s/viewfinder/lib/python2 7/sitecustomize py' % env_dir run 'ln-T-s-f~/env %s~/env' % req_rev fprint 'Environmentatrev%smarkedactive ' % req_rev
def _get_course course_key user try course get_course_with_access user 'load' course_key check_if_enrolled True except Http404 raise CourseNotFoundError 'Coursenotfound ' if not any [ tab type 'discussion' and tab is_enabled course user for tab in course tabs] raise DiscussionDisabledError 'Discussionisdisabledforthecourse ' return course
def __virtual__ if salt utils which 'csf' is None return False 'Thecsfexecutionmodulecannotbeloaded csfunavailable ' else return True
def maybe_convert_indices indices n if isinstance indices list indices np array indices if len indices 0 return np empty 0 dtype np int_ mask indices < 0 if mask any indices[mask] + nmask indices > n indices < 0 if mask any raise IndexError 'indicesareout-of-bounds' return indices
def __virtual__ if salt utils which 'apf' is None return False 'Theapfexecutionmodulecannotbeloaded apfunavailable ' elif not IPTC_IMPORTED return False 'Theapfexecutionmodulecannotbeloaded python-iptablesismissing ' else return True
def lowdata_fmt if cherrypy request method upper 'POST' returndata cherrypy request unserialized_dataif data and isinstance data collections Mapping if 'arg' in data and not isinstance data['arg'] list data['arg'] [data['arg']]cherrypy request lowstate [data]else cherrypy serving request lowstate data
def lowdata_fmt if cherrypy request method upper 'POST' returndata cherrypy request unserialized_dataif data and isinstance data collections Mapping if 'arg' in data and not isinstance data['arg'] list data['arg'] [data['arg']]cherrypy request lowstate [data]else cherrypy serving request lowstate data
def upgrade_tools name reboot False call None if call 'action' raise SaltCloudSystemExit 'Theupgrade_toolsactionmustbecalledwith-aor--action ' vm_ref salt utils vmware get_mor_by_property _get_si vim VirtualMachine name return _upg_tools_helper vm_ref reboot
def short_language_code code None if code is None code translation get_language pos code find u'-' if pos > -1 return code[ pos]return code
def select_option_by_text select_browser_query option_text def select_option query value 'Getthefirstselectelementthatmatchesthequeryandselectthedesiredvalue 'try select Select query first results[0] select select_by_visible_text value return Trueexcept StaleElementReferenceException return Falsemsg 'Selectedoption{}' format option_text EmptyPromise lambda select_option select_browser_query option_text msg fulfill
def make_inovavolumemanager_tests client_factory class Tests INovaVolumeManagerTestsMixin TestCase def setUp self super Tests self setUp self client client_factory test_case self return Tests
def getAbridgedSettings gcodeText abridgedSettings []lines archive getTextLines gcodeText settingsStart Falsefor line in lines splitLine gcodec getSplitLineBeforeBracketSemicolon line firstWord gcodec getFirstWord splitLine if firstWord ' <setting>' and settingsStart if len splitLine > 4 abridgedSettings append AbridgedSetting splitLine elif firstWord ' <settings> ' settingsStart Trueelif firstWord ' </settings> ' return abridgedSettingsreturn []
def get_secret filename '/etc/appscale/secret key' return read_file os path abspath filename chomp True
def test_resize_icon_poorly somepic get_image_path 'mozilla png' src tempfile NamedTemporaryFile mode 'r+w+b' suffix ' png' delete False dir settings TMP_PATH shutil copyfile somepic src name src_image Image open src name assert src_image size 339 128 resize_icon src name src name locally True src_image Image open src name assert src_image size 339 128
def test_resize_icon_poorly somepic get_image_path 'mozilla png' src tempfile NamedTemporaryFile mode 'r+w+b' suffix ' png' delete False dir settings TMP_PATH shutil copyfile somepic src name src_image Image open src name assert src_image size 339 128 resize_icon src name src name locally True src_image Image open src name assert src_image size 339 128
def jclouds registry xml_parent data if 'instances' in data buildWrapper XML SubElement xml_parent 'jenkins plugins jclouds compute JCloudsBuildWrapper' instances XML SubElement buildWrapper 'instancesToRun' for foo in data['instances'] for template params in foo items instance XML SubElement instances 'jenkins plugins jclouds compute InstancesToRun' XML SubElement instance 'templateName' text templateXML SubElement instance 'cloudName' text params get 'cloud-name' '' XML SubElement instance 'count' text str params get 'count' 1 XML SubElement instance 'suspendOrTerminate' text str params get 'stop-on-terminate' False lower if data get 'single-use' XML SubElement xml_parent 'jenkins plugins jclouds compute JCloudsOneOffSlave'
def jclouds registry xml_parent data if 'instances' in data buildWrapper XML SubElement xml_parent 'jenkins plugins jclouds compute JCloudsBuildWrapper' instances XML SubElement buildWrapper 'instancesToRun' for foo in data['instances'] for template params in foo items instance XML SubElement instances 'jenkins plugins jclouds compute InstancesToRun' XML SubElement instance 'templateName' text templateXML SubElement instance 'cloudName' text params get 'cloud-name' '' XML SubElement instance 'count' text str params get 'count' 1 XML SubElement instance 'suspendOrTerminate' text str params get 'stop-on-terminate' False lower if data get 'single-use' XML SubElement xml_parent 'jenkins plugins jclouds compute JCloudsOneOffSlave'
def jclouds registry xml_parent data if 'instances' in data buildWrapper XML SubElement xml_parent 'jenkins plugins jclouds compute JCloudsBuildWrapper' instances XML SubElement buildWrapper 'instancesToRun' for foo in data['instances'] for template params in foo items instance XML SubElement instances 'jenkins plugins jclouds compute InstancesToRun' XML SubElement instance 'templateName' text templateXML SubElement instance 'cloudName' text params get 'cloud-name' '' XML SubElement instance 'count' text str params get 'count' 1 XML SubElement instance 'suspendOrTerminate' text str params get 'stop-on-terminate' False lower if data get 'single-use' XML SubElement xml_parent 'jenkins plugins jclouds compute JCloudsOneOffSlave'
def _matrix_mask data mask if mask is None mask np zeros data shape np bool if isinstance mask np ndarray if mask shape data shape raise ValueError 'Maskmusthavethesameshapeasdata ' mask pd DataFrame mask index data index columns data columns dtype np bool elif isinstance mask pd DataFrame if not mask index equals data index and mask columns equals data columns err 'Maskmusthavethesameindexandcolumnsasdata 'raise ValueError err mask mask pd isnull data return mask
def _matrix_mask data mask if mask is None mask np zeros data shape np bool if isinstance mask np ndarray if mask shape data shape raise ValueError 'Maskmusthavethesameshapeasdata ' mask pd DataFrame mask index data index columns data columns dtype np bool elif isinstance mask pd DataFrame if not mask index equals data index and mask columns equals data columns err 'Maskmusthavethesameindexandcolumnsasdata 'raise ValueError err mask mask pd isnull data return mask
def all *transformers transformers tuple auto_kwargs transformer for transformer in transformers def transform data request None response None for transformer in transformers data transformer data request request response response return datareturn transform
def all *transformers transformers tuple auto_kwargs transformer for transformer in transformers def transform data request None response None for transformer in transformers data transformer data request request response response return datareturn transform
def _resolve_requirements_chain requirements chain []if isinstance requirements string_types requirements [requirements]for req_file in requirements chain append req_file chain extend _resolve_requirements_chain _find_req req_file return chain
def _resolve_requirements_chain requirements chain []if isinstance requirements string_types requirements [requirements]for req_file in requirements chain append req_file chain extend _resolve_requirements_chain _find_req req_file return chain
def _update_image facebook_id image_url image_name 'fb_image_%s jpg' % facebook_id image_temp NamedTemporaryFile try image_response urllib2 urlopen image_url except AttributeError image_response urllib request urlopen image_url image_content image_response read image_temp write image_content http_message image_response info image_size len image_content try content_type http_message typeexcept AttributeError content_type http_message get_content_type image_file InMemoryUploadedFile file image_temp name image_name field_name 'image' content_type content_type size image_size charset None image_file seek 0 image_temp flush return image_name image_file
def main opts session model for row in session query model ToolShedRepository all if 'shed_config_filename' in row metadata if row metadata['shed_config_filename'] opts bad_filename row metadata['shed_config_filename'] opts good_filenamesession add row session flush return 0
@parametrize 'table' tables mapped_classes def test_identifiers_with_names table for translation_class in table translation_classes if hasattr translation_class 'name' assert hasattr table 'identifier' table
@parametrize 'table' tables mapped_classes def test_identifiers_with_names table for translation_class in table translation_classes if hasattr translation_class 'name' assert hasattr table 'identifier' table
def wait_for_successful_query url wait_for 300 **kwargs starttime time time while True caught_exception Noneresult Nonetry result query url url **kwargs if not result get 'Error' and not result get 'error' return resultexcept Exception as exc caught_exception excif time time > starttime + wait_for if not result and caught_exception raise caught_exceptionreturn result
def get_configuration_url name urls configuration_helpers get_value 'urls' default {} return urls get name or EMPTY_URL
def test_translated_field_supports_migration fields {'charfield' TranslatedField }migration type str 'Migration' migrations Migration {'operations' [migrations CreateModel name 'MyModel' fields tuple fields items bases models Model ]} writer MigrationWriter migration output writer as_string result safe_exec output globals_ globals assert 'Migration' in result
def transplant new_net net suffix '' for p in net params p_new p + suffix if p_new not in new_net params print 'dropping' pcontinuefor i in range len net params[p] if i > len new_net params[p_new] - 1 print 'dropping' p ibreakif net params[p][i] data shape new_net params[p_new][i] data shape print 'coercing' p i 'from' net params[p][i] data shape 'to' new_net params[p_new][i] data shapeelse print 'copying' p '->' p_new inew_net params[p_new][i] data flat net params[p][i] data flat
def get_trace_db_by_live_action liveaction trace_db Nonecreated Falsetrace_context liveaction context get TRACE_CONTEXT None if trace_context trace_context _get_valid_trace_context trace_context trace_db get_trace trace_context trace_context ignore_trace_tag True if not trace_db trace_db TraceDB trace_tag trace_context trace_tag created Truereturn created trace_db parent_context executions get_parent_context liveaction_db liveaction if not trace_context and parent_context parent_execution_id parent_context get 'execution_id' None if parent_execution_id trace_db get_trace_db_by_action_execution action_execution_id parent_execution_id if not trace_db raise StackStormDBObjectNotFoundError 'Notracefoundforexecution%s' % parent_execution_id return created trace_db execution ActionExecution get liveaction__id str liveaction id if execution trace_db get_trace_db_by_action_execution action_execution execution if not trace_db trace_db TraceDB trace_tag 'execution-%s' % str liveaction id created Truereturn created trace_db
@if_conchdef generate_ssh_key key_file check_call ['ssh-keygen' '-f' key_file path '-N' '' '-q'] return Key fromFile key_file path
def NodeItem_toolTipHelper node links_in [] links_out [] desc node widget_descriptionchannel_fmt '<li>{0}</li>'title_fmt '<b>{title}</b><hr/>'title title_fmt format title escape node title inputs_list_fmt 'Inputs <ul>{inputs}</ul><hr/>'outputs_list_fmt 'Outputs <ul>{outputs}</ul>'if desc inputs inputs [channel_fmt format inp name for inp in desc inputs]inputs inputs_list_fmt format inputs '' join inputs else inputs 'Noinputs<hr/>'if desc outputs outputs [channel_fmt format out name for out in desc outputs]outputs outputs_list_fmt format outputs '' join outputs else outputs 'Nooutputs'tooltip title + inputs + outputs style 'ul{margin-top 1px margin-bottom 1px }'return TOOLTIP_TEMPLATE format style style tooltip tooltip
def fix_headers soup data header_rows soup find_all 'th' for t in header_rows if 'Version ' in t text if t next_sibling text '$Revision$' t parent extract if t next_sibling text '' t parent extract if 'Last-Modified ' in t text if '$Date$' in t next_sibling text t parent extract if t next_sibling text '' t parent extract if t text 'Title ' data['title'] t next_sibling textif t text 'Content-Type ' t parent extract if 'Version ' in t text and 'N/A' in t next_sibling text t parent extract return soup data
def fix_headers soup data header_rows soup find_all 'th' for t in header_rows if 'Version ' in t text if t next_sibling text '$Revision$' t parent extract if t next_sibling text '' t parent extract if 'Last-Modified ' in t text if '$Date$' in t next_sibling text t parent extract if t next_sibling text '' t parent extract if t text 'Title ' data['title'] t next_sibling textif t text 'Content-Type ' t parent extract if 'Version ' in t text and 'N/A' in t next_sibling text t parent extract return soup data
def slaveof master_host None master_port None host None port None db None password None if master_host and not master_port master_port 6379server _connect host port db password return server slaveof master_host master_port
def get_nodes node klass for child in node children if isinstance child klass yield child for grandchild in get_nodes child klass yield grandchild
def xsym sym op _xsym[sym]if _use_unicode return op[1]else return op[0]
def require divisions parts required None if not required return divisions parts for i in required present [j for j p in enumerate parts if p[i] is not None ]divisions tuple divisions[min present max present + 2 ] parts tuple parts[min present max present + 1 ] return divisions parts
def require divisions parts required None if not required return divisions parts for i in required present [j for j p in enumerate parts if p[i] is not None ]divisions tuple divisions[min present max present + 2 ] parts tuple parts[min present max present + 1 ] return divisions parts
def require divisions parts required None if not required return divisions parts for i in required present [j for j p in enumerate parts if p[i] is not None ]divisions tuple divisions[min present max present + 2 ] parts tuple parts[min present max present + 1 ] return divisions parts
def get_manual_interface_attributes interface module if get_interface_type interface 'svi' command 'showinterface' + interface try body execute_modified_show_for_cli_text command module [0]except IndexError ShellError return Nonecommand_list body split '\n' desc Noneadmin_state 'up'for each in command_list if 'Description ' in each line each split 'Description ' desc line[1] strip split 'MTU' [0] strip elif 'Administrativelydown' in each admin_state 'down'return dict description desc admin_state admin_state else return None
def parse_response response s _FakeSocket response response httplib HTTPResponse s strict 1 try response begin return responseexcept httplib HTTPException return None
@login_required@ensure_csrf_cookie@require_http_methods 'POST' 'PUT' 'DELETE' def signatory_detail_handler request course_key_string certificate_id signatory_id course_key CourseKey from_string course_key_string store modulestore with store bulk_operations course_key course _get_course_and_check_access course_key request user certificates_list course certificates['certificates']match_cert Nonefor index cert in enumerate certificates_list if certificate_id is not None if int cert['id'] int certificate_id match_cert certif request method 'DELETE' if not match_cert return JsonResponse status 404 CertificateManager remove_signatory request request store store course course certificate_id certificate_id signatory_id signatory_id return JsonResponse status 204
def tests_get_by_job_idx job_idx return Test objects filter job job_idx
def mod_aggregate low chunks running rules []agg_enabled ['append' 'insert']if low get 'fun' not in agg_enabled return lowfor chunk in chunks tag salt utils gen_state_tag chunk if tag in running continueif chunk get 'state' 'iptables' if '__agg__' in chunk continueif chunk get 'fun' low get 'fun' continueif chunk not in rules rules append chunk chunk['__agg__'] Trueif rules if 'rules' in low low['rules'] extend rules else low['rules'] rulesreturn low
def exec_statement statement statement textwrap dedent statement cmd ['-c' statement]return __exec_python_cmd cmd
def hash_digest text text_encoded text encode 'utf8' hash_result hashlib md5 text_encoded return hash_result hexdigest
@docstring dedent_interpddef angle_spectrum x Fs None window None pad_to None sides None return _single_spectrum_helper x x Fs Fs window window pad_to pad_to sides sides mode u'angle'
def as_template value if isinstance value Template return valueelif isinstance value collections Mapping return MappingTemplate value elif value is int return Integer elif isinstance value int return Integer value elif isinstance value type and issubclass value BASESTRING return String elif isinstance value BASESTRING return String value elif value is float return Number elif value is None return Template elif value is dict return TypeTemplate collections Mapping elif value is list return TypeTemplate collections Sequence elif isinstance value type return TypeTemplate value else raise ValueError u'cannotconverttotemplate {0 r}' format value
def names2dnsrepr x if type x is str if x and x[ -1 ] '\x00' return xx [x]res []for n in x termin '\x00'if n count ' ' 0 termin + '\x00'n '' join map lambda y chr len y + y n split ' ' + termin res append n return '' join res
def names2dnsrepr x if type x is str if x and x[ -1 ] '\x00' return xx [x]res []for n in x termin '\x00'if n count ' ' 0 termin + '\x00'n '' join map lambda y chr len y + y n split ' ' + termin res append n return '' join res
def handlerAndBytesIO output BytesIO stream outputtemplate py_logging BASIC_FORMATif _PY3 stream TextIOWrapper output encoding 'utf-8' newline '\n' formatter py_logging Formatter template handler py_logging StreamHandler stream handler setFormatter formatter return handler output
def group_from type return type split u'-' 1 [0]
def _add_junction item type_ channels _expand_one_key_dictionary item junction UnnamedStatement type 'junction' for item in channels type_ value _expand_one_key_dictionary item channel UnnamedStatement type 'channel' for val in value if _is_reference val _add_reference val channel elif _is_inline_definition val _add_inline_definition val channel junction add_child channel _current_statement add_child junction
def is_neutron return CONF use_neutron
def is_neutron return CONF use_neutron
@pytest fixturedef tutorial_disabled english return _require_project 'tutorial-disabled' 'Tutorial' english disabled True
def get_with_url url try params {'url' url 'client_id' api_key}request requests get API_BASE format 'resolve' params params request raise_for_status except requests exceptions HTTPError requests exceptions ConnectionError as e raise APIError '{}' format e json request json if not json return Noneelse return json
def ll_op left right if len left > 0 ll_gate left[0]ll_gate_is_unitary is_scalar_matrix Dagger ll_gate ll_gate _get_min_qubits ll_gate True if len left > 0 and ll_gate_is_unitary new_left left[1 len left ]new_right Dagger ll_gate + right return new_left new_right return None
def delete_floatingip floatingip_id profile None conn _auth profile return conn delete_floatingip floatingip_id
@contextmanagerdef keys purpose mode None orig_setting_dir getattr settings 'ENCRYPTED_FIELD_KEYS_DIR' None orig_setting_mode getattr settings 'ENCRYPTED_FIELD_MODE' None try if mode settings ENCRYPTED_FIELD_MODE modeif purpose keyinfo DECRYPT_AND_ENCRYPT settings ENCRYPTED_FIELD_KEYS_DIR KEY_LOCS['DECRYPT_AND_ENCRYPT'] yield keyczar Crypter Read settings ENCRYPTED_FIELD_KEYS_DIR else settings ENCRYPTED_FIELD_KEYS_DIR KEY_LOCS['ENCRYPT'] yield keyczar Encrypter Read settings ENCRYPTED_FIELD_KEYS_DIR except raisefinally settings ENCRYPTED_FIELD_KEYS_DIR orig_setting_dirif mode if orig_setting_mode settings ENCRYPTED_FIELD_MODE orig_setting_modeelse del settings ENCRYPTED_FIELD_MODE
def schema_get dbname name db_user None db_password None db_host None db_port None all_schemas schema_list dbname db_user db_user db_host db_host db_port db_port db_password db_password try return all_schemas get name None except AttributeError log error 'CouldnotretrievePostgresschema IsPostgresrunning?' return False
@_docstring 'release' def get_release_by_id id includes [] release_status [] release_type [] params _check_filter_and_make_params 'release' includes release_status release_type return _do_mb_query 'release' id includes params
@_docstring 'release' def get_release_by_id id includes [] release_status [] release_type [] params _check_filter_and_make_params 'release' includes release_status release_type return _do_mb_query 'release' id includes params
def isAddedPointOnPathIntersectingPath begin path point pointIndex segment point - begin segmentLength abs segment if segmentLength < 0 0 return FalsenormalizedSegment segment / segmentLength segmentYMirror complex normalizedSegment real - normalizedSegment imag pointRotated segmentYMirror * point beginRotated segmentYMirror * begin if euclidean isXSegmentIntersectingPath path[max 0 pointIndex - 20 pointIndex] pointRotated real beginRotated real segmentYMirror pointRotated imag return Truereturn euclidean isXSegmentIntersectingPath path[ pointIndex + 1 pointIndex + 21 ] pointRotated real beginRotated real segmentYMirror pointRotated imag
def sm_backend_conf_get_all context return IMPL sm_backend_conf_get_all context
def confirm_email token expired invalid user confirm_email_token_status token if not user or invalid invalid Truedo_flash *get_message 'INVALID_CONFIRMATION_TOKEN' if expired send_confirmation_instructions user do_flash *get_message 'CONFIRMATION_EXPIRED' email user email within _security confirm_email_within if invalid or expired return redirect get_url _security confirm_error_view or url_for 'send_confirmation' if user current_user logout_user login_user user if confirm_user user after_this_request _commit msg 'EMAIL_CONFIRMED'else msg 'ALREADY_CONFIRMED'do_flash *get_message msg return redirect get_url _security post_confirm_view or get_url _security post_login_view
@must_have_permission ADMIN @must_be_branched_from_nodedef draft_before_register_page auth node draft *args **kwargs ret serialize_node node auth primary True ret['draft'] serialize_draft_registration draft auth return ret
def noscan dev if dev not in address_ raise CommandExecutionError 'Invaliddevpassedtobluetooth noscan' cmd 'hciconfig{0}noscan' format dev __salt__['cmd run'] cmd splitlines cmd 'hciconfig{0}' format dev out __salt__['cmd run'] cmd if 'SCAN' in out return Falsereturn True
def noscan dev if dev not in address_ raise CommandExecutionError 'Invaliddevpassedtobluetooth noscan' cmd 'hciconfig{0}noscan' format dev __salt__['cmd run'] cmd splitlines cmd 'hciconfig{0}' format dev out __salt__['cmd run'] cmd if 'SCAN' in out return Falsereturn True
def disable_urllib3_warning try import requests packages urllib3requests packages urllib3 disable_warnings except Exception pass
def Lop f wrt eval_points consider_constant None disconnected_inputs 'raise' if type eval_points not in list tuple eval_points [eval_points]using_list isinstance wrt list using_tuple isinstance wrt tuple if not isinstance f list tuple f [f]f list f grads list eval_points if not isinstance wrt list tuple wrt [wrt]assert len f len grads known OrderedDict izip f grads ret grad cost None known_grads known consider_constant consider_constant wrt wrt disconnected_inputs disconnected_inputs return format_as using_list using_tuple ret
def Lop f wrt eval_points consider_constant None disconnected_inputs 'raise' if type eval_points not in list tuple eval_points [eval_points]using_list isinstance wrt list using_tuple isinstance wrt tuple if not isinstance f list tuple f [f]f list f grads list eval_points if not isinstance wrt list tuple wrt [wrt]assert len f len grads known OrderedDict izip f grads ret grad cost None known_grads known consider_constant consider_constant wrt wrt disconnected_inputs disconnected_inputs return format_as using_list using_tuple ret
def install_cache cache_name 'cache' backend 'sqlite' expire_after None allowable_codes 200 allowable_methods 'GET' session_factory CachedSession **backend_options _patch_session_factory lambda session_factory cache_name cache_name backend backend expire_after expire_after allowable_codes allowable_codes allowable_methods allowable_methods **backend_options
def EnumKey key index regenumkeyex advapi32['RegEnumKeyExW']regenumkeyex restype ctypes c_longregenumkeyex argtypes [ctypes c_void_p ctypes wintypes DWORD ctypes c_wchar_p LPDWORD LPDWORD ctypes c_wchar_p LPDWORD ctypes POINTER FileTime ]buf ctypes create_unicode_buffer 257 length ctypes wintypes DWORD 257 rc regenumkeyex key handle index ctypes cast buf ctypes c_wchar_p ctypes byref length LPDWORD ctypes c_wchar_p LPDWORD ctypes POINTER FileTime if rc 0 raise ctypes WinError 2 return ctypes wstring_at buf length value rstrip u'\x00'
def getAnalyzePluginsDirectoryPath subName '' return getJoinedPath getSkeinforgePluginsPath 'analyze_plugins' subName
def test_make_table table_types mixin_cols t table_types Table mixin_cols check_mixin_type t t['m'] mixin_cols['m'] cols list mixin_cols values t table_types Table cols names 'i' 'a' 'b' 'm' check_mixin_type t t['m'] mixin_cols['m'] t table_types Table cols check_mixin_type t t['col3'] mixin_cols['m']
def func_accepts_var_args func return any p for p in inspect signature func parameters values if p kind p VAR_POSITIONAL
def log_conditional_probability segmented_topics per_topic_postings num_docs m_lc []for s_i in segmented_topics for w_prime w_star in s_i w_prime_docs per_topic_postings[w_prime]w_star_docs per_topic_postings[w_star]co_docs w_prime_docs intersection w_star_docs if w_star_docs m_lc_i np log len co_docs / float num_docs + EPSILON / len w_star_docs / float num_docs else m_lc_i 0 0m_lc append m_lc_i return m_lc
@environmentfilterdef syntax env value lexer None filename None try import pygmentsfrom pygments import lexersfrom pygments import formattersexcept ImportError logger error u'pygmentslibraryisrequiredtousesyntaxhighlightingtags ' raise TemplateError 'Cannotloadpygments' pyg lexers get_lexer_by_name lexer if lexer else lexers guess_lexer value settings {}if hasattr env config 'syntax' settings getattr env config syntax 'options' Expando {} to_dict formatter formatters HtmlFormatter **settings code pygments highlight value pyg formatter code code replace '\n\n' '\n&nbsp \n' replace '\n' '<br/>' caption filename if filename else pyg name if hasattr env config 'syntax' if not getattr env config syntax 'use_figure' True return Markup code return Markup '<divclass "codebox"><figureclass "code">%s<figcaption>%s</figcaption></figure></div>\n\n' % code caption
@environmentfilterdef syntax env value lexer None filename None try import pygmentsfrom pygments import lexersfrom pygments import formattersexcept ImportError logger error u'pygmentslibraryisrequiredtousesyntaxhighlightingtags ' raise TemplateError 'Cannotloadpygments' pyg lexers get_lexer_by_name lexer if lexer else lexers guess_lexer value settings {}if hasattr env config 'syntax' settings getattr env config syntax 'options' Expando {} to_dict formatter formatters HtmlFormatter **settings code pygments highlight value pyg formatter code code replace '\n\n' '\n&nbsp \n' replace '\n' '<br/>' caption filename if filename else pyg name if hasattr env config 'syntax' if not getattr env config syntax 'use_figure' True return Markup code return Markup '<divclass "codebox"><figureclass "code">%s<figcaption>%s</figcaption></figure></div>\n\n' % code caption
def _handle_change_selection event params radio params['fig_selection'] radioydata event ydatalabels [label _text for label in radio labels]offset 0for idx label in enumerate labels nchans len params['selections'][label] offset + nchansif ydata < offset _set_radio_button idx params return
def _handle_change_selection event params radio params['fig_selection'] radioydata event ydatalabels [label _text for label in radio labels]offset 0for idx label in enumerate labels nchans len params['selections'][label] offset + nchansif ydata < offset _set_radio_button idx params return
def cleanPolygon elem options global numPointsRemovedFromPolygonpts parseListOfPoints elem getAttribute 'points' N len pts / 2 if N > 2 startx starty pts[ 2] endx endy pts[ -2 ]if startx endx and starty endy del pts[ -2 ]numPointsRemovedFromPolygon + 1elem setAttribute 'points' scourCoordinates pts options True
def abort http_status_code **kwargs try original_flask_abort http_status_code except HTTPException as e if len kwargs e data kwargsraise
def GetJavaJars target_list target_dicts toplevel_dir for target_name in target_list target target_dicts[target_name]for action in target get 'actions' [] for input_ in action['inputs'] if os path splitext input_ [1] ' jar' and not input_ startswith '$' if os path isabs input_ yield input_ else yield os path join os path dirname target_name input_
def umask use_sudo False func use_sudo and run_as_root or run return func 'umask'
def _send_command cmd worker lbn target profile 'default' tgt_type 'glob' ret {'code' False 'msg' 'OK' 'minions' []}func 'modjk {0}' format cmd args [worker lbn profile]response __salt__['publish publish'] target func args tgt_type errors []minions []for minion in response minions append minion if not response[minion] errors append minion if not response ret['msg'] 'noserversansweredthepublishedcommand{0}' format cmd return retelif len errors > 0 ret['msg'] 'thefollowingminionsreturnFalse'ret['minions'] errorsreturn retelse ret['code'] Trueret['msg'] 'thecommadwaspublishedsuccessfully'ret['minions'] minionsreturn ret
def _send_command cmd worker lbn target profile 'default' tgt_type 'glob' ret {'code' False 'msg' 'OK' 'minions' []}func 'modjk {0}' format cmd args [worker lbn profile]response __salt__['publish publish'] target func args tgt_type errors []minions []for minion in response minions append minion if not response[minion] errors append minion if not response ret['msg'] 'noserversansweredthepublishedcommand{0}' format cmd return retelif len errors > 0 ret['msg'] 'thefollowingminionsreturnFalse'ret['minions'] errorsreturn retelse ret['code'] Trueret['msg'] 'thecommadwaspublishedsuccessfully'ret['minions'] minionsreturn ret
def test_start_with_text hist hist start 'f' assert 'first' in hist _tmphist assert 'fourth' in hist _tmphist assert 'second' not in hist _tmphist
def get_file_encoding content encoding Nonetry lines_to_check content split u'\n' 2 for index in range 2 if len lines_to_check > index line_encoding _search_coding_line lines_to_check[index] if line_encoding encoding line_encodingbreakexcept UnicodeDecodeError as error print errorif encoding is None encoding u'UTF-8'return encoding
def get_file_encoding content encoding Nonetry lines_to_check content split u'\n' 2 for index in range 2 if len lines_to_check > index line_encoding _search_coding_line lines_to_check[index] if line_encoding encoding line_encodingbreakexcept UnicodeDecodeError as error print errorif encoding is None encoding u'UTF-8'return encoding
def get_supervisees eps list pkg_resources iter_entry_points ENTRY_POINT_GROUP return dict ep name ep load for ep in eps
def clean_whitespace chatbot statement import restatement text statement text replace '\n' '' replace '\r' '' replace ' DCTB ' '' statement text statement text strip statement text re sub '+' '' statement text return statement
def unique_path path if not os path exists syspath path return path base ext os path splitext path match re search '\\ \\d +$' base if match num int match group 1 base base[ match start ]else num 0while True num + 1new_path '%s %i%s' % base num ext if not os path exists new_path return new_path
def unique_path path if not os path exists syspath path return path base ext os path splitext path match re search '\\ \\d +$' base if match num int match group 1 base base[ match start ]else num 0while True num + 1new_path '%s %i%s' % base num ext if not os path exists new_path return new_path
def unique_path path if not os path exists syspath path return path base ext os path splitext path match re search '\\ \\d +$' base if match num int match group 1 base base[ match start ]else num 0while True num + 1new_path '%s %i%s' % base num ext if not os path exists new_path return new_path
def in_interactive_session def check_main import __main__ as mainreturn not hasattr main '__file__' or get_option 'mode sim_interactive' try return __IPYTHON__ or check_main except return check_main
def class2func path func 'CLASS_' + path replace '/' '_' replace '$' '_' replace ' ' '' return func
def class2func path func 'CLASS_' + path replace '/' '_' replace '$' '_' replace ' ' '' return func
def update_named_ports mig named_ports changed Falseexisting_ports []new_ports []if hasattr mig instance_group 'named_ports' existing_ports sorted mig instance_group named_ports key lambda x x['name'] if named_ports is not None new_ports sorted named_ports key lambda x x['name'] if existing_ports new_ports if mig instance_group set_named_ports named_ports changed Truereturn changed
def fullVersion partial if partial in '' 'latest' None return latestVersion for tag in availableVersions local False if tag startswith partial return tagreturn ''
def restrict_to_dtype dtype message_template def processor term_method _ term_instance term_dtype term_instance dtypeif term_dtype dtype raise TypeError message_template format method_name term_method __name__ expected_dtype dtype name received_dtype term_dtype return term_instancereturn preprocess self processor
def sim_otu_table sample_ids otu_ids samples otu_metadata tree num_replicates dissimilarity sample_dicts []res_sam_names []for i sample_info in enumerate samples sample_vector sample_info[0]for j in range num_replicates sample_dict {}for k in range len otu_ids otu_abundance sample_vector[k]if otu_abundance 0 continuenew_otu_id get_new_otu_id otu_ids[k] tree dissimilarity if new_otu_id in sample_dict sample_dict[new_otu_id] + otu_abundanceelse sample_dict[new_otu_id] otu_abundancesample_dicts append sample_dict res_sam_names append sample_ids[i] + ' ' + str j res_otu_mtx res_otus combine_sample_dicts sample_dicts res_otu_metadata []if otu_metadata is None or otu_metadata [] res_otu_metadata Noneelse for otu_id in res_otus try res_otu_metadata append otu_metadata[otu_ids index otu_id ] except ValueError res_otu_metadata append None return res_sam_names res_otus res_otu_mtx res_otu_metadata
def complete tab opts msg ' {0} 'if tab opts [m[len tab ] for m in opts if m startswith tab ]if len opts 1 return opts[0]if not len opts msg '{0}'return msg format ' ' join opts
def create body url build_url RESOURCE return request 'post' url json body
def dump_and_add_to_dump object_ file_ parameters None to_add None use_cpickle False protocol DEFAULT_PROTOCOL **kwargs dump object_ file_ parameters parameters use_cpickle use_cpickle protocol protocol **kwargs if to_add is not None for name obj in six iteritems to_add add_to_dump obj file_ name parameters parameters use_cpickle use_cpickle protocol protocol **kwargs
@retry exception EnvironmentError AssertionError logfun None timeout GLOBAL_TIMEOUT interval 0 001 def wait_for_file fname delete_file True empty False with open fname 'rb' as f data f read if not empty assert dataif delete_file os remove fname return data
def setup_platform hass config add_devices discovery_info None import lightifyhost config get CONF_HOST if host try bridge lightify Lightify host except socket error as err msg 'Errorconnectingtobridge {}dueto {}' format host str err _LOGGER exception msg return Falsesetup_bridge bridge add_devices else _LOGGER error 'Nohostfoundinconfiguration' return False
def add_translation key translation if not hasattr _to_save 'translations' _to_save translations {}_to_save translations setdefault key [] _to_save translations[key] append translation
@register inclusion_tag 'utilities/templatetags/utilization_graph html' def utilization_graph utilization warning_threshold 75 danger_threshold 90 return {'utilization' utilization 'warning_threshold' warning_threshold 'danger_threshold' danger_threshold}
@register inclusion_tag 'utilities/templatetags/utilization_graph html' def utilization_graph utilization warning_threshold 75 danger_threshold 90 return {'utilization' utilization 'warning_threshold' warning_threshold 'danger_threshold' danger_threshold}
def get_datacenters service_instance datacenter_names None get_all_datacenters False items [i['object'] for i in get_mors_with_properties service_instance vim Datacenter property_list ['name'] if get_all_datacenters or datacenter_names and i['name'] in datacenter_names ]return items
def get_datacenters service_instance datacenter_names None get_all_datacenters False items [i['object'] for i in get_mors_with_properties service_instance vim Datacenter property_list ['name'] if get_all_datacenters or datacenter_names and i['name'] in datacenter_names ]return items
def parse_backend_conf backend **kwargs conf settings CACHES get backend None if conf is not None args conf copy args update kwargs backend args pop 'BACKEND' location args pop 'LOCATION' '' return backend location args else try backend_cls import_by_path backend except ImproperlyConfigured as e raise InvalidCacheBackendError "Couldnotfindbackend'%s' %s" % backend e location kwargs pop 'LOCATION' '' return backend location kwargs
def parse_backend_conf backend **kwargs conf settings CACHES get backend None if conf is not None args conf copy args update kwargs backend args pop 'BACKEND' location args pop 'LOCATION' '' return backend location args else try backend_cls import_by_path backend except ImproperlyConfigured as e raise InvalidCacheBackendError "Couldnotfindbackend'%s' %s" % backend e location kwargs pop 'LOCATION' '' return backend location kwargs
def parse_backend_conf backend **kwargs conf settings CACHES get backend None if conf is not None args conf copy args update kwargs backend args pop 'BACKEND' location args pop 'LOCATION' '' return backend location args else try backend_cls import_by_path backend except ImproperlyConfigured as e raise InvalidCacheBackendError "Couldnotfindbackend'%s' %s" % backend e location kwargs pop 'LOCATION' '' return backend location kwargs
def _get_mro cls if not isinstance cls type class cls cls object passreturn cls __mro__[1 ]return cls __mro__
def check_dataset_access_permission view_func def decorate request *args **kwargs dataset kwargs get 'dataset' if dataset is not None dataset Dataset objects can_read_or_exception request dataset kwargs['dataset'] datasetreturn view_func request *args **kwargs return wraps view_func decorate
def check_dataset_access_permission view_func def decorate request *args **kwargs dataset kwargs get 'dataset' if dataset is not None dataset Dataset objects can_read_or_exception request dataset kwargs['dataset'] datasetreturn view_func request *args **kwargs return wraps view_func decorate
def create_snapshot_query_with_metadata metadata_query_string api_microversion req fakes HTTPRequest blank '/v3/snapshots?metadata ' + metadata_query_string req headers['OpenStack-API-Version'] 'volume' + api_microversion req api_version_request api_version APIVersionRequest api_microversion return req
@contextlib contextmanagerdef task_logging task old_task getattr local_context u'task' u'' local_context task tasktry yield finally local_context task old_task
def reinitialize_command self command reinit_subcommands cmd_obj _DISTUTILS_REINIT self command reinit_subcommands options self command_options get command if options self _set_command_options cmd_obj options return cmd_obj
def top **kwargs if 'id' not in kwargs['opts'] return {}cmd '{0}{1}' format __opts__['master_tops']['ext_nodes'] kwargs['opts']['id'] ndata yaml safe_load subprocess Popen cmd shell True stdout subprocess PIPE communicate [0] if not ndata log info 'master_topsext_nodescalldidnotreturnanydata' ret {}if 'environment' in ndata env ndata['environment']else env 'base'if 'classes' in ndata if isinstance ndata['classes'] dict ret[env] list ndata['classes'] elif isinstance ndata['classes'] list ret[env] ndata['classes']else return retelse log info 'master_topsext_nodescalldidnothaveadictionarywitha"classes"key ' return ret
def force_tcg force True global forceTCGforceTCG force
def force_tcg force True global forceTCGforceTCG force
def curve_keypair _check_version 3 2 'monitor' public ffi new 'char[64]' private ffi new 'char[64]' rc C zmq_curve_keypair public private _check_rc rc return ffi buffer public [ 40] ffi buffer private [ 40]
@click command u'setup-global-help' @click option u'--mariadb_root_password' def setup_global_help mariadb_root_password None from frappe installer import update_site_configfrappe local flags frappe _dict frappe local flags in_setup_help Truefrappe local flags in_install Truefrappe local lang u'en'frappe local conf frappe get_site_config sites_path u' ' update_site_config u'global_help_setup' 1 site_config_path os path join u' ' u'common_site_config json' if mariadb_root_password frappe local conf root_password mariadb_root_passwordfrom frappe utils help import syncsync
@click command u'setup-global-help' @click option u'--mariadb_root_password' def setup_global_help mariadb_root_password None from frappe installer import update_site_configfrappe local flags frappe _dict frappe local flags in_setup_help Truefrappe local flags in_install Truefrappe local lang u'en'frappe local conf frappe get_site_config sites_path u' ' update_site_config u'global_help_setup' 1 site_config_path os path join u' ' u'common_site_config json' if mariadb_root_password frappe local conf root_password mariadb_root_passwordfrom frappe utils help import syncsync
def resolve_user_restricted_access document resource resource_def app config['DOMAIN'][resource]auth resource_def['authentication']auth_field resource_def['auth_field']if auth and auth_field request_auth_value auth get_request_auth_value if request_auth_value document[auth_field] request_auth_value
def setStateNormalDisabled active widget if active widget config state settings Tkinter NORMAL else widget config state settings Tkinter DISABLED
def assert_all_finite X _assert_all_finite X data if sp issparse X else X
def send_ned_velocity velocity_x velocity_y velocity_z duration msg vehicle message_factory set_position_target_local_ned_encode 0 0 0 mavutil mavlink MAV_FRAME_LOCAL_NED 4039 0 0 0 velocity_x velocity_y velocity_z 0 0 0 0 0 for x in range 0 duration vehicle send_mavlink msg time sleep 1
def move_to_gpu data if str data dtype in tensor basic complex_dtypes return Falseif data ndim 0 and str data dtype in tensor basic discrete_dtypes return Falsereturn True
def move_to_gpu data if str data dtype in tensor basic complex_dtypes return Falseif data ndim 0 and str data dtype in tensor basic discrete_dtypes return Falsereturn True
def apply key value path __opts__['conf_file']if os path isdir path path os path join path 'master' data values data[key] valuewith salt utils fopen path 'w+' as fp_ fp_ write yaml dump data default_flow_style False
def compareFunctionName first second first getConvertedName first second getConvertedName second if first < second return -1 return first < second
@decorators api_view ['GET'] @decorators permission_classes permissions AllowAny @decorators renderer_classes JSONRenderer def section_search request query request GET get 'q' None if not query return Response {'error' 'Searchtermrequired Usethe"q"GETargtosearch '} status status HTTP_400_BAD_REQUEST project_slug request GET get 'project' None version_slug request GET get 'version' LATEST path request GET get 'path' None log debug ' APISectionSearch [%s %s]%s' project_slug version_slug query results search_section request request query query project_slug project_slug version_slug version_slug path path return Response {'results' results}
def openshift_deploy_canceller registry xml_parent data osb XML SubElement xml_parent 'com openshift jenkins plugins pipeline OpenShiftDeployCanceller' mapping [ 'api-url' 'apiURL' 'https //openshift default svc cluster local' 'dep-cfg' 'depCfg' 'frontend' 'namespace' 'namespace' 'test' 'auth-token' 'authToken' '' 'verbose' 'verbose' False ]helpers convert_mapping_to_xml osb data mapping fail_required True
def openshift_deploy_canceller registry xml_parent data osb XML SubElement xml_parent 'com openshift jenkins plugins pipeline OpenShiftDeployCanceller' mapping [ 'api-url' 'apiURL' 'https //openshift default svc cluster local' 'dep-cfg' 'depCfg' 'frontend' 'namespace' 'namespace' 'test' 'auth-token' 'authToken' '' 'verbose' 'verbose' False ]helpers convert_mapping_to_xml osb data mapping fail_required True
def openshift_deploy_canceller registry xml_parent data osb XML SubElement xml_parent 'com openshift jenkins plugins pipeline OpenShiftDeployCanceller' mapping [ 'api-url' 'apiURL' 'https //openshift default svc cluster local' 'dep-cfg' 'depCfg' 'frontend' 'namespace' 'namespace' 'test' 'auth-token' 'authToken' '' 'verbose' 'verbose' False ]helpers convert_mapping_to_xml osb data mapping fail_required True
def openshift_deploy_canceller registry xml_parent data osb XML SubElement xml_parent 'com openshift jenkins plugins pipeline OpenShiftDeployCanceller' mapping [ 'api-url' 'apiURL' 'https //openshift default svc cluster local' 'dep-cfg' 'depCfg' 'frontend' 'namespace' 'namespace' 'test' 'auth-token' 'authToken' '' 'verbose' 'verbose' False ]helpers convert_mapping_to_xml osb data mapping fail_required True
def openshift_deploy_canceller registry xml_parent data osb XML SubElement xml_parent 'com openshift jenkins plugins pipeline OpenShiftDeployCanceller' mapping [ 'api-url' 'apiURL' 'https //openshift default svc cluster local' 'dep-cfg' 'depCfg' 'frontend' 'namespace' 'namespace' 'test' 'auth-token' 'authToken' '' 'verbose' 'verbose' False ]helpers convert_mapping_to_xml osb data mapping fail_required True
def random_symbols expr try return list expr atoms RandomSymbol except AttributeError return []
def _stub_islink path return False
def _stub_islink path return False
def _stub_islink path return False
@csrf_exempt@gzip_page@require_sync_session@api_handle_error_with_jsondef device_download data session zone session client_device get_zone devicezones list DeviceZone all_objects filter zone zone device__in data['devices'] devices [devicezone device for devicezone in devicezones]session models_downloaded + len devices + len devicezones return JsonResponse {'devices' serialize devices + devicezones dest_version session client_version ensure_ascii False }
@pytest mark skipif 'notHAS_YAML' def test_csv_ecsv_colnames_mismatch lines copy copy SIMPLE_LINES header_index lines index 'abc' lines[header_index] 'abd'with pytest raises ValueError as err ascii read lines format 'ecsv' assert "columnnamesfromECSVheader['a' 'b' 'c']" in str err
@pytest mark skipif 'notHAS_YAML' def test_csv_ecsv_colnames_mismatch lines copy copy SIMPLE_LINES header_index lines index 'abc' lines[header_index] 'abd'with pytest raises ValueError as err ascii read lines format 'ecsv' assert "columnnamesfromECSVheader['a' 'b' 'c']" in str err
def _process_item v rpc_type Noneif isinstance v Column if isinstance v type sqlalchemy Enum if v type convert_unicode rpc_type primitive Unicode values v type enums else rpc_type primitive String values v type enums elif v type in _type_map rpc_type _type_map[v type]elif type v type in _type_map rpc_type _type_map[type v type ]else raise Exception 'soap_typewasnotfound maybe_type_mapneedsanewentry %r' % v elif isinstance v RelationshipProperty v enable_typechecks Falserpc_type Array v argument return rpc_type
def _process_item v rpc_type Noneif isinstance v Column if isinstance v type sqlalchemy Enum if v type convert_unicode rpc_type primitive Unicode values v type enums else rpc_type primitive String values v type enums elif v type in _type_map rpc_type _type_map[v type]elif type v type in _type_map rpc_type _type_map[type v type ]else raise Exception 'soap_typewasnotfound maybe_type_mapneedsanewentry %r' % v elif isinstance v RelationshipProperty v enable_typechecks Falserpc_type Array v argument return rpc_type
def unlink_cohort_partition_group cohort CourseUserGroupPartitionGroup objects filter course_user_group cohort delete
def dispatch c id methodname args kwds {} c send id methodname args kwds kind result c recv if kind '#RETURN' return resultraise convert_to_error kind result
def update_floatingip floatingip_id port profile None conn _auth profile return conn update_floatingip floatingip_id port
def init_compare print 'Initializingcomparisonfeature'subprocess Popen ['hg' 'clone' ' ' 'a'] wait subprocess Popen ['hg' 'clone' ' ' 'b'] wait
def generate_fake_facility_groups names 'Class4E' 'Class5B' facilities None if not facilities facilities generate_fake_facilities facility_groups []for facility in facilities for name in names found_facility_groups FacilityGroup objects filter facility facility name name if found_facility_groups facility_group found_facility_groups[0]logging info "Retrievedfacilitygroup'%s'" % name else facility_group FacilityGroup facility facility name name facility_group save logging info "Createdfacilitygroup'%s'" % name facility_groups append facility_group return facility_groups facilities
def _recurring_component_to_events component rrule_as_str component get 'rrule' to_ical recur_rule rrule rrulestr rrule_as_str dtstart component decoded 'dtstart' recur_set rrule rruleset recur_set rrule recur_rule if 'exdate' in component for exdate_line in component decoded 'exdate' for exdate in exdate_line dts recur_set exdate exdate dt utcnow now later utcnow + datetime timedelta days MAX_FUTURE start_times recur_set between utcnow later event_length component decoded 'dtend' - component decoded 'dtstart' events []for start in start_times events append {'start' start 'end' start + event_length 'summary' component decoded 'summary' 'uid' component decoded 'uid' 'last_modified' component decoded 'last-modified' } return events
def set_file_utime filename desired_time try os utime filename desired_time desired_time except OSError as e if e errno errno EPERM raise eraise SetFileUtimeError 'Thefilewasdownloaded butattemptingtomodifytheutimeofthefilefailed Isthefileownedbyanotheruser?'
def absent name **kwargs ret {'name' name 'result' True 'changes' {} 'comment' []}current_beacons __salt__['beacons list'] return_yaml False if name in current_beacons if 'test' in __opts__ and __opts__['test'] kwargs['test'] Trueresult __salt__['beacons delete'] name **kwargs ret['comment'] append result['comment'] else result __salt__['beacons delete'] name **kwargs if not result['result'] ret['result'] result['result']ret['comment'] result['comment']return retelse ret['comment'] append 'Removed{0}frombeacons' format name else ret['comment'] append '{0}notconfiguredinbeacons' format name ret['comment'] '\n' join ret['comment'] return ret
def parse_startup pkt values []plen endian_int pkt[0 4] pkt pkt[8 ]tmp ''for i in xrange plen - 8 tmp + pkt[i] decode 'hex' if pkt[i] '00' values append tmp tmp ''return values
def parse_startup pkt values []plen endian_int pkt[0 4] pkt pkt[8 ]tmp ''for i in xrange plen - 8 tmp + pkt[i] decode 'hex' if pkt[i] '00' values append tmp tmp ''return values
def check_named option opt value if isinstance value dict return valuevalues []for value in check_csv option opt value if value find ' ' -1 values append value split ' ' 1 elif value find ' ' -1 values append value split ' ' 1 if values return dict values msg 'option%s invalidnamedvalue%r shouldbe<NAME> <VALUE>or<NAME> <VALUE>'raise OptionValueError msg % opt value
def contrasting_color_generator def rgb_to_hex rgb return u'#%02x%02x%02x' % tuple rgb triples [ 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 ]n 1 << 7 so_far [[0 0 0]]while True if n 0 yield u'#000000' copy_so_far list so_far for triple in triples for previous in copy_so_far rgb [ n * triple[i] + previous[i] for i in range 3 ]so_far append rgb yield rgb_to_hex rgb n >> 1
def traceroute destination source None ttl None timeout None return __proxy__['napalm call'] 'traceroute' **{'destination' destination 'source' source 'ttl' ttl 'timeout' timeout}
def traceroute destination source None ttl None timeout None return __proxy__['napalm call'] 'traceroute' **{'destination' destination 'source' source 'ttl' ttl 'timeout' timeout}
def traceroute destination source None ttl None timeout None return __proxy__['napalm call'] 'traceroute' **{'destination' destination 'source' source 'ttl' ttl 'timeout' timeout}
def _read_password_file b_path content Noneif os path exists b_path with open b_path 'rb' as f b_content f read rstrip content to_text b_content errors 'surrogate_or_strict' return content
def _read_password_file b_path content Noneif os path exists b_path with open b_path 'rb' as f b_content f read rstrip content to_text b_content errors 'surrogate_or_strict' return content
def _read_password_file b_path content Noneif os path exists b_path with open b_path 'rb' as f b_content f read rstrip content to_text b_content errors 'surrogate_or_strict' return content
def ZigZagEncode value if value > 0 return value << 1 return value << 1 ^ ~ 0
def ext_pillar minion_id pillar *args **kwargs return POSTGRESExtPillar fetch minion_id pillar *args **kwargs
def crawl links [] domains [] delay 20 0 parse HTMLLinkParser parse sort FIFO method DEPTH **kwargs crawler Crawler links domains delay parse sort bind crawler 'visit' lambda crawler link source None setattr crawler 'crawled' link source while not crawler done crawler crawled None None crawler crawl method **kwargs yield crawler crawled
def evaluate_object obj params if isinstance obj basestring return parse_string obj params elif isinstance obj list new_list []for item in obj new_list append evaluate_object item params return new_listelif isinstance obj dict new_dict {}for key in obj new_dict[key] evaluate_object obj[key] params return new_dictelse return copy deepcopy obj
def update_app id config if 'id' not in config config['id'] idconfig pop 'version' None config pop 'fetch' None data json dumps config try response salt utils http query '{0}/v2/apps/{1}?force true' format _base_url id method 'PUT' decode_type 'json' decode True data data header_dict {'Content-Type' 'application/json' 'Accept' 'application/json'} log debug 'updateresponse %s' response return response['dict']except Exception as ex log error 'unabletoupdatemarathonapp %s' get_error_message ex return {'exception' {'message' get_error_message ex }}
def getfilesystemencoding encoding sys getfilesystemencoding if encoding is None encoding PREFERRED_ENCODINGreturn encoding
def getfilesystemencoding encoding sys getfilesystemencoding if encoding is None encoding PREFERRED_ENCODINGreturn encoding
def _load_module filename import magics marshalfp open filename 'rb' magic fp read 4 try version magics versions[magic]except KeyError raise ImportError 'Unknownmagicnumber%sin%s' % ord magic[0] + 256 * ord magic[1] filename if version '2 7' raise ImportError 'ThisisaPython%sfile OnlyPython2 7filesaresupported ' % version fp read 4 co marshal load fp fp close return version co
def build_auxiliary_node_connectivity G directed G is_directed mapping {}H nx DiGraph for i node in enumerate G mapping[node] iH add_node '%dA' % i id node H add_node '%dB' % i id node H add_edge '%dA' % i '%dB' % i capacity 1 edges []for source target in G edges edges append '%sB' % mapping[source] '%sA' % mapping[target] if not directed edges append '%sB' % mapping[target] '%sA' % mapping[source] H add_edges_from edges capacity 1 H graph['mapping'] mappingreturn H
def build_auxiliary_node_connectivity G directed G is_directed mapping {}H nx DiGraph for i node in enumerate G mapping[node] iH add_node '%dA' % i id node H add_node '%dB' % i id node H add_edge '%dA' % i '%dB' % i capacity 1 edges []for source target in G edges edges append '%sB' % mapping[source] '%sA' % mapping[target] if not directed edges append '%sB' % mapping[target] '%sA' % mapping[source] H add_edges_from edges capacity 1 H graph['mapping'] mappingreturn H
def load_auth_tokens user None if user is None user users get_current_user if user is None return {}pickled_tokens memcache get 'gdata_pickled_tokens %s' % user if pickled_tokens return pickle loads pickled_tokens user_tokens TokenCollection all filter 'user ' user get if user_tokens memcache set 'gdata_pickled_tokens %s' % user user_tokens pickled_tokens return pickle loads user_tokens pickled_tokens return {}
def answer_callback_query token callback_query_id text None show_alert None url None cache_time None method_url 'answerCallbackQuery'payload {'callback_query_id' callback_query_id}if text payload['text'] textif show_alert payload['show_alert'] show_alertif url payload['url'] urlif cache_time payload['cache_time'] cache_timereturn _make_request token method_url params payload method 'post'
def window_by_index idx if not window_registry raise NoWindow else key sorted window_registry [idx]return window_registry[key]
def _get_email_config intent require_valid_intent intent return config_domain Registry get_config_property feconf VALID_MODERATOR_ACTIONS[intent]['email_config']
def _get_email_config intent require_valid_intent intent return config_domain Registry get_config_property feconf VALID_MODERATOR_ACTIONS[intent]['email_config']
def build_name_setter registry xml_parent data build_name_setter XML SubElement xml_parent 'org jenkinsci plugins buildnameupdater BuildNameUpdater' mapping [ 'name' 'buildName' 'version txt' 'template' 'macroTemplate' '#${BUILD_NUMBER}' 'file' 'fromFile' False 'macro' 'fromMacro' False 'macro-first' 'macroFirst' False ]convert_mapping_to_xml build_name_setter data mapping fail_required True
def build_name_setter registry xml_parent data build_name_setter XML SubElement xml_parent 'org jenkinsci plugins buildnameupdater BuildNameUpdater' mapping [ 'name' 'buildName' 'version txt' 'template' 'macroTemplate' '#${BUILD_NUMBER}' 'file' 'fromFile' False 'macro' 'fromMacro' False 'macro-first' 'macroFirst' False ]convert_mapping_to_xml build_name_setter data mapping fail_required True
def build_name_setter registry xml_parent data build_name_setter XML SubElement xml_parent 'org jenkinsci plugins buildnameupdater BuildNameUpdater' mapping [ 'name' 'buildName' 'version txt' 'template' 'macroTemplate' '#${BUILD_NUMBER}' 'file' 'fromFile' False 'macro' 'fromMacro' False 'macro-first' 'macroFirst' False ]convert_mapping_to_xml build_name_setter data mapping fail_required True
def run_experiments shallow basic_conv omit_FC dbl_conv activation_fn sigmoid dbl_conv_relu expanded_data n 100 expanded_data n 300 expanded_data n 1000 expanded_data_double_fc n 100 expanded_data_double_fc n 300 expanded_data_double_fc n 1000 nets double_fc_dropout 0 5 0 5 0 5 5 error_locations erroneous_predictions ensemble nets plt plot_errors error_locations erroneous_predictions plt savefig 'ensemble_errors png' plt plot_filters nets[0] 0 5 4 plt savefig 'net_full_layer_0 png' plt plot_filters nets[0] 1 8 5 plt savefig 'net_full_layer_1 png'
def run_experiments shallow basic_conv omit_FC dbl_conv activation_fn sigmoid dbl_conv_relu expanded_data n 100 expanded_data n 300 expanded_data n 1000 expanded_data_double_fc n 100 expanded_data_double_fc n 300 expanded_data_double_fc n 1000 nets double_fc_dropout 0 5 0 5 0 5 5 error_locations erroneous_predictions ensemble nets plt plot_errors error_locations erroneous_predictions plt savefig 'ensemble_errors png' plt plot_filters nets[0] 0 5 4 plt savefig 'net_full_layer_0 png' plt plot_filters nets[0] 1 8 5 plt savefig 'net_full_layer_1 png'
def run_experiments shallow basic_conv omit_FC dbl_conv activation_fn sigmoid dbl_conv_relu expanded_data n 100 expanded_data n 300 expanded_data n 1000 expanded_data_double_fc n 100 expanded_data_double_fc n 300 expanded_data_double_fc n 1000 nets double_fc_dropout 0 5 0 5 0 5 5 error_locations erroneous_predictions ensemble nets plt plot_errors error_locations erroneous_predictions plt savefig 'ensemble_errors png' plt plot_filters nets[0] 0 5 4 plt savefig 'net_full_layer_0 png' plt plot_filters nets[0] 1 8 5 plt savefig 'net_full_layer_1 png'
def aes_cbc_decrypt data key enc_iv cipher AES new key AES MODE_CBC enc_iv return cipher decrypt data
def show_instance name call None if call 'action' raise SaltCloudSystemExit 'Theshow_instanceactionmustbecalledwith-aor--action ' return _get_node name
def get_interfaces_dict module command 'showinterfacestatus'try body execute_show_command command module [0]except IndexError body {}interfaces {'ethernet' [] 'svi' [] 'loopback' [] 'management' [] 'portchannel' [] 'nve' [] 'unknown' []}interface_list body get 'TABLE_interface' ['ROW_interface']for index in interface_list intf index['interface']intf_type get_interface_type intf interfaces[intf_type] append intf return interfaces
def get_interfaces_dict module command 'showinterfacestatus'try body execute_show_command command module [0]except IndexError body {}interfaces {'ethernet' [] 'svi' [] 'loopback' [] 'management' [] 'portchannel' [] 'nve' [] 'unknown' []}interface_list body get 'TABLE_interface' ['ROW_interface']for index in interface_list intf index['interface']intf_type get_interface_type intf interfaces[intf_type] append intf return interfaces
@hook regex 'vimeo com/ [0-9]+ ' def vimeo_url match info http get_json 'http //vimeo com/api/v2/video/%s json' % match group 1 if info info[0]['duration'] timeformat format_time info[0]['duration'] info[0]['stats_number_of_likes'] format info[0]['stats_number_of_likes'] ' d' info[0]['stats_number_of_plays'] format info[0]['stats_number_of_plays'] ' d' return '\x02% title s\x02-length\x02% duration s\x02-\x02% stats_number_of_likes s\x02likes-\x02% stats_number_of_plays s\x02plays-\x02% user_name s\x02on\x02% upload_date s\x02' % info[0]
@hook regex 'vimeo com/ [0-9]+ ' def vimeo_url match info http get_json 'http //vimeo com/api/v2/video/%s json' % match group 1 if info info[0]['duration'] timeformat format_time info[0]['duration'] info[0]['stats_number_of_likes'] format info[0]['stats_number_of_likes'] ' d' info[0]['stats_number_of_plays'] format info[0]['stats_number_of_plays'] ' d' return '\x02% title s\x02-length\x02% duration s\x02-\x02% stats_number_of_likes s\x02likes-\x02% stats_number_of_plays s\x02plays-\x02% user_name s\x02on\x02% upload_date s\x02' % info[0]
def open_tasks_for_project def prep r tablename 'project_project's3 crud_strings[tablename] title_list T 'OpenTasksforProject' s3 crud_labels READ s3 crud_labels UPDATE T 'Select' s3db configure tablename deletable False listadd False return Trues3 prep prepdef postp r output if r interactive and not r component tasklist_url URL f 'task' vars {'project' '[id]'} s3_action_buttons r deletable False read_url tasklist_url update_url tasklist_url return outputs3 postp postpreturn s3_rest_controller module 'project' hide_filter False
def open_tasks_for_project def prep r tablename 'project_project's3 crud_strings[tablename] title_list T 'OpenTasksforProject' s3 crud_labels READ s3 crud_labels UPDATE T 'Select' s3db configure tablename deletable False listadd False return Trues3 prep prepdef postp r output if r interactive and not r component tasklist_url URL f 'task' vars {'project' '[id]'} s3_action_buttons r deletable False read_url tasklist_url update_url tasklist_url return outputs3 postp postpreturn s3_rest_controller module 'project' hide_filter False
def open_tasks_for_project def prep r tablename 'project_project's3 crud_strings[tablename] title_list T 'OpenTasksforProject' s3 crud_labels READ s3 crud_labels UPDATE T 'Select' s3db configure tablename deletable False listadd False return Trues3 prep prepdef postp r output if r interactive and not r component tasklist_url URL f 'task' vars {'project' '[id]'} s3_action_buttons r deletable False read_url tasklist_url update_url tasklist_url return outputs3 postp postpreturn s3_rest_controller module 'project' hide_filter False
def strip_version idstr parts idstr split 'v' return parts[0]
def strip_version idstr parts idstr split 'v' return parts[0]
def setup_scanner hass config see hosts [Host ip dev_id hass config for dev_id ip in config[const CONF_HOSTS] items ]interval timedelta seconds len hosts * config[CONF_PING_COUNT] + DEFAULT_SCAN_INTERVAL _LOGGER info 'Startedpingtrackerwithinterval %sonhosts %s' interval ' ' join [host ip_address for host in hosts] def update now 'Updateallthehostsoneveryintervaltime 'for host in hosts host update see track_point_in_utc_time hass update now + interval return Truereturn update util dt utcnow
def _build_cluster_status_fsm_table S ClusterStatusStatesI ClusterStatusInputsO ClusterStatusOutputstable TransitionTable table table addTransitions S DISCONNECTED {I CONNECTED_TO_CONTROL_SERVICE [O STORE_CLIENT] S IGNORANT I SHUTDOWN [] S SHUTDOWN } table table addTransitions S IGNORANT {I DISCONNECTED_FROM_CONTROL_SERVICE [] S DISCONNECTED I STATUS_UPDATE [O UPDATE_STATUS] S KNOWLEDGEABLE I SHUTDOWN [O DISCONNECT] S SHUTDOWN } table table addTransitions S KNOWLEDGEABLE {I STATUS_UPDATE [O UPDATE_STATUS] S KNOWLEDGEABLE I DISCONNECTED_FROM_CONTROL_SERVICE [O STOP] S DISCONNECTED I SHUTDOWN [O STOP O DISCONNECT] S SHUTDOWN } table table addTransitions S SHUTDOWN {I DISCONNECTED_FROM_CONTROL_SERVICE [] S SHUTDOWN I STATUS_UPDATE [] S SHUTDOWN } return table
def sdm_LC f K if not f return K zeroelse return f[0][1]
def _get_param param param_dict default None result param_dict get param default if param in ['max-keys'] and result return long result return result
@before each_scenariodef reset_data scenario LOGGER debug 'Flushingthetestdatabase ' call_command 'flush' interactive False verbosity 0 world absorb {} 'scenario_dict'
@before each_scenariodef reset_data scenario LOGGER debug 'Flushingthetestdatabase ' call_command 'flush' interactive False verbosity 0 world absorb {} 'scenario_dict'
def ValidateToken token targets def GetSubjectForError if len targets 1 return list targets [0]else return Noneif not token raise access_control UnauthorizedAccess 'Mustgiveanauthorizationtokenfor%s' % targets subject GetSubjectForError token CheckExpiry if not token username raise access_control UnauthorizedAccess 'Mustspecifyausernameforaccessto%s ' % targets subject GetSubjectForError return True
def _tabulate results metrics formats column_width max max len k for k in formats + 1 8 first_width max len k for k in metrics head_fmt '{ <{fw}s}' + '{ >{cw}s}' * len formats row_fmt '{ <{fw}s}' + '{ >{cw} 3f}' * len formats print head_fmt format 'Metric' cw column_width fw first_width *formats for metric row in zip metrics results[ -1 -1 -1 ] print row_fmt format metric cw column_width fw first_width *row
@dodef upload_python_packages scratch_directory target_bucket top_level output error check_call ['python' 'setup py' 'sdist' '--dist-dir {}' format scratch_directory path 'bdist_wheel' '--dist-dir {}' format scratch_directory path ] cwd top_level path stdout output stderr error files set [f basename for f in scratch_directory children ] yield Effect UploadToS3Recursively source_path scratch_directory target_bucket target_bucket target_key 'python' files files
def extend_api route '' api None base_url '' def decorator extend_with apply_to_api hug API api if api else hug api from_object extend_with for extended_api in extend_with apply_to_api extend extended_api route base_url return extend_withreturn decorator
def set_user_password uid mode 'set_password' password None **kwargs with _IpmiCommand **kwargs as s s set_user_password uid mode 'set_password' password password return True
def set_current_canvas canvas canvas context _do_CURRENT_command Trueif canvasses and canvasses[ -1 ] is canvas returncc [c for c in canvasses if c is not None ]while canvas in cc cc remove canvas cc append canvas canvasses[ ] [weakref ref c for c in cc]
@cacheitdef _has_simple_delta expr index if expr has KroneckerDelta if _is_simple_delta expr index return Trueif expr is_Add or expr is_Mul for arg in expr args if _has_simple_delta arg index return Truereturn False
def task_log_begin_task context task_name period_beginning period_ending host task_items None message None return IMPL task_log_begin_task context task_name period_beginning period_ending host task_items message
def signal_to_noise_oir_ccd t source_eps sky_eps dark_eps rd npix gain 1 0 signal t * source_eps * gain noise np sqrt t * source_eps * gain + npix * sky_eps * gain + dark_eps + npix * rd ** 2 return signal / noise
def signal_to_noise_oir_ccd t source_eps sky_eps dark_eps rd npix gain 1 0 signal t * source_eps * gain noise np sqrt t * source_eps * gain + npix * sky_eps * gain + dark_eps + npix * rd ** 2 return signal / noise
def signal_to_noise_oir_ccd t source_eps sky_eps dark_eps rd npix gain 1 0 signal t * source_eps * gain noise np sqrt t * source_eps * gain + npix * sky_eps * gain + dark_eps + npix * rd ** 2 return signal / noise
def is_prerequisite_courses_enabled return settings FEATURES get 'ENABLE_PREREQUISITE_COURSES' and settings FEATURES get 'MILESTONES_APP'
def processor n fountain_url responder_urls zpullsock zcontext socket zmq PULL zpullsock connect fountain_url zreqsock zcontext socket zmq REQ for url in responder_urls zreqsock connect url while True word zpullsock recv zreqsock send word print n zreqsock recv
def build_path_result_tests name return build_schema_test name str name + u'Tests' schema {u'$ref' u'/endpoints json#/definitions/' + name } schema_store SCHEMAS failing_instances {'additionalProperties' [{u'Err' u'' u'Mountpoint' u'/x' u'extra' u'y'} {u'Result' u'hello'}] 'required' [{} {u'Mountpoint' u'/x'}] 'type' [[] u'' None]} passing_instances [{u'Err' u'Somethingwentwrong '} {u'Err' u'' u'Mountpoint' u'/x/'}]
def build_path_result_tests name return build_schema_test name str name + u'Tests' schema {u'$ref' u'/endpoints json#/definitions/' + name } schema_store SCHEMAS failing_instances {'additionalProperties' [{u'Err' u'' u'Mountpoint' u'/x' u'extra' u'y'} {u'Result' u'hello'}] 'required' [{} {u'Mountpoint' u'/x'}] 'type' [[] u'' None]} passing_instances [{u'Err' u'Somethingwentwrong '} {u'Err' u'' u'Mountpoint' u'/x/'}]
def char_from_number number base 26rval ''if number 0 rval 'A'while number 0 remainder number % base new_char chr ord 'A' + remainder rval new_char + rval number // basereturn rval
def char_from_number number base 26rval ''if number 0 rval 'A'while number 0 remainder number % base new_char chr ord 'A' + remainder rval new_char + rval number // basereturn rval
def char_from_number number base 26rval ''if number 0 rval 'A'while number 0 remainder number % base new_char chr ord 'A' + remainder rval new_char + rval number // basereturn rval
def show_user id_ return show_item 'user' id_
def set_wake_on_network enabled state salt utils mac_utils validate_enabled enabled cmd 'systemsetup-setwakeonnetworkaccess{0}' format state salt utils mac_utils execute_return_success cmd return salt utils mac_utils confirm_updated state get_wake_on_network
def request_user_has_webhook_permission permission_type from st2common models db webhook import WebhookDBdef decorate func @wraps func def func_wrapper *args **kwargs hook '/' join args[1 ] webhook_db WebhookDB name hook resource_db webhook_dbutils assert_request_user_has_resource_db_permission request pecan request resource_db resource_db permission_type permission_type return func *args **kwargs return func_wrapperreturn decorate
def request_user_has_webhook_permission permission_type from st2common models db webhook import WebhookDBdef decorate func @wraps func def func_wrapper *args **kwargs hook '/' join args[1 ] webhook_db WebhookDB name hook resource_db webhook_dbutils assert_request_user_has_resource_db_permission request pecan request resource_db resource_db permission_type permission_type return func *args **kwargs return func_wrapperreturn decorate
@jsonifydef delete_record self arg_dict cmd ['xenstore-rm' '/local/domain/% dom_id s/% path s' % arg_dict ] ret result _run_command cmd return result
def is_field_shared_factory field_name class IsFieldShared permissions BasePermission '\nGrantsaccessifaparticularprofilefieldissharedwiththerequestinguser \n'def has_permission self request view url_username request parser_context get 'kwargs' {} get 'username' '' if request user username lower url_username lower return Trueif request user is_staff return Trueuser get_object_or_404 User username__iexact url_username if field_name in visible_fields user profile user return Trueraise Http404 return IsFieldShared
def is_field_shared_factory field_name class IsFieldShared permissions BasePermission '\nGrantsaccessifaparticularprofilefieldissharedwiththerequestinguser \n'def has_permission self request view url_username request parser_context get 'kwargs' {} get 'username' '' if request user username lower url_username lower return Trueif request user is_staff return Trueuser get_object_or_404 User username__iexact url_username if field_name in visible_fields user profile user return Trueraise Http404 return IsFieldShared
def is_field_shared_factory field_name class IsFieldShared permissions BasePermission '\nGrantsaccessifaparticularprofilefieldissharedwiththerequestinguser \n'def has_permission self request view url_username request parser_context get 'kwargs' {} get 'username' '' if request user username lower url_username lower return Trueif request user is_staff return Trueuser get_object_or_404 User username__iexact url_username if field_name in visible_fields user profile user return Trueraise Http404 return IsFieldShared
def getRoutes app for rule in app _url_map iter_rules methods rule methods difference ['HEAD'] path translate_werkzeug_rule rule rule attributes vars app _endpoints[rule endpoint] copy if 'segment_count' in attributes del attributes['segment_count'] yield KleinRoute methods methods path path endpoint rule endpoint attributes attributes
def parse_flags flags flags_dict show_unknown_flags True separator '' flags_list []mask 1while mask < flags if flags & mask if mask in flags_dict flags_list append flags_dict[mask] elif show_unknown_flags flags_list append '???' mask mask << 1 return separator join flags_list
def reject_fairness experiment num_heads len [flip for flip in experiment if flip] return num_heads < 469 or num_heads > 531
def insert_read_more_link instance if type instance contents Article returnSUMMARY_MAX_LENGTH instance settings get 'SUMMARY_MAX_LENGTH' READ_MORE_LINK instance settings get 'READ_MORE_LINK' None READ_MORE_LINK_FORMAT instance settings get 'READ_MORE_LINK_FORMAT' '<aclass "read-more"href "/{url}">{text}</a>' if not SUMMARY_MAX_LENGTH and READ_MORE_LINK and READ_MORE_LINK_FORMAT returnif hasattr instance '_summary' and instance _summary summary instance _summaryelse summary truncate_html_words instance content SUMMARY_MAX_LENGTH if summary instance content read_more_link READ_MORE_LINK_FORMAT format url instance url text READ_MORE_LINK instance _summary insert_into_last_element summary read_more_link
def BuildClonePostBody file_tuples file_list []for tup in file_tuples path tup[1]tup tup[2 ]file_list append TUPLE_DELIMITER join [path] + list tup return LIST_DELIMITER join file_list
def BuildClonePostBody file_tuples file_list []for tup in file_tuples path tup[1]tup tup[2 ]file_list append TUPLE_DELIMITER join [path] + list tup return LIST_DELIMITER join file_list
def _indent text amount indentation amount * '' return indentation + '\n' + indentation join text split '\n'
def get_schema frame name flavor None keys None con None dtype None pandas_sql pandasSQL_builder con con flavor flavor return pandas_sql _create_sql_schema frame name keys keys dtype dtype
def build_conversion option opt value parser try with open value 'r' as input data pickle load input except raise ConfigurationException 'FileNotFound%s' % value with open value + ' dump' 'w' as output for dk dv in data iteritems output write '[%s]\n\n' % dk if isinstance dv values list output write '\n' join [ '[%d] %d' % vk vv for vk vv in enumerate dv values ] elif isinstance data[k] values dict output write '\n' join [ '[%d] %d' % vk vv for vk vv in dv values iteritems ] else raise ConfigurationException 'Datastoreiscorrupted%s' % value output write '\n\n' exit
def make_user_coach user master_course_key coach_role_on_master_course CourseCcxCoachRole master_course_key coach_role_on_master_course add_users user
def getHost ip try return socket gethostbyaddr ip [0]except Exception return False
def test_add_to_end a dedent '\nclassAbc \ndefabc self \nself x 3\n\nclassTwo Abc \ndefh self \nself\n' b 'defg self \nself 'assert jedi Script a 8 12 'example py' completions assert jedi Script a + b path 'example py' completions a a[ -1 ] + ' \n' assert jedi Script a 8 13 'example py' completions assert jedi Script a + b path 'example py' completions
def test_add_to_end a dedent '\nclassAbc \ndefabc self \nself x 3\n\nclassTwo Abc \ndefh self \nself\n' b 'defg self \nself 'assert jedi Script a 8 12 'example py' completions assert jedi Script a + b path 'example py' completions a a[ -1 ] + ' \n' assert jedi Script a 8 13 'example py' completions assert jedi Script a + b path 'example py' completions
def test_add_to_end a dedent '\nclassAbc \ndefabc self \nself x 3\n\nclassTwo Abc \ndefh self \nself\n' b 'defg self \nself 'assert jedi Script a 8 12 'example py' completions assert jedi Script a + b path 'example py' completions a a[ -1 ] + ' \n' assert jedi Script a 8 13 'example py' completions assert jedi Script a + b path 'example py' completions
def test_add_to_end a dedent '\nclassAbc \ndefabc self \nself x 3\n\nclassTwo Abc \ndefh self \nself\n' b 'defg self \nself 'assert jedi Script a 8 12 'example py' completions assert jedi Script a + b path 'example py' completions a a[ -1 ] + ' \n' assert jedi Script a 8 13 'example py' completions assert jedi Script a + b path 'example py' completions
def constrain_rgb r g b w - min [0 r g b] if w > 0 r + wg + wb + wreturn r g b
def constrain_rgb r g b w - min [0 r g b] if w > 0 r + wg + wb + wreturn r g b
def test_matching_should_be_case_insensitive completer text u'foo'collection [u'Foo' u'FOO' u'fOO']matches completer find_matches text collection assert len matches 3
def scope2index scope descr where None try return scopes index scope except ValueError raise ValueError "{0}{1}hasanunsupportedscopevalue'{2}'" format descr 'from{0}' format where if where else '' scope
def add_ignored_fields patterns assert isinstance patterns list tuple ignored_fields extend patterns
def get_service hass config discovery_info None pushetta_service PushettaNotificationService config[CONF_API_KEY] config[CONF_CHANNEL_NAME] config[CONF_SEND_TEST_MSG] if pushetta_service is_valid return pushetta_service
def _recv_tag_raw sock s sock recv 4 * 4 if len s 16 raise RuntimeError 'Notenoughbytesreceived somethingiswrong Makesurethemne_rt_serverisrunning ' tag Tag *np fromstring s '>i4' n_received 0rec_buff [s]while n_received < tag size n_buffer min 4096 tag size - n_received this_buffer sock recv n_buffer rec_buff append this_buffer n_received + len this_buffer if n_received tag size raise RuntimeError 'Notenoughbytesreceived somethingiswrong Makesurethemne_rt_serverisrunning ' buff '' join rec_buff return tag buff
def unmountvolume volume putaway volume
def set_device request device u'' response redirect add_cache_bypass next_url request or u'/' set_cookie response u'mezzanine-device' device 60 * 60 * 24 * 365 return response
def set_device request device u'' response redirect add_cache_bypass next_url request or u'/' set_cookie response u'mezzanine-device' device 60 * 60 * 24 * 365 return response
def set_device request device u'' response redirect add_cache_bypass next_url request or u'/' set_cookie response u'mezzanine-device' device 60 * 60 * 24 * 365 return response
def test_operators for op in Operator inheritors check_export description 'AssertthattheTPOT{}operatorexportsasexpected' format op __name__ yield check_export op
def test_operators for op in Operator inheritors check_export description 'AssertthattheTPOT{}operatorexportsasexpected' format op __name__ yield check_export op
@register filterdef pprint value break_after 10 value to_unicode value return mark_safe u'<span></span>' join [escape value[i i + break_after ] for i in range 0 len value break_after ]
@register filterdef pprint value break_after 10 value to_unicode value return mark_safe u'<span></span>' join [escape value[i i + break_after ] for i in range 0 len value break_after ]
@register filterdef pprint value break_after 10 value to_unicode value return mark_safe u'<span></span>' join [escape value[i i + break_after ] for i in range 0 len value break_after ]
def setup_platform hass config add_devices discovery_info None station config get CONF_STATION zone_id wmo_id config get CONF_ZONE_ID config get CONF_WMO_ID if station is not None if zone_id and wmo_id _LOGGER warning 'Usingconfig"%s" not"%s"and"%s"forBOMsensor' CONF_STATION CONF_ZONE_ID CONF_WMO_ID elif zone_id and wmo_id station '{} {}' format zone_id wmo_id else station closest_station config get CONF_LATITUDE config get CONF_LONGITUDE hass config config_dir if station is None _LOGGER error 'CouldnotgetBOMweatherstationfromlat/lon' return Falserest BOMCurrentData hass station try rest update except ValueError as err _LOGGER error 'ReceivederrorfromBOM_Current %s' err return Falseadd_devices [BOMCurrentSensor rest variable config get CONF_NAME for variable in config[CONF_MONITORED_CONDITIONS]] return True
def params_set module changed Falsemodule params['ovs-vsctl'] module get_bin_path 'ovs-vsctl' True fmt '% ovs-vsctl s-t% timeout sget% table s% record s% col s % key s'cmd fmt % module params _ output _ cmd_run module cmd False if module params['value'] not in output fmt '% ovs-vsctl s-t% timeout sset% table s% record s% col s % key s % value s'cmd fmt % module params rtc _ err cmd_run module cmd if rtc 0 module fail_json msg err changed Truemodule exit_json changed changed
@utils arg 'address' metavar '<ipaddress>' help _ 'NewIPAddress ' @utils arg 'port' metavar '<port>' help _ 'NewPort ' def do_cloudpipe_configure cs args cs cloudpipe update args address args port
def _require_sender_id_is_valid intent sender_id if intent not in SENDER_VALIDATORS raise Exception 'Invalidemailintentstring %s' % intent elif not SENDER_VALIDATORS[intent] sender_id logging error "Invalidsender_id%sforemailwithintent'%s'" % sender_id intent raise Exception "Invalidsender_idforemailwithintent'%s'" % intent
def _init_setting_completions log completion debug 'Initializingsettingcompletion ' _instances[usertypes Completion section] configmodel SettingSectionCompletionModel _instances[usertypes Completion option] {}_instances[usertypes Completion value] {}for sectname in configdata DATA opt_model configmodel SettingOptionCompletionModel sectname _instances[usertypes Completion option][sectname] opt_model_instances[usertypes Completion value][sectname] {}for opt in configdata DATA[sectname] val_model configmodel SettingValueCompletionModel sectname opt _instances[usertypes Completion value][sectname][opt] val_model
def locate_cuda if 'CUDAHOME' in os environ home os environ['CUDAHOME']nvcc pjoin home 'bin' 'nvcc' else default_path pjoin os sep 'usr' 'local' 'cuda' 'bin' nvcc find_in_path 'nvcc' os environ['PATH'] + os pathsep + default_path if nvcc is None raise EnvironmentError 'Thenvccbinarycouldnotbelocatedinyour$PATH Eitheraddittoyourpath orset$CUDAHOME' home os path dirname os path dirname nvcc cudaconfig {'home' home 'nvcc' nvcc 'include' pjoin home 'include' 'lib64' pjoin home 'lib64' }for k v in cudaconfig iteritems if not os path exists v raise EnvironmentError 'TheCUDA%spathcouldnotbelocatedin%s' % k v return cudaconfig
def locate_cuda if 'CUDAHOME' in os environ home os environ['CUDAHOME']nvcc pjoin home 'bin' 'nvcc' else default_path pjoin os sep 'usr' 'local' 'cuda' 'bin' nvcc find_in_path 'nvcc' os environ['PATH'] + os pathsep + default_path if nvcc is None raise EnvironmentError 'Thenvccbinarycouldnotbelocatedinyour$PATH Eitheraddittoyourpath orset$CUDAHOME' home os path dirname os path dirname nvcc cudaconfig {'home' home 'nvcc' nvcc 'include' pjoin home 'include' 'lib64' pjoin home 'lib64' }for k v in cudaconfig iteritems if not os path exists v raise EnvironmentError 'TheCUDA%spathcouldnotbelocatedin%s' % k v return cudaconfig
@cache_permissiondef can_edit_subproject user project return check_permission user project 'trans change_subproject'
def literalquery statement dialect None if isinstance statement sqlalchemy orm Query if dialect is None dialect statement session get_bind statement _mapper_zero_or_none dialectstatement statement statementif dialect is None dialect getattr statement bind 'dialect' None if dialect is None from sqlalchemy dialects import mysqldialect mysql dialect Compiler type statement _compiler dialect class LiteralCompiler Compiler visit_bindparam Compiler render_literal_bindparamdef render_literal_value self value type_ if isinstance value Decimal long return str value elif isinstance value datetime return repr str value else value super LiteralCompiler self render_literal_value value type_ if isinstance value unicode return value encode 'UTF-8' else return valuereturn LiteralCompiler dialect statement
def s_one_pre topics s_one_pre []for top_words in topics s_one_pre_t []for w_prime_index w_prime in enumerate top_words[1 ] for w_star in top_words[ w_prime_index + 1 ] s_one_pre_t append w_prime w_star s_one_pre append s_one_pre_t return s_one_pre
def s_one_pre topics s_one_pre []for top_words in topics s_one_pre_t []for w_prime_index w_prime in enumerate top_words[1 ] for w_star in top_words[ w_prime_index + 1 ] s_one_pre_t append w_prime w_star s_one_pre append s_one_pre_t return s_one_pre
@register simple_tagdef language_name code return escape force_text Language objects get code code
def __get_image conn vm_ img config get_cloud_config_value 'image' vm_ __opts__ default 'debian-7' search_global False return conn ex_get_image img
def __get_image conn vm_ img config get_cloud_config_value 'image' vm_ __opts__ default 'debian-7' search_global False return conn ex_get_image img
def to_dnf expr simplify False expr sympify expr if not isinstance expr BooleanFunction return exprif simplify return simplify_logic expr 'dnf' True if is_dnf expr return exprexpr eliminate_implications expr return distribute_or_over_and expr
def guvectorize ftylist signature **kws if isinstance ftylist str ftylist [ftylist]def wrap func guvec GUVectorize func signature **kws for fty in ftylist guvec add fty return guvec build_ufunc return wrap
def guvectorize ftylist signature **kws if isinstance ftylist str ftylist [ftylist]def wrap func guvec GUVectorize func signature **kws for fty in ftylist guvec add fty return guvec build_ufunc return wrap
def mquantiles_cimj data prob [0 25 0 5 0 75] alpha 0 05 axis None alpha min alpha 1 - alpha z norm ppf 1 - alpha / 2 0 xq mstats mquantiles data prob alphap 0 betap 0 axis axis smj mjci data prob axis axis return xq - z * smj xq + z * smj
def run start_date datetime date 2013 8 27 approve '"action" %s' % rvw ACTION_APPROVE reject '"action" %s' % rvw ACTION_REJECT al ActivityLog objects filter Q _details__contains approve Q _details__contains reject action amo LOG THEME_REVIEW id created__lte start_date for chunk in chunked al 50 _batch_award_points delay chunk
def run start_date datetime date 2013 8 27 approve '"action" %s' % rvw ACTION_APPROVE reject '"action" %s' % rvw ACTION_REJECT al ActivityLog objects filter Q _details__contains approve Q _details__contains reject action amo LOG THEME_REVIEW id created__lte start_date for chunk in chunked al 50 _batch_award_points delay chunk
def run start_date datetime date 2013 8 27 approve '"action" %s' % rvw ACTION_APPROVE reject '"action" %s' % rvw ACTION_REJECT al ActivityLog objects filter Q _details__contains approve Q _details__contains reject action amo LOG THEME_REVIEW id created__lte start_date for chunk in chunked al 50 _batch_award_points delay chunk
def run start_date datetime date 2013 8 27 approve '"action" %s' % rvw ACTION_APPROVE reject '"action" %s' % rvw ACTION_REJECT al ActivityLog objects filter Q _details__contains approve Q _details__contains reject action amo LOG THEME_REVIEW id created__lte start_date for chunk in chunked al 50 _batch_award_points delay chunk
def run start_date datetime date 2013 8 27 approve '"action" %s' % rvw ACTION_APPROVE reject '"action" %s' % rvw ACTION_REJECT al ActivityLog objects filter Q _details__contains approve Q _details__contains reject action amo LOG THEME_REVIEW id created__lte start_date for chunk in chunked al 50 _batch_award_points delay chunk
def xfail reason '' __tracebackhide__ Trueraise XFailed reason
def create_network_acl_entry network_acl_id None rule_number None protocol None rule_action None cidr_block None egress None network_acl_name None icmp_code None icmp_type None port_range_from None port_range_to None region None key None keyid None profile None kwargs locals return _create_network_acl_entry **kwargs
def AbsoluteNode node if node attributes for name value in node attributes items if name in ['InheritedPropertySheets' 'RelativePath' 'AdditionalIncludeDirectories' 'IntermediateDirectory' 'OutputDirectory' 'AdditionalLibraryDirectories'] path_list value split ' ' new_list FixFilenames path_list os path dirname ARGUMENTS[1] node setAttribute name ' ' join new_list if not value node removeAttribute name
def AbsoluteNode node if node attributes for name value in node attributes items if name in ['InheritedPropertySheets' 'RelativePath' 'AdditionalIncludeDirectories' 'IntermediateDirectory' 'OutputDirectory' 'AdditionalLibraryDirectories'] path_list value split ' ' new_list FixFilenames path_list os path dirname ARGUMENTS[1] node setAttribute name ' ' join new_list if not value node removeAttribute name
def set_preprint_providers providers {'osf' 'OpenScienceFramework' 'socarxiv' 'SocArXiv' 'engrxiv' 'EngrXiv' 'psyarxiv' 'PsyArXiv'}for key value in providers items provider factories PreprintProviderFactory provider _id keyprovider name valuetry provider save except KeyExistsException continue
def test_cache_metadata config_stub tmpdir config_stub data {'storage' {'cache-size' 1024} 'general' {'private-browsing' False}}url 'http //qutebrowser org'metadata QNetworkCacheMetaData metadata setUrl QUrl url assert metadata isValid disk_cache cache DiskCache str tmpdir device disk_cache prepare metadata device write 'foobar' disk_cache insert device assert disk_cache metaData QUrl url metadata
def cleanup_stubs pass
def md5_hash text return md5 to_bytes text hexdigest
def create_welcome_forum if User query count < 1 return Falseuser User query filter_by id 1 first category Category title u'MyCategory' position 1 category save forum Forum title u'Welcome' description u'Yourfirstforum' category_id category id forum save topic Topic title u'Welcome ' post Post content u'HavefunwithyournewFlaskBBForum ' topic save user user forum forum post post return True
def new key msg None digestmod None return HMAC key msg digestmod
def _process_glsl_template template colors for i in range len colors - 1 -1 -1 color colors[i]assert len color 4 vec4_color 'vec4 % 3f % 3f % 3f % 3f ' % tuple color template template replace '$color_%d' % i vec4_color return template
@cli command 'display' @processordef display_cmd images for image in images click echo 'Displaying"%s"' % image filename image show yield image
def ode_linear_coefficients eq func order match return ode_1st_homogeneous_coeff_best eq func order match
def present name profile 'github' **kwargs ret {'name' name 'changes' {} 'result' None 'comment' ''}target __salt__['github get_user'] name profile profile **kwargs if not target ret['result'] Falseret['comment'] 'Couldntfinduser{0}' format name elif isinstance target bool and target ret['comment'] 'User{0}isalreadyintheorg' format name ret['result'] Trueelif not target get 'in_org' False and target get 'membership_state' 'pending' if __opts__['test'] ret['comment'] 'User{0}willbeaddedtotheorg' format name return retresult __salt__['github add_user'] name profile profile **kwargs if result ret['changes'] setdefault 'old' None ret['changes'] setdefault 'new' 'User{0}existsintheorgnow' format name ret['result'] Trueelse ret['result'] Falseret['comment'] 'Failedtoadduser{0}totheorg' format name else ret['comment'] 'User{0}hasalreadybeeninvited ' format name ret['result'] Truereturn ret
def read_git_file path result Noneif path and is_git_file path header u'gitdir 'data core read path strip if data startswith header result data[len header ]return result
def function_variable *a **kw return a kw
def InlineLegend chart show Falselabels []label_positions []for series in chart data if series label is None labels append '' else labels append series label show Truelabel_positions append series data[ -1 ] if show chart right min chart left minchart right max chart left maxchart right labels labelschart right label_positions label_positionschart _show_legend False
def InlineLegend chart show Falselabels []label_positions []for series in chart data if series label is None labels append '' else labels append series label show Truelabel_positions append series data[ -1 ] if show chart right min chart left minchart right max chart left maxchart right labels labelschart right label_positions label_positionschart _show_legend False
def _tgrep_node_literal_value node return node label if _istree node else text_type node
def generate_pools cidr gateway_ip net netaddr IPNetwork cidr ip_version net versionfirst netaddr IPAddress net first ip_version last netaddr IPAddress net last ip_version if first last return [netaddr IPRange first last ]first_ip first + 1 last_ip last - ip_version 4 if first_ip > last_ip return []ipset netaddr IPSet netaddr IPRange first_ip last_ip if gateway_ip ipset remove netaddr IPAddress gateway_ip ip_version return list ipset iter_ipranges
def generate_pools cidr gateway_ip net netaddr IPNetwork cidr ip_version net versionfirst netaddr IPAddress net first ip_version last netaddr IPAddress net last ip_version if first last return [netaddr IPRange first last ]first_ip first + 1 last_ip last - ip_version 4 if first_ip > last_ip return []ipset netaddr IPSet netaddr IPRange first_ip last_ip if gateway_ip ipset remove netaddr IPAddress gateway_ip ip_version return list ipset iter_ipranges
def generate_pools cidr gateway_ip net netaddr IPNetwork cidr ip_version net versionfirst netaddr IPAddress net first ip_version last netaddr IPAddress net last ip_version if first last return [netaddr IPRange first last ]first_ip first + 1 last_ip last - ip_version 4 if first_ip > last_ip return []ipset netaddr IPSet netaddr IPRange first_ip last_ip if gateway_ip ipset remove netaddr IPAddress gateway_ip ip_version return list ipset iter_ipranges
def mapping_delete index doc_type hosts None profile None es _get_instance hosts profile try result es indices delete_mapping index index doc_type doc_type if result get 'acknowledged' False return Trueexcept elasticsearch exceptions NotFoundError return Nonereturn None
def register_linker name linker if name in predefined_linkers raise ValueError 'Linkernamealreadytaken %s' % name predefined_linkers[name] linker
def register_linker name linker if name in predefined_linkers raise ValueError 'Linkernamealreadytaken %s' % name predefined_linkers[name] linker
def _set_rpm_probes probes return __salt__['probes set_probes'] _ordered_dict_to_dict probes commit False
def _set_rpm_probes probes return __salt__['probes set_probes'] _ordered_dict_to_dict probes commit False
def i0 x return tt switch tt lt x 5 1 + x ** 2 / 4 + x ** 4 / 64 + x ** 6 / 2304 + x ** 8 / 147456 + x ** 10 / 14745600 + x ** 12 / 2123366400 np e ** x / 2 * np pi * x ** 0 5 * 1 + 1 / 8 * x + 9 / 128 * x ** 2 + 225 / 3072 * x ** 3 + 11025 / 98304 * x ** 4
def strip_unneeded bkts sufficient_funds bkts sorted bkts key lambda bkt bkt value for i in range len bkts if not sufficient_funds bkts[ i + 1 ] return bkts[i ]return bkts
def generate_probable_prime **kwargs exact_bits kwargs pop 'exact_bits' None randfunc kwargs pop 'randfunc' None prime_filter kwargs pop 'prime_filter' lambda x True if kwargs print 'Unknownparameters ' kwargs keys if exact_bits is None raise ValueError 'Missingexact_bitsparameter' if exact_bits < 160 raise ValueError 'Primenumberisnotbigenough ' if randfunc is None randfunc Random new readresult COMPOSITEwhile result COMPOSITE candidate Integer random exact_bits exact_bits randfunc randfunc 1 if not prime_filter candidate continueresult test_probable_prime candidate randfunc return candidate
def _start_date year_ago date today - timedelta days 365 return date year_ago year year_ago month 1
def _start_date year_ago date today - timedelta days 365 return date year_ago year year_ago month 1
def temp_db_filename try h test_db_fname tempfile mkstemp '_BioSQL db' dir '/dev/shm' except OSError h test_db_fname tempfile mkstemp '_BioSQL db' os close h return test_db_fname
def add_s3 command_table session **kwargs utils rename_command command_table 's3' 's3api' command_table['s3'] S3 session
def ParseNolintSuppressions filename raw_line linenum error matched _RE_SUPPRESSION search raw_line if matched category matched group 1 if category in None ' * ' _error_suppressions setdefault None set add linenum elif category startswith ' ' and category endswith ' ' category category[1 -1 ]if category in _ERROR_CATEGORIES _error_suppressions setdefault category set add linenum else error filename linenum 'readability/nolint' 5 'UnknownNOLINTerrorcategory %s' % category
def tree2conllstr t lines [u'' join token for token in tree2conlltags t ]return u'\n' join lines
def setLoggerClass loggingClass assert issubclass loggingClass ILogger 'loggingClassmustsubclassILogger'global _LoggerClass_LoggerClass loggingClass
def _evalcode_python executor code input_type global_dict gdb parse_and_eval 'PyEval_GetGlobals ' local_dict gdb parse_and_eval 'PyEval_GetLocals ' if pointervalue global_dict 0 or pointervalue local_dict 0 raise gdb GdbError 'UnabletofindthelocalsorglobalsofthemostrecentPythonfunction relativetotheselectedframe ' return executor evalcode code input_type global_dict local_dict
def test_colormap_endian cmap cm get_cmap u'jet' a [ -0 5 0 0 5 1 1 5 np nan]for dt in [u'f2' u'f4' u'f8'] anative np ma masked_invalid np array a dtype dt aforeign anative byteswap newbyteorder assert_array_equal cmap anative cmap aforeign
def test_colormap_endian cmap cm get_cmap u'jet' a [ -0 5 0 0 5 1 1 5 np nan]for dt in [u'f2' u'f4' u'f8'] anative np ma masked_invalid np array a dtype dt aforeign anative byteswap newbyteorder assert_array_equal cmap anative cmap aforeign
def proportions_chisquare_allpairs count nobs multitest_method 'hs' all_pairs lzip *np triu_indices len count 1 pvals [proportions_chisquare count[list pair ] nobs[list pair ] [1] for pair in all_pairs]return AllPairsResults pvals all_pairs multitest_method multitest_method
def proportions_chisquare_allpairs count nobs multitest_method 'hs' all_pairs lzip *np triu_indices len count 1 pvals [proportions_chisquare count[list pair ] nobs[list pair ] [1] for pair in all_pairs]return AllPairsResults pvals all_pairs multitest_method multitest_method
def required_modules_error name docstring modules required_module_list docstring if not modules return ''filename os path basename name split ' ' [0]msg "'{0}'requiresthesepythonmodules {1}"return msg format filename ' ' join modules
def required_modules_error name docstring modules required_module_list docstring if not modules return ''filename os path basename name split ' ' [0]msg "'{0}'requiresthesepythonmodules {1}"return msg format filename ' ' join modules
def required_modules_error name docstring modules required_module_list docstring if not modules return ''filename os path basename name split ' ' [0]msg "'{0}'requiresthesepythonmodules {1}"return msg format filename ' ' join modules
def set_default_retry_params retry_params _thread_local_settings default_retry_params copy copy retry_params
def _nice_case line l line lower s ''i 0nextCap 1while i < len l c l[i]if c > 'a' and c < 'z' and nextCap c c upper nextCap 0elif c '' or c ' ' or c ' ' or c ' ' or c ' ' or c ' DCTB ' or c '-' or c '_' nextCap 1s + ci + 1return s
def enforce_le_validity domain domain enforce_domain_sanity domain if not re match '^[A-Za-z0-9 -]*$' domain raise errors ConfigurationError '{0}containsaninvalidcharacter ValidcharactersareA-Z a-z 0-9 and- ' format domain labels domain split ' ' if len labels < 2 raise errors ConfigurationError '{0}needsatleasttwolabels' format domain for label in labels if label startswith '-' raise errors ConfigurationError 'label"{0}"indomain"{1}"cannotstartwith"-"' format label domain if label endswith '-' raise errors ConfigurationError 'label"{0}"indomain"{1}"cannotendwith"-"' format label domain return domain
def enforce_le_validity domain domain enforce_domain_sanity domain if not re match '^[A-Za-z0-9 -]*$' domain raise errors ConfigurationError '{0}containsaninvalidcharacter ValidcharactersareA-Z a-z 0-9 and- ' format domain labels domain split ' ' if len labels < 2 raise errors ConfigurationError '{0}needsatleasttwolabels' format domain for label in labels if label startswith '-' raise errors ConfigurationError 'label"{0}"indomain"{1}"cannotstartwith"-"' format label domain if label endswith '-' raise errors ConfigurationError 'label"{0}"indomain"{1}"cannotendwith"-"' format label domain return domain
@receiver post_save def create_profile sender instance created **kwargs if sender get_user_model user instanceprofile_model get_profile_model if profile_model UserProfile and created profile new UserProfile objects get_or_create user instance
@receiver post_save def create_profile sender instance created **kwargs if sender get_user_model user instanceprofile_model get_profile_model if profile_model UserProfile and created profile new UserProfile objects get_or_create user instance
def get_cap_alert_addresses_opts T current Tgtable current s3db pr_grouprows current db gtable deleted True select gtable id gtable name return [ row id s3_str T row name for row in rows]
def chained func def wrapper *args **kwargs for xs in func *args **kwargs for x in xs yield x return wrapper
def chained func def wrapper *args **kwargs for xs in func *args **kwargs for x in xs yield x return wrapper
def trace msg html False write msg 'TRACE' html
def trace msg html False write msg 'TRACE' html
def selStochasticUniversalSampling individuals k s_inds sorted individuals key attrgetter 'fitness' reverse True sum_fits sum ind fitness values[0] for ind in individuals distance sum_fits / float k start random uniform 0 distance points [ start + i * distance for i in xrange k ]chosen []for p in points i 0sum_ s_inds[i] fitness values[0]while sum_ < p i + 1sum_ + s_inds[i] fitness values[0]chosen append s_inds[i] return chosen
def create_catalog_api_client user catalog_integration scopes ['email' 'profile']expires_in settings OAUTH_ID_TOKEN_EXPIRATIONjwt JwtBuilder user build_token scopes expires_in return EdxRestApiClient catalog_integration internal_api_url jwt jwt
def create_catalog_api_client user catalog_integration scopes ['email' 'profile']expires_in settings OAUTH_ID_TOKEN_EXPIRATIONjwt JwtBuilder user build_token scopes expires_in return EdxRestApiClient catalog_integration internal_api_url jwt jwt
def check_solutions eq s diophantine eq factors Mul make_args eq var list eq free_symbols var sort key default_sort_key while s solution s pop for f in factors if diop_simplify f subs zip var solution 0 breakelse return Falsereturn True
def check_solutions eq s diophantine eq factors Mul make_args eq var list eq free_symbols var sort key default_sort_key while s solution s pop for f in factors if diop_simplify f subs zip var solution 0 breakelse return Falsereturn True
def has_sub_tasks task if istask task return Trueelif isinstance task list return any has_sub_tasks i for i in task else return False
def draw_shell G **kwargs nlist kwargs get 'nlist' None if nlist is not None del kwargs['nlist']draw G shell_layout G nlist nlist **kwargs
def strip_sys_meta_prefix server_type key return key[len get_sys_meta_prefix server_type ]
def placeholder shape None ndim None dtype None sparse False name None if dtype is None dtype floatx if not shape if ndim shape tuple [None for _ in range ndim ] if sparse x tf sparse_placeholder dtype name name x _dims len shape else x tf placeholder dtype shape shape name name x _keras_shape shapex _uses_learning_phase Falsereturn x
def test_ast_bad_assert cant_compile u' assert ' cant_compile u' assert123 ' cant_compile u' assert1[12]3 '
def _is_number_geographical numobj num_type number_type numobj return num_type PhoneNumberType FIXED_LINE or num_type PhoneNumberType FIXED_LINE_OR_MOBILE or numobj country_code in _GEO_MOBILE_COUNTRIES and num_type PhoneNumberType MOBILE
def setup_editor qtbot text 'a 1\nprint a \n\nx 2'editorStack EditorStack None [] editorStack set_introspector Mock editorStack set_find_widget Mock editorStack set_io_actions Mock Mock Mock Mock finfo editorStack new 'foo py' 'utf-8' text qtbot addWidget editorStack return editorStack finfo editor
def setup_editor qtbot text 'a 1\nprint a \n\nx 2'editorStack EditorStack None [] editorStack set_introspector Mock editorStack set_find_widget Mock editorStack set_io_actions Mock Mock Mock Mock finfo editorStack new 'foo py' 'utf-8' text qtbot addWidget editorStack return editorStack finfo editor
def setup_editor qtbot text 'a 1\nprint a \n\nx 2'editorStack EditorStack None [] editorStack set_introspector Mock editorStack set_find_widget Mock editorStack set_io_actions Mock Mock Mock Mock finfo editorStack new 'foo py' 'utf-8' text qtbot addWidget editorStack return editorStack finfo editor
def setup_editor qtbot text 'a 1\nprint a \n\nx 2'editorStack EditorStack None [] editorStack set_introspector Mock editorStack set_find_widget Mock editorStack set_io_actions Mock Mock Mock Mock finfo editorStack new 'foo py' 'utf-8' text qtbot addWidget editorStack return editorStack finfo editor
def reply_published_cb sender user reply trivial **kwargs siteconfig SiteConfiguration objects get_current if siteconfig get u'mail_send_review_mail' and not trivial mail_reply reply user
def test_ast_good_lambda can_compile u' lambda[] ' can_compile u' lambda[]1 '
def _get_user_model custom_model getattr settings setting_name 'USER_MODEL' None if custom_model return module_member custom_model try from mongoengine django mongo_auth models import get_user_documentreturn get_user_document except ImportError return module_member 'mongoengine django auth User'
def fmt_whitespace value value WHITESPACE_RE sub u'<spanclass "hlspace">\\1</span>' value value value replace u' DCTB ' SPACE_TAB format _ u'Tabcharacter' return value
def get_variables scope None suffix None candidates tf get_collection MODEL_VARIABLES scope [ ]if suffix is not None candidates [var for var in candidates if var op name endswith suffix ]return candidates
def subf pattern format string count 0 flags 0 pos None endpos None concurrent None **kwargs return _compile pattern flags kwargs subf format string count pos endpos concurrent
def worker_activate worker lbn profile 'default' return _worker_ctl worker lbn 'a' profile
def worker_activate worker lbn profile 'default' return _worker_ctl worker lbn 'a' profile
def test_cp12403 superConsole SendKeys 'outputRedirectStart{ }{ }{ENTER}' superConsole SendKeys 'raiseException{ }"Somestringexception"{ }{ENTER}' expected ['Traceback mostrecentcalllast ' 'File"<stdin>" line1 in<module>' 'Exception Somestringexception' '']superConsole SendKeys 'outputRedirectStop{ }{ }{ENTER}' AreEqual removePrompts getTestOutput [0] [] errlines getTestOutput [1]for i in xrange len errlines Assert errlines[i] startswith expected[i] str errlines + ' ' + str expected
def test_cp12403 superConsole SendKeys 'outputRedirectStart{ }{ }{ENTER}' superConsole SendKeys 'raiseException{ }"Somestringexception"{ }{ENTER}' expected ['Traceback mostrecentcalllast ' 'File"<stdin>" line1 in<module>' 'Exception Somestringexception' '']superConsole SendKeys 'outputRedirectStop{ }{ }{ENTER}' AreEqual removePrompts getTestOutput [0] [] errlines getTestOutput [1]for i in xrange len errlines Assert errlines[i] startswith expected[i] str errlines + ' ' + str expected
def clean_xml_string s return u'' join c for c in s if is_valid_xml_char_ordinal ord c
def FindRendererForObject rdf_obj return ValueRenderer rdf_obj
def _parallel_predict_log_proba estimators estimators_features X n_classes n_samples X shape[0]log_proba np empty n_samples n_classes log_proba fill - np inf all_classes np arange n_classes dtype np int for estimator features in zip estimators estimators_features log_proba_estimator estimator predict_log_proba X[ features] if n_classes len estimator classes_ log_proba np logaddexp log_proba log_proba_estimator else log_proba[ estimator classes_] np logaddexp log_proba[ estimator classes_] log_proba_estimator[ range len estimator classes_ ] missing np setdiff1d all_classes estimator classes_ log_proba[ missing] np logaddexp log_proba[ missing] - np inf return log_proba
def _parallel_predict_log_proba estimators estimators_features X n_classes n_samples X shape[0]log_proba np empty n_samples n_classes log_proba fill - np inf all_classes np arange n_classes dtype np int for estimator features in zip estimators estimators_features log_proba_estimator estimator predict_log_proba X[ features] if n_classes len estimator classes_ log_proba np logaddexp log_proba log_proba_estimator else log_proba[ estimator classes_] np logaddexp log_proba[ estimator classes_] log_proba_estimator[ range len estimator classes_ ] missing np setdiff1d all_classes estimator classes_ log_proba[ missing] np logaddexp log_proba[ missing] - np inf return log_proba
def test_keypoints_censure_moon_image_star detector CENSURE mode 'star' detector detect rescale img 0 25 expected_keypoints np array [[23 27] [29 89] [30 86] [107 59] [109 64] [111 67] [113 70]] expected_scales np array [3 2 4 2 5 3 2] assert_array_equal expected_keypoints detector keypoints assert_array_equal expected_scales detector scales
def enum *sequential return type str u'Enum' dict zip sequential sequential
def start *args **kwargs return _query 'server/start'
def lookupService name timeout None return getResolver lookupService name timeout
def strip_whitespace tokens for i token in enumerate tokens if token type u'S' breakelse return []tokens tokens[i ]while tokens and tokens[ -1 ] type u'S' tokens pop return tokens
def _calculate_meta meta bases winner metafor base in bases base_meta type base if issubclass winner base_meta continueif issubclass base_meta winner winner base_metacontinueraise TypeError 'metaclassconflict themetaclassofaderivedclassmustbea non-strict subclassofthemetaclassesofallitsbases' return winner
def isTipcAvailable if not hasattr socket 'AF_TIPC' return Falseif not os path isfile '/proc/modules' return Falsewith open '/proc/modules' as f for line in f if line startswith 'tipc' return Truereturn False
def isTipcAvailable if not hasattr socket 'AF_TIPC' return Falseif not os path isfile '/proc/modules' return Falsewith open '/proc/modules' as f for line in f if line startswith 'tipc' return Truereturn False
def isTipcAvailable if not hasattr socket 'AF_TIPC' return Falseif not os path isfile '/proc/modules' return Falsewith open '/proc/modules' as f for line in f if line startswith 'tipc' return Truereturn False
def isTipcAvailable if not hasattr socket 'AF_TIPC' return Falseif not os path isfile '/proc/modules' return Falsewith open '/proc/modules' as f for line in f if line startswith 'tipc' return Truereturn False
@publicdef xfield symbols domain order lex _field FracField symbols domain order return _field _field gens
@publicdef xfield symbols domain order lex _field FracField symbols domain order return _field _field gens
def auth_info encoded_info request headers get 'X-Endpoint-API-UserInfo' None if encoded_info info_json _base64_decode encoded_info user_info json loads info_json else user_info {'id' 'anonymous'}return jsonify user_info
def ec2_error req request_id code message LOG error _ '% code s % message s' % locals resp webob Response resp status 400resp headers['Content-Type'] 'text/xml'resp body str '<?xmlversion "1 0"?>\n<Response><Errors><Error><Code>%s</Code><Message>%s</Message></Error></Errors><RequestID>%s</RequestID></Response>' % utils xhtml_escape utils utf8 code utils xhtml_escape utils utf8 message utils xhtml_escape utils utf8 request_id return resp
@contextmanagerdef in_dir dir None cwd os getcwd if dir is None yield cwd returnos chdir dir yield dir os chdir cwd
@contextmanagerdef in_dir dir None cwd os getcwd if dir is None yield cwd returnos chdir dir yield dir os chdir cwd
@contextmanagerdef in_dir dir None cwd os getcwd if dir is None yield cwd returnos chdir dir yield dir os chdir cwd
@contextmanagerdef in_dir dir None cwd os getcwd if dir is None yield cwd returnos chdir dir yield dir os chdir cwd
@contextmanagerdef in_dir dir None cwd os getcwd if dir is None yield cwd returnos chdir dir yield dir os chdir cwd
def test_resize_promo_img resize_size [1050]final_size [ 1050 591 640 360 320 180 ]_promo_img_uploader resize_size final_size
def organization_follower_list context data_dict _check_access 'organization_follower_list' context data_dict return _follower_list context data_dict ckan logic schema default_follow_group_schema context['model'] UserFollowingGroup
def build_summary layout level 1 assert level > 0 level - 1summary List klass u'summary' for child in layout children if not isinstance child Section continuelabel layout_title child if not label and not child id continueif not child id child id label replace '' '-' node Link u'#' + child id label label or child id if level and [n for n in child children if isinstance n Section ] node Paragraph [node build_summary child level ] summary append node return summary
def randu nchars return '' join np random choice RANDU_CHARS nchars
def test_temporary with pytest raises falcon http_status HTTPStatus as redirect hug redirect temporary '/' assert '307' in redirect value status
def set_cache_dir cache_dir if cache_dir is not None and not op exists cache_dir raise IOError 'Directory%sdoesnotexist' % cache_dir set_config 'MNE_CACHE_DIR' cache_dir set_env False
def collect_feature_locations paths strict True locations []for path in paths if os path isdir path for dirpath dirnames filenames in os walk path dirnames sort for filename in sorted filenames if filename endswith ' feature' location FileLocation os path join dirpath filename locations append location elif path startswith '@' locations extend FeatureListParser parse_file path[1 ] else location FileLocationParser parse path if not location filename endswith ' feature' raise InvalidFilenameError location filename elif location exists locations append location elif strict raise FileNotFoundError path return locations
def getRotatedComplexLists planeAngle pointLists rotatedComplexLists []for pointList in pointLists rotatedComplexLists append getRotatedComplexes planeAngle pointList return rotatedComplexLists
def histogram_bins timeseries series scipy array [x[1] for x in timeseries] t tail_avg timeseries h np histogram series bins 15 bins h[1]for index bin_size in enumerate h[0] if bin_size < 20 if index 0 if t < bins[0] return Trueelif t > bins[index] and t < bins[ index + 1 ] return Truereturn False
def histogram_bins timeseries series scipy array [x[1] for x in timeseries] t tail_avg timeseries h np histogram series bins 15 bins h[1]for index bin_size in enumerate h[0] if bin_size < 20 if index 0 if t < bins[0] return Trueelif t > bins[index] and t < bins[ index + 1 ] return Truereturn False
def ping host None core_name None ret _get_return_dict if _get_none_or_value core_name is None and _check_for_cores success Truefor name in __opts__['solr cores'] resp _get_admin_info 'ping' host host core_name name if resp['success'] data {name {'status' resp['data']['status']}}else success Falsedata {name {'status' None}}ret _update_return_dict ret success data resp['errors'] return retelse resp _get_admin_info 'ping' host host core_name core_name return resp
def select_device device_id context devices get_context device_id return context device
def read_element_string stream size return _read stream size decode 'ascii'
def static path try return staticfiles_storage url path except ValueError return settings STATIC_URL + path
def test_show_examples skip_if_no_matplotlib skip_if_no_data with open 'temp yaml' 'w' as f f write "\n obj pylearn2 datasets mnist MNIST{\nwhich_set 'train'\n}\n" show_examples 'temp yaml' 28 28 out 'garbage png' os remove 'temp yaml'
def _contains_bad_names file_names return any [xml_unsafe search f for f in file_names]
def _contains_bad_names file_names return any [xml_unsafe search f for f in file_names]
def write trees handle plain False **kwargs return Writer trees write handle plain plain **kwargs
@doctest_depends_on modules 'lxml' def c2p mml simple False if not mml startswith '<math' mml add_mathml_headers mml if simple return apply_xsl mml 'mathml/data/simple_mmlctop xsl' return apply_xsl mml 'mathml/data/mmlctop xsl'
@doctest_depends_on modules 'lxml' def c2p mml simple False if not mml startswith '<math' mml add_mathml_headers mml if simple return apply_xsl mml 'mathml/data/simple_mmlctop xsl' return apply_xsl mml 'mathml/data/mmlctop xsl'
def snapshot name suffix None return _virt_call name 'snapshot' 'saved' 'Snapshothasbeentaken' suffix suffix
def ppa name auto_accept True keyserver None assert name startswith 'ppa ' user repo name[4 ] split '/' 2 release float distrib_release if release > 12 04 repo repo replace ' ' '_' auto_accept '--yes' if auto_accept else '' else auto_accept ''if not isinstance keyserver basestring and keyserver keyserver keyserver[0]if keyserver keyserver '--keyserver' + keyserver else keyserver ''distrib distrib_codename source '/etc/apt/sources list d/% user s-% repo s-% distrib s list' % locals if not is_file source if release > 14 04 package 'software-properties-common' else package 'python-software-properties' run_as_root 'add-apt-repository% auto_accept s% keyserver s% name s' % locals pty False update_index
def remove_featured_activity activity_type activity_id featured_references get_featured_activity_references activity_reference_found Falsenew_activity_references []for reference in featured_references if reference type activity_type or reference id activity_id new_activity_references append reference else activity_reference_found Trueif activity_reference_found logging info 'The%swithid%swasremovedfromthefeaturedlist ' % activity_type activity_id update_featured_activity_references new_activity_references
def pci_device_get_all_by_node context node_id return IMPL pci_device_get_all_by_node context node_id
def textile text head_offset 0 html_type 'xhtml' encoding None output None return Textile textile text head_offset head_offset html_type html_type
def textile text head_offset 0 html_type 'xhtml' encoding None output None return Textile textile text head_offset head_offset html_type html_type
def test_nonexistent_extra_warns_user_no_wheel script data result script pip 'install' '--no-binary all ' '--no-index' '--find-links ' + data find_links 'simple[nonexistent]' expect_stderr True assert "simple3 0doesnotprovidetheextra'nonexistent'" in result stderr
def test_nonexistent_extra_warns_user_no_wheel script data result script pip 'install' '--no-binary all ' '--no-index' '--find-links ' + data find_links 'simple[nonexistent]' expect_stderr True assert "simple3 0doesnotprovidetheextra'nonexistent'" in result stderr
def test_nonexistent_extra_warns_user_no_wheel script data result script pip 'install' '--no-binary all ' '--no-index' '--find-links ' + data find_links 'simple[nonexistent]' expect_stderr True assert "simple3 0doesnotprovidetheextra'nonexistent'" in result stderr
def test_nonexistent_extra_warns_user_no_wheel script data result script pip 'install' '--no-binary all ' '--no-index' '--find-links ' + data find_links 'simple[nonexistent]' expect_stderr True assert "simple3 0doesnotprovidetheextra'nonexistent'" in result stderr
def test_nonexistent_extra_warns_user_no_wheel script data result script pip 'install' '--no-binary all ' '--no-index' '--find-links ' + data find_links 'simple[nonexistent]' expect_stderr True assert "simple3 0doesnotprovidetheextra'nonexistent'" in result stderr
def test_nonexistent_extra_warns_user_no_wheel script data result script pip 'install' '--no-binary all ' '--no-index' '--find-links ' + data find_links 'simple[nonexistent]' expect_stderr True assert "simple3 0doesnotprovidetheextra'nonexistent'" in result stderr
def test_nonexistent_extra_warns_user_no_wheel script data result script pip 'install' '--no-binary all ' '--no-index' '--find-links ' + data find_links 'simple[nonexistent]' expect_stderr True assert "simple3 0doesnotprovidetheextra'nonexistent'" in result stderr
def user_home request return shortcuts redirect horizon get_user_home request user
def suiteVisit suite visitor warnings warn 'TestvisitorsdeprecatedinTwisted8 0' category DeprecationWarning for case in suite _tests visit getattr case 'visit' None if visit is not None visit visitor elif isinstance case pyunit TestCase case itrial ITestCase case case visit visitor elif isinstance case pyunit TestSuite suiteVisit case visitor else case visit visitor
def suiteVisit suite visitor warnings warn 'TestvisitorsdeprecatedinTwisted8 0' category DeprecationWarning for case in suite _tests visit getattr case 'visit' None if visit is not None visit visitor elif isinstance case pyunit TestCase case itrial ITestCase case case visit visitor elif isinstance case pyunit TestSuite suiteVisit case visitor else case visit visitor
def set_ key value service None profile None client _get_conn profile client set key value return get key service profile
def access_message user_profile message_id try message Message objects select_related get id message_id except Message DoesNotExist raise JsonableError _ 'Invalidmessage s ' try user_message UserMessage objects select_related get user_profile user_profile message message except UserMessage DoesNotExist user_message Noneif user_message is None if message recipient type Recipient STREAM raise JsonableError _ 'Invalidmessage s ' stream Stream objects get id message recipient type_id if not stream is_public raise JsonableError _ 'Invalidmessage s ' if stream realm user_profile realm raise JsonableError _ 'Invalidmessage s ' return message user_message
def access_message user_profile message_id try message Message objects select_related get id message_id except Message DoesNotExist raise JsonableError _ 'Invalidmessage s ' try user_message UserMessage objects select_related get user_profile user_profile message message except UserMessage DoesNotExist user_message Noneif user_message is None if message recipient type Recipient STREAM raise JsonableError _ 'Invalidmessage s ' stream Stream objects get id message recipient type_id if not stream is_public raise JsonableError _ 'Invalidmessage s ' if stream realm user_profile realm raise JsonableError _ 'Invalidmessage s ' return message user_message
def decov h return DeCov h
def test_angle_format_roundtripping a1 Angle 0 unit u radian a2 Angle 10 unit u degree a3 Angle 0 543 unit u degree a4 Angle u'1d2m3 4s' assert Angle str a1 degree a1 degree assert Angle str a2 degree a2 degree assert Angle str a3 degree a3 degree assert Angle str a4 degree a4 degree ra Longitude u'1h2m3 4s' dec Latitude u'1d2m3 4s' assert_allclose Angle str ra degree ra degree assert_allclose Angle str dec degree dec degree
def test_angle_format_roundtripping a1 Angle 0 unit u radian a2 Angle 10 unit u degree a3 Angle 0 543 unit u degree a4 Angle u'1d2m3 4s' assert Angle str a1 degree a1 degree assert Angle str a2 degree a2 degree assert Angle str a3 degree a3 degree assert Angle str a4 degree a4 degree ra Longitude u'1h2m3 4s' dec Latitude u'1d2m3 4s' assert_allclose Angle str ra degree ra degree assert_allclose Angle str dec degree dec degree
def paranoid_urlparser_method check def check_wrapper parser *args **kwargs return UrlParser perform_paranoid_check parser check *args **kwargs return check_wrapper
def paranoid_urlparser_method check def check_wrapper parser *args **kwargs return UrlParser perform_paranoid_check parser check *args **kwargs return check_wrapper
def mkdir path try os mkdir path except OSError logging error "OSErrorwhilecreatingdir'{0}'" format path return Falsereturn True
@contextlib contextmanagerdef TempChangeField obj field_name new_value old_value getattr obj field_name setattr obj field_name new_value yield old_value setattr obj field_name old_value
@contextlib contextmanagerdef TempChangeField obj field_name new_value old_value getattr obj field_name setattr obj field_name new_value yield old_value setattr obj field_name old_value
def get_file_systems filesystemid None keyid None key None profile None region None **kwargs result Noneclient _get_conn key key keyid keyid profile profile region region if filesystemid response client describe_file_systems FileSystemId filesystemid result response['FileSystems']else response client describe_file_systems result response['FileSystems']while 'NextMarker' in response response client describe_file_systems Marker response['NextMarker'] result extend response['FileSystems'] return result
def find_mapreduce_yaml dir os path dirname __file__ while dir for mr_yaml_name in MR_YAML_NAMES yaml_path os path join dir mr_yaml_name if os path exists yaml_path return yaml_pathparent os path dirname dir if parent dir breakdir parentreturn None
def smoothstep edge0 edge1 x x np clip x - edge0 / edge1 - edge0 0 0 1 0 return x * x * 3 - 2 * x
def get_configuration root getLogger name_levels [ '' logging getLevelName root level ]name_levels extend name logging getLevelName logger level for name logger in root manager loggerDict items if hasattr logger 'level' config_string ' ' join '%s %s' % x for x in name_levels return dict config_string config_string log_json SLogger manager log_json
def get_configuration root getLogger name_levels [ '' logging getLevelName root level ]name_levels extend name logging getLevelName logger level for name logger in root manager loggerDict items if hasattr logger 'level' config_string ' ' join '%s %s' % x for x in name_levels return dict config_string config_string log_json SLogger manager log_json
def ensure_utf8 s if isinstance s unicode return s encode 'utf8' return s
def ensure_utf8 s if isinstance s unicode return s encode 'utf8' return s
def ensure_utf8 s if isinstance s unicode return s encode 'utf8' return s
def _TestBuildArchive tester user_cookie user_id device_id tester GetIdsFromCookie user_cookie request_dict {'email' 'user1@emailscrubbed com'}actual_dict tester SendRequest 'build_archive' user_cookie request_dict tester _CompareResponseDicts 'build_archive' user_id request_dict {} actual_dict return actual_dict
def build_queries_from f regex cap_types cap_pr encoding u'ascii' queries []for line in f matches regex match line if matches query reorder_to list zip matches groups cap_types cap_pr queries append query return queries
def simplegeneric func registry {}def wrapper *args **kw ob args[0]try cls ob __class__except AttributeError cls type ob try mro cls __mro__except AttributeError try class cls cls object passmro cls __mro__[1 ]except TypeError mro object for t in mro if t in registry return registry[t] *args **kw else return func *args **kw try wrapper __name__ func __name__except TypeError AttributeError passdef register typ func None if func is None return lambda f register typ f registry[typ] funcreturn funcwrapper __dict__ func __dict__wrapper __doc__ func __doc__wrapper register registerreturn wrapper
def check device minor _validate_device device try int minor except Exception raise CommandExecutionError 'Invalidminornumberpassedtopartition check' cmd 'parted-m-s{0}check{1}' format device minor out __salt__['cmd run'] cmd splitlines return out
def Normal name mean std return rv name NormalDistribution mean std
@register_uncanonicalize@gof local_optimizer [T MaxAndArgmax] def local_max_and_argmax node if isinstance node op T MaxAndArgmax axis node op get_params node if len node outputs[1] clients 0 new CAReduce scal maximum axis node inputs[0] return [new None]if len node outputs[0] clients 0 return [None T _argmax node inputs[0] axis ]
@register_uncanonicalize@gof local_optimizer [T MaxAndArgmax] def local_max_and_argmax node if isinstance node op T MaxAndArgmax axis node op get_params node if len node outputs[1] clients 0 new CAReduce scal maximum axis node inputs[0] return [new None]if len node outputs[0] clients 0 return [None T _argmax node inputs[0] axis ]
def parse handle format if not isinstance format basestring raise TypeError 'Needastringforthefileformat lowercase ' if not format raise ValueError 'Formatrequired lowercasestring ' if format format lower raise ValueError "Formatstring'%s'shouldbelowercase" % format with as_handle handle 'rU' as fp if format in _FormatToIterator iterator_generator _FormatToIterator[format]i iterator_generator fp else raise ValueError "Unknownformat'%s'" % format for r in i yield r
def test_hexoct class foo object def __hex__ self return selfdef __oct__ self return selfclass bar def __hex__ self return selfdef __oct__ self return selfAssertError TypeError hex foo AssertError TypeError oct foo AssertError TypeError hex bar AssertError TypeError oct bar
def getHeaders document return domhelpers findElements document lambda n m re compile 'h[23]$' match m n nodeName
def getHeaders document return domhelpers findElements document lambda n m re compile 'h[23]$' match m n nodeName
def DNSServiceRegister flags 0 interfaceIndex kDNSServiceInterfaceIndexAny name None regtype _NO_DEFAULT domain None host None port _NO_DEFAULT txtRecord '' callBack None _NO_DEFAULT check regtype _NO_DEFAULT check port port socket htons port if not txtRecord txtLen txtRecord 1 '\x00' else txtLen txtRecord _string_to_length_and_void_p txtRecord @_DNSServiceRegisterReplydef _callback sdRef flags errorCode name regtype domain context if callBack is not None callBack sdRef flags errorCode name decode regtype decode domain decode _global_lock acquire try sdRef _DNSServiceRegister flags interfaceIndex name regtype domain host port txtLen txtRecord _callback None finally _global_lock release sdRef _add_callback _callback return sdRef
def list_members_without_mfa profile 'github' ignore_cache False key 'github {0} non_mfa_users' format _get_config_value profile 'org_name' if key not in __context__ or ignore_cache client _get_client profile organization client get_organization _get_config_value profile 'org_name' filter_key 'filter'if hasattr github Team Team 'membership' filter_key 'filter_'__context__[key] [m login lower for m in _get_members organization {filter_key '2fa_disabled'} ]return __context__[key]
def export_pipeline exported_pipeline pipeline_tree expr_to_tree exported_pipeline pipeline_text generate_import_code exported_pipeline pipeline_text + pipeline_code_wrapper generate_pipeline_code pipeline_tree return pipeline_text
def get_access_token_from_code code redirect_uri app_id app_secret args {'code' code 'redirect_uri' redirect_uri 'client_id' app_id 'client_secret' app_secret}response urllib urlopen 'https //graph facebook com/oauth/access_token' + '?' + urllib urlencode args read query_str parse_qs response if 'access_token' in query_str result {'access_token' query_str['access_token'][0]}if 'expires' in query_str result['expires'] query_str['expires'][0]return resultelse response json loads response raise GraphAPIError response
def remove mod persist False pre_mods lsmod __salt__['cmd run_all'] 'kldunload{0}' format mod python_shell False post_mods lsmod mods _rm_mods pre_mods post_mods persist_mods set if persist persist_mods _remove_persistent_module mod return sorted list mods persist_mods
def nands print 'HelloWorld'
def nands print 'HelloWorld'
def libvlc_media_player_get_title_count p_mi f _Cfunctions get 'libvlc_media_player_get_title_count' None or _Cfunction 'libvlc_media_player_get_title_count' 1 None ctypes c_int MediaPlayer return f p_mi
def test_unsafe_url eq_ 'Allyour{"<ahref "http //xx yy com/grover png"rel "nofollow">xx yy com/grover png</a>"}baseare' linkify 'Allyour{"xx yy com/grover png"}baseare'
def test_unsafe_url eq_ 'Allyour{"<ahref "http //xx yy com/grover png"rel "nofollow">xx yy com/grover png</a>"}baseare' linkify 'Allyour{"xx yy com/grover png"}baseare'
def incr_mean_variance_axis X axis last_mean last_var last_n _raise_error_wrong_axis axis if isinstance X sp csr_matrix if axis 0 return _incr_mean_var_axis0 X last_mean last_mean last_var last_var last_n last_n else return _incr_mean_var_axis0 X T last_mean last_mean last_var last_var last_n last_n elif isinstance X sp csc_matrix if axis 0 return _incr_mean_var_axis0 X last_mean last_mean last_var last_var last_n last_n else return _incr_mean_var_axis0 X T last_mean last_mean last_var last_var last_n last_n else _raise_typeerror X
def incr_mean_variance_axis X axis last_mean last_var last_n _raise_error_wrong_axis axis if isinstance X sp csr_matrix if axis 0 return _incr_mean_var_axis0 X last_mean last_mean last_var last_var last_n last_n else return _incr_mean_var_axis0 X T last_mean last_mean last_var last_var last_n last_n elif isinstance X sp csc_matrix if axis 0 return _incr_mean_var_axis0 X last_mean last_mean last_var last_var last_n last_n else return _incr_mean_var_axis0 X T last_mean last_mean last_var last_var last_n last_n else _raise_typeerror X
def get_user_data userid profile 'grafana' if isinstance profile string_types profile __salt__['config option'] profile response requests get '{0}/api/users/{1}' format profile['grafana_url'] userid auth _get_auth profile headers _get_headers profile timeout profile get 'grafana_timeout' 3 if response status_code > 400 response raise_for_status return response json
def cleanup_sff flowgrams header outhandle None outdir '/tmp' min_length 150 max_length 400 clean_filename ''if not outhandle fd clean_filename mkstemp dir outdir prefix 'cleanup_sff' suffix ' sff txt' close fd outhandle open clean_filename 'w' l filter_sff_file flowgrams header [ lambda f within_length f min_length max_length lambda f f hasProperKey ] outhandle return clean_filename l
def setup_platform hass config add_devices discovery_info None lights []for address device_config in config[CONF_DEVICES] items device {}device['name'] device_config[CONF_NAME]device['key'] device_config[CONF_API_KEY]device['address'] addresslight DecoraLight device if light is_valid lights append light add_devices lights
def test_print_op b tensor fmatrix f theano function [b] theano printing Print b * 2 mode mode_with_gpu topo f maker fgraph toposort assert isinstance topo[0] op GpuFromHost assert isinstance topo[1] op theano printing Print assert isinstance topo[2] op GpuElemwise assert topo[3] op host_from_gpu f numpy random random 5 5 astype 'float32'
def test_print_op b tensor fmatrix f theano function [b] theano printing Print b * 2 mode mode_with_gpu topo f maker fgraph toposort assert isinstance topo[0] op GpuFromHost assert isinstance topo[1] op theano printing Print assert isinstance topo[2] op GpuElemwise assert topo[3] op host_from_gpu f numpy random random 5 5 astype 'float32'
def test_print_op b tensor fmatrix f theano function [b] theano printing Print b * 2 mode mode_with_gpu topo f maker fgraph toposort assert isinstance topo[0] op GpuFromHost assert isinstance topo[1] op theano printing Print assert isinstance topo[2] op GpuElemwise assert topo[3] op host_from_gpu f numpy random random 5 5 astype 'float32'
def loaded_vispy_modules import_module depth None all_modules False vispy_dir os path dirname os path dirname vispy __file__ code "importsys %s print ' ' join sys modules " % import_module res run_subprocess [sys executable '-c' code] cwd vispy_dir [0]loaded_modules [name strip for name in res split ' ' ]if all_modules return loaded_modulesvispy_modules set for m in loaded_modules if m startswith 'vispy' and '__future__' not in m if depth parts m split ' ' m ' ' join parts[ depth] vispy_modules add m return vispy_modules
def loaded_vispy_modules import_module depth None all_modules False vispy_dir os path dirname os path dirname vispy __file__ code "importsys %s print ' ' join sys modules " % import_module res run_subprocess [sys executable '-c' code] cwd vispy_dir [0]loaded_modules [name strip for name in res split ' ' ]if all_modules return loaded_modulesvispy_modules set for m in loaded_modules if m startswith 'vispy' and '__future__' not in m if depth parts m split ' ' m ' ' join parts[ depth] vispy_modules add m return vispy_modules
def loaded_vispy_modules import_module depth None all_modules False vispy_dir os path dirname os path dirname vispy __file__ code "importsys %s print ' ' join sys modules " % import_module res run_subprocess [sys executable '-c' code] cwd vispy_dir [0]loaded_modules [name strip for name in res split ' ' ]if all_modules return loaded_modulesvispy_modules set for m in loaded_modules if m startswith 'vispy' and '__future__' not in m if depth parts m split ' ' m ' ' join parts[ depth] vispy_modules add m return vispy_modules
def _default_content_type_rewriter state if not 'Content-Type' in state headers state headers['Content-Type'] 'text/html'
def pool_health_monitor_create request **kwargs body {'health_monitor' {'type' kwargs['type'] 'delay' kwargs['delay'] 'timeout' kwargs['timeout'] 'max_retries' kwargs['max_retries'] 'http_method' kwargs['http_method'] 'url_path' kwargs['url_path'] 'expected_codes' kwargs['expected_codes'] 'admin_state_up' kwargs['admin_state_up']}}mon quantumclient request create_health_monitor body get 'health_monitor' body {'health_monitor' {'id' mon['id']}}quantumclient request associate_health_monitor kwargs['pool_id'] body return PoolMonitor mon
def verify text None user None filename None gnupghome None gpg _create_gpg user if text verified gpg verify text elif filename with salt utils flopen filename 'rb' as _fp verified gpg verify_file _fp else raise SaltInvocationError 'filenameortextmustbepassed ' ret {}if verified trust_level is not None ret['res'] Trueret['username'] verified usernameret['key_id'] verified key_idret['trust_level'] VERIFY_TRUST_LEVELS[str verified trust_level ]ret['message'] 'Thesignatureisverified 'else ret['res'] Falseret['message'] 'Thesignaturecouldnotbeverified 'return ret
def const_ext x n axis -1 if n < 1 return xleft_end axis_slice x start 0 stop 1 axis axis ones_shape [1] * x ndim ones_shape[axis] nones np ones ones_shape dtype x dtype left_ext ones * left_end right_end axis_slice x start -1 axis axis right_ext ones * right_end ext np concatenate left_ext x right_ext axis axis return ext
def const_ext x n axis -1 if n < 1 return xleft_end axis_slice x start 0 stop 1 axis axis ones_shape [1] * x ndim ones_shape[axis] nones np ones ones_shape dtype x dtype left_ext ones * left_end right_end axis_slice x start -1 axis axis right_ext ones * right_end ext np concatenate left_ext x right_ext axis axis return ext
def test_assumptions m n symbols 'mn' integer True positive True diof diophantine n ** 2 + m * n - 500 assert diof set [ 5 20 40 10 95 5 121 4 248 2 499 1 ] a b symbols 'ab' integer True positive False diof diophantine a * b + 2 * a + 3 * b - 6 assert diof set [ -15 -3 -9 -4 -7 -5 -6 -6 -5 -8 -4 -14 ]
def test_assumptions m n symbols 'mn' integer True positive True diof diophantine n ** 2 + m * n - 500 assert diof set [ 5 20 40 10 95 5 121 4 248 2 499 1 ] a b symbols 'ab' integer True positive False diof diophantine a * b + 2 * a + 3 * b - 6 assert diof set [ -15 -3 -9 -4 -7 -5 -6 -6 -5 -8 -4 -14 ]
def copy self CopySource Bucket Key ExtraArgs None Callback None SourceClient None Config None subscribers Noneif Callback is not None subscribers [ProgressCallbackInvoker Callback ]config Configif config is None config TransferConfig with create_transfer_manager self config as manager future manager copy copy_source CopySource bucket Bucket key Key extra_args ExtraArgs subscribers subscribers source_client SourceClient return future result
def _blockdevicevolume_from_dataset_id dataset_id size attached_to None return BlockDeviceVolume size size attached_to attached_to dataset_id dataset_id blockdevice_id u'block-{0}' format dataset_id
def get_maintenance if not database return Nonemaintenance_state database maintenance find_one {'maintenance' True} if maintenance_state return {'start' maintenance_state get 'start' 'end' maintenance_state get 'end' }else return None
def conv x y mode 2 warnings warn "Usenumpy convolve x y mode 'full' " DeprecationWarning return np convolve x y mode
def gatling registry xml_parent data gatling XML SubElement xml_parent 'io gatling jenkins GatlingPublisher' XML SubElement gatling 'enabled' text 'true'
def gatling registry xml_parent data gatling XML SubElement xml_parent 'io gatling jenkins GatlingPublisher' XML SubElement gatling 'enabled' text 'true'
def SOPform variables minterms dontcares None variables [sympify v for v in variables]if minterms [] return falseminterms [list i for i in minterms]dontcares [list i for i in dontcares or [] ]for d in dontcares if d in minterms raise ValueError '%sinmintermsisalsoindontcares' % d old Nonenew minterms + dontcares while new old old newnew _simplified_pairs old essential _rem_redundancy new minterms return Or *[_convert_to_varsSOP x variables for x in essential]
def SOPform variables minterms dontcares None variables [sympify v for v in variables]if minterms [] return falseminterms [list i for i in minterms]dontcares [list i for i in dontcares or [] ]for d in dontcares if d in minterms raise ValueError '%sinmintermsisalsoindontcares' % d old Nonenew minterms + dontcares while new old old newnew _simplified_pairs old essential _rem_redundancy new minterms return Or *[_convert_to_varsSOP x variables for x in essential]
def SOPform variables minterms dontcares None variables [sympify v for v in variables]if minterms [] return falseminterms [list i for i in minterms]dontcares [list i for i in dontcares or [] ]for d in dontcares if d in minterms raise ValueError '%sinmintermsisalsoindontcares' % d old Nonenew minterms + dontcares while new old old newnew _simplified_pairs old essential _rem_redundancy new minterms return Or *[_convert_to_varsSOP x variables for x in essential]
def SOPform variables minterms dontcares None variables [sympify v for v in variables]if minterms [] return falseminterms [list i for i in minterms]dontcares [list i for i in dontcares or [] ]for d in dontcares if d in minterms raise ValueError '%sinmintermsisalsoindontcares' % d old Nonenew minterms + dontcares while new old old newnew _simplified_pairs old essential _rem_redundancy new minterms return Or *[_convert_to_varsSOP x variables for x in essential]
def stack_partitions dfs divisions join 'outer' meta methods concat [df _meta for df in dfs] join join empty strip_unknown_categories meta name 'concat-{0}' format tokenize *dfs dsk {}i 0for df in dfs dsk update df dask try df _meta meta match Trueexcept ValueError TypeError match Falsefor key in df _keys if match dsk[ name i ] keyelse dsk[ name i ] methods concat [empty key] 0 join i + 1return new_dd_object dsk name meta divisions
def stack_partitions dfs divisions join 'outer' meta methods concat [df _meta for df in dfs] join join empty strip_unknown_categories meta name 'concat-{0}' format tokenize *dfs dsk {}i 0for df in dfs dsk update df dask try df _meta meta match Trueexcept ValueError TypeError match Falsefor key in df _keys if match dsk[ name i ] keyelse dsk[ name i ] methods concat [empty key] 0 join i + 1return new_dd_object dsk name meta divisions
def stack_partitions dfs divisions join 'outer' meta methods concat [df _meta for df in dfs] join join empty strip_unknown_categories meta name 'concat-{0}' format tokenize *dfs dsk {}i 0for df in dfs dsk update df dask try df _meta meta match Trueexcept ValueError TypeError match Falsefor key in df _keys if match dsk[ name i ] keyelse dsk[ name i ] methods concat [empty key] 0 join i + 1return new_dd_object dsk name meta divisions
def mkfs os_type fs_label target run_as_root True specified_fs None mkfs_command _MKFS_COMMAND get os_type _DEFAULT_MKFS_COMMAND or '' % {'fs_label' fs_label 'target' target} if mkfs_command utils execute run_as_root run_as_root *mkfs_command split else if not specified_fs specified_fs CONF default_ephemeral_formatif not specified_fs specified_fs _DEFAULT_FS_BY_OSTYPE get os_type _DEFAULT_FILE_SYSTEM utils mkfs specified_fs target fs_label run_as_root run_as_root
def mkfs os_type fs_label target run_as_root True specified_fs None mkfs_command _MKFS_COMMAND get os_type _DEFAULT_MKFS_COMMAND or '' % {'fs_label' fs_label 'target' target} if mkfs_command utils execute run_as_root run_as_root *mkfs_command split else if not specified_fs specified_fs CONF default_ephemeral_formatif not specified_fs specified_fs _DEFAULT_FS_BY_OSTYPE get os_type _DEFAULT_FILE_SYSTEM utils mkfs specified_fs target fs_label run_as_root run_as_root
def greedyPolicy Ts R discountFactor V dim len V numA len Ts Vnext V * discountFactor + R policy zeros dim numA for si in range dim actions all_argmax [dot T[si ] Vnext for T in Ts] for a in actions policy[ si a ] 1 0 / len actions return policy collapsedTransitions Ts policy
def _get_old_unhelpful old_formatted {}cursor connection cursor cursor execute 'SELECTdoc_id yes no\nFROM\n SELECTwiki_revision document_idasdoc_id \nSUM limitedvotes helpful asyes \nSUM NOT limitedvotes helpful asno\nFROM\n SELECT*FROMwiki_helpfulvote\nWHEREcreated< DATE_SUB CURDATE INTERVAL1WEEK \nANDcreated> DATE_SUB DATE_SUB CURDATE \nINTERVAL1WEEK INTERVAL1WEEK \n aslimitedvotes\nINNERJOINwiki_revisionON\nlimitedvotes revision_id wiki_revision id\nINNERJOINwiki_documentON\nwiki_document id wiki_revision document_id\nWHEREwiki_document locale "en-US"\nGROUPBYdoc_id\nHAVINGno>yes\n ascalculated' old_data cursor fetchall for data in old_data doc_id data[0]yes float data[1] no float data[2] total yes + no if total 0 continueold_formatted[doc_id] {'total' total 'percentage' yes / total }return old_formatted
def to_subscription_key uid event return u'{}_{}' format uid event
def doctest_depends_on exe None modules None disable_viewers None pyglet Falseif modules is not None and 'pyglet' in modules pyglet Truedef depends_on_deco fn fn _doctest_depends_on dict exe exe modules modules disable_viewers disable_viewers pyglet pyglet if inspect isclass fn fn _doctest_depdends_on no_attrs_in_subclass fn fn _doctest_depends_on return fnreturn depends_on_deco
def doctest_depends_on exe None modules None disable_viewers None pyglet Falseif modules is not None and 'pyglet' in modules pyglet Truedef depends_on_deco fn fn _doctest_depends_on dict exe exe modules modules disable_viewers disable_viewers pyglet pyglet if inspect isclass fn fn _doctest_depdends_on no_attrs_in_subclass fn fn _doctest_depends_on return fnreturn depends_on_deco
def doctest_depends_on exe None modules None disable_viewers None pyglet Falseif modules is not None and 'pyglet' in modules pyglet Truedef depends_on_deco fn fn _doctest_depends_on dict exe exe modules modules disable_viewers disable_viewers pyglet pyglet if inspect isclass fn fn _doctest_depdends_on no_attrs_in_subclass fn fn _doctest_depends_on return fnreturn depends_on_deco
def doctest_depends_on exe None modules None disable_viewers None pyglet Falseif modules is not None and 'pyglet' in modules pyglet Truedef depends_on_deco fn fn _doctest_depends_on dict exe exe modules modules disable_viewers disable_viewers pyglet pyglet if inspect isclass fn fn _doctest_depdends_on no_attrs_in_subclass fn fn _doctest_depends_on return fnreturn depends_on_deco
def doctest_depends_on exe None modules None disable_viewers None pyglet Falseif modules is not None and 'pyglet' in modules pyglet Truedef depends_on_deco fn fn _doctest_depends_on dict exe exe modules modules disable_viewers disable_viewers pyglet pyglet if inspect isclass fn fn _doctest_depdends_on no_attrs_in_subclass fn fn _doctest_depends_on return fnreturn depends_on_deco
def get_thumbnailer obj relative_name None if hasattr obj 'easy_thumbnails_thumbnailer' return obj easy_thumbnails_thumbnailerif isinstance obj Thumbnailer return objelif isinstance obj FieldFile if not relative_name relative_name obj namereturn ThumbnailerFieldFile obj instance obj field relative_name source_storage Noneif isinstance obj six string_types relative_name objobj Noneif not relative_name raise ValueError 'IfobjectisnotaFieldFileorThumbnailerinstance therelativenamemustbeprovided' if isinstance obj File obj obj fileif isinstance obj Storage or obj default_storage source_storage objobj Nonereturn Thumbnailer file obj name relative_name source_storage source_storage remote_source obj is not None
def add_model_for_resource resource model _RESOURCE_TO_MODEL_MAP[resource] model
def connect_on_app_finalize callback _on_app_finalizers add callback return callback
@mock_ec2def test_igw_detach_unattached conn boto connect_vpc u'the_key' u'the_secret' igw conn create_internet_gateway vpc conn create_vpc VPC_CIDR with assert_raises EC2ResponseError as cm conn detach_internet_gateway igw id vpc id cm exception code should equal u'Gateway NotAttached' cm exception status should equal 400 cm exception request_id should_not be none
def to_padded_yaml data level 0 indent 2 **kw if data in [None ''] return ''try transformed yaml dump data indent indent allow_unicode True default_flow_style False Dumper AnsibleDumper **kw padded '\n' join [ '' * level * indent + line for line in transformed splitlines ] return to_text '\n{0}' format padded except Exception as my_e raise errors AnsibleFilterError 'Failedtoconvert %s' % my_e
def read_text_file filename if PYTHON3 return open filename 'r' encoding 'utf-8' read else return open filename 'r' read
def _do_mb_delete path return _mb_request path 'DELETE' AUTH_YES True
def path_getrootdir path drive _ os path splitdrive path if drive return drive + os path sep return os path sep
def import_submodules context root_module path for loader module_name is_pkg in pkgutil walk_packages path root_module + ' ' module __import__ module_name globals locals ['__name__'] for k v in six iteritems vars module if not k startswith '_' context[k] vcontext[module_name] module
def get_accessible_discussion_xblocks course user include_all False all_xblocks modulestore get_items course id qualifiers {'category' 'discussion'} include_orphans False return [xblock for xblock in all_xblocks if has_required_keys xblock and include_all or has_access user 'load' xblock course id ]
def convertToPix vertices pos units win unit2pixFunc _unit2PixMappings get units if unit2pixFunc return unit2pixFunc vertices pos win else msg 'Theunittype[{0}]isnotregisteredwithPsychoPy'raise ValueError msg format units
def _get_options ret None defaults {'priority' '0'}attrs {'pushover_profile' 'profile' 'user' 'user' 'device' 'device' 'token' 'token' 'priority' 'priority' 'title' 'title' 'api_version' 'api_version' 'expire' 'expire' 'retry' 'retry' 'sound' 'sound'}profile_attr 'pushover_profile'profile_attrs {'user' 'user' 'device' 'device' 'token' 'token' 'priority' 'priority' 'title' 'title' 'api_version' 'api_version' 'expire' 'expire' 'retry' 'retry' 'sound' 'sound'}_options salt returners get_returner_options __virtualname__ ret attrs profile_attr profile_attr profile_attrs profile_attrs __salt__ __salt__ __opts__ __opts__ defaults defaults return _options
def createKeyPair type bits pkey crypto PKey pkey generate_key type bits return pkey
def getDecoders decoder_paths import_base os path commonprefix decoder_paths split os path sep [ -1 ]decoders {}for path in decoder_paths import_path path split os path sep [len import_base ]for f in glob iglob '%s/* py' % path name os path splitext os path basename f [0]if name '__init__' decoders[name] ' ' join import_path + [name] return decoders
def _should_send_email self try return self get_profile should_send_emailexcept Profile DoesNotExist return True
def _should_send_email self try return self get_profile should_send_emailexcept Profile DoesNotExist return True
def set_random_state estimator random_state 0 if isinstance estimator DBSCAN returnif 'random_state' in estimator get_params estimator set_params random_state random_state
def load_source name pathname if six PY2 import impreturn imp load_source name pathname else loader importlib machinery SourceFileLoader name pathname return loader load_module name
def makeService options service AppService service port int options['port'] return service
def makeService options service AppService service port int options['port'] return service
def findObject name names name split ' ' topLevelPackage NonemoduleNames names[ ]while not topLevelPackage trialname ' ' join moduleNames if len trialname 0 return False None try topLevelPackage __import__ trialname except ImportError exc_info sys exc_info if len traceback extract_tb exc_info[2] > 1 try del sys modules[trialname]except KeyError passraise exc_info[0] exc_info[1] exc_info[2]moduleNames pop obj topLevelPackagefor n in names[1 ] try obj getattr obj n except AttributeError return False obj return True obj
def download_all recommended False restart True to_download _get_available recommended restart for name in to_download download name return list_downloads
@Panel registerdef rate_limit panel task_name rate_limit **kwargs try timeutils rate rate_limit except ValueError as exc return {'error' 'Invalidratelimitstring %s' % exc }try tasks[task_name] rate_limit rate_limitexcept KeyError panel logger error 'Ratelimitattemptforunknowntask%s' task_name exc_info sys exc_info return {'error' 'unknowntask'}if not hasattr panel consumer ready_queue 'refresh' panel logger error 'Ratelimitattempt butratelimitsdisabled ' return {'error' 'ratelimitsdisabled'}panel consumer ready_queue refresh if not rate_limit panel logger info 'Disabledratelimitsfortasksoftype%s' task_name return {'ok' 'ratelimitdisabledsuccessfully'}panel logger info 'Newratelimitfortasksoftype%s %s ' task_name rate_limit return {'ok' 'newratelimitsetsuccessfully'}
def base64_b64decode instr if six PY3 b salt utils to_bytes instr data base64 b64decode b try return salt utils to_str data except UnicodeDecodeError return datareturn base64 b64decode instr
def delete_serving_url blob_key rpc None rpc delete_serving_url_async blob_key rpc rpc get_result
def delete_serving_url blob_key rpc None rpc delete_serving_url_async blob_key rpc rpc get_result
def get_create_or_change_title request instance name_field None if not instance pk return _ u'New%s' % instance _meta verbose_name if name_field name getattr instance name_field None else name u'%s' % instance if name return force_text name return _ u'Unnamed%s' % instance _meta verbose_name
def _logistic_loss_and_grad w X y alpha sample_weight None n_samples n_features X shapegrad np empty_like w w c yz _intercept_dot w X y if sample_weight is None sample_weight np ones n_samples out - np sum sample_weight * log_logistic yz + 0 5 * alpha * np dot w w z expit yz z0 sample_weight * z - 1 * y grad[ n_features] safe_sparse_dot X T z0 + alpha * w if grad shape[0] > n_features grad[ -1 ] z0 sum return out grad
def _resize_error src instance user mkt log mkt LOG VIDEO_ERROR instance user user instance delete
def get_glove_info glove_file_name with smart_open glove_file_name as f num_lines sum 1 for line in f with smart_open glove_file_name as f num_dims len f readline split - 1 return num_lines num_dims
def get_glove_info glove_file_name with smart_open glove_file_name as f num_lines sum 1 for line in f with smart_open glove_file_name as f num_dims len f readline split - 1 return num_lines num_dims
def serialize_num val if isinstance val bool return str int val return str val
def serialize_num val if isinstance val bool return str int val return str val
def serialize_num val if isinstance val bool return str int val return str val
def show_address kwargs None call None if call 'function' raise SaltCloudSystemExit 'Theshow_snapshotfunctionmustbecalledwith-for--function ' if not kwargs or 'name' not in kwargs log error 'Mustspecifyname ' return Falseif not kwargs or 'region' not in kwargs log error 'Mustspecifyregion ' return Falseconn get_conn return _expand_address conn ex_get_address kwargs['name'] kwargs['region']
@event u'manager db_cleanup' def db_cleanup manager session existing_tasks list manager tasks + [None] session query SimpleKeyValue filter ~ SimpleKeyValue task in_ existing_tasks delete synchronize_session False
def setup settings current deployment_settingstasks s3 tasks
def read_worksheet xml_source parent preset_title string_table style_table workbook_name None sheet_codename None if workbook_name and sheet_codename ws IterableWorksheet parent preset_title workbook_name sheet_codename xml_source else ws Worksheet parent preset_title fast_parse ws xml_source string_table style_table return ws
def get_field java_object field_name command proto FIELD_COMMAND_NAME + proto FIELD_GET_SUBCOMMAND_NAME + java_object _target_id + u'\n' + field_name + u'\n' + proto END_COMMAND_PART answer java_object _gateway_client send_command command if answer proto NO_MEMBER_COMMAND or is_error answer [0] raise Py4JError u'nofield{0}inobject{1}' format field_name java_object _target_id else return get_return_value answer java_object _gateway_client java_object _target_id field_name
def kneighbors_graph X n_neighbors mode 'connectivity' metric 'minkowski' p 2 metric_params None include_self False n_jobs 1 if not isinstance X KNeighborsMixin X NearestNeighbors n_neighbors metric metric p p metric_params metric_params n_jobs n_jobs fit X else _check_params X metric p metric_params query _query_include_self X include_self return X kneighbors_graph X query n_neighbors n_neighbors mode mode
def notify_list_member_added e target e['target']if target['screen_name'] c['original_name'] returnsource e['source']target_object [e['target_object']]created_at e['created_at']source_user cycle_color source['name'] + color_func c['NOTIFICATION']['source_nick'] '@' + source['screen_name'] notify color_func c['NOTIFICATION']['notify'] 'addedyoutoalist' date parser parse created_at clock fallback_humanize date clock color_func c['NOTIFICATION']['clock'] clock meta c['NOTIFY_FORMAT']meta source_user join meta split '#source_user' meta notify join meta split '#notify' meta clock join meta split '#clock' meta emojize meta printNicely '' printNicely meta print_list target_object noti True
def _validate_ui_config obj_type ui_config reference_dict UI_CONFIG_SPECS[obj_type]assert set ui_config keys < set reference_dict keys for key value in ui_config iteritems schema_utils normalize_against_schema value reference_dict[key]
def _new_versions quay conda sconda set conda squay set quay if quay else set return sconda - squay
@pytest mark modelsdef test_unit_end_gazetteer EN matcher Matcher EN vocab {u'MemberNames' u'PERSON' {} [[{LOWER u'cal'}] [{LOWER u'cal'} {LOWER u'henderson'}]] } doc EN u'whoiscalthemanagerof?' if len list doc ents 0 ents matcher doc assert len ents 1 doc ents + tuple ents EN entity doc assert list doc ents [0] text u'cal'
def get_sys cmd ''if salt utils which 'localectl' cmd 'localectl grepKeymap sed-e"s/ / /"-e"s/^[ DCTB ]*//"'elif 'RedHat' in __grains__['os_family'] cmd 'grepLAYOUT/etc/sysconfig/keyboard grep-vE"^#"'elif 'Debian' in __grains__['os_family'] cmd 'grepXKBLAYOUT/etc/default/keyboard grep-vE"^#"'elif 'Gentoo' in __grains__['os_family'] cmd 'grep"^keymap"/etc/conf d/keymaps grep-vE"^#"'out __salt__['cmd run'] cmd python_shell True split ' ' ret out[1] replace '"' '' return ret
def get_ninja_project_file path extension ' nja'return get_ninja_file path extension only_first True
def getTime return monotonicClock getTime
def ShortenParameterNames params out {}for name value in params iteritems short_name LONG_NAMES get name name if short_name in out raise KeyError 'Bothlongandshortversionofparameter%s %s found Itisunclearwhichonetouse ' % name short_name out[short_name] valuereturn out
def ShortenParameterNames params out {}for name value in params iteritems short_name LONG_NAMES get name name if short_name in out raise KeyError 'Bothlongandshortversionofparameter%s %s found Itisunclearwhichonetouse ' % name short_name out[short_name] valuereturn out
def mon_create **kwargs return ceph_cfg mon_create **kwargs
def humanBitSize size divisor 1000if size < divisor return ngettext '%ubit' '%ubits' size % size units [u'Kbit' u'Mbit' u'Gbit' u'Tbit']size float size for unit in units size size / divisor if size < divisor return '% 1f%s' % size unit return u'%u%s' % size unit
def getMinimumByPathComplex path minimum complex 999999999 0 999999999 0 for point in path minimum getMinimum minimum point return minimum
def _ra_pid_for dev pid_file _ra_file dev 'pid' if os path exists pid_file with open pid_file 'r' as f return int f read
def cut_threshold labels rag thresh in_place True if not in_place rag rag copy to_remove [ x y for x y d in rag edges_iter data True if d['weight'] > thresh ]rag remove_edges_from to_remove comps nx connected_components rag map_array np arange labels max + 1 dtype labels dtype for i nodes in enumerate comps for node in nodes for label in rag node[node]['labels'] map_array[label] ireturn map_array[labels]
def Save root None formats None **options clf options pop 'clf' True Config **options if formats is None formats ['pdf' 'eps']try formats remove 'plotly' Plotly clf False except ValueError passif root for fmt in formats SaveFormat root fmt if clf Clf
def indexable *iterables result []for X in iterables if sp issparse X result append X tocsr elif hasattr X '__getitem__' or hasattr X 'iloc' result append X elif X is None result append X else result append np array X check_consistent_length *result return result
def _get_adapter_name_and_ip_address network_adapters mac_address adapter_name Noneip_address Nonefor network_adapter in network_adapters if network_adapter['mac-address'] mac_address lower adapter_name network_adapter['name']ip_address network_adapter['ip-address']breakreturn adapter_name ip_address
def leaks url 'http //localhost 8080/manager' timeout 180 return _wget 'findleaks' {'statusLine' 'true'} url timeout timeout ['msg']
def rev_parse committish repo_dir None head read_ref committish repo_dir repo_dir if head debug2 'resolvedfromref commit %s\n' % head encode 'hex' return headpL PackIdxList repo 'objects/pack' repo_dir repo_dir if len committish 40 try hash committish decode 'hex' except TypeError return Noneif pL exists hash return hashreturn None
def pull resource_type resource_id **kwargs callback _get_manager get_callback resource_type obj callback resource_type resource_id **kwargs if obj if not isinstance obj base NeutronObject or resource_type obj obj_name raise exceptions CallbackWrongResourceType resource_type resource_type return obj
def pull resource_type resource_id **kwargs callback _get_manager get_callback resource_type obj callback resource_type resource_id **kwargs if obj if not isinstance obj base NeutronObject or resource_type obj obj_name raise exceptions CallbackWrongResourceType resource_type resource_type return obj
def pull resource_type resource_id **kwargs callback _get_manager get_callback resource_type obj callback resource_type resource_id **kwargs if obj if not isinstance obj base NeutronObject or resource_type obj obj_name raise exceptions CallbackWrongResourceType resource_type resource_type return obj
def libvlc_log_get_object ctx id f _Cfunctions get 'libvlc_log_get_object' None or _Cfunction 'libvlc_log_get_object' 1 2 2 1 None None Log_ptr ListPOINTER ctypes c_char_p ListPOINTER ctypes c_char_p ctypes POINTER ctypes c_uint return f ctx id
def update_url url debug if not url return urloriginal_url urlurl url replace ' //raw github com/' ' //raw githubusercontent com/' url url replace ' //nodeload github com/' ' //codeload github com/' url re sub '^ https //codeload github com/[^/]+/[^/]+/ zipball / * $' '\\1zip\\2' url if url 'https //sublime wbond net/repositories json' or url 'https //sublime wbond net/channel json' url 'https //packagecontrol io/channel_v3 json'if debug and url original_url console_write u'\nFixedURLfrom%sto%s\n' original_url url return url
def get_doc_version version parsed_version parse_version version if is_release version and parsed_version documentation_revision is not None return parsed_version releaseelse return version
def touch path times None if os path isdir path os utime path times else with open path 'ab' os utime path times
def ward y return linkage y method 'ward' metric 'euclidean'
def ward y return linkage y method 'ward' metric 'euclidean'
def ward y return linkage y method 'ward' metric 'euclidean'
def read_null_list start end addr_space return null_list read_addr_range start end addr_space end - start
@register filterdef comment_filter comment_text filter_func settings COMMENT_FILTERif not filter_func def filter_func s return linebreaksbr urlize s autoescape True autoescape True elif not callable filter_func filter_func import_dotted_path filter_func return filter_func comment_text
@register filterdef comment_filter comment_text filter_func settings COMMENT_FILTERif not filter_func def filter_func s return linebreaksbr urlize s autoescape True autoescape True elif not callable filter_func filter_func import_dotted_path filter_func return filter_func comment_text
def get_or_bust data_dict keys if isinstance keys basestring keys [keys]import ckan logic schema as schemaschema schema create_schema_for_required_keys keys data_dict errors _validate data_dict schema if errors raise ValidationError errors values [data_dict[key] for key in keys]if len values 1 return values[0]return tuple values
def colorscale_to_colors colorscale color_list []for item in colorscale color_list append item[1] return color_list
def add_prerequisite_course course_key prerequisite_course_key if not is_prerequisite_courses_enabled return Nonemilestone_name _ 'Course{course_id}requires{prerequisite_course_id}' format course_id unicode course_key prerequisite_course_id unicode prerequisite_course_key milestone milestones_api add_milestone {'name' milestone_name 'namespace' unicode prerequisite_course_key 'description' _ 'Systemdefinedmilestone' } milestones_api add_course_milestone course_key 'requires' milestone milestones_api add_course_milestone prerequisite_course_key 'fulfills' milestone
def snapshot_absent name force False recursive False return _absent name 'snapshot' force recursive
def force_text s encoding 'utf-8' if isinstance s Text return selif isinstance s binary_type return s decode encoding else raise TypeError 'force_textexpectsastringtype'
def diop_ternary_quadratic_normal eq var coeff diop_type classify_diop eq _dict False if diop_type 'homogeneous_ternary_quadratic_normal' return _diop_ternary_quadratic_normal var coeff
def _get context namespace_name resource_type_name namespace_id resource_type_id session try query session query models MetadefNamespaceResourceType filter_by namespace_id namespace_id resource_type_id resource_type_id db_rec query one except sa_orm exc NoResultFound LOG debug 'Themetadatadefinitionresource-typeassociationofresource_type % resource_type_name stonamespace_name % namespace_name swasnotfound ' {'resource_type_name' resource_type_name 'namespace_name' namespace_name} raise exc MetadefResourceTypeAssociationNotFound resource_type_name resource_type_name namespace_name namespace_name return db_rec
def get_hosts_and_tests host_info {}q dbmodels Q test_name__startswith 'kernbench' dbmodels Q test_name__startswith 'dbench' dbmodels Q test_name__startswith 'tbench' dbmodels Q test_name__startswith 'unixbench' dbmodels Q test_name__startswith 'iozone' test_query models TestView objects filter q values 'test_name' 'hostname' 'machine_idx' distinct for result_dict in test_query hostname result_dict['hostname']test result_dict['test_name']machine_idx result_dict['machine_idx']host_info setdefault hostname {} host_info[hostname] setdefault 'tests' [] host_info[hostname]['tests'] append test host_info[hostname]['id'] machine_idxreturn rpc_utils prepare_for_serialization host_info
def get_hosts_and_tests host_info {}q dbmodels Q test_name__startswith 'kernbench' dbmodels Q test_name__startswith 'dbench' dbmodels Q test_name__startswith 'tbench' dbmodels Q test_name__startswith 'unixbench' dbmodels Q test_name__startswith 'iozone' test_query models TestView objects filter q values 'test_name' 'hostname' 'machine_idx' distinct for result_dict in test_query hostname result_dict['hostname']test result_dict['test_name']machine_idx result_dict['machine_idx']host_info setdefault hostname {} host_info[hostname] setdefault 'tests' [] host_info[hostname]['tests'] append test host_info[hostname]['id'] machine_idxreturn rpc_utils prepare_for_serialization host_info
def get_hosts_and_tests host_info {}q dbmodels Q test_name__startswith 'kernbench' dbmodels Q test_name__startswith 'dbench' dbmodels Q test_name__startswith 'tbench' dbmodels Q test_name__startswith 'unixbench' dbmodels Q test_name__startswith 'iozone' test_query models TestView objects filter q values 'test_name' 'hostname' 'machine_idx' distinct for result_dict in test_query hostname result_dict['hostname']test result_dict['test_name']machine_idx result_dict['machine_idx']host_info setdefault hostname {} host_info[hostname] setdefault 'tests' [] host_info[hostname]['tests'] append test host_info[hostname]['id'] machine_idxreturn rpc_utils prepare_for_serialization host_info
def get_hosts_and_tests host_info {}q dbmodels Q test_name__startswith 'kernbench' dbmodels Q test_name__startswith 'dbench' dbmodels Q test_name__startswith 'tbench' dbmodels Q test_name__startswith 'unixbench' dbmodels Q test_name__startswith 'iozone' test_query models TestView objects filter q values 'test_name' 'hostname' 'machine_idx' distinct for result_dict in test_query hostname result_dict['hostname']test result_dict['test_name']machine_idx result_dict['machine_idx']host_info setdefault hostname {} host_info[hostname] setdefault 'tests' [] host_info[hostname]['tests'] append test host_info[hostname]['id'] machine_idxreturn rpc_utils prepare_for_serialization host_info
def make_fastq_multi in_fasta quals out_fp label_transform split_lib_transform mkdir out_fp seen_libs defaultdict list for rec label in iter_fastq in_fasta quals label_transform lib_id seq_id label rsplit '_' 1 seen_libs[lib_id] append rec for lib recs in seen_libs items if lib is None continueoutfile open out_fp + '/' + lib + ' fastq' 'w' outfile write '\n' join recs outfile close
def normalize_path filename return os path normcase os path realpath filename
def _rotateLeft x n return x << n x >> 32 - n
def transferCoincidences network fromElementName toElementName coincidenceHandle getLockedHandle runtimeElement network getElement fromElementName expression 'self _cd _W' network getElement toElementName setParameter 'coincidencesAbove' coincidenceHandle
@np deprecate message 'splineisdeprecatedinscipy0 19 0 useBsplineclassinstead ' def spline xk yk xnew order 3 kind 'smoothest' conds None return spleval splmake xk yk order order kind kind conds conds xnew
@np deprecate message 'splineisdeprecatedinscipy0 19 0 useBsplineclassinstead ' def spline xk yk xnew order 3 kind 'smoothest' conds None return spleval splmake xk yk order order kind kind conds conds xnew
def pick seq func maxobj None maxscore Nonefor obj in seq score func obj if maxscore is None or maxscore < score maxscore maxobj score obj return maxobj
def absent name ret {'name' name 'changes' {} 'result' True 'comment' ''}index_exists __salt__['elasticsearch index_exists'] index name if index_exists if __opts__['test'] ret['comment'] 'Index{0}willberemoved' format name ret['result'] Noneelse ret['result'] __salt__['elasticsearch index_delete'] index name if ret['result'] ret['comment'] 'Removedindex{0}successfully' format name else ret['comment'] 'Failedtoremoveindex{0}' format name elif not index_exists ret['comment'] 'Index{0}isalreadyabsent' format name else ret['comment'] 'Failedtodeterminewhetherindex{0}isabsent seeMinionlogformoreinformation' format name ret['result'] Falsereturn ret
def inet_ntoa address if len address 4 raise dns exception SyntaxErrorreturn '%u %u %u %u' % ord address[0] ord address[1] ord address[2] ord address[3]
def find_lexer_for_filename filename filename filename or '' root ext os path splitext filename if ext in custom_extension_lexer_mapping lexer get_lexer_by_name custom_extension_lexer_mapping[ext] else try lexer get_lexer_for_filename filename except ClassNotFound return TextLexer return lexer
def get_descendant_ids root_id children Page objects filter parent root_id values_list u'pk' flat True for child_id in children iterator yield child_id for descendant_id in get_descendant_ids child_id yield descendant_id
def get_descendant_ids root_id children Page objects filter parent root_id values_list u'pk' flat True for child_id in children iterator yield child_id for descendant_id in get_descendant_ids child_id yield descendant_id
def fmtTimeSpan time pad 0 point 0 short False after False unit 99 type point optimalPeriod time point unit time convertSecondsTo time type if not point time int round time if short fmt shortTimeFmt type elif after fmt afterTimeTable[type] _pluralCount time point else fmt timeTable[type] _pluralCount time point timestr '% a d % b df' % {'a' pad 'b' point} return locale format_string '%' + fmt % timestr time
@deprecated_renamed_argument 'clobber' 'overwrite' '1 3' def writeto filename data header None output_verify 'exception' overwrite False checksum False hdu _makehdu data header if hdu is_image and not isinstance hdu PrimaryHDU hdu PrimaryHDU data header header hdu writeto filename overwrite overwrite output_verify output_verify checksum checksum
@deprecated_renamed_argument 'clobber' 'overwrite' '1 3' def writeto filename data header None output_verify 'exception' overwrite False checksum False hdu _makehdu data header if hdu is_image and not isinstance hdu PrimaryHDU hdu PrimaryHDU data header header hdu writeto filename overwrite overwrite output_verify output_verify checksum checksum
def rm_job user path mask cmd mask str mask upper for item in mask split ' ' if item not in _MASK_TYPES return 'Invalidmasktype {0}' format item lst list_tab user ret 'absent'rm_ Nonefor ind in range len lst['crons'] if rm_ is not None breakif path lst['crons'][ind]['path'] if cmd lst['crons'][ind]['cmd'] if mask lst['crons'][ind]['mask'] rm_ indif rm_ is not None lst['crons'] pop rm_ ret 'removed'comdat _write_incron_lines user _render_tab lst if comdat['retcode'] return comdat['stderr']return ret
def launch core print_status 'LaunchingMetasploitandattackingthesystemsspecified Thismaytakeamoment ' try child pexpect spawn '{0}-r{1}\r\n\r\n' format os path join core meta_path + 'msfconsole' os path join core setdir + 'autopwn answer' child interact except Exception as error core log error
def launch core print_status 'LaunchingMetasploitandattackingthesystemsspecified Thismaytakeamoment ' try child pexpect spawn '{0}-r{1}\r\n\r\n' format os path join core meta_path + 'msfconsole' os path join core setdir + 'autopwn answer' child interact except Exception as error core log error
def anonymous_name id import socket randomname '%s_%s_%s_%s' % id socket gethostname os getpid random randint 0 sys maxsize name name replace ' ' '_' name name replace '-' '_' return name replace ' ' '_'
@pytest mark parametrize 'parallel' [True False] def test_many_columns parallel read_basic text '' join [str i for i in range 500 ] text + '\n' + text + '\n' + text table read_basic text parallel parallel expected Table [[i i] for i in range 500 ] names [str i for i in range 500 ] assert_table_equal table expected
def get_repository_version pear_output lines pear_output split '\n' for line in lines if 'Latest' in line return line rsplit None 1 [ -1 ] strip return None
def get_repository_version pear_output lines pear_output split '\n' for line in lines if 'Latest' in line return line rsplit None 1 [ -1 ] strip return None
def _create_event return windll kernel32 CreateEventA pointer SECURITY_ATTRIBUTES BOOL True BOOL False None
def test_reader_macro entry tokenize '#^ ' assert entry[0][0] HySymbol 'dispatch_reader_macro' assert entry[0][1] HyString '^' assert len entry[0] 3
def logistic_function value return 1 0 / 1 0 + math exp - value
def project_versions request project_slug project get_object_or_404 Project objects protected request user slug project_slug versions Version objects public user request user project project only_active False active_versions versions filter active True inactive_versions versions filter active False inactive_filter VersionSlugFilter request GET queryset inactive_versions active_filter VersionSlugFilter request GET queryset active_versions wiped request GET get 'wipe' '' wiped_version versions filter slug wiped if wiped and wiped_version count messages success request 'Versionwiped ' + wiped return render_to_response 'projects/project_version_list html' {'inactive_filter' inactive_filter 'active_filter' active_filter 'project' project} context_instance RequestContext request
def project_versions request project_slug project get_object_or_404 Project objects protected request user slug project_slug versions Version objects public user request user project project only_active False active_versions versions filter active True inactive_versions versions filter active False inactive_filter VersionSlugFilter request GET queryset inactive_versions active_filter VersionSlugFilter request GET queryset active_versions wiped request GET get 'wipe' '' wiped_version versions filter slug wiped if wiped and wiped_version count messages success request 'Versionwiped ' + wiped return render_to_response 'projects/project_version_list html' {'inactive_filter' inactive_filter 'active_filter' active_filter 'project' project} context_instance RequestContext request
def _silent_no_wrap func modname return func modname
def addPrefixDictionary dictionary keys value for key in keys dictionary[key lstrip '_' ] value
def show_help help_text N_ u'\nKeyboardShortcuts\n------------------\nJ Down MoveDown\nK Up MoveUp\nEnter EditSelectedFiles\nSpacebar OpenFileUsingDefaultApplication\nCtrl+L FocusTextEntryField\n? ShowHelp\n\nTheupanddownarrowschangefocusbetweenthetextentryfield\nandtheresults \n' title N_ u'Help-FindFiles' return text text_dialog help_text title
def init opts log debug 'Openingconnectiontojunos' thisproxy['conn'] jnpr junos Device user opts['proxy']['username'] host opts['proxy']['host'] password opts['proxy']['passwd'] thisproxy['conn'] open thisproxy['conn'] bind cu jnpr junos utils config Config thisproxy['conn'] bind sw jnpr junos utils sw SW thisproxy['initialized'] True
def encode_mirror_url raw_url_or_path remote_domain None is_scheme None is_escape False if is_escape _raw_url_or_path raw_url_or_path replace 'r\\/' '/' else _raw_url_or_path raw_url_or_pathsp urlsplit _raw_url_or_path if '/extdomains/' sp path[ 12] return raw_url_or_pathdomain remote_domain or sp netloc or parse remote_domain or target_domain if domain not in allowed_domains_set return raw_url_or_pathif is_scheme is not False if _raw_url_or_path[ 2] '//' our_prefix '//' + my_host_name elif is_scheme or sp scheme our_prefix myurl_prefixelse our_prefix ''else our_prefix ''if is_external_domain domain middle_part '/extdomains/' + domain else middle_part ''result urljoin our_prefix + middle_part + '/' extract_url_path_and_query _raw_url_or_path lstrip '/' if is_escape result s_esc result return result
def _create_dot_graph graph show_connectinfo False simple_form True logger debug u'creatingdotgraph' pklgraph nx DiGraph for edge in graph edges data graph get_edge_data *edge srcname get_print_name edge[0] simple_form simple_form destname get_print_name edge[1] simple_form simple_form if show_connectinfo pklgraph add_edge srcname destname l str data[u'connect'] else pklgraph add_edge srcname destname return pklgraph
def scheme_node_from_element node_el registry try widget_desc registry widget node_el get 'qualified_name' except KeyError as ex raise UnknownWidgetDefinition *ex args title node_el get 'title' pos node_el get 'position' if pos is not None pos tuple_eval pos return SchemeNode widget_desc title title position pos
def getSSLContext keyfile os path join _GAME_DIR 'server' 'ssl key' certfile os path join _GAME_DIR 'server' 'ssl cert' verify_SSL_key_and_cert keyfile certfile return twisted_ssl DefaultOpenSSLContextFactory keyfile certfile
def read_element_date stream size if size 8 raise SizeError size nanoseconds unpack '>q' _read stream 8 [0]return datetime 2001 1 1 0 0 0 0 None + timedelta microseconds nanoseconds // 1000
def pull_dkr url name index return _pull_image 'dkr' url name index index
def pull_dkr url name index return _pull_image 'dkr' url name index index
def _gaussian_loglik_scorer est X y None precision est get_precision n_samples n_features X shapelog_like np zeros n_samples log_like -0 5 * X * np dot X precision sum axis 1 log_like - 0 5 * n_features * log 2 0 * np pi - _logdet precision out np mean log_like return out
def _gaussian_loglik_scorer est X y None precision est get_precision n_samples n_features X shapelog_like np zeros n_samples log_like -0 5 * X * np dot X precision sum axis 1 log_like - 0 5 * n_features * log 2 0 * np pi - _logdet precision out np mean log_like return out
def _get_reg_software ignore_list [u'AddressBook' u'ConnectionManager' u'DirectDrawEx' u'Fontcore' u'IE40' u'IE4Data' u'IE5BAKEX' u'IEData' u'MobileOptionPack' u'SchedulingAgent' u'WIC' u'NotFound' u' valuenotset ' u'' None]reg_software {}hive u'HKLM'key u'Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall'def update hive key reg_key use_32bit d_name u''d_vers u''d_name __salt__[u'reg read_value'] hive u'{0}\\{1}' format key reg_key u'DisplayName' use_32bit [u'vdata']d_vers __salt__[u'reg read_value'] hive u'{0}\\{1}' format key reg_key u'DisplayVersion' use_32bit [u'vdata']if d_name not in ignore_list reg_software update {d_name str d_vers } for reg_key in __salt__[u'reg list_keys'] hive key update hive key reg_key False for reg_key in __salt__[u'reg list_keys'] hive key True update hive key reg_key True return reg_software
def _get_reg_software ignore_list [u'AddressBook' u'ConnectionManager' u'DirectDrawEx' u'Fontcore' u'IE40' u'IE4Data' u'IE5BAKEX' u'IEData' u'MobileOptionPack' u'SchedulingAgent' u'WIC' u'NotFound' u' valuenotset ' u'' None]reg_software {}hive u'HKLM'key u'Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall'def update hive key reg_key use_32bit d_name u''d_vers u''d_name __salt__[u'reg read_value'] hive u'{0}\\{1}' format key reg_key u'DisplayName' use_32bit [u'vdata']d_vers __salt__[u'reg read_value'] hive u'{0}\\{1}' format key reg_key u'DisplayVersion' use_32bit [u'vdata']if d_name not in ignore_list reg_software update {d_name str d_vers } for reg_key in __salt__[u'reg list_keys'] hive key update hive key reg_key False for reg_key in __salt__[u'reg list_keys'] hive key True update hive key reg_key True return reg_software
def _get_reg_software ignore_list [u'AddressBook' u'ConnectionManager' u'DirectDrawEx' u'Fontcore' u'IE40' u'IE4Data' u'IE5BAKEX' u'IEData' u'MobileOptionPack' u'SchedulingAgent' u'WIC' u'NotFound' u' valuenotset ' u'' None]reg_software {}hive u'HKLM'key u'Software\\Microsoft\\Windows\\CurrentVersion\\Uninstall'def update hive key reg_key use_32bit d_name u''d_vers u''d_name __salt__[u'reg read_value'] hive u'{0}\\{1}' format key reg_key u'DisplayName' use_32bit [u'vdata']d_vers __salt__[u'reg read_value'] hive u'{0}\\{1}' format key reg_key u'DisplayVersion' use_32bit [u'vdata']if d_name not in ignore_list reg_software update {d_name str d_vers } for reg_key in __salt__[u'reg list_keys'] hive key update hive key reg_key False for reg_key in __salt__[u'reg list_keys'] hive key True update hive key reg_key True return reg_software
def make_loc_files file_creator size None if size body '*' * size else body 'Thisisatest 'filename1 file_creator create_file os path join 'some_directory' 'text1 txt' body filename2 file_creator create_file os path join 'some_directory' 'another_directory' 'text2 txt' body filename1 six text_type filename1 filename2 six text_type filename2 return [filename1 filename2 os path dirname filename2 os path dirname filename1 ]
def pandas_read_text reader b header kwargs dtypes None columns None write_header True enforce False bio BytesIO if write_header and not b startswith header rstrip bio write header bio write b bio seek 0 df reader bio **kwargs if dtypes coerce_dtypes df dtypes if enforce and columns and list df columns list columns raise ValueError 'Columnsdonotmatch' df columns columns elif columns df columns columnsreturn df
def trg_redirect uid res_type res_id new_rid cr assert isinstance new_rid long int return WorkflowService new cr uid res_type res_id redirect new_rid
def rewrite_config CONFIG['MobSF']['dir'] expanduser '~' + CONFIG['MobSF']['dir'] CONFIG['MobSF']['downloads'] CONFIG['MobSF']['dir'] + CONFIG['MobSF']['downloads'] CONFIG['MobSF']['tools'] CONFIG['MobSF']['dir'] + CONFIG['MobSF']['tools'] CONFIG['MobSF']['samples'] CONFIG['MobSF']['dir'] + CONFIG['MobSF']['samples'] CONFIG['MobSF']['key_path'] CONFIG['MobSF']['dir'] + CONFIG['MobSF']['key_path'] CONFIG['MobSF']['priv_key'] CONFIG['MobSF']['key_path'] + CONFIG['MobSF']['priv_key'] CONFIG['MobSF']['pub_key'] CONFIG['MobSF']['key_path'] + CONFIG['MobSF']['pub_key'] with open os path join CONFIG_PATH CONFIG_FILE 'w' as configfile CONFIG write configfile
def rewrite_config CONFIG['MobSF']['dir'] expanduser '~' + CONFIG['MobSF']['dir'] CONFIG['MobSF']['downloads'] CONFIG['MobSF']['dir'] + CONFIG['MobSF']['downloads'] CONFIG['MobSF']['tools'] CONFIG['MobSF']['dir'] + CONFIG['MobSF']['tools'] CONFIG['MobSF']['samples'] CONFIG['MobSF']['dir'] + CONFIG['MobSF']['samples'] CONFIG['MobSF']['key_path'] CONFIG['MobSF']['dir'] + CONFIG['MobSF']['key_path'] CONFIG['MobSF']['priv_key'] CONFIG['MobSF']['key_path'] + CONFIG['MobSF']['priv_key'] CONFIG['MobSF']['pub_key'] CONFIG['MobSF']['key_path'] + CONFIG['MobSF']['pub_key'] with open os path join CONFIG_PATH CONFIG_FILE 'w' as configfile CONFIG write configfile
def _check_method estimator method if method 'transform' and not hasattr estimator 'transform' method 'predict'if not hasattr estimator method ValueError 'base_estimatordoesnothave`%s`method ' % method return method
def save_project project file_path json dump project codecs open file_path u'w' u'utf8' indent 4
def _dummy_ name token **kwargs global _dummiesif not name token in _dummies _dummies[ name token ] Dummy name **kwargs return _dummies[ name token ]
def _dummy_ name token **kwargs global _dummiesif not name token in _dummies _dummies[ name token ] Dummy name **kwargs return _dummies[ name token ]
def sub keys d return dict [ k d[k] for k in keys]
def sub keys d return dict [ k d[k] for k in keys]
def _GetValidMain module if not hasattr module 'main' return Nonemain module mainif not hasattr main '__call__' return Nonedefaults main __defaults__if defaults default_argcount len defaults else default_argcount 0if main __code__ co_argcount - default_argcount 0 return mainelse return None
def get_file_name blob_key if not blob_key raise files InvalidArgumentError 'Emptyblobkey' if not isinstance blob_key blobstore BlobKey basestring raise files InvalidArgumentError 'Expectedstringorblobstore BlobKey' return '%s%s' % _BLOBSTORE_DIRECTORY blob_key
def copy_header_subset from_r to_r condition for k v in from_r headers items if condition k to_r headers[k] v
def copy_header_subset from_r to_r condition for k v in from_r headers items if condition k to_r headers[k] v
def s3_debug message value None output 'S3Debug %s' % s3_unicode message if value output '%s %s' % output s3_unicode value try print >>sys stderr outputexcept print >>sys stderr 'Debugcrashed'
def s3_debug message value None output 'S3Debug %s' % s3_unicode message if value output '%s %s' % output s3_unicode value try print >>sys stderr outputexcept print >>sys stderr 'Debugcrashed'
@requires_segment_infodef capslock_indicator pl segment_info text u'CAPS' if not vim_func_exists u'CapsLockStatusline' return Nonereturn text if vim eval u'CapsLockStatusline ' else None
def get_pending_computer_name current get_computer_name pending __salt__['reg read_value'] 'HKLM' 'SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters' 'NVHostname' ['vdata']if pending return pending if pending current else None return False
def setup_platform hass config add_devices discovery_info None host config get CONF_HOST port config get CONF_PORT name config get CONF_NAME username config get CONF_USERNAME password config get CONF_PASSWORD monitored_types config get CONF_MONITORED_VARIABLES url 'http //{} {}/jsonrpc' format host port try nzbgetapi NZBGetAPI api_url url username username password password nzbgetapi update except requests exceptions ConnectionError requests exceptions HTTPError as conn_err _LOGGER error 'ErrorsettingupNZBGetAPI %s' conn_err return Falsedevices []for ng_type in monitored_types new_sensor NZBGetSensor api nzbgetapi sensor_type SENSOR_TYPES get ng_type client_name name devices append new_sensor add_devices devices
def kpsewhich filename try find_cmd 'kpsewhich' proc subprocess Popen ['kpsewhich' filename] stdout subprocess PIPE stderr subprocess PIPE stdout stderr proc communicate return stdout strip decode 'utf8' 'replace' except FindCmdError pass
def get_subset_from_bitstring super_set bitstring if len super_set len bitstring raise ValueError 'Thesizesofthelistsarenotequal' return [super_set[i] for i j in enumerate bitstring if bitstring[i] '1' ]
def mod_aggregate low chunks running pkgs []agg_enabled ['installed' 'latest' 'removed' 'purged']if low get 'fun' not in agg_enabled return lowfor chunk in chunks tag salt utils gen_state_tag chunk if tag in running continueif chunk get 'state' 'pkg' if '__agg__' in chunk continueif chunk get 'fun' low get 'fun' continueif chunk get 'fromrepo' low get 'fromrepo' continueif 'pkgs' in chunk pkgs extend chunk['pkgs'] chunk['__agg__'] Trueelif 'name' in chunk pkgs append chunk['name'] chunk['__agg__'] Trueif pkgs if 'pkgs' in low low['pkgs'] extend pkgs else low['pkgs'] pkgsreturn low
def mod_aggregate low chunks running pkgs []agg_enabled ['installed' 'latest' 'removed' 'purged']if low get 'fun' not in agg_enabled return lowfor chunk in chunks tag salt utils gen_state_tag chunk if tag in running continueif chunk get 'state' 'pkg' if '__agg__' in chunk continueif chunk get 'fun' low get 'fun' continueif chunk get 'fromrepo' low get 'fromrepo' continueif 'pkgs' in chunk pkgs extend chunk['pkgs'] chunk['__agg__'] Trueelif 'name' in chunk pkgs append chunk['name'] chunk['__agg__'] Trueif pkgs if 'pkgs' in low low['pkgs'] extend pkgs else low['pkgs'] pkgsreturn low
def find_subsections section result []for child in section children if isinstance child nodes section result append child continueresult extend find_subsections child return result
def get_port_status cluster lswitch_id port_id try r do_single_request HTTP_GET '/ws v1/lswitch/%s/lport/%s/status' % lswitch_id port_id cluster cluster r json loads r except NvpApiClient ResourceNotFound as e LOG error _ 'Portnotfound Error %s' str e raise exception PortNotFound port_id port_id net_id lswitch_id except NvpApiClient NvpApiException as e raise exception QuantumException if r['link_status_up'] is True return constants PORT_STATUS_ACTIVEelse return constants PORT_STATUS_DOWN
def _get_cibpath cibpath os path join __opts__['cachedir'] 'pcs' __env__ log trace 'cibpath {0}' format cibpath return cibpath
def _get_cibpath cibpath os path join __opts__['cachedir'] 'pcs' __env__ log trace 'cibpath {0}' format cibpath return cibpath
def _setupSyslog testCase logMessages []class fakesyslogobserver object def __init__ self prefix logMessages append prefix def emit self eventDict logMessages append eventDict testCase patch syslog 'SyslogObserver' fakesyslogobserver return logMessages
def set_palette palette n_colors None desat None color_codes False colors palettes color_palette palette n_colors desat if mpl_ge_150 from cycler import cyclercyl cycler 'color' colors mpl rcParams['axes prop_cycle'] cylelse mpl rcParams['axes color_cycle'] list colors mpl rcParams['patch facecolor'] colors[0]if color_codes palettes set_color_codes palette
def set_palette palette n_colors None desat None color_codes False colors palettes color_palette palette n_colors desat if mpl_ge_150 from cycler import cyclercyl cycler 'color' colors mpl rcParams['axes prop_cycle'] cylelse mpl rcParams['axes color_cycle'] list colors mpl rcParams['patch facecolor'] colors[0]if color_codes palettes set_color_codes palette
def test_lambda_list_keywords_kwonly kwonly_demo u' fn[&kwonlya[b2]] printab 'if PY3 code can_compile kwonly_demo for i kwonlyarg_name in enumerate u'a' u'b' assert kwonlyarg_name code body[0] args kwonlyargs[i] arg assert code body[0] args kw_defaults[0] is None assert code body[0] args kw_defaults[1] n 2 else exception cant_compile kwonly_demo assert isinstance exception HyTypeError message exception argsassert message u'keyword-onlyargumentsareonlyavailableunderPython3'
def test_lambda_list_keywords_kwonly kwonly_demo u' fn[&kwonlya[b2]] printab 'if PY3 code can_compile kwonly_demo for i kwonlyarg_name in enumerate u'a' u'b' assert kwonlyarg_name code body[0] args kwonlyargs[i] arg assert code body[0] args kw_defaults[0] is None assert code body[0] args kw_defaults[1] n 2 else exception cant_compile kwonly_demo assert isinstance exception HyTypeError message exception argsassert message u'keyword-onlyargumentsareonlyavailableunderPython3'
def moduleProvides *interfaces frame sys _getframe 1 locals frame f_localsif locals is not frame f_globals or '__name__' not in locals raise TypeError 'moduleProvidescanonlybeusedfromamoduledefinition ' if '__provides__' in locals raise TypeError 'moduleProvidescanonlybeusedonceinamoduledefinition ' locals['__provides__'] Provides ModuleType *_normalizeargs interfaces
@frappe whitelist def get_due_date posting_date party_type party company due_date Noneif posting_date and party due_date posting_date credit_days_based_on credit_days get_credit_days party_type party company if credit_days_based_on u'FixedDays' and credit_days due_date add_days posting_date credit_days elif credit_days_based_on u'LastDayoftheNextMonth' due_date get_first_day posting_date 0 2 + datetime timedelta -1 strftime u'%Y-%m-%d' return due_date
def run returnfor file_ in File objects filter version__addon__type__in [amo ADDON_EXTENSION amo ADDON_THEME] try data parse_addon file_ file_path addon file_ version addon if data get 'is_multi_package' file_ update is_multi_package True except log error 'Failedcheckingfile{0}' format file_ pk
def run returnfor file_ in File objects filter version__addon__type__in [amo ADDON_EXTENSION amo ADDON_THEME] try data parse_addon file_ file_path addon file_ version addon if data get 'is_multi_package' file_ update is_multi_package True except log error 'Failedcheckingfile{0}' format file_ pk
def run returnfor file_ in File objects filter version__addon__type__in [amo ADDON_EXTENSION amo ADDON_THEME] try data parse_addon file_ file_path addon file_ version addon if data get 'is_multi_package' file_ update is_multi_package True except log error 'Failedcheckingfile{0}' format file_ pk
@cronjobs registerdef cleanup_image_files log info 'Removingonedayoldtemporaryimagefilesforthemes ' for folder in 'persona_footer' 'persona_header' root os path join settings TMP_PATH folder if not os path exists root continuefor path in os listdir root full_path os path join root path age time time - os stat full_path st_atime if age > 60 * 60 * 24 log debug 'Removingimagefile %s %dsecsold ' % full_path age os unlink full_path
def make_cache_key *args **kwargs path request pathargs str hash frozenset request args items return path + args encode 'ascii' 'ignore'
def make_cache_key *args **kwargs path request pathargs str hash frozenset request args items return path + args encode 'ascii' 'ignore'
@log_calldef metadef_namespace_get_all context marker None limit None sort_key 'created_at' sort_dir 'desc' filters None resource_types filters get 'resource_types' [] if filters else [] visibility filters get 'visibility' None if filters else None namespaces []for namespace in DATA['metadef_namespaces'] if not _is_namespace_visible context namespace continueif visibility and namespace['visibility'] visibility continueif resource_types for association in DATA['metadef_namespace_resource_types'] if association['namespace_id'] namespace['id'] if association['name'] in resource_types breakelse continuenamespaces append namespace return namespaces
def label_rgb colors return 'rgb %s %s %s ' % colors[0] colors[1] colors[2]
def start_server server version req servers_service_pb StartServerRequest req set_server server req set_version version resp servers_service_pb StartServerResponse try apiproxy_stub_map MakeSyncCall 'servers' 'StartServer' req resp except apiproxy_errors ApplicationError as e if e application_error servers_service_pb ServersServiceError INVALID_VERSION raise InvalidVersionError elif e application_error servers_service_pb ServersServiceError UNEXPECTED_STATE raise UnexpectedStateError elif e application_error servers_service_pb ServersServiceError TRANSIENT_ERROR raise TransientError else raise Error
def _split_module_dicts if not isinstance __salt__ dict return __salt__mod_dict dict __salt__ for module_func_name mod_fun in six iteritems mod_dict copy mod fun module_func_name split ' ' 1 if mod not in mod_dict mod_dict[mod] lambda None setattr mod_dict[mod] fun mod_fun return mod_dict
def aws_provisioner access_key secret_access_token keyname region zone security_groups instance_type 'm3 large' session_token None conn connect_to_region region aws_access_key_id access_key aws_secret_access_key secret_access_token security_token session_token if conn is None raise ValueError 'Invalidregion {}' format region return AWSProvisioner _connection conn _keyname keyname _security_groups security_groups _zone zone _default_size instance_type
def split_string_separator txt size if len txt > size txt '' join [re sub u'\\ ?P<ends>[^ ]* $' ' \n\n\\g<ends>' txt[i i + size ] 1 for i in xrange 0 len txt size ] return txt
@open_file 1 mode 'wb' def write_gpickle G path protocol pickle HIGHEST_PROTOCOL pickle dump G path protocol
def getLabelString dictionary for key in dictionary labelIndex key find 'label' if labelIndex > 0 return dictionary[key]return ''
def flip_axis_multi x axis is_random False if is_random factor np random uniform -1 1 if factor > 0 results []for data in x data np asarray data swapaxes axis 0 data data[ -1 ]data data swapaxes 0 axis results append data return np asarray results else return np asarray x else results []for data in x data np asarray data swapaxes axis 0 data data[ -1 ]data data swapaxes 0 axis results append data return np asarray results
def initialize import loggingimport osimport settings_localos environ['RBSITE_PYTHONPATH'] os path dirname settings_local __file__ from Crypto import Randomfrom django conf import settingsfrom django db import DatabaseErrorfrom djblets import logfrom djblets cache serials import generate_ajax_serialfrom reviewboard import signalsfrom reviewboard admin siteconfig import load_site_configfrom reviewboard extensions base import get_extension_managerimport reviewboard site templatetagsis_running_test getattr settings u'RUNNING_TEST' False if not is_running_test Random atfork log init_logging load_site_config if not is_running_test if settings DEBUG logging debug u'LogfileforReviewBoardv%s PID%s ' % get_version_string os getpid generate_ajax_serial if not getattr settings u'TEMPLATE_SERIAL' None settings TEMPLATE_SERIAL settings AJAX_SERIALtry get_extension_manager load except DatabaseError passsignals initializing send sender None
def test_approve_addons_approve_files use_case mozilla_user addon file1 file2 review_type use_caseapprove_addons approve_files [ file1 review_type file2 review_type ] assert file1 reload status amo STATUS_PUBLIC assert file2 reload status amo STATUS_PUBLIC logs AddonLog objects filter addon addon assert len logs 2 file1_log file2_log logsassert file1_log activity_log details['comments'] u'bulkapproval' assert file1_log activity_log user mozilla_user assert file2_log activity_log details['comments'] u'bulkapproval' assert file2_log activity_log user mozilla_user assert not ReviewerScore objects all
def quickstart_generator war_path sdk_root None if not sdk_root sdk_root _SDK_ROOTquickstart_xml_path os path join war_path 'WEB-INF' 'quickstart-web xml' if os path exists quickstart_xml_path os remove quickstart_xml_path java_home exec_suffix java_utils JavaHomeAndSuffix java_command os path join java_home 'bin' 'java' + exec_suffix quickstartgenerator_jar os path join sdk_root _QUICKSTART_JAR_PATH webdefaultxml os path join sdk_root _JAVA_VMRUNTIME_PATH 'etc' 'webdefault xml' command [java_command '-jar' quickstartgenerator_jar war_path webdefaultxml]subprocess check_call command with open quickstart_xml_path as f return f read quickstart_xml_path
def get_oauth_client server_token_url client_id client_secret if not is_valid_url server_token_url returnclient BackendApplicationClient client_id client_id oauth_ccxcon OAuth2Session client client oauth_ccxcon fetch_token token_url server_token_url client_id client_id client_secret client_secret timeout CCXCON_REQUEST_TIMEOUT return oauth_ccxcon
def test_withall tmpmod with tmpmod mkdir 'xontrib' join 'spameggs py' open 'w' as x x write "\n__all__ 'spam' '_foobar'\nspam 1\neggs 2\n_foobar 3\n" ctx xontrib_context 'spameggs' assert ctx {'spam' 1 '_foobar' 3}
def values_eq_approx_high_tol a b rtol Noneif a size > 100000 rtol 5e-05return CudaNdarrayType values_eq_approx a b rtol rtol
def _enrollment_mode_display enrollment_mode verification_status course_id course_mode_slugs [mode slug for mode in CourseMode modes_for_course course_id ]if enrollment_mode CourseMode VERIFIED if verification_status in [VERIFY_STATUS_NEED_TO_VERIFY VERIFY_STATUS_SUBMITTED VERIFY_STATUS_APPROVED] display_mode DISPLAY_VERIFIEDelif DISPLAY_HONOR in course_mode_slugs display_mode DISPLAY_HONORelse display_mode DISPLAY_AUDITelif enrollment_mode in [CourseMode PROFESSIONAL CourseMode NO_ID_PROFESSIONAL_MODE] display_mode DISPLAY_PROFESSIONALelse display_mode enrollment_modereturn display_mode
def oldgc phenny input query input group 2 if not query return phenny reply ' gcwhat?' query query encode 'utf-8' num formatnumber google_count query phenny say query + ' ' + num
def confirmation_option *param_decls **attrs def decorator f def callback ctx param value if not value ctx abort attrs setdefault 'is_flag' True attrs setdefault 'callback' callback attrs setdefault 'expose_value' False attrs setdefault 'prompt' 'Doyouwanttocontinue?' attrs setdefault 'help' 'Confirmtheactionwithoutprompting ' return option * param_decls or '--yes' **attrs f return decorator
def random_dense_design_matrix_for_regression rng num_examples dim reg_min reg_max X rng randn num_examples dim Y rng randint reg_min reg_max num_examples 1 return DenseDesignMatrix X X y Y
def AddPost content t time strftime '%c' time localtime DB append t content
def _has_wiki_staff_access user wiki_slug modstore course_keys modstore get_courses_for_wiki wiki_slug if wiki_slug endswith '_' and slug_is_numerical wiki_slug[ -1 ] course_keys extend modstore get_courses_for_wiki wiki_slug[ -1 ] for course_key in course_keys course modstore get_course course_key if courseware access has_access user 'staff' course course_key return Truereturn False
def logmgf_exact q priv_eps l if q < 0 5 t_one 1 - q * math pow 1 - q / 1 - math exp priv_eps * q l t_two q * math exp priv_eps * l t t_one + t_two try log_t math log t except ValueError print 'GotValueErrorinmath logforvalues ' + str q priv_eps l t log_t priv_eps * l else log_t priv_eps * l return min 0 5 * priv_eps * priv_eps * l * l + 1 log_t priv_eps * l
def _setConfAttributes debugMsg 'initializingtheconfiguration'logger debug debugMsg conf authUsername Noneconf authPassword Noneconf boundaries []conf cj Noneconf dbmsConnector Noneconf dbmsHandler Noneconf dnsServer Noneconf dumpPath Noneconf hashDB Noneconf hashDBFile Noneconf httpHeaders []conf hostname Noneconf ipv6 Falseconf multipleTargets Falseconf outputPath Noneconf paramDict {}conf parameters {}conf path Noneconf port Noneconf proxyList Noneconf resultsFilename Noneconf resultsFP Noneconf scheme Noneconf tests []conf trafficFP Noneconf wFileType None
def trustRootFromCertificates certificates certs []for cert in certificates if isinstance cert CertBase cert cert originalelse raise TypeError 'certificatesitemsmustbetwisted internet ssl CertBaseinstances' certs append cert return OpenSSLCertificateAuthorities certs
def getDotProductPlusOne firstComplex secondComplex return 1 0 + getDotProduct firstComplex secondComplex
def lottery_promoted_links sr_names n 10 promo_tuples get_live_promotions sr_names weights {p p weight or 0 001 for p in promo_tuples}selected []while weights and len selected < n s weighted_lottery weights del weights[s]selected append s return selected
def create_scenario actions logs None keys [str i for i in range len actions ]key_provider create_mock_key_provider keys digest_provider MockDigestProvider actions logs digest_validator Mock def validate bucket key public_key digest_data digest_str if '_invalid' in digest_data raise DigestError 'invaliderror' digest_validator validate validatereturn key_provider digest_provider digest_validator
def saxify element_or_tree content_handler return ElementTreeProducer element_or_tree content_handler saxify
def setup_platform hass config add_devices_callback discovery_info None add_devices_callback [DemoSwitch 'DecorativeLights' True None True DemoSwitch 'AC' False 'mdi air-conditioner' False ]
def print_result results file_dict retval print_results section log_printer file_diff_dict ignore_ranges console_printer min_severity_str str section get 'min_severity' 'INFO' upper min_severity RESULT_SEVERITY str_dict get min_severity_str 'INFO' results list filter lambda result type result is Result and result severity > min_severity and not check_result_ignore result ignore_ranges results patched_results autoapply_actions results file_dict file_diff_dict section log_printer print_results log_printer section patched_results file_dict file_diff_dict console_printer return retval or len results > 0 patched_results
def print_result results file_dict retval print_results section log_printer file_diff_dict ignore_ranges console_printer min_severity_str str section get 'min_severity' 'INFO' upper min_severity RESULT_SEVERITY str_dict get min_severity_str 'INFO' results list filter lambda result type result is Result and result severity > min_severity and not check_result_ignore result ignore_ranges results patched_results autoapply_actions results file_dict file_diff_dict section log_printer print_results log_printer section patched_results file_dict file_diff_dict console_printer return retval or len results > 0 patched_results
def get_form_data form prepared False if isinstance form FormGroup data {}for subform in form forms values data update get_form_data subform prepared prepared return dataif isinstance form BaseFormSet data {}for subform in chain [form management_form] form forms data update get_form_data subform prepared prepared return datadata {}for name field in form fields items prefixed_name form add_prefix name data_value field widget value_from_datadict form data form files prefixed_name if data_value value data_valueelse value form initial get name field initial if callable value value value if prepared value field prepare_value value if value is None continuedata[prefixed_name] valuereturn data
def get_form_data form prepared False if isinstance form FormGroup data {}for subform in form forms values data update get_form_data subform prepared prepared return dataif isinstance form BaseFormSet data {}for subform in chain [form management_form] form forms data update get_form_data subform prepared prepared return datadata {}for name field in form fields items prefixed_name form add_prefix name data_value field widget value_from_datadict form data form files prefixed_name if data_value value data_valueelse value form initial get name field initial if callable value value value if prepared value field prepare_value value if value is None continuedata[prefixed_name] valuereturn data
def eaGenerateUpdate toolbox ngen halloffame None stats None verbose __debug__ logbook tools Logbook logbook header ['gen' 'nevals'] + stats fields if stats else [] for gen in xrange ngen population toolbox generate fitnesses toolbox map toolbox evaluate population for ind fit in zip population fitnesses ind fitness values fitif halloffame is not None halloffame update population toolbox update population record stats compile population if stats is not None else {} logbook record gen gen nevals len population **record if verbose print logbook streamreturn population logbook
def eaGenerateUpdate toolbox ngen halloffame None stats None verbose __debug__ logbook tools Logbook logbook header ['gen' 'nevals'] + stats fields if stats else [] for gen in xrange ngen population toolbox generate fitnesses toolbox map toolbox evaluate population for ind fit in zip population fitnesses ind fitness values fitif halloffame is not None halloffame update population toolbox update population record stats compile population if stats is not None else {} logbook record gen gen nevals len population **record if verbose print logbook streamreturn population logbook
def reset_template_info_cache _template_info_cache clear
def infer_import self context None asname True name context lookupnameif name is None raise InferenceError if asname yield self do_import_module self real_name name else yield self do_import_module name
def orthogonal_procrustes A B check_finite True if check_finite A np asarray_chkfinite A B np asarray_chkfinite B else A np asanyarray A B np asanyarray B if A ndim 2 raise ValueError 'expectedndimtobe2 butobserved%s' % A ndim if A shape B shape raise ValueError 'theshapesofAandBdiffer %svs%s ' % A shape B shape u w vt svd B T dot A T R u dot vt scale w sum return R scale
@contextlib contextmanagerdef reset_setup_stop_context setup_stop_after distutils core _setup_stop_afterdistutils core _setup_stop_after None yield distutils core _setup_stop_after setup_stop_after
def extract_configs configs parent_dir config_dir os path join parent_dir 'configs' if os path isdir configs shutil copytree configs config_dir symlinks True elif tarfile is_tarfile configs with tarfile open configs 'r' as tar tar extractall config_dir else raise errors Error 'Unknownconfigurationsfiletype' return config_dir
def extract_configs configs parent_dir config_dir os path join parent_dir 'configs' if os path isdir configs shutil copytree configs config_dir symlinks True elif tarfile is_tarfile configs with tarfile open configs 'r' as tar tar extractall config_dir else raise errors Error 'Unknownconfigurationsfiletype' return config_dir
def init_siteconfig app created_models verbosity db None **kwargs try site Site objects get_current except Site DoesNotExist from django contrib sites management import create_default_sitecreate_default_site app created_models verbosity db db site Site objects get_current siteconfig is_new SiteConfiguration objects get_or_create site site new_version get_version_string if is_new if Site not in created_models print u'***Migratingsettingsfromsettings_local pytothedatabase ' migrate_settings siteconfig if Site not in created_models print u'***IfyouhavepreviouslyconfiguredReviewBoardthrougha' print u'***settings_local pyfile pleaseensurethatthemigration' print u'***wassuccessfulbyverifyingyoursettingsat' print u'***%s //%s%sadmin/settings/' % siteconfig get u'site_domain_method' site domain settings SITE_ROOT siteconfig version new_versionsiteconfig save elif siteconfig version new_version print u'UpgradingReviewBoardfrom%sto%s' % siteconfig version new_version siteconfig version new_versionsiteconfig save
def monomial_count V N from sympy import factorialreturn factorial V + N / factorial V / factorial N
def dense2cvxopt value import cvxoptreturn cvxopt matrix value tc 'd'
def source name uri distribution *components from fabtools require import file as require_filepath '/etc/apt/sources list d/% name s list' % locals components '' join components source_line 'deb% uri s% distribution s% components s\n' % locals with watch path as config require_file path path contents source_line use_sudo True if config changed puts 'AddedAPTrepository %s' % source_line update_index
def induce_pcfg start productions pcount {}lcount {}for prod in productions lcount[prod lhs ] lcount get prod lhs 0 + 1 pcount[prod] pcount get prod 0 + 1 prods [ProbabilisticProduction p lhs p rhs prob pcount[p] / lcount[p lhs ] for p in pcount]return PCFG start prods
@dispatch Expr object def pre_compute leaf data scope None **kwargs return data
@contextmanagerdef persistent_grades_feature_flags global_flag enabled_for_all_courses False course_id None enabled_for_course False PersistentGradesEnabledFlag objects create enabled global_flag enabled_for_all_courses enabled_for_all_courses if course_id CoursePersistentGradesFlag objects create course_id course_id enabled enabled_for_course yield
@contextmanagerdef persistent_grades_feature_flags global_flag enabled_for_all_courses False course_id None enabled_for_course False PersistentGradesEnabledFlag objects create enabled global_flag enabled_for_all_courses enabled_for_all_courses if course_id CoursePersistentGradesFlag objects create course_id course_id enabled enabled_for_course yield
def interface_field interfaces **field_kwargs if not isinstance interfaces tuple raise TypeError 'The``interfaces``argumentmustbeatuple Got { r}' format interfaces original_invariant field_kwargs pop 'invariant' None def invariant value error_messages []if original_invariant is not None original_invariant_result _original_invariant_message original_invariant value if original_invariant_result error_messages append original_invariant_result missing_interfaces []for interface in interfaces if not interface providedBy value missing_interfaces append interface getName if missing_interfaces error_messages append 'Thevalue{ r}didnotprovidetheserequiredinterfaces {}' format value ' ' join missing_interfaces if error_messages return False '\n' join error_messages else return True '' field_kwargs['invariant'] invariantreturn field **field_kwargs
def interface_field interfaces **field_kwargs if not isinstance interfaces tuple raise TypeError 'The``interfaces``argumentmustbeatuple Got { r}' format interfaces original_invariant field_kwargs pop 'invariant' None def invariant value error_messages []if original_invariant is not None original_invariant_result _original_invariant_message original_invariant value if original_invariant_result error_messages append original_invariant_result missing_interfaces []for interface in interfaces if not interface providedBy value missing_interfaces append interface getName if missing_interfaces error_messages append 'Thevalue{ r}didnotprovidetheserequiredinterfaces {}' format value ' ' join missing_interfaces if error_messages return False '\n' join error_messages else return True '' field_kwargs['invariant'] invariantreturn field **field_kwargs
def interface_field interfaces **field_kwargs if not isinstance interfaces tuple raise TypeError 'The``interfaces``argumentmustbeatuple Got { r}' format interfaces original_invariant field_kwargs pop 'invariant' None def invariant value error_messages []if original_invariant is not None original_invariant_result _original_invariant_message original_invariant value if original_invariant_result error_messages append original_invariant_result missing_interfaces []for interface in interfaces if not interface providedBy value missing_interfaces append interface getName if missing_interfaces error_messages append 'Thevalue{ r}didnotprovidetheserequiredinterfaces {}' format value ' ' join missing_interfaces if error_messages return False '\n' join error_messages else return True '' field_kwargs['invariant'] invariantreturn field **field_kwargs
def create_pool lb name method 'ROUND_ROBIN' if __opts__['load_balancers'] get lb None username password list __opts__['load_balancers'][lb] values else raise Exception 'Unabletofind`{0}`loadbalancer' format lb F5 F5Mgmt lb username password F5 create_pool name method return True
def _GivePropertiesFromGeneralToSpecific handler_list for i j in itertools combinations xrange len handler_list 2 if handler_list[j] MatchesAll handler_list[i] if isinstance handler_list[i] SimpleHandler handler_list[i] handler_list[i] CreateOverlappedHandler handler_list[i] AddMatchingHandler handler_list[j]
def intensity_range image range_values 'image' clip_negative False if range_values 'dtype' range_values image dtype typeif range_values 'image' i_min np min image i_max np max image elif range_values in DTYPE_RANGE i_min i_max DTYPE_RANGE[range_values]if clip_negative i_min 0else i_min i_max range_valuesreturn i_min i_max
def set_up_gae_environment sdk_path if 'google' in sys modules reload_module sys modules['google'] sys path insert 0 sdk_path import dev_appserverdev_appserver fix_sys_path import google appengine tools os_compat
def reverse viewname urlconf None args None kwargs None prefix None current_app None add_prefix True prefixer get_url_prefix if prefixer prefix prefix or '/' url django_reverse viewname urlconf args kwargs prefix current_app if prefixer and add_prefix return prefixer fix url else return url
def _osbornei e d def f rv if not isinstance rv TrigonometricFunction return rva rv args[0] xreplace {d S One} if rv func is sin return sinh a / I elif rv func is cos return cosh a elif rv func is tan return tanh a / I elif rv func is cot return coth a * I elif rv func is sec return 1 / cosh a elif rv func is csc return I / sinh a else raise NotImplementedError 'unhandled%s' % rv func return bottom_up e f
def nzb_redirect wdir nzbname pp script cat priority files []for root _dirs names in os walk wdir for name in names files append os path join root name for file_ in files if os path splitext file_ [1] lower ' nzb' return Noneif len files 1 nzbname Nonefor file_ in files dirscanner ProcessSingleFile os path split file_ [1] file_ pp script cat priority priority keep False dup_check False nzbname nzbname return files
def nzb_redirect wdir nzbname pp script cat priority files []for root _dirs names in os walk wdir for name in names files append os path join root name for file_ in files if os path splitext file_ [1] lower ' nzb' return Noneif len files 1 nzbname Nonefor file_ in files dirscanner ProcessSingleFile os path split file_ [1] file_ pp script cat priority priority keep False dup_check False nzbname nzbname return files
def decode_result found return {True 'Countermodelfound' False 'Nocountermodelfound' None 'None'}[found]
def experimental api_name if not chainer disable_experimental_feature_warning warnings warn '{}isexperimental Theinterfacecanchangeinthefuture ' format api_name FutureWarning
def DumpLocalGroups path 'WinNT //%s computer' % local_name ob ADsGetObject path IID_IADsContainer ob put_Filter ['Group'] for sub_ob in ob print 'Group %s %s ' % sub_ob Name sub_ob ADsPath members sub_ob Members for member in members print 'Groupmember %s %s ' % member Name member ADsPath
def FDistribution name d1 d2 return rv name FDistributionDistribution d1 d2
def has_site_permission user mw u'mezzanine core middleware SitePermissionMiddleware'if mw not in get_middleware_setting from warnings import warnwarn mw + u'missingfromsettings MIDDLEWARE-persitepermissionsnotapplied' return user is_staff and user is_active return getattr user u'has_site_permission' False
def has_site_permission user mw u'mezzanine core middleware SitePermissionMiddleware'if mw not in get_middleware_setting from warnings import warnwarn mw + u'missingfromsettings MIDDLEWARE-persitepermissionsnotapplied' return user is_staff and user is_active return getattr user u'has_site_permission' False
def get_grid grid_url raw False fid parse_grid_id_args None grid_url response v2 grids content fid parsed_content response json if raw return parsed_contentreturn Grid parsed_content fid
def expm_multiply A B start None stop None num None endpoint None if all arg is None for arg in start stop num endpoint X _expm_multiply_simple A B else X status _expm_multiply_interval A B start stop num endpoint return X
def bpic trace model None model modelcontext model mean_deviance -2 * np mean [model logp pt for pt in trace] free_rv_means {rv name trace[rv name] mean axis 0 for rv in model free_RVs}deviance_at_mean -2 * model logp free_rv_means return 3 * mean_deviance - 2 * deviance_at_mean
def test_remove_astropy_time t1 Time '2007 001' scale 'tai' assert 'astropy_time' not in t1 FORMATS with pytest raises ValueError as err Time t1 format 'astropy_time' assert 'formatmustbeoneof' in str err
def fs2web path return '/' join path split os path sep
def fs2web path return '/' join path split os path sep
def _get_prerequisite_milestone prereq_content_key milestones milestones_api get_milestones '{usage_key}{qualifier}' format usage_key prereq_content_key qualifier GATING_NAMESPACE_QUALIFIER if not milestones log warning 'CouldnotfindgatingmilestoneforprereqUsageKey%s' prereq_content_key return Noneif len milestones > 1 log warning 'MultiplegatingmilestonesfoundforprereqUsageKey%s' prereq_content_key return milestones[0]
@contextmanagerdef set_siteconfig_settings settings siteconfig SiteConfiguration objects get_current old_settings {}for setting value in six iteritems settings old_settings[setting] siteconfig get setting siteconfig set setting value siteconfig save load_site_config try yield finally for setting value in six iteritems old_settings siteconfig set setting value siteconfig save load_site_config
def migration_get_by_instance_and_status context instance_uuid status return IMPL migration_get_by_instance_and_status context instance_uuid status
def limitedTime second func *args **kw return func *args **kw
def set_special user special cmd lst list_tab user for cron in lst['special'] if special cron['spec'] and cmd cron['cmd'] return 'present'spec {'spec' special 'cmd' cmd}lst['special'] append spec comdat _write_cron_lines user _render_tab lst if comdat['retcode'] return comdat['stderr']return 'new'
@intercept_errors UserAPIInternalError ignore_errors [UserAPIRequestError] def update_user_preferences requesting_user update user None if not user or isinstance user basestring user _get_authorized_user requesting_user user else _check_authorized requesting_user user username errors {}serializers {}for preference_key in update keys preference_value update[preference_key]if preference_value is not None try serializer create_user_preference_serializer user preference_key preference_value validate_user_preference_serializer serializer preference_key preference_value serializers[preference_key] serializerexcept PreferenceValidationError as error preference_error error preference_errors[preference_key]errors[preference_key] {'developer_message' preference_error['developer_message'] 'user_message' preference_error['user_message']}if errors raise PreferenceValidationError errors for preference_key in update keys preference_value update[preference_key]if preference_value is not None try serializer serializers[preference_key]serializer save except Exception as error raise _create_preference_update_error preference_key preference_value error else delete_user_preference requesting_user preference_key
def nagios_from_file results_file data open results_file read strip pieces data split ' ' if not len pieces 4 state 'UNKNOWN'ret 3data 'Resultsfilemalformed'else timestamp int pieces[0] time_diff time time - timestamp if time_diff > 60 * 2 ret 3state 'UNKNOWN'data 'Resultsfileisstale'else ret int pieces[1] state pieces[2]data pieces[3]return ret '%s %s' % state data
def nagios_from_file results_file data open results_file read strip pieces data split ' ' if not len pieces 4 state 'UNKNOWN'ret 3data 'Resultsfilemalformed'else timestamp int pieces[0] time_diff time time - timestamp if time_diff > 60 * 2 ret 3state 'UNKNOWN'data 'Resultsfileisstale'else ret int pieces[1] state pieces[2]data pieces[3]return ret '%s %s' % state data
def threads request document_slug doc get_document document_slug request try sort int request GET get 'sort' 0 except ValueError sort 0try desc int request GET get 'desc' 0 except ValueError desc 0desc_toggle 0 if desc else 1 threads_ sort_threads doc thread_set sort desc threads_ paginate request threads_ per_page kbforums THREADS_PER_PAGE feed_urls reverse 'wiki discuss threads feed' args [document_slug] ThreadsFeed title doc is_watching_forum request user is_authenticated and NewThreadEvent is_notifying request user doc return render request 'kbforums/threads html' {'document' doc 'threads' threads_ 'is_watching_forum' is_watching_forum 'sort' sort 'desc_toggle' desc_toggle 'feeds' feed_urls}
def threads request document_slug doc get_document document_slug request try sort int request GET get 'sort' 0 except ValueError sort 0try desc int request GET get 'desc' 0 except ValueError desc 0desc_toggle 0 if desc else 1 threads_ sort_threads doc thread_set sort desc threads_ paginate request threads_ per_page kbforums THREADS_PER_PAGE feed_urls reverse 'wiki discuss threads feed' args [document_slug] ThreadsFeed title doc is_watching_forum request user is_authenticated and NewThreadEvent is_notifying request user doc return render request 'kbforums/threads html' {'document' doc 'threads' threads_ 'is_watching_forum' is_watching_forum 'sort' sort 'desc_toggle' desc_toggle 'feeds' feed_urls}
def find_router_gw_port context cluster router_id results query_lrouter_lports cluster router_id relations 'LogicalPortAttachment' for lport in results if '_relations' in lport attachment lport['_relations'] get 'LogicalPortAttachment' if attachment and attachment get 'type' 'L3GatewayAttachment' return lport
def _read_uint64 f return np uint64 struct unpack '>Q' f read 8 [0]
def send text connections **kwargs if not isinstance connections collections Iterable connections [connections]router get_router message router new_outgoing_message text text connections connections **kwargs router send_outgoing message return message
def set_email_preferences_for_exploration user_id exploration_id mute_feedback_notifications None mute_suggestion_notifications None exploration_user_model user_models ExplorationUserDataModel get user_id exploration_id if exploration_user_model is None exploration_user_model user_models ExplorationUserDataModel create user_id exploration_id if mute_feedback_notifications is not None exploration_user_model mute_feedback_notifications mute_feedback_notificationsif mute_suggestion_notifications is not None exploration_user_model mute_suggestion_notifications mute_suggestion_notificationsexploration_user_model put
def set_email_preferences_for_exploration user_id exploration_id mute_feedback_notifications None mute_suggestion_notifications None exploration_user_model user_models ExplorationUserDataModel get user_id exploration_id if exploration_user_model is None exploration_user_model user_models ExplorationUserDataModel create user_id exploration_id if mute_feedback_notifications is not None exploration_user_model mute_feedback_notifications mute_feedback_notificationsif mute_suggestion_notifications is not None exploration_user_model mute_suggestion_notifications mute_suggestion_notificationsexploration_user_model put
def _GetLines line_strings lines []for line_string in line_strings line list map int line_string split '-' 1 if line[0] < 1 raise errors YapfError 'invalidstartoflinerange %r' % line if line[0] > line[1] raise errors YapfError 'endcomesbeforestartinlinerange %r' line lines append tuple line return lines
def _GetLines line_strings lines []for line_string in line_strings line list map int line_string split '-' 1 if line[0] < 1 raise errors YapfError 'invalidstartoflinerange %r' % line if line[0] > line[1] raise errors YapfError 'endcomesbeforestartinlinerange %r' line lines append tuple line return lines
def _GetLines line_strings lines []for line_string in line_strings line list map int line_string split '-' 1 if line[0] < 1 raise errors YapfError 'invalidstartoflinerange %r' % line if line[0] > line[1] raise errors YapfError 'endcomesbeforestartinlinerange %r' line lines append tuple line return lines
def GetWordIds text vocab pad_len None pad_id None ids []for w in text split i vocab WordToId w if i > 0 ids append i else ids append vocab WordToId UNKNOWN_TOKEN if pad_len is not None return Pad ids pad_id pad_len return ids
def main query_ip Metadata _QUERY_IPoptions parse_command_line if options options mock from tornado import testingport testing get_unused_port class Handler web RequestHandler def get self path self write path split '/' [ -1 ] application web Application [ '/ * ' Handler ] application listen port query_ip 'localhost {0}' format port def _MetadataCallback metadata print metadataioloop IOLoop current stop Metadata callback _MetadataCallback query_ip query_ip ioloop IOLoop current start return 0
def download_output project_id cluster_id output_bucket job_id print 'Downloadingoutputfile'client storage Client project project_id bucket client get_bucket output_bucket output_blob 'google-cloud-dataproc-metainfo/{}/jobs/{}/driveroutput 000000000' format cluster_id job_id return bucket blob output_blob download_as_string
def format_instances instances features header featuresdatarows [[getattr x f for f in features] for x in instances]return header datarows
def getMinimumByVector3Path path minimum Vector3 9 876543219876543e+17 9 876543219876543e+17 9 876543219876543e+17 for point in path minimum minimize point return minimum
def signal_committed_filefields sender instance **kwargs for field_name in getattr instance '_uncommitted_filefields' fieldfile getattr instance field_name if fieldfile signals saved_file send_robust sender sender fieldfile fieldfile
def signal_committed_filefields sender instance **kwargs for field_name in getattr instance '_uncommitted_filefields' fieldfile getattr instance field_name if fieldfile signals saved_file send_robust sender sender fieldfile fieldfile
def signal_committed_filefields sender instance **kwargs for field_name in getattr instance '_uncommitted_filefields' fieldfile getattr instance field_name if fieldfile signals saved_file send_robust sender sender fieldfile fieldfile
def signal_committed_filefields sender instance **kwargs for field_name in getattr instance '_uncommitted_filefields' fieldfile getattr instance field_name if fieldfile signals saved_file send_robust sender sender fieldfile fieldfile
def retry *dargs **dkw if len dargs 1 and callable dargs[0] def wrap_simple f @six wraps f def wrapped_f *args **kw return Retrying call f *args **kw return wrapped_freturn wrap_simple dargs[0] else def wrap f @six wraps f def wrapped_f *args **kw return Retrying *dargs **dkw call f *args **kw return wrapped_freturn wrap
def predict_help_ver args ns _ HELP_VER_PREDICTOR_PARSER parse_known_args args pred ns help is not None or ns version is not None return pred
def get_service services service_name config None if service_name not in services mod __import__ 'services ' + service_name globals globals locals locals fromlist ['Service'] level -1 services[service_name] mod Service services[service_name] init services[service_name] config configreturn services[service_name]
def split_datastore_path datastore_path splits datastore_path split '[' 1 [1] split ']' 1 datastore_name Nonefolder_path Nonefile_name Noneif len splits 1 datastore_name splits[0]else datastore_name path splitssplits path split '/' file_name splits[ len splits - 1 ]folder_path path[ - len file_name ]return datastore_name strip folder_path strip file_name strip
def connectServerAndClient test clientFactory serverFactory addr '127 0 0 1' clientBroker clientFactory buildProtocol addr serverBroker serverFactory buildProtocol addr clientTransport StringIO serverTransport StringIO clientBroker makeConnection protocol FileWrapper clientTransport serverBroker makeConnection protocol FileWrapper serverTransport pump IOPump clientBroker serverBroker clientTransport serverTransport def maybeDisconnect broker if not broker disconnected broker connectionLost failure Failure main CONNECTION_DONE def disconnectClientFactory clientFactory clientConnectionLost connector None reason failure Failure main CONNECTION_DONE test addCleanup maybeDisconnect clientBroker test addCleanup maybeDisconnect serverBroker test addCleanup disconnectClientFactory pump pump return clientBroker serverBroker pump
def connectServerAndClient test clientFactory serverFactory addr '127 0 0 1' clientBroker clientFactory buildProtocol addr serverBroker serverFactory buildProtocol addr clientTransport StringIO serverTransport StringIO clientBroker makeConnection protocol FileWrapper clientTransport serverBroker makeConnection protocol FileWrapper serverTransport pump IOPump clientBroker serverBroker clientTransport serverTransport def maybeDisconnect broker if not broker disconnected broker connectionLost failure Failure main CONNECTION_DONE def disconnectClientFactory clientFactory clientConnectionLost connector None reason failure Failure main CONNECTION_DONE test addCleanup maybeDisconnect clientBroker test addCleanup maybeDisconnect serverBroker test addCleanup disconnectClientFactory pump pump return clientBroker serverBroker pump
def connectServerAndClient test clientFactory serverFactory addr '127 0 0 1' clientBroker clientFactory buildProtocol addr serverBroker serverFactory buildProtocol addr clientTransport StringIO serverTransport StringIO clientBroker makeConnection protocol FileWrapper clientTransport serverBroker makeConnection protocol FileWrapper serverTransport pump IOPump clientBroker serverBroker clientTransport serverTransport def maybeDisconnect broker if not broker disconnected broker connectionLost failure Failure main CONNECTION_DONE def disconnectClientFactory clientFactory clientConnectionLost connector None reason failure Failure main CONNECTION_DONE test addCleanup maybeDisconnect clientBroker test addCleanup maybeDisconnect serverBroker test addCleanup disconnectClientFactory pump pump return clientBroker serverBroker pump
def undefine vm_ dom _get_domain vm_ return dom undefine 0
def globalsfilter input_dict check_all False filters None exclude_private None exclude_capitalized None exclude_uppercase None exclude_unsupported None excluded_names None output_dict {}for key value in list input_dict items excluded exclude_private and key startswith '_' or exclude_capitalized and key[0] isupper or exclude_uppercase and key isupper and len key > 1 and not key[1 ] isdigit or key in excluded_names or exclude_unsupported and not is_supported value check_all check_all filters filters if not excluded output_dict[key] valuereturn output_dict
@config command @click argument 'option' @click argument 'value' @configurationdef set option value from sentry import optionsfrom sentry options manager import UnknownOptiontry options set option value except UnknownOption raise click ClickException 'unknownoption %s' % option except TypeError as e raise click ClickException six text_type e
def _SortChunksByFile chunks chunks_by_file defaultdict list for chunk in chunks filepath chunk[u'range'][u'start'][u'filepath']chunks_by_file[filepath] append chunk return chunks_by_file
def expect_mc dist func lambda x 1 size 50000 def fun x return func x rvs dist rvs size size return fun rvs mean 0
def expect_mc dist func lambda x 1 size 50000 def fun x return func x rvs dist rvs size size return fun rvs mean 0
def expect_mc dist func lambda x 1 size 50000 def fun x return func x rvs dist rvs size size return fun rvs mean 0
@context quietfunc@with_devicedef write path data '' with tempfile NamedTemporaryFile as temp misc write temp name data push temp name path
@context quietfunc@with_devicedef write path data '' with tempfile NamedTemporaryFile as temp misc write temp name data push temp name path
def get_config_status cmd 'Get-DscConfigurationStatus Select-Object-PropertyHostName Status MetaData @{Name "StartDate" Expression {Get-Date $_ StartDate -Formatg}} Type Mode RebootRequested NumberofResources'return _pshell cmd
def get_config_status cmd 'Get-DscConfigurationStatus Select-Object-PropertyHostName Status MetaData @{Name "StartDate" Expression {Get-Date $_ StartDate -Formatg}} Type Mode RebootRequested NumberofResources'return _pshell cmd
def delete_net_dev dev if device_exists dev try utils execute 'ip' 'link' 'delete' dev run_as_root True check_exit_code [0 2 254] LOG debug "Netdeviceremoved '%s'" dev except processutils ProcessExecutionError with excutils save_and_reraise_exception LOG error _LE "Failedremovingnetdevice '%s'" dev
@register inclusion_tag 'utilities/render_form html' def render_form form return {'form' form}
def _make_segment_dict record return {'id' record id NETWORK_TYPE record network_type PHYSICAL_NETWORK record physical_network SEGMENTATION_ID record segmentation_id}
def masi_distance label1 label2 len_intersection len label1 intersection label2 len_union len label1 union label2 len_label1 len label1 len_label2 len label2 if len_label1 len_label2 and len_label1 len_intersection m 1elif len_intersection min len_label1 len_label2 m 0 67elif len_intersection > 0 m 0 33else m 0return 1 - len_intersection / float len_union * m
def build_tables print u'Buildinglexerandparsertables ' sys path insert 0 os path dirname __file__ from xonsh parser import ParserParser lexer_table u'lexer_table' yacc_table u'parser_table' outputdir u'xonsh' yacc_debug True sys path pop 0
def get_minibatch doc_iter size pos_class positive_class data [ u'{title}\n\n{body}' format **doc pos_class in doc['topics'] for doc in itertools islice doc_iter size if doc['topics']]if not len data return np asarray [] dtype int np asarray [] dtype int X_text y zip *data return X_text np asarray y dtype int
def skipif condition *args **kwargs if is_called_from_pytest import pytestreturn pytest mark skipif condition *args **kwargs else from nose decorators import skipifreturn skipif condition *args **kwargs
def get_version_and_flavor executable version_string get_version_from_pycaffe if version_string is None version_string get_version_from_cmdline executable if version_string is None version_string get_version_from_soname executable if version_string is None raise ValueError 'CouldnotfindversioninformationforCaffebuild' + 'at"%s" Upgradeyourinstallation' % executable version parse_version version_string if parse_version 0 99 0 > version > parse_version 0 9 0 flavor 'NVIDIA'minimum_version '0 11 0'if version < parse_version minimum_version raise ValueError 'Requiredversion"%s"isgreaterthan"%s" Upgradeyourinstallation ' % minimum_version version_string else flavor 'BVLC'return version_string flavor
def _get_random_string return '' join random choice string ascii_uppercase + string digits for _ in range RAND_LENGTH_SIZE
def make_buffer_from_bit_pattern pattern on_freq off_freq last_bit pattern[ -1 ]output_buffer []offset 0for i in range len pattern bit pattern[i]if i < len pattern - 1 next_bit pattern[ i + 1 ]else next_bit pattern[0]freq on_freq if bit '1' else off_freq tone quietnet tone freq DATASIZE offset offset output_buffer + quietnet envelope tone left last_bit '0' right next_bit '0' offset + DATASIZElast_bit bitreturn quietnet pack_buffer output_buffer
def generate_random_alphanumeric length return '' join random choice string ascii_uppercase + string digits for _x in range length
@register tag 'static_with_version' def do_static_with_version parser token return StaticWithVersionNode handle_token parser token
@pytest fixture def link_headers manager def headers response links {}for link in requests utils parse_header_links response headers get u'link' url link[u'url']page int re search u' ?< per_ page \\d ' url group 1 links[link[u'rel']] dict url url page page return linksreturn headers
def APITester request response print 'notyetimplemented'
def _comp_split_codons hsp seq_type scodon_moves scodons []for idx in range len scodon_moves[seq_type] pair scodon_moves[seq_type][idx]if not any pair continueelse assert not all pair a b pairanchor_pair hsp[ '%s_ranges' % seq_type ][ idx // 2 ]strand 1 if hsp[ '%s_strand' % seq_type ] > 0 else -1 if a func max if strand 1 else min anchor func anchor_pair start_c end_c anchor + a * strand * -1 anchor elif b func min if strand 1 else max anchor func anchor_pair start_c end_c anchor + b * strand anchor scodons append min start_c end_c max start_c end_c return scodons
def _collapse_wspace text if text is not None return '' join text split
def test_get_debug_values_no_debugger prev_value config compute_test_valuetry config compute_test_value 'off'x T vector for x_val in op get_debug_values x assert Falsefinally config compute_test_value prev_value
def constrains *args return attrsetter '_constrains' args
def MakeError name message return JsToPyException ERRORS[name] Js message
def _eintr_retry func *args while True try return func *args except OSError select error as e if e args[0] errno EINTR raise
def _label2rgb_avg label_field image bg_label 0 bg_color 0 0 0 out np zeros_like image labels np unique label_field bg labels bg_label if bg any labels labels[ labels bg_label ]out[bg] bg_colorfor label in labels mask label_field label nonzero color image[mask] mean axis 0 out[mask] colorreturn out
def test_disk_store_alt_name_relpath with TestConfig DISK_TEST_CONFIG as directory object_store empty_dataset MockDataset 1 directory write '' 'files1/000/dataset_1 dat' directory write 'foo' 'foo txt' try assert object_store get_data empty_dataset extra_dir 'dataset_1_files' alt_name ' / / /foo txt' 'foo' except ObjectInvalid pass
def test_close_process_when_aborted with pipeline get_cat_pipeline pipeline PIPE pipeline PIPE as pl assert len pl commands 1 assert pl commands[0] _process poll is None pl abort pipeline_wait pl
def getGlobalRepositoryDialogValues global globalRepositoryDialogListTablereturn euclidean getListTableElements globalRepositoryDialogListTable
def transpose_inplace x **kwargs dims list range x ndim - 1 -1 -1 return elemwise DimShuffle x broadcastable dims inplace True x
def transpose_inplace x **kwargs dims list range x ndim - 1 -1 -1 return elemwise DimShuffle x broadcastable dims inplace True x
def read_pref_mode_from_name name return _MONGOS_MODES index name
def mvstdtprob a b R df ieps 1e-05 quadkwds None mvstkwds None kwds dict args a b R df epsabs 0 0001 epsrel 0 01 limit 150 if not quadkwds is None kwds update quadkwds res err integrate quad funbgh2 *chi ppf [ieps 1 - ieps ] df **kwds prob res * bghfactor df return prob
def footprint sobject n 0for a in sobject __keylist__ v getattr sobject a if v is None continueif isinstance v Object n + footprint v continueif hasattr v '__len__' if len v n + 1continuen + 1return n
def _count_supplied_tokens args return sum 1 for arg in args if not isinstance arg astroid Keyword
def escape_abbr text return re sub '\\ ? \\s $ ' ' \\@' text
def create_youtube_string module youtube_ids [module youtube_id_0_75 module youtube_id_1_0 module youtube_id_1_25 module youtube_id_1_5]youtube_speeds ['0 75' '1 00' '1 25' '1 50']return ' ' join [' ' join pair for pair in zip youtube_speeds youtube_ids if pair[1]]
def libvlc_media_player_set_hwnd p_mi drawable f _Cfunctions get 'libvlc_media_player_set_hwnd' None or _Cfunction 'libvlc_media_player_set_hwnd' 1 1 None None MediaPlayer ctypes c_void_p return f p_mi drawable
def destroy_vm session instance vm_ref None try if not vm_ref vm_ref get_vm_ref session instance LOG debug 'DestroyingtheVM' instance instance destroy_task session _call_method session vim 'Destroy_Task' vm_ref session _wait_for_task destroy_task LOG info _LI 'DestroyedtheVM' instance instance except Exception LOG exception _LE 'DestroyVMfailed' instance instance
def dirichlet_likelihood weights alpha None if type weights is Variable n_topics weights data shape[1]else n_topics weights W data shape[1]if alpha is None alpha 1 0 / n_topics if type weights is Variable log_proportions F log_softmax weights else log_proportions F log_softmax weights W loss alpha - 1 0 * log_proportions return - F sum loss
@taskdef setup_python_macos HOMEBREW_URL 'https //raw githubusercontent com/Homebrew/install/master/install'local '/usr/bin/ruby-e"$ curl-fsSL%s "' % HOMEBREW_URL local 'echoexportPATH /usr/local/bin /usr/local/sbin $PATH>>~/ bash_profile' local 'brewinstallpython' local 'brewupdate' local 'pipinstallvirtualenvwrapper' local 'echosource/usr/local/bin/virtualenvwrapper sh>>~/ bash_profile'
def ssm_create_association name None kwargs None instance_id None call None if call 'action' raise SaltCloudSystemExit 'Thessm_create_associationactionmustbecalledwith-aor--action ' if not kwargs kwargs {}if 'instance_id' in kwargs instance_id kwargs['instance_id']if name and not instance_id instance_id _get_node name ['instanceId']if not name and not instance_id log error 'Eitheranameoraninstance_idisrequired ' return Falseif 'ssm_document' not in kwargs log error 'Assm_documentisrequired ' return Falseparams {'Action' 'CreateAssociation' 'InstanceId' instance_id 'Name' kwargs['ssm_document']}result aws query params return_root True location get_location provider get_provider product 'ssm' opts __opts__ sigver '4' log info result return result
def get_cpu_list ip user passwd cmd 'statcpu-iter1-t'showcpu_list run_ssh_thread ip user passwd cmd cpu_list []line_num 0for line in showcpu_list line_num + 1if line_num > 3 cpu_stats line split if len cpu_stats > 2 cpu_list append cpu_stats[0] split ' ' [0] return cpu_list
@contextlib contextmanagerdef expect_warnings_on db *messages **kw spec db_spec db if isinstance db util string_types and not spec config _current yield else with expect_warnings *messages **kw yield
def getPointMinimum firstPoint secondPoint return Vector3 min firstPoint x secondPoint x min firstPoint y secondPoint y min firstPoint z secondPoint z
def _initialize_backends_from_django_settings backends clear config getattr settings 'TRACKING_BACKENDS' {} for name values in config iteritems if values engine values['ENGINE']options values get 'OPTIONS' {} backends[name] _instantiate_backend_from_name engine options
def _initialize_backends_from_django_settings backends clear config getattr settings 'TRACKING_BACKENDS' {} for name values in config iteritems if values engine values['ENGINE']options values get 'OPTIONS' {} backends[name] _instantiate_backend_from_name engine options
def _item_to_entry iterator entry_pb loggers resource MessageToDict entry_pb return entry_from_resource resource iterator client loggers
def refresh_info_cache_for_instance context instance if instance info_cache is not None instance info_cache refresh
def _handlers module_name handlers_module try_import '%s handlers' % module_name if handlers_module is None return []if not hasattr handlers_module '__path__' raise HandlerError 'Module%smustbeadirectory ' % handlers_module __name__ files find_python_files handlers_module __path__[0] module_names [ '%s %s' % handlers_module __name__ file for file in files]modules [try_import mod_name for mod_name in module_names]return [get_class mod BaseHandler for mod in modules if mod]
def patch module attribute_name attribute_replacement attribute getattr module attribute_name setattr attribute_replacement __BACKUP_ATTRIBUTE_NAME attribute setattr module attribute_name attribute_replacement return is_patched module attribute_name
def patch module attribute_name attribute_replacement attribute getattr module attribute_name setattr attribute_replacement __BACKUP_ATTRIBUTE_NAME attribute setattr module attribute_name attribute_replacement return is_patched module attribute_name
def get_non_supported lang lang lang lower langs dict k lower v for k v in settings NON_SUPPORTED_LOCALES items if lang in langs if langs[lang] is None return settings LANGUAGE_CODEreturn langs[lang]return None
def update_deps post lang task task file_dep update [p for p in post fragment_deps lang if not p startswith '####MAGIC####' ]
def update_deps post lang task task file_dep update [p for p in post fragment_deps lang if not p startswith '####MAGIC####' ]
def load_class dotted_path dotted_path_split dotted_path split ' ' if len dotted_path_split > 1 klass_name dotted_path_split[ -1 ]module_name ' ' join dotted_path_split[ -1 ] module load_module module_name if has_attribute module klass_name klass getattr module klass_name return klasselse raise AttributeError 'Module%sdoesnothaveclassattribute%s' % module_name klass_name else raise ValueError 'Dottedmodulepath%smustcontainamodulenameandaclassname' % dotted_path
def load_class dotted_path dotted_path_split dotted_path split ' ' if len dotted_path_split > 1 klass_name dotted_path_split[ -1 ]module_name ' ' join dotted_path_split[ -1 ] module load_module module_name if has_attribute module klass_name klass getattr module klass_name return klasselse raise AttributeError 'Module%sdoesnothaveclassattribute%s' % module_name klass_name else raise ValueError 'Dottedmodulepath%smustcontainamodulenameandaclassname' % dotted_path
def run_bokchoy options passthrough_options test_suite BokChoyTestSuite 'bok-choy' passthrough_options passthrough_options **options msg colorize 'green' 'Runningtestsusing{default_store}modulestore ' format default_store test_suite default_store print msgtest_suite run
def estimate_graph_size old_chunks new_chunks crossed_size reduce mul len oc + len nc for oc nc in zip old_chunks new_chunks return crossed_size
def estimate_graph_size old_chunks new_chunks crossed_size reduce mul len oc + len nc for oc nc in zip old_chunks new_chunks return crossed_size
def validate instance schema cls None use_default True allow_default_none False *args **kwargs instance copy deepcopy instance schema_type schema get 'type' None instance_is_dict isinstance instance dict if use_default and allow_default_none schema modify_schema_allow_default_none schema schema if use_default and schema_type 'object' and instance_is_dict instance assign_default_values instance instance schema schema jsonschema validate instance instance schema schema cls cls *args **kwargs return instance
def validate instance schema cls None use_default True allow_default_none False *args **kwargs instance copy deepcopy instance schema_type schema get 'type' None instance_is_dict isinstance instance dict if use_default and allow_default_none schema modify_schema_allow_default_none schema schema if use_default and schema_type 'object' and instance_is_dict instance assign_default_values instance instance schema schema jsonschema validate instance instance schema schema cls cls *args **kwargs return instance
def update_schema_task cursor None num_updated 0 batch_size 100 reload models_v2 query models_v2 Picture query pictures next_cursor more query fetch_page batch_size start_cursor cursor to_put []for picture in pictures picture num_votes 1picture avg_rating 5to_put append picture if to_put ndb put_multi to_put num_updated + len to_put logging info 'Put{}entitiestoDatastoreforatotalof{}' format len to_put num_updated if more deferred defer update_schema_task cursor next_cursor num_updated num_updated else logging debug 'update_schema_taskcompletewith{0}updates ' format num_updated
def add_diversity table table['Obscure'] [ 'Mozilla/5 0 compatible Googlebot/2 1 +http //www google com/bot html' 'GoogleBot' 'Wget/1 16 1 linux-gnu ' 'wget1 16 1' 'curl/7 40 0' 'curl7 40 0' ]return table
def save_subs_to_store subs subs_id item language 'en' filedata json dumps subs indent 2 filename subs_filename subs_id language return save_to_store filedata filename 'application/json' item location
def download url u'' method GET query {} timeout 10 cached True throttle 0 proxy None user_agent USER_AGENT referrer REFERRER authentication None unicode False return URL url method query download timeout cached throttle proxy user_agent referrer authentication unicode
def get_source_line node while node if node source or node line return node source node line node node parentreturn None None
def id_to_ec2_id instance_id template 'i-%08x' return template % int instance_id
def render_to_link_list link_list return render_to_js_vardef 'tinyMCELinkList' link_list
def create_categories cr categories p_id Nonecategory []while categories category append categories[0] xml_id 'module_category_' + '_' join map lambda x x lower category replace '&' 'and' replace '' '_' cr execute 'SELECTres_idFROMir_model_dataWHEREname %sANDmodule %sANDmodel %s' xml_id 'base' 'ir module category' c_id cr fetchone if not c_id cr execute 'INSERTINTOir_module_category name parent_id VALUES %s %s RETURNINGid' categories[0] p_id c_id cr fetchone [0]cr execute 'INSERTINTOir_model_data module name res_id model VALUES %s %s %s %s ' 'base' xml_id c_id 'ir module category' else c_id c_id[0]p_id c_idcategories categories[1 ]return p_id
def get_service hass config discovery_info None return DemoNotificationService hass
def cifs registry xml_parent data console_prefix 'CIFS 'plugin_tag 'jenkins plugins publish__over__cifs CifsPublisherPlugin'publisher_tag 'jenkins plugins publish__over__cifs CifsPublisher'transfer_tag 'jenkins plugins publish__over__cifs CifsTransfer'plugin_reference_tag 'jenkins plugins publish_over_cifs CifsPublisherPlugin'base_publish_over xml_parent data console_prefix plugin_tag publisher_tag transfer_tag plugin_reference_tag
def cifs registry xml_parent data console_prefix 'CIFS 'plugin_tag 'jenkins plugins publish__over__cifs CifsPublisherPlugin'publisher_tag 'jenkins plugins publish__over__cifs CifsPublisher'transfer_tag 'jenkins plugins publish__over__cifs CifsTransfer'plugin_reference_tag 'jenkins plugins publish_over_cifs CifsPublisherPlugin'base_publish_over xml_parent data console_prefix plugin_tag publisher_tag transfer_tag plugin_reference_tag
def start_webserver_any_free_port ip webroot handler WebHandler web_server HTTPServer ip 0 webroot handler server_thread threading Thread target web_server serve_forever server_thread name 'WebServer'server_thread daemon Trueserver_thread start web_server wait_for_start return server_thread web_server get_port
@utils template_doc **get_config_file def mpl_to_plotly fig resize False strip_style False verbose False if matplotlylib renderer matplotlylib PlotlyRenderer matplotlylib Exporter renderer run fig if resize renderer resize if strip_style renderer strip_style if verbose print renderer msgreturn renderer plotly_figelse warnings warn "TousePlotly'smatplotlylibfunctionality you'llneedtohavematplotlibsuccessfullyinstalledwithallofitsdependencies You'regettingthiserrorbecausematplotliboroneofitsdependenciesdoesn'tseemtobeinstalledcorrectly "
def validate_ok_for_replace replacement validate_is_mapping 'replacement' replacement if replacement and not isinstance replacement RawBSONDocument first next iter replacement if first startswith '$' raise ValueError 'replacementcannotinclude$operators'
def default_serialize_error req resp exception representation Nonepreferred req client_prefers 'application/xml' 'text/xml' 'application/json' if preferred is None accept req accept lower if '+json' in accept preferred 'application/json'elif '+xml' in accept preferred 'application/xml'if preferred is not None if preferred 'application/json' representation exception to_json else representation exception to_xml resp body representationresp content_type preferred + ' charset UTF-8' resp append_header 'Vary' 'Accept'
def update_record_field table sys_id field value client _get_client client table tableresponse client update {field value} sys_id return response
def update_record_field table sys_id field value client _get_client client table tableresponse client update {field value} sys_id return response
@taskdef test_sympy with cd '/home/vagrant/repos/sympy' run ' /setup pytest'
def denoise_nl_means image patch_size 7 patch_distance 11 h 0 1 multichannel None fast_mode True if multichannel is None warn 'denoise_nl_meanswilldefaulttomultichannel Falseinv0 15' multichannel Trueif image ndim 2 image image[ np newaxis]multichannel Trueif image ndim 3 raise NotImplementedError 'Non-localmeansdenoisingisonlyimplementedfor2DgrayscaleandRGBimagesor3-Dgrayscaleimages ' if multichannel if fast_mode return np squeeze np array _fast_nl_means_denoising_2d image patch_size patch_distance h else return np squeeze np array _nl_means_denoising_2d image patch_size patch_distance h elif fast_mode return np array _fast_nl_means_denoising_3d image s patch_size d patch_distance h h else return np array _nl_means_denoising_3d image patch_size patch_distance h
def generate_addon_preview addon color random choice ImageColor colormap keys im Image new 'RGB' 320 480 color p Preview objects create addon addon caption 'Screenshot1' position 1 f tempfile NamedTemporaryFile im save f 'png' resize_preview f name p
def _specify_repositories base disablerepo enablerepo base read_all_repos repos base reposfor repo_pattern in disablerepo for repo in repos get_matching repo_pattern repo disable for repo_pattern in enablerepo for repo in repos get_matching repo_pattern repo enable
def _AddConditionalProperty properties condition name value if name not in properties properties[name] {}values properties[name]if value not in values values[value] []conditions values[value]conditions append condition
def make_sure_keen_schemas_match source_collection destination_collection keen_client source_schema keen_client get_collection source_collection destination_schema keen_client get_collection destination_collection return source_schema destination_schema
def make_sure_keen_schemas_match source_collection destination_collection keen_client source_schema keen_client get_collection source_collection destination_schema keen_client get_collection destination_collection return source_schema destination_schema
def VerifyNoCollidingTargets targets used {}for target in targets path name target rsplit ' ' 1 subdir gyp os path split path if not subdir subdir ' 'key subdir + ' ' + name if key in used raise GypError 'Duplicatetargetname"%s"indirectory"%s"usedbothin"%s"and"%s" ' % name subdir gyp used[key] used[key] gyp
def write_course_block_detail_report course_data with open 'xblock_course_detail csv' 'wb' as csvfile detail_writer csv writer csvfile delimiter ' ' quotechar '"' quoting csv QUOTE_ALL detail_writer writerow ['XBLOCK_TYPE_NAME' 'COURSE_NAME' 'COURSE_ID' 'COURSE_START' 'COURSE_END' 'NUM_XBLOCK_INSTANCES'] for course in course_data for block_type count in course get BLOCK_COUNTS_KEY [] items if count > 0 detail_writer writerow [block_type course get COURSE_NAME_KEY '' encode 'utf-8' course get COURSE_ID_KEY '' course get COURSE_START_KEY '' course get COURSE_END_KEY '' str count ] csvfile close
def write_course_block_detail_report course_data with open 'xblock_course_detail csv' 'wb' as csvfile detail_writer csv writer csvfile delimiter ' ' quotechar '"' quoting csv QUOTE_ALL detail_writer writerow ['XBLOCK_TYPE_NAME' 'COURSE_NAME' 'COURSE_ID' 'COURSE_START' 'COURSE_END' 'NUM_XBLOCK_INSTANCES'] for course in course_data for block_type count in course get BLOCK_COUNTS_KEY [] items if count > 0 detail_writer writerow [block_type course get COURSE_NAME_KEY '' encode 'utf-8' course get COURSE_ID_KEY '' course get COURSE_START_KEY '' course get COURSE_END_KEY '' str count ] csvfile close
def FilterFnTable fn_table symbol new_table list for entry in fn_table if entry[0] symbol new_table append entry return new_table
def do_one *rules def do_one_rl expr for rl in rules result rl expr if result expr return resultreturn exprreturn do_one_rl
def do_one *rules def do_one_rl expr for rl in rules result rl expr if result expr return resultreturn exprreturn do_one_rl
def tokenize source may_divide Falsepos 0lineno 1end len source while pos < end for token_type rule in rules match rule match source pos if match is not None breakelse if may_divide match division_re match source pos token_type 'operator'else match regex_re match source pos token_type 'regexp'if match is None pos + 1continuetoken_value match group if token_type is not None token Token token_type token_value lineno may_divide indicates_division token yield token lineno + len line_re findall token_value pos match end
def _generate_tar_package target sources sources_dir suffix mode _get_tar_mode_from_suffix suffix tar tarfile open target mode manifest _archive_package_sources tar add sources sources_dir manifest_path '%s MANIFEST' % target m open manifest_path 'w' print >>m '\n' join manifest + '\n' m close tar add manifest_path _PACKAGE_MANIFEST tar close return None
def _generate_tar_package target sources sources_dir suffix mode _get_tar_mode_from_suffix suffix tar tarfile open target mode manifest _archive_package_sources tar add sources sources_dir manifest_path '%s MANIFEST' % target m open manifest_path 'w' print >>m '\n' join manifest + '\n' m close tar add manifest_path _PACKAGE_MANIFEST tar close return None
def test_LogNorm ln mcolors LogNorm clip True vmax 5 assert_array_equal ln [1 6] [0 1 0]
@verbosedef expand_tweetids_demo ids_f StringIO '588665495492124672\n588665495487909888\n588665495508766721\n588665495513006080\n588665495517200384\n588665495487811584\n588665495525588992\n588665495487844352\n588665495492014081\n588665495512948737' oauth credsfromfile client Query **oauth hydrated client expand_tweetids ids_f for tweet in hydrated id_str tweet['id_str']print 'id {}' format id_str text tweet['text']if text startswith '@null' text '[Tweetnotavailable]'print text + '\n'
def get_public_modules path base_package None result []for subdir _ files in os walk path if any [part startswith '_' for part in subdir split os path sep ] continue _ rel_dir subdir split path rel_dir rel_dir lstrip os path sep for filename in files if is_valid_module filename mod_name _ os path splitext filename rel_path os path join rel_dir mod_name if base_package is not None rel_path os path join base_package rel_path rel_path rel_path replace os path sep ' ' if mod_name '__init__' result append rel_path[ - len ' __init__' ] else result append rel_path return result
def trace backlinks source_sents_lens target_sents_lens links []position len source_sents_lens len target_sents_lens while position 0 0 and all p > 0 for p in position try s t backlinks[position]except TypeError position position[0] - 1 position[1] - 1 continuefor i in range s for j in range t links append position[0] - i - 1 position[1] - j - 1 position position[0] - s position[1] - t return links[ -1 ]
def write_translations_file app lang full_dict None app_messages None if not app_messages app_messages get_messages_for_app app if not app_messages returntpath frappe get_pymodule_path app u'translations' frappe create_folder tpath write_csv_file os path join tpath lang + u' csv' app_messages full_dict or get_full_dict lang
def read_installed_packages_list def read_from_file config_filename '\nReadsaninstalled lstfilefromagivenlocation\n\n paramconfig_filename theconfigurationfiletoread\n'global installed_packages_listtry installed_list_file open config_filename except IOError passelse for line in installed_list_file l line rstrip split '' if l installed_packages_list[l[0]] this_package package_info config_filename l[0] l[1] l[2] urllib unquote l[3] urllib unquote l[4] else passif super_powers read_from_file os path join dataset_conf_path 'installed lst' else paths [os path join root_conf_path 'installed lst' os path join user_conf_path 'installed lst' ]try paths + [os path join x 'installed lst' for x in re split ' ' os environ['PYLEARN2_DATA_PATH'] ]except Exception passfor path in paths read_from_file path if len installed_packages_list 0 logger warning '[cf]noinstall lstfound willbecreatedoninstall/upgrade '
def group_create groupname user None host None port None maintenance_db None password None createdb None createuser None createroles None encrypted None login None inherit None superuser None replication None rolepassword None groups None runas None return _role_create groupname user user typ_ 'group' host host port port maintenance_db maintenance_db password password createdb createdb createroles createroles createuser createuser encrypted encrypted login login inherit inherit superuser superuser replication replication rolepassword rolepassword groups groups runas runas
def delete_virtual hostname username password name ret {'name' name 'changes' {} 'result' False 'comment' ''}if __opts__['test'] return _test_output ret 'delete' params {'hostname' hostname 'username' username 'password' password 'name' name} existing __salt__['bigip list_virtual'] hostname username password name if existing['code'] 200 deleted __salt__['bigip delete_virtual'] hostname username password name if deleted['code'] 200 ret['result'] Trueret['comment'] 'Virtualwassuccessfullydeleted 'ret['changes']['old'] existing['content']ret['changes']['new'] {}else ret _load_result deleted ret elif existing['code'] 404 ret['result'] Trueret['comment'] 'Thisvirtualalreadydoesnotexist Nochangesmade 'ret['changes']['old'] {}ret['changes']['new'] {}else ret _load_result existing ret return ret
def moment X n c 0 condition None **kwargs return expectation X - c ** n condition **kwargs
@decoratordef needs_sqlite f self *a **kw if sqlite3 is None or not self enabled return []else return f self *a **kw
@pytest fixturedef topicsread user topic last_read topicsread TopicsRead topicsread user_id user idtopicsread topic_id topic idtopicsread forum_id topic forum_idtopicsread last_read last_readtopicsread save return topicsread
def binipdisplay s if len s % 4 0 raise EnvironmentErrorol []for i in range len s / 4 s1 s[ 4]s s[4 ]ip []for j in s1 ip append str ord j ol append string join ip ' ' return ol
def _shouldEnableNewStyle value os environ get 'TWISTED_NEWSTYLE' '' if value in ['' 'no' 'false' 'False' '0'] return Falseelse return True
def _do_mb_put path return _mb_request path 'PUT' AUTH_YES True
def absent name purge False force False ret {'name' name 'changes' {} 'result' True 'comment' ''}lusr __salt__['user info'] name if lusr if __opts__['test'] ret['result'] Noneret['comment'] 'User{0}setforremoval' format name return retbeforegroups set salt utils get_group_list name ret['result'] __salt__['user delete'] name purge force aftergroups set [g for g in beforegroups if __salt__['group info'] g ] if ret['result'] ret['changes'] {}for g in beforegroups - aftergroups ret['changes']['{0}group' format g ] 'removed'ret['changes'][name] 'removed'ret['comment'] 'Removeduser{0}' format name else ret['result'] Falseret['comment'] 'Failedtoremoveuser{0}' format name return retret['comment'] 'User{0}isnotpresent' format name return ret
def absent name purge False force False ret {'name' name 'changes' {} 'result' True 'comment' ''}lusr __salt__['user info'] name if lusr if __opts__['test'] ret['result'] Noneret['comment'] 'User{0}setforremoval' format name return retbeforegroups set salt utils get_group_list name ret['result'] __salt__['user delete'] name purge force aftergroups set [g for g in beforegroups if __salt__['group info'] g ] if ret['result'] ret['changes'] {}for g in beforegroups - aftergroups ret['changes']['{0}group' format g ] 'removed'ret['changes'][name] 'removed'ret['comment'] 'Removeduser{0}' format name else ret['result'] Falseret['comment'] 'Failedtoremoveuser{0}' format name return retret['comment'] 'User{0}isnotpresent' format name return ret
def apply_path_dir_wildcard dirs path_dir_wildcard path_globs tuple pg for d in dirs dependencies for pg in PathGlob create_from_spec d stat d path path_dir_wildcard remainder return PathsExpansion Paths tuple path_globs
def create_channels logger log_info 'Creatingdefaultchannels ' goduser get_god_player for channeldict in settings DEFAULT_CHANNELS channel create create_channel **channeldict channel connect goduser
def udpateDatabaseFromKippt global settingsif not os path exists os path join alp local settings['sqliteDB'] createDatabase clips readAllClips if clips None returnconn lite connect settings['sqliteDB'] with conn cur conn cursor cur execute 'DELETEFROMClips' for clip in clips title clip['title']subtitle clip['title']url clip['url']app_url clip['app_url']notes clip['notes']sql 'INSERTINTOClipsVALUES NULL 'sql + '"%s" ' % title sql + '"%s" ' % subtitle sql + '"%s" ' % url sql + '"%s" ' % app_url sql + '"%s"' % notes sql + ' 'cur execute sql
def udpateDatabaseFromKippt global settingsif not os path exists os path join alp local settings['sqliteDB'] createDatabase clips readAllClips if clips None returnconn lite connect settings['sqliteDB'] with conn cur conn cursor cur execute 'DELETEFROMClips' for clip in clips title clip['title']subtitle clip['title']url clip['url']app_url clip['app_url']notes clip['notes']sql 'INSERTINTOClipsVALUES NULL 'sql + '"%s" ' % title sql + '"%s" ' % subtitle sql + '"%s" ' % url sql + '"%s" ' % app_url sql + '"%s"' % notes sql + ' 'cur execute sql
def identify str if not str return 'empty_ae_name_'rv ''ok string ascii_letters + '_' ok2 ok + string digits for c in str if c in ok rv rv + c elif c '' rv rv + '_' else rv rv + '_%02 2x_' % ord c ok ok2if keyword iskeyword rv rv rv + '_' return rv
def nonsingular vmin vmax expander 0 001 tiny 1e-15 increasing True if not np isfinite vmin or not np isfinite vmax return - expander expander swapped Falseif vmax < vmin vmin vmax vmax vmin swapped Trueif vmax - vmin < max abs vmin abs vmax * tiny if vmin 0 0 vmin - expander vmax expanderelse vmin - expander * abs vmin vmax + expander * abs vmax if swapped and not increasing vmin vmax vmax vmin return vmin vmax
def get_theme_dirs themes_dir None return [_dir for _dir in os listdir themes_dir if is_theme_dir themes_dir / _dir ]
@_docstring 'labels' browse True def browse_labels release None includes [] limit None offset None valid_includes VALID_BROWSE_INCLUDES['labels']params {'release' release}return _browse_impl 'label' includes valid_includes limit offset params
def bsd_jail_path jid if jid 0 jls_output call GET_BSD_JAIL_PATH % jid [] if len jls_output 2 and len jls_output[1] split 4 return jls_output[1] split [3]return None
def QueryValueEx key value_name regqueryvalueex advapi32['RegQueryValueExW']regqueryvalueex restype ctypes c_longregqueryvalueex argtypes [ctypes c_void_p ctypes c_wchar_p LPDWORD LPDWORD LPBYTE LPDWORD]size 256data_type ctypes wintypes DWORD while True tmp_size ctypes wintypes DWORD size buf ctypes create_string_buffer size rc regqueryvalueex key handle value_name LPDWORD ctypes byref data_type ctypes cast buf LPBYTE ctypes byref tmp_size if rc ERROR_MORE_DATA breakif size > 10 * 1024 * 1024 raise exceptions WindowsError 'ValuetoobigtobereadbyGRR ' size * 2if rc ERROR_SUCCESS raise ctypes WinError 2 return Reg2Py buf tmp_size value data_type value data_type value
def _get_profile self if not hasattr self u'_profile' self _profile Profile objects get_or_create user self [0]self _profile user selfif self _profile extra_data is None self _profile extra_data {}return self _profile
def get_testfile_path name return os path join os path dirname os path realpath __file__ name
def _convert_args args converted []for arg in args if isinstance arg dict for key in list arg keys if key '__kwarg__' continueconverted append '{0} {1}' format key arg[key] else converted append arg return converted
def _strip_exc exc return re sub '^Command[\\\'"] +[\\\'"]failed ' '' exc strerror
def header msg printprint msgprint '-----------------------------'
def dir_size start_path total_size 0for dirpath dirnames filenames in os walk start_path for f in filenames fp os path join dirpath f if not os path islink fp total_size + os path getsize fp return total_size
def _netmaskToPrefixlen netmask netlen _count0Bits netmask masklen _count1Bits netmask _checkNetmask netmask masklen return masklen - netlen
def entails expr formula_set {} formula_set list formula_set formula_set append Not expr return not satisfiable And *formula_set
def entails expr formula_set {} formula_set list formula_set formula_set append Not expr return not satisfiable And *formula_set
def run_wsgi_app application run_bare_wsgi_app add_wsgi_middleware application
def run_wsgi_app application run_bare_wsgi_app add_wsgi_middleware application
def relpath_to_site lang target_lang path _SITES_RELPATH_DB get lang target_lang None if path is None siteurl _SITE_DB get lang _MAIN_SITEURL target_siteurl _SITE_DB get target_lang _MAIN_SITEURL path posixpath relpath get_site_path target_siteurl get_site_path siteurl _SITES_RELPATH_DB[ lang target_lang ] pathreturn path
def relpath_to_site lang target_lang path _SITES_RELPATH_DB get lang target_lang None if path is None siteurl _SITE_DB get lang _MAIN_SITEURL target_siteurl _SITE_DB get target_lang _MAIN_SITEURL path posixpath relpath get_site_path target_siteurl get_site_path siteurl _SITES_RELPATH_DB[ lang target_lang ] pathreturn path
def notify_list_user_subscribed e target e['target']if target['screen_name'] c['original_name'] returnsource e['source']target_object [e['target_object']]created_at e['created_at']source_user cycle_color source['name'] + color_func c['NOTIFICATION']['source_nick'] '@' + source['screen_name'] notify color_func c['NOTIFICATION']['notify'] 'subscribedtoyourlist' date parser parse created_at clock fallback_humanize date clock color_func c['NOTIFICATION']['clock'] clock meta c['NOTIFY_FORMAT']meta source_user join meta split '#source_user' meta notify join meta split '#notify' meta clock join meta split '#clock' meta emojize meta printNicely '' printNicely meta print_list target_object noti True
def absent name vname None use_32bit_registry False ret {'name' name 'result' True 'changes' {} 'comment' ''} hive key _parse_key name reg_check __salt__['reg read_value'] hive hive key key vname vname use_32bit_registry use_32bit_registry if not reg_check['success'] or reg_check['vdata'] ' valuenotset ' ret['comment'] '{0}isalreadyabsent' format name return retremove_change {'Key' '{0}\\{1}' format hive key 'Entry' '{0}' format vname if vname else ' Default ' }if __opts__['test'] ret['result'] Noneret['changes'] {'reg' {'Willremove' remove_change}}return retret['result'] __salt__['reg delete_value'] hive hive key key vname vname use_32bit_registry use_32bit_registry if not ret['result'] ret['changes'] {}ret['comment'] 'Failedtoremove{0}from{1}' format key hive else ret['changes'] {'reg' {'Removed' remove_change}}ret['comment'] 'Removed{0}from{1}' format key hive return ret
def get_all_specs context filters None marker None limit None offset None sort_keys None sort_dirs None return objects QualityOfServiceSpecsList get_all context filters filters marker marker limit limit offset offset sort_keys sort_keys sort_dirs sort_dirs
def calculate_group_scores messages user_email now datetime datetime now message_ids_to_scores {}molecules_dict defaultdict set def get_message_list_weight message_ids return sum [message_ids_to_scores[m_id] for m_id in message_ids] for msg in messages participants _get_participants msg [user_email] if len participants > MIN_GROUP_SIZE molecules_dict[tuple participants ] add msg id message_ids_to_scores[msg id] _get_message_weight now msg date if len molecules_dict > SOCIAL_MOLECULE_LIMIT return {}if len molecules_dict < SOCIAL_MOLECULE_EXPANSION_LIMIT _expand_molecule_pool molecules_dict molecules_list [ set emails set msgs for emails msgs in molecules_dict iteritems if get_message_list_weight msgs > MIN_MESSAGE_COUNT ]molecules_list _subsume_molecules molecules_list get_message_list_weight molecules_list _combine_similar_molecules molecules_list return {' ' join sorted g get_message_list_weight m for g m in molecules_list}
def calculate_group_scores messages user_email now datetime datetime now message_ids_to_scores {}molecules_dict defaultdict set def get_message_list_weight message_ids return sum [message_ids_to_scores[m_id] for m_id in message_ids] for msg in messages participants _get_participants msg [user_email] if len participants > MIN_GROUP_SIZE molecules_dict[tuple participants ] add msg id message_ids_to_scores[msg id] _get_message_weight now msg date if len molecules_dict > SOCIAL_MOLECULE_LIMIT return {}if len molecules_dict < SOCIAL_MOLECULE_EXPANSION_LIMIT _expand_molecule_pool molecules_dict molecules_list [ set emails set msgs for emails msgs in molecules_dict iteritems if get_message_list_weight msgs > MIN_MESSAGE_COUNT ]molecules_list _subsume_molecules molecules_list get_message_list_weight molecules_list _combine_similar_molecules molecules_list return {' ' join sorted g get_message_list_weight m for g m in molecules_list}
def calculate_group_scores messages user_email now datetime datetime now message_ids_to_scores {}molecules_dict defaultdict set def get_message_list_weight message_ids return sum [message_ids_to_scores[m_id] for m_id in message_ids] for msg in messages participants _get_participants msg [user_email] if len participants > MIN_GROUP_SIZE molecules_dict[tuple participants ] add msg id message_ids_to_scores[msg id] _get_message_weight now msg date if len molecules_dict > SOCIAL_MOLECULE_LIMIT return {}if len molecules_dict < SOCIAL_MOLECULE_EXPANSION_LIMIT _expand_molecule_pool molecules_dict molecules_list [ set emails set msgs for emails msgs in molecules_dict iteritems if get_message_list_weight msgs > MIN_MESSAGE_COUNT ]molecules_list _subsume_molecules molecules_list get_message_list_weight molecules_list _combine_similar_molecules molecules_list return {' ' join sorted g get_message_list_weight m for g m in molecules_list}
def generateSimpleSequences nCoinc 10 seqLength [5 6 7] nSeq 100 coincList range nCoinc seqList []for i in xrange nSeq if max seqLength < nCoinc seqList append random sample coincList random choice seqLength else len random choice seqLength seq []for x in xrange len seq append random choice coincList seqList append seq return seqList
def wrap_draft item item is_draft item location revision MongoRevisionKey draft item location item location replace revision MongoRevisionKey published return item
def wrap_draft item item is_draft item location revision MongoRevisionKey draft item location item location replace revision MongoRevisionKey published return item
def revision save False **kwargs doc Noneif 'document' not in kwargs doc document save True else doc kwargs['document']defaults {'summary' 'Somesummary' 'content' 'Somecontent' 'comment' 'Somecomment' 'creator' kwargs get 'creator' get_user 'document' doc 'tags' '"some" "tags"' 'toc_depth' 1}defaults update kwargs rev Revision **defaults if save rev save return rev
def dot x y if hasattr x 'getnnz' x as_sparse_variable x if hasattr y 'getnnz' y as_sparse_variable y x_is_sparse_variable _is_sparse_variable x y_is_sparse_variable _is_sparse_variable y if not x_is_sparse_variable and not y_is_sparse_variable raise TypeError return _dot x y
@signals worker_process_init connectdef attach_models *args **kwargs if settings USE_POSTGRES logger debug 'NotsettingstoragebackendsbecauseUSE_POSTGRES True' returnset_up_storage models MODELS storage MongoStorage
def apply_parallel function array chunks None depth 0 mode None extra_arguments extra_keywords {} if not dask_available raise RuntimeError "Couldnotimport'dask' Pleaseinstallusing'pipinstalldask'" if chunks is None shape array shapetry ncpu cpu_count except NotImplementedError ncpu 4chunks _get_chunks shape ncpu if mode 'wrap' mode 'periodic'elif mode 'symmetric' mode 'reflect'elif mode 'edge' mode 'nearest'def wrapped_func arr return function arr *extra_arguments **extra_keywords darr da from_array array chunks chunks return darr map_overlap wrapped_func depth boundary mode compute
def apply_parallel function array chunks None depth 0 mode None extra_arguments extra_keywords {} if not dask_available raise RuntimeError "Couldnotimport'dask' Pleaseinstallusing'pipinstalldask'" if chunks is None shape array shapetry ncpu cpu_count except NotImplementedError ncpu 4chunks _get_chunks shape ncpu if mode 'wrap' mode 'periodic'elif mode 'symmetric' mode 'reflect'elif mode 'edge' mode 'nearest'def wrapped_func arr return function arr *extra_arguments **extra_keywords darr da from_array array chunks chunks return darr map_overlap wrapped_func depth boundary mode compute
def name_to_cat fname cat None if cat is None and fname startswith '{{' n fname find '}}' if n > 2 cat fname[2 n] strip fname fname[ n + 2 ] strip logging debug 'Job%shascategory%s' fname cat return fname cat
def init proxy_factory ProxyFactory objreg register 'proxy-factory' proxy_factory QNetworkProxyFactory setApplicationProxyFactory proxy_factory
def propagate method1 method2 if method1 for attr in INHERITED_ATTRS if hasattr method1 attr and not hasattr method2 attr setattr method2 attr getattr method1 attr return method2
def text_date_synonym name def getter self return getattr self name def setter self value if isinstance value basestring try setattr self name datetime strptime value u'%Y-%m-%d' except ValueError setattr self name None else setattr self name value return synonym name descriptor property getter setter
def execute_runner runner workunit_factory None workunit_name None workunit_labels None workunit_log_config None if not isinstance runner Executor Runner raise ValueError u'TherunnerargumentmustbeajavaExecutor Runnerinstance given{}oftype{}' format runner type runner if workunit_factory is None return runner run else workunit_labels [WorkUnitLabel TOOL WorkUnitLabel NAILGUN if isinstance runner executor NailgunExecutor else WorkUnitLabel JVM ] + workunit_labels or [] with workunit_factory name workunit_name labels workunit_labels cmd runner cmd log_config workunit_log_config as workunit ret runner run stdout workunit output u'stdout' stderr workunit output u'stderr' workunit set_outcome WorkUnit FAILURE if ret else WorkUnit SUCCESS return ret
def plot_images_together images fig plt figure images [image[ 3 25] for image in images]image np concatenate images axis 1 ax fig add_subplot 1 1 1 ax matshow image cmap matplotlib cm binary plt xticks np array [] plt yticks np array [] plt show
def plot_images_together images fig plt figure images [image[ 3 25] for image in images]image np concatenate images axis 1 ax fig add_subplot 1 1 1 ax matshow image cmap matplotlib cm binary plt xticks np array [] plt yticks np array [] plt show
def ec2_add_priv_launch_key argument_table operation_model session **kwargs argument_table['priv-launch-key'] LaunchKeyArgument session operation_model 'priv-launch-key'
def unarmor pem_bytes multiple False generator _unarmor pem_bytes if not multiple return next generator return generator
def unarmor pem_bytes multiple False generator _unarmor pem_bytes if not multiple return next generator return generator
def unarmor pem_bytes multiple False generator _unarmor pem_bytes if not multiple return next generator return generator
def test_local_abstractconv_gemm image tensor ftensor4 W tensor ftensor4 conv tensor nnet conv2d image W input_shape 1 32 32 32 filter_shape 32 32 3 3 border_mode 'half' f theano function [image W] [conv] mode mode_with_gpu f numpy random rand 1 32 32 32 astype 'float32' numpy random rand 32 32 3 3 astype 'float32'
def create_new_user_confirmation user_address id_chars string ascii_letters + string digits rand random SystemRandom random_id '' join [rand choice id_chars for i in range 42 ] record UserConfirmationRecord user_address user_address id random_id record put return 'https //{}/user/confirm?code {}' format socket getfqdn socket gethostname random_id
def CopyReversedLines instream outstream blocksize 2 ** 16 line_count 0instream seek 0 2 last_block instream tell // blocksize spillover ''for iblock in xrange last_block + 1 -1 -1 instream seek iblock * blocksize data instream read blocksize lines data splitlines True lines[ -1 ] '' join lines[ -1 ] + [spillover] splitlines True if lines and not lines[ -1 ] endswith '\n' lines[ -1 ] + '\n'lines reverse if lines and iblock > 0 spillover lines pop if lines line_count + len lines data '' join lines replace '\x00' '\n DCTB ' outstream write data return line_count
def CopyReversedLines instream outstream blocksize 2 ** 16 line_count 0instream seek 0 2 last_block instream tell // blocksize spillover ''for iblock in xrange last_block + 1 -1 -1 instream seek iblock * blocksize data instream read blocksize lines data splitlines True lines[ -1 ] '' join lines[ -1 ] + [spillover] splitlines True if lines and not lines[ -1 ] endswith '\n' lines[ -1 ] + '\n'lines reverse if lines and iblock > 0 spillover lines pop if lines line_count + len lines data '' join lines replace '\x00' '\n DCTB ' outstream write data return line_count
def introspect rebulk context None return Introspection rebulk context
def introspect rebulk context None return Introspection rebulk context
def pem_managed name text backup False **kwargs file_args kwargs _get_file_args name **kwargs file_args['contents'] __salt__['x509 get_pem_entry'] text text return __states__['file managed'] **file_args
def leakage Cls *args **kwargs mem []for i in range 100 Cls *args **kwargs mem append info getMemoryUsage if mem[i] - mem[0] > THRESHOLD breakproportion i / 99 0 return round mem[i] - mem[0] / proportion 1
def DestroyInteractiveWindow global editif edit is not None and edit currentView is not None if edit currentView GetParentFrame win32ui GetMainFrame passelse edit Close edit None
def _prep_headers_to_info headers server_type meta {}sysmeta {}other {}for key val in dict headers items lkey key lower if is_user_meta server_type lkey meta[strip_user_meta_prefix server_type lkey ] valelif is_sys_meta server_type lkey sysmeta[strip_sys_meta_prefix server_type lkey ] valelse other[lkey] valreturn other meta sysmeta
@register filterdef display_url url url force_bytes url errors 'replace' return urllib unquote url decode 'utf-8' errors 'replace'
def norm_constraint tensor_var max_norm norm_axes None epsilon 1e-07 ndim tensor_var ndimif norm_axes is not None sum_over tuple norm_axes elif ndim 2 sum_over 0 elif ndim in [3 4 5] sum_over tuple range 1 ndim else raise ValueError 'Unsupportedtensordimensionality{} Mustspecify`norm_axes`' format ndim dtype np dtype theano config floatX typenorms T sqrt T sum T sqr tensor_var axis sum_over keepdims True target_norms T clip norms 0 dtype max_norm constrained_output tensor_var * target_norms / dtype epsilon + norms return constrained_output
def norm_constraint tensor_var max_norm norm_axes None epsilon 1e-07 ndim tensor_var ndimif norm_axes is not None sum_over tuple norm_axes elif ndim 2 sum_over 0 elif ndim in [3 4 5] sum_over tuple range 1 ndim else raise ValueError 'Unsupportedtensordimensionality{} Mustspecify`norm_axes`' format ndim dtype np dtype theano config floatX typenorms T sqrt T sum T sqr tensor_var axis sum_over keepdims True target_norms T clip norms 0 dtype max_norm constrained_output tensor_var * target_norms / dtype epsilon + norms return constrained_output
def DEFINE_multi parser serializer name default help flag_values FLAGS **args DEFINE_flag MultiFlag parser serializer name default help **args flag_values
def DEFINE_multi parser serializer name default help flag_values FLAGS **args DEFINE_flag MultiFlag parser serializer name default help **args flag_values
def test_message_hiding qtbot view with qtbot waitSignal view _clear_timer timeout view show_message usertypes MessageLevel info 'test' assert not view _messages
def get_config config_file repo ctx dir config_file basic_util strip_path config_file for changeset in reversed_upper_bounded_changelog repo ctx changeset_ctx repo changectx changeset for ctx_file in changeset_ctx files ctx_file_name basic_util strip_path ctx_file if ctx_file_name config_file return get_named_tmpfile_from_ctx changeset_ctx ctx_file dir return None
def find_dest_path_comp_key files src_path None src files['src']dest files['dest']src_type src['type']dest_type dest['type']if src_path is None src_path src['path']sep_table {'s3' '/' 'local' os sep}if files['dir_op'] rel_path src_path[len src['path'] ]else rel_path src_path split sep_table[src_type] [ -1 ]compare_key rel_path replace sep_table[src_type] '/' if files['use_src_name'] dest_path dest['path']dest_path + rel_path replace sep_table[src_type] sep_table[dest_type] else dest_path dest['path']return dest_path compare_key
def find_dest_path_comp_key files src_path None src files['src']dest files['dest']src_type src['type']dest_type dest['type']if src_path is None src_path src['path']sep_table {'s3' '/' 'local' os sep}if files['dir_op'] rel_path src_path[len src['path'] ]else rel_path src_path split sep_table[src_type] [ -1 ]compare_key rel_path replace sep_table[src_type] '/' if files['use_src_name'] dest_path dest['path']dest_path + rel_path replace sep_table[src_type] sep_table[dest_type] else dest_path dest['path']return dest_path compare_key
def process_figure_for_rasterizing fig bbox_inches_restore fixed_dpi None bbox_inches restore_bbox bbox_inches_restorerestore_bbox r adjust_bbox fig bbox_inches fixed_dpi return bbox_inches r
def filter_labels train label classes None if isinstance train theano tensor sharedvar SharedVariable train train get_value borrow True if isinstance label theano tensor sharedvar SharedVariable label label get_value borrow True if not isinstance train numpy ndarray or scipy sparse issparse train raise TypeError 'trainmustbeanumpyarray ascipysparsematrix oratheanosharedarray' if classes is not None label label[ classes]if scipy sparse issparse train idx label sum axis 1 nonzero [0]return train[idx] label[idx] condition label any axis 1 return tuple var compress condition axis 0 for var in train label
def SetupSharedModules module_dict output_dict {}for module_name module in module_dict iteritems if module is None continueif IsEncodingsModule module_name output_dict[module_name] modulecontinueshared_prefix ModuleNameHasPrefix module_name SHARED_MODULE_PREFIXES banned_prefix ModuleNameHasPrefix module_name NOT_SHARED_MODULE_PREFIXES if shared_prefix and not banned_prefix output_dict[module_name] modulereturn output_dict
def cs_int func func argtypes [CS_PTR POINTER c_uint ]func restype c_intfunc errcheck check_cs_getreturn func
def reduce_along_dim img dim weights indicies other_dim abs dim - 1 if other_dim 0 weights np tile weights[np newaxis np newaxis] img shape[other_dim] 1 1 3 out_img img[ indicies ] * weights out_img np sum out_img axis 2 else weights np tile weights[ np newaxis np newaxis] 1 1 img shape[other_dim] 3 out_img img[indicies ] * weights out_img np sum out_img axis 1 return out_img
def dashboard_mark_activities_old context data_dict _check_access 'dashboard_mark_activities_old' context data_dict model context['model']user_id model User get context['user'] idmodel Dashboard get user_id activity_stream_last_viewed datetime datetime utcnow if not context get 'defer_commit' model repo commit
def _get_related_models m related_models [subclass for subclass in m __subclasses__ if issubclass subclass models Model ]related_fields_models set for f in m _meta get_fields include_parents True include_hidden True if f is_relation and f related_model is not None and not isinstance f related_model str related_fields_models add f model related_models append f related_model opts m _metaif opts proxy and m in related_fields_models related_models append opts concrete_model return related_models
def parse_identifier source start throw True start pass_white source start end startif not end < len source if throw raise SyntaxError 'Missingidentifier ' return Noneif source[end] not in IDENTIFIER_START if throw raise SyntaxError 'Invalididentifierstart "%s"' % source[end] return Noneend + 1while end < len source and source[end] in IDENTIFIER_PART end + 1if not is_valid_lval source[start end] if throw raise SyntaxError 'Invalididentifiername "%s"' % source[start end] return Nonereturn source[start end] end
def __virtual__ if salt utils is_windows and 'ip get_interface' in __salt__ return __virtualname__return False
def getVersion proj base 'twisted' if proj 'core' vfile os path join base '_version py' else vfile os path join base proj '_version py' ns {'__name__' 'Nothingtoseehere'}execfile vfile ns return ns['version'] base
@before alldef start_video_server video_source_dir '{}/data/video' format settings TEST_ROOT video_server VideoSourceHttpService port_num settings VIDEO_SOURCE_PORT video_server config['root_dir'] video_source_dirworld video_source video_server
def computeNearestNeighbor username users distances []for user in users if user username distance manhattan users[user] users[username] distances append distance user distances sort return distances
@get '/scan/<taskid>/log' def scan_log taskid json_log_messages list if taskid not in DataStore tasks logger warning '[%s]InvalidtaskIDprovidedtoscan_log ' % taskid return jsonize {'success' False 'message' 'InvalidtaskID'} for time_ level message in DataStore current_db execute 'SELECTtime level messageFROMlogsWHEREtaskid ?ORDERBYidASC' taskid json_log_messages append {'time' time_ 'level' level 'message' message} logger debug '[%s]Retrievedscanlogmessages' % taskid return jsonize {'success' True 'log' json_log_messages}
def _PutSecret io_loop secret _GetSecretsManager PutSecret secret sys stdin read io_loop stop
def drop_redundant_messages messages sorted_messages sorted messages key len reverse True filtered_messages set for message in sorted_messages for filtered_message in filtered_messages if message in filtered_message breakelse filtered_messages add message return filtered_messages
def extra_job_filters not_yet_run False running False finished False assert not not_yet_run and running or not_yet_run and finished or running and finished 'Cannotspecifymorethanonefiltertothisfunction'not_queued ' SELECTjob_idFROMafe_host_queue_entriesWHEREstatus "%s" ' % models HostQueueEntry Status QUEUED not_finished ' SELECTjob_idFROMafe_host_queue_entriesWHEREnotcomplete 'if not_yet_run where [ 'idNOTIN' + not_queued ]elif running where [ ' idIN%s AND idIN%s ' % not_queued not_finished ]elif finished where [ 'idNOTIN' + not_finished ]else return {}return {'where' where}
def get_all_mfa_devices user_name region None key None keyid None profile None user get_user user_name region key keyid profile if not user msg 'Username{0}doesnotexist'log error msg format user_name return Falseconn _get_conn region region key key keyid keyid profile profile try result conn get_all_mfa_devices user_name devices result['list_mfa_devices_response']['list_mfa_devices_result']['mfa_devices']return devicesexcept boto exception BotoServerError as e log debug e if 'NotFound' in e log info 'Couldnotfinduser{0} ' format user_name return []msg 'FailedtogetallMFAdevicesforuser{0} 'log error msg format user_name return False
def retry exception_processor generic_exception_processor def yield_new_function_from f def shim *args **kwargs exc_processor_cxt Nonewhile True gevent sleep 0 1 try return f *args **kwargs except KeyboardInterrupt raiseexcept exception_info_tuple Nonetry exception_info_tuple sys exc_info exc_processor_cxt exception_processor exception_info_tuple exc_processor_cxt exc_processor_cxt finally del exception_info_tuplereturn functools wraps f shim return yield_new_function_from
def empty_if_not_sysadmin key data errors context from ckan lib navl validators import emptyuser context get 'user' ignore_auth context get 'ignore_auth' if ignore_auth or user and authz is_sysadmin user returnempty key data errors context
def CreateDefaultGUI appClass None if appClass is None import intpyappappClass intpyapp InteractivePythonAppappClass InitInstance
def Deterministic name var model None model modelcontext model var name model name_for name model deterministics append var model add_random_variable var return var
def _make_transform_graph_docs import inspectfrom textwrap import dedentfrom extern import sixfrom baseframe import BaseCoordinateFrame frame_transform_graphisclass inspect isclasscoosys [item for item in six itervalues globals if isclass item and issubclass item BaseCoordinateFrame ]graphstr frame_transform_graph to_dot_graph addnodes coosys docstr '\nThediagrambelowshowsallofthecoordinatesystemsbuiltintothe\n`~astropy coordinates`package theiraliases usefulforconverting\nothercoordinatestothemusingattribute-styleaccess andthe\npre-definedtransformationsbetweenthem Theuserisfreeto\noverrideanyofthesetransformationsbydefiningnewtransformations\nbetweenthesesystems butthepre-definedtransformationsshouldbe\nsufficientfortypicalusage \n\nThegraphalsoindicatesthepriorityforeachtransformationasa\nnumbernexttothearrow Theseprioritiesareusedtodecidethe\npreferredorderwhentwotransformationpathshavethesamenumber\nofsteps Theseprioritiesaredefinedsuchthatthepathwitha\n*smaller*totalpriorityisfavored \n\n\n graphviz \n\n'return dedent docstr + '' + graphstr replace '\n' '\n'
def dropout_layer state_before use_noise trng proj tensor switch use_noise state_before * trng binomial state_before shape p 0 5 n 1 dtype state_before dtype state_before * 0 5 return proj
@csrf_exempt@require_sync_session@api_handle_error_with_jsondef model_upload data session if 'models' not in data return JsonResponseMessageError 'Mustprovidemodels ' data {'saved_model_count' 0} status 400 try result save_serialized_models data['models'] src_version session client_version except Exception as e print 'Exceptionuploadingmodels inapi_views %s %s %s' % e __class__ __name__ e message e args result {'error' e message 'saved_model_count' 0}session models_uploaded + result['saved_model_count']session errors + result has_key 'error' return JsonResponse result
def test_scenario_has_name scenario Scenario from_string SCENARIO1 assert isinstance scenario Scenario assert_equals scenario name 'Addingsomestudentstomyuniversitydatabase'
@taskdef test_varnish ctx proc apiserver ctx wait False autoreload False try sleep 5 test_module ctx module 'api/caching/tests/test_caching py' finally proc kill
def reset return '< -- spam\nContent-Type text/html\n\n<bodybgcolor "#f0f0f8"><fontcolor "#f0f0f8"size "-5">-->\n<bodybgcolor "#f0f0f8"><fontcolor "#f0f0f8"size "-5">-->-->\n</font></font></font></script></object></blockquote></pre>\n</table></table></table></table></table></font></font></font>'
def install from tornado import ioloopassert not ioloop IOLoop initialized or ioloop IOLoop instance is IOLoop instance 'tornadoIOLoopalreadyinitialized'if tornado_version > 3 ioloop IOLoop configure ZMQIOLoop else ioloop IOLoop _instance IOLoop instance
def _LC f ring f ringk ring ngensyring ring clone symbols ring symbols[ k - 1 ] y yring gens[0]degf _deg f lcf yring zerofor monom coeff in f iterterms if monom[ -1 ] degf lcf + coeff * y ** monom[ -1 ] return lcf
def _LC f ring f ringk ring ngensyring ring clone symbols ring symbols[ k - 1 ] y yring gens[0]degf _deg f lcf yring zerofor monom coeff in f iterterms if monom[ -1 ] degf lcf + coeff * y ** monom[ -1 ] return lcf
def assert_samelines testcase text1 text2 msg None testcase assertEqual text1 splitlines text2 splitlines msg
def assert_samelines testcase text1 text2 msg None testcase assertEqual text1 splitlines text2 splitlines msg
@require_contextdef attachment_specs_get context attachment_id rows _attachment_specs_query context attachment_id all result {row['key'] row['value'] for row in rows}return result
def build_datastore_path datastore_name path return '[%s]%s' % datastore_name path
def isStackingAvailable retVal Falseif PAYLOAD TECHNIQUE STACKED in kb injection data retVal Trueelse for technique in getPublicTypeMembers PAYLOAD TECHNIQUE True _ getTechniqueData technique if _ and 'stacked' in _['title'] lower retVal Truebreakreturn retVal
def create_trunk_port network_switch port_name native_vlan 1 allowed_vlans 'all' debug Falsenew_port SwitchPort objects get_or_create port_name port_name mode 'trunk' trunk_native_vlan native_vlan trunk_allowed_vlans allowed_vlans network_switch network_switch if debug print new_port
def magic_set obj def decorator func is_class isinstance obj six class_types args varargs varkw defaults inspect getargspec func if not args or args[0] not in 'self' 'cls' 'klass' if is_class replacement staticmethod func else replacement funcelif args[0] 'self' if is_class replacement funcelse def replacement *args **kw return func obj *args **kw try replacement __name__ func __name__except passelif is_class replacement classmethod func else def replacement *args **kw return func obj __class__ *args **kw try replacement __name__ func __name__except passsetattr obj func __name__ replacement return replacementreturn decorator
def open_cover hass entity_id None data {ATTR_ENTITY_ID entity_id} if entity_id else None hass services call DOMAIN SERVICE_OPEN_COVER data
def _get_overrides_for_ccx ccx overrides_cache request_cache get_cache 'ccx-overrides' if ccx not in overrides_cache overrides {}query CcxFieldOverride objects filter ccx ccx for override in query block_overrides overrides setdefault override location {} block_overrides[override field] json loads override value block_overrides[ override field + '_id' ] override idblock_overrides[ override field + '_instance' ] overrideoverrides_cache[ccx] overridesreturn overrides_cache[ccx]
def usd value return Money value u'USD'
def usd value return Money value u'USD'
def get_data url try request requests get url request raise_for_status except requests exceptions HTTPError requests exceptions ConnectionError as e raise ParseError e items microdata get_items request text for item in items if item itemtype [microdata URI 'http //schema org/Recipe' ] return itemraise ParseError 'Norecipedatafound'
def get_data url try request requests get url request raise_for_status except requests exceptions HTTPError requests exceptions ConnectionError as e raise ParseError e items microdata get_items request text for item in items if item itemtype [microdata URI 'http //schema org/Recipe' ] return itemraise ParseError 'Norecipedatafound'
def get_data url try request requests get url request raise_for_status except requests exceptions HTTPError requests exceptions ConnectionError as e raise ParseError e items microdata get_items request text for item in items if item itemtype [microdata URI 'http //schema org/Recipe' ] return itemraise ParseError 'Norecipedatafound'
def _limit_max_file_size f @functools wraps f def wrapper *args **kwargs test_max_file_size constraints MAX_FILE_SIZEif constraints MAX_FILE_SIZE > sys maxsize test_max_file_size 2 ** 30 + 2 with mock patch object constraints 'MAX_FILE_SIZE' test_max_file_size return f *args **kwargs return wrapper
def _make_values_bytes dict_ return {k six b v for k v in dict_ items }
def _make_values_bytes dict_ return {k six b v for k v in dict_ items }
def get_unique_variable name candidates tf get_collection tf GraphKeys GLOBAL_VARIABLES name if not candidates raise ValueError 'Couldntfindvariable%s' % name for candidate in candidates if candidate op name name return candidateraise ValueError 'Variable%sdoesnotuniquelyidentifyavariable' name
def _task_info_get context task_id session None session session or get_session query session query models TaskInfo query query filter_by task_id task_id try task_info_ref query one except sa_orm exc NoResultFound LOG debug 'TaskInfowasnotfoundfortaskwithid% task_id s' {'task_id' task_id} task_info_ref Nonereturn task_info_ref
def _task_info_get context task_id session None session session or get_session query session query models TaskInfo query query filter_by task_id task_id try task_info_ref query one except sa_orm exc NoResultFound LOG debug 'TaskInfowasnotfoundfortaskwithid% task_id s' {'task_id' task_id} task_info_ref Nonereturn task_info_ref
def start_server config args logger info 'Startservermode' global serverfrom glances server import GlancesServerserver GlancesServer cached_time args cached_time config config args args print 'Glancesserverisrunningon{} {}' format args bind_address args port if args password '' server add_user args username args password server serve_forever server server_close
@mock_ec2def test_modify_attribute_blockDeviceMapping conn boto ec2 connect_to_region u'us-east-1' reservation conn run_instances u'ami-1234abcd' instance reservation instances[0]with assert_raises JSONResponseError as ex instance modify_attribute u'blockDeviceMapping' {u'/dev/sda1' True} dry_run True ex exception reason should equal u'DryRunOperation' ex exception status should equal 400 ex exception message should equal u'Anerroroccurred DryRunOperation whencallingtheModifyInstanceAttributeoperation Requestwouldhavesucceeded butDryRunflagisset' instance modify_attribute u'blockDeviceMapping' {u'/dev/sda1' True} instance ec2_backends[conn region name] get_instance instance id instance block_device_mapping should have key u'/dev/sda1' instance block_device_mapping[u'/dev/sda1'] delete_on_termination should be True
def get_port context port_id with context session begin subtransactions True try record context session query models_v2 Port enable_eagerloads False filter models_v2 Port id startswith port_id one return recordexcept exc NoResultFound returnexcept exc MultipleResultsFound LOG error _LE 'Multipleportshaveport_idstartingwith%s' port_id return
@register simple_tagdef get_gravatar email size 80 rating 'g' default None protocol PROTOCOL gravatar_protocols {'http' 'http //www' 'https' 'https //secure'}url '%s gravatar com/avatar/%s' % gravatar_protocols[protocol] md5 email strip lower encode 'utf-8' hexdigest options {'s' size 'r' rating}if default options['d'] defaulturl '%s?%s' % url urlencode options return url replace '&' '&amp '
@fixturedef db global _dbif _db is None _db orm new_session_factory 'sqlite /// memory ' echo True user orm User name getuser user servers append orm Server hub orm Hub server orm Server _db add user _db add hub _db commit return _db
def get_datacenter_id return config get_cloud_config_value 'datacenter_id' get_configured_provider __opts__ search_global False
def unique iterable seen set return [x for x in iterable if x not in seen and not seen add x ]
def static path root None mimetype None p os path join root or '' path p os path realpath p return cp lib static serve_file p content_type mimetype
def model_pk_lock func def decorator self *args **kwargs from taiga base utils db import get_typename_for_model_classpk self kwargs get self pk_url_kwarg None tn get_typename_for_model_class self get_queryset model key '{0} {1}' format tn pk with advisory_lock key return func self *args **kwargs return decorator
def model_pk_lock func def decorator self *args **kwargs from taiga base utils db import get_typename_for_model_classpk self kwargs get self pk_url_kwarg None tn get_typename_for_model_class self get_queryset model key '{0} {1}' format tn pk with advisory_lock key return func self *args **kwargs return decorator
def model_pk_lock func def decorator self *args **kwargs from taiga base utils db import get_typename_for_model_classpk self kwargs get self pk_url_kwarg None tn get_typename_for_model_class self get_queryset model key '{0} {1}' format tn pk with advisory_lock key return func self *args **kwargs return decorator
def _indent elem level 0 i '\n' + level * '' if len elem if not elem text or not elem text strip elem text i + '' for e in elem _indent e level + 1 if not e tail or not e tail strip e tail i + '' if not e tail or not e tail strip e tail ielif level and not elem tail or not elem tail strip elem tail i
def _indent elem level 0 i '\n' + level * '' if len elem if not elem text or not elem text strip elem text i + '' for e in elem _indent e level + 1 if not e tail or not e tail strip e tail i + '' if not e tail or not e tail strip e tail ielif level and not elem tail or not elem tail strip elem tail i
def FindStartOfExpressionInLine line endpos depth startchar endchar for i in xrange endpos -1 -1 if line[i] endchar depth + 1elif line[i] startchar depth - 1if depth 0 return i 0 return -1 depth
def collect_quantum_ports bridges root_helper ports []for bridge in bridges ovs ovs_lib OVSBridge bridge root_helper ports + [port port_name for port in ovs get_vif_ports ]return ports
def test_parse_no_timezone d iso8601 parse_date '2007-01-01T08 00 00' assert d year 2007 assert d month 1 assert d day 1 assert d hour 8 assert d minute 0 assert d second 0 assert d microsecond 0 assert d tzinfo iso8601 UTC
def test_parse_no_timezone d iso8601 parse_date '2007-01-01T08 00 00' assert d year 2007 assert d month 1 assert d day 1 assert d hour 8 assert d minute 0 assert d second 0 assert d microsecond 0 assert d tzinfo iso8601 UTC
def test_parse_no_timezone d iso8601 parse_date '2007-01-01T08 00 00' assert d year 2007 assert d month 1 assert d day 1 assert d hour 8 assert d minute 0 assert d second 0 assert d microsecond 0 assert d tzinfo iso8601 UTC
def get_image_files_json image_id files_json get_image_files_cache image_id if files_json return files_jsonimage_path store image_layer_path image_id with tempfile TemporaryFile as tmp_fobj for buf in store stream_read image_path tmp_fobj write buf tmp_fobj seek 0 files_json json dumps get_image_files_from_fobj tmp_fobj set_image_files_cache image_id files_json return files_json
def has_key k trie return _retrive_branch k trie is not None
def has_key k trie return _retrive_branch k trie is not None
def create_api_key name description enabled True stageKeys None region None key None keyid None profile None try stageKeys list if stageKeys is None else stageKeys conn _get_conn region region key key keyid keyid profile profile response conn create_api_key name name description description enabled enabled stageKeys stageKeys if not response return {'created' False}return {'created' True 'apiKey' _convert_datetime_str response }except ClientError as e return {'created' False 'error' salt utils boto3 get_error e }
def defaultFactoryMethod rowClass data kw newObject rowClass newObject __dict__ update kw return newObject
def defaultFactoryMethod rowClass data kw newObject rowClass newObject __dict__ update kw return newObject
@depends HAS_ESX_CLI def esxcli_cmd cmd_str host None username None password None protocol None port None esxi_hosts None ret {}if esxi_hosts if not isinstance esxi_hosts list raise CommandExecutionError "'esxi_hosts'mustbealist " for esxi_host in esxi_hosts response salt utils vmware esxcli host username password cmd_str protocol protocol port port esxi_host esxi_host if response['retcode'] 0 ret update {esxi_host {'Error' response get 'stdout' }} else ret update {esxi_host response} else response salt utils vmware esxcli host username password cmd_str protocol protocol port port if response['retcode'] 0 ret update {host {'Error' response get 'stdout' }} else ret update {host response} return ret
@depends HAS_ESX_CLI def esxcli_cmd cmd_str host None username None password None protocol None port None esxi_hosts None ret {}if esxi_hosts if not isinstance esxi_hosts list raise CommandExecutionError "'esxi_hosts'mustbealist " for esxi_host in esxi_hosts response salt utils vmware esxcli host username password cmd_str protocol protocol port port esxi_host esxi_host if response['retcode'] 0 ret update {esxi_host {'Error' response get 'stdout' }} else ret update {esxi_host response} else response salt utils vmware esxcli host username password cmd_str protocol protocol port port if response['retcode'] 0 ret update {host {'Error' response get 'stdout' }} else ret update {host response} return ret
def subrange_exercise mult lb ub m MultisetPartitionTraverser assert m count_partitions mult m count_partitions_slow mult ma MultisetPartitionTraverser mc MultisetPartitionTraverser md MultisetPartitionTraverser a_it ma enum_range mult lb ub b_it part_range_filter multiset_partitions_taocp mult lb ub c_it part_range_filter mc enum_small mult ub lb sum mult d_it part_range_filter md enum_large mult lb 0 ub for sa sb sc sd in zip_longest a_it b_it c_it d_it assert compare_multiset_states sa sb assert compare_multiset_states sa sc assert compare_multiset_states sa sd
def test_pkl_yaml_src_field try fd fn mkstemp close fd o DumDum o x 'a' 'b' 'c' serial save fn o yaml " pkl '" + fn + "'\n" loaded load yaml assert loaded x 'a' 'b' 'c' assert loaded yaml_src yaml finally os remove fn
def initial_has_dependencies tree to_process has_dependencies []for n in tree nontips include_self True if n not in to_process has_dependencies append n return has_dependencies
def initial_has_dependencies tree to_process has_dependencies []for n in tree nontips include_self True if n not in to_process has_dependencies append n return has_dependencies
def initial_has_dependencies tree to_process has_dependencies []for n in tree nontips include_self True if n not in to_process has_dependencies append n return has_dependencies
@taskdef mongorestore ctx path drop False db settings DB_NAMEport settings DB_PORTcmd 'mongorestore--db{db}--port{port}' format db db port port pty True if settings DB_USER cmd + '--username{0}' format settings DB_USER if settings DB_PASS cmd + '--password{0}' format settings DB_PASS if drop cmd + '--drop'cmd + '' + path ctx run cmd echo True
@taskdef mongorestore ctx path drop False db settings DB_NAMEport settings DB_PORTcmd 'mongorestore--db{db}--port{port}' format db db port port pty True if settings DB_USER cmd + '--username{0}' format settings DB_USER if settings DB_PASS cmd + '--password{0}' format settings DB_PASS if drop cmd + '--drop'cmd + '' + path ctx run cmd echo True
@taskdef mongorestore ctx path drop False db settings DB_NAMEport settings DB_PORTcmd 'mongorestore--db{db}--port{port}' format db db port port pty True if settings DB_USER cmd + '--username{0}' format settings DB_USER if settings DB_PASS cmd + '--password{0}' format settings DB_PASS if drop cmd + '--drop'cmd + '' + path ctx run cmd echo True
def get_preferred_submodules if 'submodules' in modules_db return modules_db['submodules']mods ['numpy' 'scipy' 'sympy' 'pandas' 'networkx' 'statsmodels' 'matplotlib' 'sklearn' 'skimage' 'mpmath' 'os' 'PIL' 'OpenGL' 'array' 'audioop' 'binascii' 'cPickle' 'cStringIO' 'cmath' 'collections' 'datetime' 'errno' 'exceptions' 'gc' 'imageop' 'imp' 'itertools' 'marshal' 'math' 'mmap' 'msvcrt' 'nt' 'operator' 'parser' 'rgbimg' 'signal' 'strop' 'sys' 'thread' 'time' 'wx' 'xxsubtype' 'zipimport' 'zlib' 'nose' 'PyQt4' 'PySide' 'os path']submodules []for m in mods submods get_submodules m submodules + submodsmodules_db['submodules'] submodulesreturn submodules
def tokenize readline tokeneater printtoken try tokenize_loop readline tokeneater except StopTokenizing pass
def tokenize readline tokeneater printtoken try tokenize_loop readline tokeneater except StopTokenizing pass
@event u'manager startup' def init_parsers manager for parser_type in PARSER_TYPES parsers[parser_type] {}for p in plugin get_plugins interface parser_type + u'_parser' parsers[parser_type][p name replace u'parser_' u'' ] p instancefunc_name u'parse_' + parser_type default_parsers[parser_type] max iter parsers[parser_type] items key lambda p getattr getattr p[1] func_name u'priority' 0 [0]log debug u'settingdefault%sparserto%s options %s ' % parser_type default_parsers[parser_type] parsers[parser_type]
def place board queens r c d g if not queens solutions append None if len queens > len board returnif len queens > len r returnif len queens > len c returnif len queens > len d returnif len queens > len g returnfor ip pos in enumerate board ar ac ad ag attackmap[pos]attacked frozenset union ar ac ad ag nboard [b for b in board[ ip + 1 ] if b not in attacked ]place nboard queens[1 ] r - ar c - ac d - ad g - ag
def place board queens r c d g if not queens solutions append None if len queens > len board returnif len queens > len r returnif len queens > len c returnif len queens > len d returnif len queens > len g returnfor ip pos in enumerate board ar ac ad ag attackmap[pos]attacked frozenset union ar ac ad ag nboard [b for b in board[ ip + 1 ] if b not in attacked ]place nboard queens[1 ] r - ar c - ac d - ad g - ag
def _integer_rational_reconstruction c m domain if c < 0 c + m r0 s0 m domain zero r1 s1 c domain one bound sqrt m / 2 while r1 > bound quo r0 // r1 r0 r1 r1 r0 - quo * r1 s0 s1 s1 s0 - quo * s1 if abs s1 > bound return Noneif s1 < 0 a b - r1 - s1 elif s1 > 0 a b r1 s1 else return Nonefield domain get_field return field a / field b
def get_random_user_agent return random choice user_agents_list
def is_empty G return not any G adj values
def capture_output environ start_response application warnings warn 'wsgilib capture_outputhasbeendeprecatedinfavorofwsgilib intercept_output' DeprecationWarning 2 data []output StringIO def replacement_start_response status headers exc_info None if data data[ ] []data append status data append headers start_response status headers exc_info return output writeapp_iter application environ replacement_start_response try for item in app_iter output write item finally if hasattr app_iter 'close' app_iter close if not data data append None if len data < 2 data append None data append output getvalue return data
def ReverseBitsInt64 v v v >> 1 & 6148914691236517205 v & 6148914691236517205 << 1 v v >> 2 & 3689348814741910323 v & 3689348814741910323 << 2 v v >> 4 & 1085102592571150095 v & 1085102592571150095 << 4 v v >> 8 & 71777214294589695 v & 71777214294589695 << 8 v v >> 16 & 281470681808895 v & 281470681808895 << 16 v int v >> 32 v << 32 & 18446744073709551615L return v
@register tagdef render_table parser token bits token split_contents bits pop 0 table parser compile_filter bits pop 0 template parser compile_filter bits pop 0 if bits else None return RenderTableNode table template
def _get_used_lun_ids_for_mappings mappings used_luns set map lambda lun int lun['lun'] mappings used_luns add 0 return used_luns
def _get_used_lun_ids_for_mappings mappings used_luns set map lambda lun int lun['lun'] mappings used_luns add 0 return used_luns
def label2rgb label image None colors None alpha 0 3 bg_label -1 bg_color 0 0 0 image_alpha 1 kind 'overlay' if kind 'overlay' return _label2rgb_overlay label image colors alpha bg_label bg_color image_alpha else return _label2rgb_avg label image bg_label bg_color
def create_normal_player session name password if _throttle session maxlim 5 timeout 5 * 60 session msg '{RYoumadetoomanyconnectionattempts Tryagaininafewminutes {n' return Noneplayer authenticate username name password password if not player session msg 'Incorrectlogininformationgiven ' _throttle session player PlayerDB objects get_player_from_name name if player player at_failed_login session return Nonebans ServerConfig objects conf 'server_bans' if bans and any tup[0] player name lower for tup in bans or any tup[2] match session address for tup in bans if tup[2] string '{rYouhavebeenbannedandcannotcontinuefromhere \nIfyoufeelthisbanisinerror pleaseemailanadmin {x'session msg string session sessionhandler disconnect session 'Goodbye Disconnecting ' return Nonereturn player
def _leftmost_descendants node try treepos node treepositions except AttributeError return []return [node[x] for x in treepos[1 ] if all y 0 for y in x ]
def Rebuild verbose 1 clsidToTypelib clear infos GetGeneratedInfos if verbose and len infos print 'RebuildingcacheofgeneratedfilesforCOMsupport 'for info in infos iid lcid major minor infoif verbose print 'Checking' GetGeneratedFileName *info try AddModuleToCache iid lcid major minor verbose 0 except print 'Couldnotaddmodule%s-%s %s' % info sys exc_info [0] sys exc_info [1] if verbose and len infos print 'Done '_SaveDicts
def find predicate seq for element in seq if predicate element return elementreturn None
def find predicate seq for element in seq if predicate element return elementreturn None
def keyboard_role name rawtext text lineno inliner options {} content [] new_element nodes literal rawtext text new_element set_class 'kbd' return [new_element] []
def skill if auth permission format 'popup' return ''def prep r if r method 'create' return Falsereturn Trues3 prep prepoutput s3_rest_controller 'hrm' 'skill' return output
def dmp_slice_in f m n j u K if j < 0 or j > u raise IndexError '-%s< j<%sexpected got%s' % u u j if not u return dup_slice f m n K f g dmp_to_dict f u {} for monom coeff in f items k monom[j]if k < m or k > n monom monom[ j] + 0 + monom[ j + 1 ] if monom in g g[monom] + coeffelse g[monom] coeffreturn dmp_from_dict g u K
def get_preferred_output_encoding if hasattr locale u'LC_MESSAGES' return locale getlocale locale LC_MESSAGES [1] or locale getdefaultlocale [1] or u'ascii' return locale getdefaultlocale [1] or u'ascii'
def get_rtd_version version None version_str get_docs_version version version positions 3 return 'latest' if version_str 'dev' else 'stable-%s' % version_str
def get_rtd_version version None version_str get_docs_version version version positions 3 return 'latest' if version_str 'dev' else 'stable-%s' % version_str
def render_modal_workflow request html_template js_template template_vars None response_keyvars []if html_template html render_to_string html_template template_vars or {} request request response_keyvars append u"'html' %s" % json dumps html if js_template js render_to_string js_template template_vars or {} request request response_keyvars append u"'onload' %s" % js response_text u'{%s}' % u' ' join response_keyvars return HttpResponse response_text content_type u'text/javascript'
def getAlterationLines fileName return archive getTextLines getAlterationFile fileName
def createURLs urls []for x in range 0 randint 20 100 name 'DEBUG_API'if randint 0 5 5 name ''urls append name + '' join sample string ascii_letters randint 10 20 return urls
def imresize img cropped_width cropped_height width_scale float cropped_width / img shape[1] height_scale float cropped_height / img shape[0] if len img shape 2 img np tile img[ np newaxis] 1 1 3 order np argsort [height_scale width_scale] scale [height_scale width_scale]out_dim [cropped_height cropped_width]weights [0 0]indicies [0 0]for i in range 0 2 weights[i] indicies[i] contribution img shape[i] out_dim[i] scale[i] for i in range 0 len order img reduce_along_dim img order[i] weights[order[i]] indicies[order[i]] return img
def imresize img cropped_width cropped_height width_scale float cropped_width / img shape[1] height_scale float cropped_height / img shape[0] if len img shape 2 img np tile img[ np newaxis] 1 1 3 order np argsort [height_scale width_scale] scale [height_scale width_scale]out_dim [cropped_height cropped_width]weights [0 0]indicies [0 0]for i in range 0 2 weights[i] indicies[i] contribution img shape[i] out_dim[i] scale[i] for i in range 0 len order img reduce_along_dim img order[i] weights[order[i]] indicies[order[i]] return img
def _get_server_type doc if not doc get 'ok' return SERVER_TYPE Unknownif doc get 'isreplicaset' return SERVER_TYPE RSGhostelif doc get 'setName' if doc get 'hidden' return SERVER_TYPE RSOtherelif doc get 'ismaster' return SERVER_TYPE RSPrimaryelif doc get 'secondary' return SERVER_TYPE RSSecondaryelif doc get 'arbiterOnly' return SERVER_TYPE RSArbiterelse return SERVER_TYPE RSOtherelif doc get 'msg' 'isdbgrid' return SERVER_TYPE Mongoselse return SERVER_TYPE Standalone
def _get_server_type doc if not doc get 'ok' return SERVER_TYPE Unknownif doc get 'isreplicaset' return SERVER_TYPE RSGhostelif doc get 'setName' if doc get 'hidden' return SERVER_TYPE RSOtherelif doc get 'ismaster' return SERVER_TYPE RSPrimaryelif doc get 'secondary' return SERVER_TYPE RSSecondaryelif doc get 'arbiterOnly' return SERVER_TYPE RSArbiterelse return SERVER_TYPE RSOtherelif doc get 'msg' 'isdbgrid' return SERVER_TYPE Mongoselse return SERVER_TYPE Standalone
def _get_code_w_scope data position obj_end opts element_name code_end position + _UNPACK_INT data[position position + 4 ] [0] code position _get_string data position + 4 code_end opts element_name scope position _get_object data position code_end opts element_name if position code_end raise InvalidBSON 'scopeoutsideofjavascriptcodeboundaries' return Code code scope position
def subtract d1 d2 warnings warn 'deprecated' DeprecationWarning if len d1 > len d2 for k in d2 keys if d1 has_key k del d1[k]else for k in d1 keys if d2 has_key k del d1[k]return d1
def load_base_library library dict library update read_style_directory BASE_LIBRARY_PATH return library
def load_base_library library dict library update read_style_directory BASE_LIBRARY_PATH return library
def get_dynamic_link_map for_delete False if getattr frappe local 'dynamic_link_map' None None or frappe flags in_test dynamic_link_map {}for df in get_dynamic_links meta frappe get_meta df parent if meta issingle dynamic_link_map setdefault meta name [] append df else links frappe db sql_list 'selectdistinct{options}from`tab{parent}`' format **df for doctype in links dynamic_link_map setdefault doctype [] append df frappe local dynamic_link_map dynamic_link_mapreturn frappe local dynamic_link_map
def set_time_server time_server 'time apple com' cmd 'systemsetup-setnetworktimeserver{0}' format time_server salt utils mac_utils execute_return_success cmd return time_server in get_time_server
@pytest mark django_dbdef test_cross_sell_plugin_type shop get_default_shop supplier get_default_supplier product create_product 'test-sku' shop shop supplier supplier stock_behavior StockBehavior UNSTOCKED context get_jinja_context product product type_counts ProductCrossSellType RELATED 1 ProductCrossSellType RECOMMENDED 2 ProductCrossSellType BOUGHT_WITH 3 for type count in type_counts _create_cross_sell_products product shop supplier type count assert ProductCrossSell objects filter product1 product type type count count for type count in type_counts assert len list product_helpers get_product_cross_sells context product type count count
@pytest mark django_dbdef test_cross_sell_plugin_type shop get_default_shop supplier get_default_supplier product create_product 'test-sku' shop shop supplier supplier stock_behavior StockBehavior UNSTOCKED context get_jinja_context product product type_counts ProductCrossSellType RELATED 1 ProductCrossSellType RECOMMENDED 2 ProductCrossSellType BOUGHT_WITH 3 for type count in type_counts _create_cross_sell_products product shop supplier type count assert ProductCrossSell objects filter product1 product type type count count for type count in type_counts assert len list product_helpers get_product_cross_sells context product type count count
def get_http_expiry _Expirestype _num if _Expirestype 'd' expire_date datetime datetime now + datetime timedelta days _num elif _Expirestype 'h' expire_date datetime datetime now + datetime timedelta hours _num else expire_date datetime datetime now + datetime timedelta minutes _num return expire_date strftime '%a %d%b%Y%H %M %SGMT'
def get_http_expiry _Expirestype _num if _Expirestype 'd' expire_date datetime datetime now + datetime timedelta days _num elif _Expirestype 'h' expire_date datetime datetime now + datetime timedelta hours _num else expire_date datetime datetime now + datetime timedelta minutes _num return expire_date strftime '%a %d%b%Y%H %M %SGMT'
def build_pdf branch os chdir os path join gitdname 'statsmodels' 'docs' sphinx_dir os path join virtual_dir 'bin' retcode subprocess call '' join ['make' 'latexpdf' 'SPHINXBUILD ' + sphinx_dir + '/sphinx-build' ] shell True if retcode 0 msg 'Couldnotbuildthepdfdocsforbranch%s' % branch raise Exception msg os chdir dname
def build_pdf branch os chdir os path join gitdname 'statsmodels' 'docs' sphinx_dir os path join virtual_dir 'bin' retcode subprocess call '' join ['make' 'latexpdf' 'SPHINXBUILD ' + sphinx_dir + '/sphinx-build' ] shell True if retcode 0 msg 'Couldnotbuildthepdfdocsforbranch%s' % branch raise Exception msg os chdir dname
def build_pdf branch os chdir os path join gitdname 'statsmodels' 'docs' sphinx_dir os path join virtual_dir 'bin' retcode subprocess call '' join ['make' 'latexpdf' 'SPHINXBUILD ' + sphinx_dir + '/sphinx-build' ] shell True if retcode 0 msg 'Couldnotbuildthepdfdocsforbranch%s' % branch raise Exception msg os chdir dname
def svd a full_matrices 1 compute_uv 1 return SVD full_matrices compute_uv a
def svd a full_matrices 1 compute_uv 1 return SVD full_matrices compute_uv a
def _dt_to_epoch_ns dt_series index pd to_datetime dt_series values if index tzinfo is None index index tz_localize 'UTC' else index index tz_convert 'UTC' return index view np int64
def positive_int_list argument if ' ' in argument entries argument split ' ' else entries argument split return [positive_int entry for entry in entries]
def positive_int_list argument if ' ' in argument entries argument split ' ' else entries argument split return [positive_int entry for entry in entries]
def is_current_user_capable api_name user get_current_user if not user sys stderr write 'userisnotloggedin-cannotuseapi' + api_name + '\n' return Falseemail user email return is_user_capable email api_name
def make_fastq_single in_fasta quals out_fp label_transform split_lib_transform outfile open out_fp 'w' for rec seq_id in iter_fastq in_fasta quals label_transform outfile write rec + '\n' outfile close